---
title: "How to Build Custom Evals for LLMs"
slug: how-to-build-custom-ai-evals-for-llms
publishedAt: "2025-09-14"
createdAt: "2025-07-08"
updatedAt: "2025-09-14"
summary: "Learn how to build custom advanced LLM evaluations for enterprise AI deployment. Discover why public benchmarks fail and create internal evaluation pipelines that ensure quality, reduce costs, and improve user satisfaction."
authors:
  - name: "Pranav Patel"
category: "LLM Evaluations"
image: "https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/686d98251703724ec9db3479_Screenshot%202025-07-09%20at%2003.43.52.png"
---

<p><em>NOTE THAT THIS IS GOING TO BE A VERY VERY COMPREHENSIVE OVERVIEW ABOUT CREATING EVALUATION PROCESSES AND EVALUATION DATASETS. WE PLAN TO RELEASE MANY TOOLS AND MANY MORE NICHE GUIDES ABOUT THIS FIELD IN THE UPCOMING FUTURE. </em><strong><em>IF YOU NEED ANY ASSISTANCE IN EVALUATING YOUR MODEL, </em></strong><a href="https://www.mercity.ai/contacts"><strong><em>PLEASE REACH OUT</em></strong></a><strong><em>!</em></strong></p><p>You can watch this if the blog is too long for you:</p><div data-rt-embed-type='true'><div style="position: relative; padding-bottom: 64.90384615384616%; height: 0;"><iframe src="https://www.loom.com/embed/ab25eee1980749dea355bff3f8b50d82?sid=d77c303f-9247-483e-9730-ef6ad79e062f" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;"></iframe></div></div><p>AI is on the rise. Everyone wants an AI model in their business, automating tasks, improving efficiency, and making life easier. Users want AI on every platform. There have been great models like LLaMA, Mistral, GPTs, and Claude, etc. The problem becomes what model to pick and go with; that‚Äôs where custom evaluation becomes relevant.</p><p>We need to be able to evaluate what model works the best for your users, and now with this mass of models, it is more necessary than ever. It is important to pick the right model and use it correctly, usually just using it 5-10 times doesn‚Äôt give you a strong understanding of where the model might be lacking. In this blog, we are going to talk about how you can create internal benchmarks or evaluation sets for your models, and test them before deploying it internally or to your users and pick the best model.</p><p><em>Read on</em></p><h2 id="what-are-llm-evals">What are LLM Evals?</h2><figure class="w-richtext-figure-type-image w-richtext-align-center" style="max-width:1600px" data-rt-type="image" data-rt-align="center" data-rt-max-width="1600px"><div><img src="https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c72474df9a2299a68c1876_8b0f2ae3.png" width="auto" height="auto" alt="" loading="auto"></div></figure><p><em>Evals are simply the process of evaluating the outputs of your model.</em> As simple as that. Evals help you compare models, identify weaknesses, and choose the best one for your specific application. You can evaluate various capabilities of your models like, reasoning, math, creative writing, linguistic understanding, and even <a href="https://www.mercity.ai/blog-post/custom-gpt-4-chatbot">RAG </a>capabilities. They are essential in evaluating any model pipeline before deploying and pushing it to other people. Using these evaluations you can catch your model early, where it is lacking, and improve it accordingly, or build systems around it to help the model.</p><p>Evals for LLMs are very straightforward to understand, but there are nuances that make them much harder to actually work with at times.</p><h3 id="difference-between-evals-and-benchmarks">Difference between Evals and Benchmarks</h3><p>A Benchmark is more of a standardized test that produces a quantitative score for comparison. Think of MMLU, which tests a model's knowledge across various subjects. Its purpose is to rank models against each other on a public leaderboard. It provides a single, comparable number but doesn't tell you if they're a good fit for a specific, complex job.</p><p>Whereas<strong> </strong>an Evaluation is the <em>process</em> of determining if an LLM is good enough for your specific business use case. <em>An eval framework can include running benchmarks</em>, but it also includes qualitative measures, human feedback, prompt adherence tests, and use-case-specific testing. It is supposed to be much more specific to your usecase, and rather than measuring how good a model is in general, it mesures how good a model is in your narrow usecase.</p><p>There have been many benchmarks over the years which helped in measuring progress, like <a href="https://en.wikipedia.org/wiki/MMLU">MMLU</a>, <a href="https://arxiv.org/abs/2311.12022">GPQA</a>, etc. These benchmarks are simple and measure AI performance on human knowledge tasks, and <strong>they are very near being saturated </strong>as the new models are massively stronger. And as our workflows get more complex, these benchmarks are not able to properly capture the essence of the tasks we perform in the real world, hence the need for custom and personal Evaluations, especially for enterprises and companies that deploy models to millions of users.</p><h3 id="good-benchmarks-to-track">Good Benchmarks to Track</h3><p>As mentioned before, most of the benchmarks are nearing saturation, models constantly score more then 80% on MMLU now, and Sonnet 3.7 is at around 75% on GPQA. This is leading to creation of more and more complex benchmarks, MMLU Pro, GPQA Diamond and whatnot. And it is getting more and more difficult to understand what benchmarks are really relevant in the real world and which ones are just for very specific and niche use cases.</p><p>Here are some lesser known benchmarks that you can still track for the general capabilities of the model.</p><ul><li><a href="https://lmarena.ai/"><strong>Chatbot Arena by LymSys</strong></a><strong>:</strong> This is perhaps the best and most important benchmark you should keep an eye on. They created a ‚Äúbattleground‚Äù of sorts of models and paired them against each other, with actual real humans being the judges of quality.&nbsp; ‚Äì Overall assessment of models, they have specific categories on the website too.</li><li><strong>ùúè-Bench (Tau-Bench):</strong> Tool calling benchmark, simulates <strong>real-world agent environments</strong> combining human conversations and API interactions across 12 enterprise domains.</li><li><strong>Fiction Live:</strong> Long context benchmark, evaluates LLMs' ability to understand and track complex fictional narratives over extended contexts. ‚Äì Good and important to track when working with large documents and contexts.</li><li><strong>MT-Bench:</strong> Evaluates multi-turn conversation abilities through 80 curated questions (160 turns) across writing, reasoning, and coding domains. NECESSARY benchmark to track on especially for enterprise as chats span across multiple messages and across topics. There‚Äôs also MT-Bench++ which is even more enhanced for enterprise and real world use cases.</li><li><strong>BOLD: </strong>Measures bias in open ended language tasks might not seem important, but in experience enterprises always need to make sure the model they are deploying doesn‚Äôt have any biases. Minor biases can really create major issues for big companies.</li></ul><p>These are some sophisticated benchmarks that you should always measure your model against. These are not very well known, and hence not very saturated yet either. And the best part is that most of them are based on real world usage, so you can always pick the best model that works for you AND your users, not just internally.</p><h2 id="why-do-you-need-your-own-custom-evals">Why do you need your own Custom Evals?</h2><p>We have suggested some good and still relevant benchmarks above, but they still might not be enough. There are many issues with the public benchmarks. They are very often not the right indicator to evaluate real business value and performance. Which is why you still need some internal benchmarking and evaluation.</p><p>Let‚Äôs go deeper into why you need to be creating your own LLM Evaluation Sets.</p><h3 id="popular-ones-are-useless">Popular ones are useless</h3><p>Most of the popular benchmarks are very outdated. As mentioned before, benchmarks like MMLU and GPQA are indeed useful for some areas, but they are not at all a good indicator of what a model is capable of. <strong>There is a very strong disconnect between the academic vs. business applications of LLMs</strong>. Most of the popular benchmarks lean heavily on the academic side of tasks, rather than day to day business applications. Benchmarks like MMLU tests knowledge, not application and actual performance.</p><p>This, along with researchers training intentionally to overfit the benchmarks that ruins the whole purpose of these rather good benchmarks too! This is a massive issue in space right now, which is often caught early, but sometimes not. A good example of this would be the Llama 4 series, which performed very well on the benchmarks, but performed poorly when people tried it out.<em> (Do note that there were some issues with the implementation of Llama4 initially, which were fixed later, but even after that, the performance has been underwhelming)</em>.</p><h3 id="llms-get-saturated-on-the-benchmarks-and-public-data">LLMs get saturated on the Benchmarks and Public Data</h3><figure class="w-richtext-figure-type-image w-richtext-align-center" style="max-width:1600px" data-rt-type="image" data-rt-align="center" data-rt-max-width="1600px"><div><img src="https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c72474df9a2299a68c1879_7a879c33.png" width="auto" height="auto" alt="" loading="auto"></div></figure><p>When training LLMs, most of the data comes from scraping publicly available sources on the internet. Some companies do add in some private data to the mix, too. This massive amount of data is necessary for the model to learn and evolve. But very often this means training and even saturating the model on the data and tasks that are unnecessary to our use cases.</p><p>Doing this does deliver better performance in some areas, but often translates to needless training on tasks that are not necessary to the businesses and users, just to the benchmarks. For example, getting better performance on MMLU-like benchmarks is a big deal to researchers as it represents word knowledge, but it is mostly about STEM areas and are very sophisticated and niche questions. Something that has no place in business applications.</p><p>This saturation is the cause of a lot of slowdown and misunderstanding of the model's performance. People often think if a model scored 96% on a benchmark it must be better than a model that scored only 93%. But the truth is that <strong>going from 93% to 96% leads to barely any gains in real world performance.</strong> <em>Once you are above 90% accuracy in the most important benchmarks, small gains don‚Äôt really matter.</em>&nbsp;</p><p>This is why you should depend more on usage-based tests than benchmarks when past a threshold of performance, and most closed-source models are past that. You definitely need your own evals to accurately judge LLMs on your own set of tasks.</p><h3 id="your-use-cases-are-too-niche-and-specific-to-be-reflected-in-other-benchmarks">Your use cases are too niche and specific to be reflected in other benchmarks</h3><p>In our experience, very often companies present with use cases that are very specific to their industry or to their application. Sometimes generating very long data flow JSONs from predetermined sources, and sometimes role-playing as a famous person, but also subtly advertising for certain products. These can be tricky to implement sometimes. For example, a model might start a sales pitch instead of pushing products subtly. Something like that would be disastrous if it reaches production.</p><figure class="w-richtext-figure-type-image w-richtext-align-center" style="max-width:1600px" data-rt-type="image" data-rt-align="center" data-rt-max-width="1600px"><div><img src="https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c72474df9a2299a68c187c_b4100554.png" width="auto" height="auto" alt="" loading="auto"></div></figure><p>And when these use cases are tricky to implement, it becomes even trickier to evaluate these properly. <strong>And there is almost no way that a public benchmark would cover your use case.</strong></p><h3 id="quality-is-of-utmost-importance">Quality is of utmost importance</h3><p>Last, but perhaps most important, QUALITY. Companies always want to deliver the best solution possible. This means doing better than competitors, improving on what is already out there and often entering into novel areas. This is hard to work on and harder to properly capture. The issue being when you try to do something better than everyone else, you cannot measure yourself on the same scale as others. Methodology matters, and it is not always quantitative. This requires not only building, but even iterating and internally testing your benchmarks.</p><p>It is very difficult to build quality products using LLMs, it is necessary to properly test and iterate on the issues before releasing these products.</p><h2 id="how-to-pick-what-factors-to-evaluate-your-model-on">How to pick what factors to Evaluate your model on?</h2><p>When designing your own Evaluation, or when picking the right one from the massive set of benchmarks that exist out there, it is necessary to understand what exactly you <em>need</em> to test your model on. If you are building a simple chatbot application, you might not need any strong math or reasoning skills. Similarly, if you are building a simple Email CTA Line generator, you might not even need to look at multi-turn chat performance. It is important to understand what are the necessary factors to test based on your use case so you can pick the right model.</p><p>One simple way to determine what factors are important is to break down the <em>User Journey</em> of your LLM or application into smaller bits, both quantitative and qualitative. For example, if you are building a therapy bot, you‚Äôd need good performance on tasks like instruction following, empathy, multi-turn conversation skills as the chat gets long, etc.&nbsp;</p><p>The core task or the journey always comprises multiple smaller subtasks. In a way, you can think of what skills the model would need to do that task, and then the model should be good in all of those skills. Generating answers in a <a href="https://www.mercity.ai/blog-post/custom-gpt-4-chatbot">RAG</a> scenario would require <em>good long context understanding, multi-turn conversation skills, low hallucination and good tool calling, etc.</em> All of these are events in how the user will interact with the model, they will send chat messages, and expect the model to call the RAG tool and ingest a lot of data, and then provide answers accurately. <em>If your model is good at the sub skills, it's a safe bet to assume the model will be pretty good at combining them too.</em></p><h2 id="how-to-create-your-own-evals-for-llms">How to create your own Evals for LLMs?</h2><p>Once we understand what factors are necessary and how different evaluations work, we can use that information to build custom evals. These evals will help you evaluate your models, improve, and push out a version that is properly tested and covers as many edge cases as possible.</p><p>Most of the testing can be put into largely two categories, Qualitative and Quantitative evals. Lets explore these and see how we can build evals to test both of these aspects of the LLMs.</p><h3 id="quantitative-testing">Quantitative Testing</h3><p>Quantitative testing is when you can properly measure and accurately classify if the output is wrong or right. This is perhaps the easier one. There are very specific applications and niche areas where this is necessary. For example, if you are building a Math tutor bot, you ofc want it to be good at math. If you are building a History Teacher bot, you want it to be very high in factual correctness.</p><p>Let‚Äôs talk about a few quantitative evaluation factors in detail.</p><h4 id="math">Math</h4><p>This is a rather simple one to understand. A very basic knowledge of math is essential to talk and have basic conversations. And most models have become very good at it, some can even go beyond that and can act as a math tutor to many in most situations. If you are planning to deal with normal conversations, and or up to high school math, you should be able to pick any frontier model and be able to get decent performance.</p><p>You can often follow a good, famous math benchmark, and it will be good enough. But if you want to build one for internal use, there are two methods:</p><ul><li>Evaluating Steps: This is also known as Process Supervision. If you evaluate every step of the model and decide if it is moving in the right direction or not, if not, you kind of know that the answer is most likely going to be wrong too. OpenAI worked on this in the paper titled ‚Äú<a href="https://arxiv.org/abs/2305.20050">Lets Verify Step by Step</a>‚Äù.</li><li>Evaluating on Answer: This is more open-ended. Sometimes there are multiple paths to the same problem, like when answering a riddle. Or maybe even in math questions. This is where you evaluate the answer and let the model figure out the intermediate steps naturally. In the <a href="https://arxiv.org/abs/2402.03300">DeepSeek R1 Paper</a> Deepseek team was able to not use any process reward models and still get amazing results!</li></ul><p>Both approaches tend to work. But evolution, it is always suggested to evaluate on steps if possible and if applicable to your task. An example would be solving math equations, the steps need to be correct as well, and the probability that the answer will be correct without the steps being correct is very low.</p><figure class="w-richtext-figure-type-image w-richtext-align-center" style="max-width:1600px" data-rt-type="image" data-rt-align="center" data-rt-max-width="1600px"><div><img src="https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c724d0e724789b67d06e07_63fd0324.png" width="auto" height="auto" alt="" loading="auto"></div></figure><h4 id="function-calling-and-tool-usage">Function Calling and Tool Usage</h4><p>Tool calling capabilities are very important when you are not just using LLMs, but also integrating them into your stack. Which most of the companies are now doing. It is very important to evaluate models on your internal specific tools and documentation. LLMs have improved a lot on function calling and with <a href="https://docs.anthropic.com/en/docs/claude-code/mcp">MCP protocols</a> they are improving faster than ever. But with very niche areas, they still struggle.</p><figure class="w-richtext-figure-type-image w-richtext-align-center" style="max-width:1600px" data-rt-type="image" data-rt-align="center" data-rt-max-width="1600px"><div><img src="https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c724d0e724789b67d06e01_b474d5f2.png" width="auto" height="auto" alt="" loading="auto"></div></figure><p>These numbers from the <a href="https://www.anthropic.com/news/claude-4">Claude-4 launch</a> show how tool use is still at 80% accuracy for Retail and us just at 80% for the Airline industry. These are very low numbers. 80% accuracy means it fails at one out of five-tool calls. This doesn‚Äôt seem bad until you realize that one agentic call can take multiple tool calls, sometimes more than 10. Such executions can never work at an 80% error rate, as a single failure can cause the whole thing to fail.</p><p>This makes it necessary to evaluate tool calls in your own industry.</p><p>To create your tool calling evaluation, you can simply use an LLM to create lots of functions for your industry with sophisticated documentation for each function. Then you can design queries that take multiple of those function calls to execute to answer.</p><p>Check out the prompt in the image:</p><figure class="w-richtext-figure-type-image w-richtext-align-center" style="max-width:1600px" data-rt-type="image" data-rt-align="center" data-rt-max-width="1600px"><div><img src="https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c724d0e724789b67d06e0d_e48abbeb.png" width="auto" height="auto" alt="" loading="auto"></div></figure><p>The outputs can now be used to evaluate your model on these tool call chains of your system, or can be used to further train a model.</p><p>The given prompts should help you design your own tool call evaluation set. We are doing extensive work in this area internally, with more sophisticated writeups coming up soon! We would <a href="https://www.mercity.ai/contacts">suggest reaching out to us</a> if you are looking for help on this!</p><h4 id="factuality">Factuality</h4><p>This is a rather simple one to test. Organizations often want to test factual correctness over their knowledge bases and after their fine-tuning. A very simple way to test for this is to look at MMLU-based benchmarks to test for knowledge retention.</p><p>To test your own knowledge bases, you can take your documents/texts and generate question-answer pairs over them, and then have your model generate answers and have a cheap verifier model check if the answers are correct or not. If you don‚Äôt want to use a verifier model, you can simply ask the model to output in a JSON format where you have an ‚Äúanswer‚Äù key which you can statically match with the generated correct answers.</p><h4 id="long-context-handling">Long Context Handling</h4><p>Long context capabilities are becoming increasingly important as more people integrate with LLMs. When working with agents and large knowledgebases, long context is super important, you need to be able to rely on it, if not, you need to build fancy mechanisms like context summarization or chunking to deal with it.</p><p><a href="https://fiction.live/stories/Fiction-liveBench-April-6-2025/oQdzQvKHw8JyXbN87">Fiction.live</a> benchmark is rather the most comprehensive and the best benchmark for this. It is built on sophisticated stories. It requires the model to do long context thinking and reasoning with a lot of data points and potential answers to correctly reach an answer. As of writing this, Gemini is the best model for long context.</p><figure class="w-richtext-figure-type-image w-richtext-align-center" style="max-width:1208px" data-rt-type="image" data-rt-align="center" data-rt-max-width="1208px"><div><img src="https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c724d0e724789b67d06e0a_9f0b4d8a.png" width="auto" height="auto" alt="" loading="auto"></div></figure><p>But if you want to build your own evaluation process to test for long context, a simple <a href="https://arxiv.org/abs/2407.01437">Needle in the Hay Stack</a> type test would work. You simply take a document or a paragraph/chunk of text and insert it in a large unrelated or related dump of text, and ask the LLM questions that could only be answered using the paragraph you inserted. A good example would be putting a specific medical record of a patient across the sea of various unnecessary medical information or medical information of other patients. This requires the model to locate the right source of information and correctly ignore the incorrect data. You can also move around the inserted text to test the model on various levels of depth.</p><figure class="w-richtext-figure-type-image w-richtext-align-center" style="max-width:1600px" data-rt-type="image" data-rt-align="center" data-rt-max-width="1600px"><div><img src="https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c724d0e724789b67d06e04_d8c8efd6.png" width="auto" height="auto" alt="" loading="auto"></div></figure><h3 id="qualitative-testing">Qualitative Testing</h3><p>Qualitative testing is much harder than quantitative testing. Simply because it's harder, if not impossible, to measure. The measure of quality is very subjective too. However, it is also significantly more important than quantitative testing, simply because users care most about the quality of the responses above all else. Things like math tutoring and function calling are like the goals achieved using the LLMs, it matters just as much if not more how we achieve the goals.</p><p>If your AI math tutor is 100% accurate all the time, but is rude and sometimes don‚Äôt understand what exactly you are asking to explain further, you would not want to use such a bot at all.</p><p><em>User satisfaction is largely driven by high quality responses.</em></p><p>These qualitative evaluations are so much more important if you are deployig to real users and not using the LLM in an agentic or automation setting.</p><h4 id="human-evaluation">Human Evaluation</h4><p>The best way to perform Qualitative testing is to have actual humans test it out. This is a time-consuming and manual process, but it is the best way to evaluate an AI. You can use fancy methods like LLM-as-a-Judge or build a classifier on top of your AI‚Äôs responses to classify it as a good or bad response. But nothing will give you as good results as simply sitting down and going through at least a hundred responses from the AI across various topics. <strong>Human eye and feedback are absolutely necessary to improve your AI.</strong> You cannot replace this with other models because it's humans who will be using your LLM, not other LLMs.</p><p>Here‚Äôs a quick overview of how we perform human evaluation internally at Mercity</p><figure class="w-richtext-figure-type-image w-richtext-align-center" style="max-width:1600px" data-rt-type="image" data-rt-align="center" data-rt-max-width="1600px"><div><img src="https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c724f4ab3f53d704f45869_3a884f10.png" width="auto" height="auto" alt="" loading="auto"></div></figure><p>We have also built a much more sophisticated tool to collect human feedback on AI responses on chat settings here:</p><p><a href="https://github.com/Mercity-AI/LLM-Feedback-Collector">Mercity AI LLM Feedback Collector</a></p><figure class="w-richtext-figure-type-image w-richtext-align-center" style="max-width:1600px" data-rt-type="image" data-rt-align="center" data-rt-max-width="1600px"><div><img src="https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c724f4ab3f53d704f4586c_bf1ad19f.png" width="auto" height="auto" alt="" loading="auto"></div></figure><p><em>More on this tool in later sections!</em></p><p>It is very very important that you always collect as much human feedback as possible. It is good to not only know where the model is failing but also how it can be made better. A very important point is to always collect as much information as possible when collecting feedback. We collect:</p><ul><li>Acceptable or Not: Required. Binary, is the response acceptable or not?</li><li>Rating /10: Required. Rate the response to the query out of 10.</li><li>Why: Optional. Why is the output acceptable or not?</li><li>How to make it better: Optional. How can the response be made better? Or where did the model mess up? Or what specifically do you like about the model response?</li><li>Next Message: What is the next message to the response of the LLM? - This can reveal important information about model behaviour too.</li></ul><p>Just these 4 data points can massively assist you in analyzing the model responses. This question set helps you answer crucial questions like ‚ÄúWhat length of responses do people like?‚Äù to ‚ÄúWhat tone of voice do people prefer?‚Äù and ‚ÄúWhat are common likes and dislikes about the model?‚Äù&nbsp;</p><p>We have always collected at least 100 datapoints when in the testing phase, when working with LLMs, before going out to release it to a wider audience.</p><h4 id="creativity">Creativity</h4><p>This is a difficult one to evaluate. Creativity requires you to truly think outside the box and produce outputs that no one has ever done before. It can also be defined as diversity in outputs.</p><p>There is no direct way to measure creativity on a scale, but one good way is to measure the diversity of outputs with maintained quality. The idea comes from <a href="https://arxiv.org/abs/2503.17126">this</a> underrated paper. And that is something we can measure.</p><figure class="w-richtext-figure-type-image w-richtext-align-center" style="max-width:1600px" data-rt-type="image" data-rt-align="center" data-rt-max-width="1600px"><div><img src="https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c724f4ab3f53d704f45870_5af9f3d6.png" width="auto" height="auto" alt="" loading="auto"></div></figure><p>The idea is to generate multiple responses on low and high temperature settings, where the model gets to explore and output various outputs, and we convert them into embeddings and measure the distance or standard deviation between them. These outputs are also taken and put into a larger, smarter thinking model with a proper ruleset for making sure that these are still high-quality outputs, the ones that are not ignored.</p><figure class="w-richtext-figure-type-image w-richtext-align-center" style="max-width:1600px" data-rt-type="image" data-rt-align="center" data-rt-max-width="1600px"><div><img src="https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c724f4ab3f53d704f45882_d38685db.png" width="auto" height="auto" alt="" loading="auto"></div></figure><p>Once we have this dataset, we can go look at the points where a high enough quality or adherence to the ruleset is maintained, and a large enough standard deviation is maintained, and manually check those outputs and verify if they are good or not. If they are, we have the settings for the most creative outputs of the model.</p><p>This measure of standard deviation gives us a rough idea of diversity in the model‚Äôs outputs, and that gives us a rough idea of the creativity of the model.</p><figure class="w-richtext-figure-type-image w-richtext-align-center" style="max-width:783px" data-rt-type="image" data-rt-align="center" data-rt-max-width="783px"><div><img src="https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c724f4ab3f53d704f45876_cef858ec.png" width="auto" height="auto" alt="" loading="auto"></div></figure><p><em>Here you can see various genres encoded and plotted as embeddings. Farther these embeddings are, more diverse or different the genres and the content in them.</em></p><h4 id="communication-quality">Communication Quality</h4><p>This is a very small and subtle thing. Very often, a model makes mistakes or messes up ever so slightly that it is acceptable for an LLM to do. But completely unacceptable for an human being.</p><p>Here is a very simple example:</p><figure class="w-richtext-figure-type-image w-richtext-align-center" style="max-width:1600px" data-rt-type="image" data-rt-align="center" data-rt-max-width="1600px"><div><img src="https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c724f4ab3f53d704f45873_65a00063.png" width="auto" height="auto" alt="" loading="auto"></div></figure><p>Even after mentioning ‚ÄúEinstein‚Äù in the context already, the model asks what roleplay scenario and character we want. It should be inferred from context that we want the model to roleplay as Einstein. This is a simple example.</p><p>Very small things like missing subtle details/instructions in the prompt, calling a tool unnecessarily when the information is present, and making incorrect assumptions are hallmarks of a model that is not very good at communicating. These things are slowly fading away with more training and adaptation.</p><p>But there are still some things that you should test for:</p><ul><li>Not asking questions: Sometimes, the LLM should ask clarifying questions before proceeding with an answer</li><li>Making assumptions: LLMs make assumptions all the time, some of them are wrong, and they are called Hallucinations. It is suggested to check aggressively for these.</li><li>Under answering: Not all questions or sub-questions are answered</li><li>Overanswering: Answering more than necessary</li></ul><p>These issues are pretty common and happen often in a subtle manner. If your LLM is making these mistakes way too often, maybe consider changing the LLM or tuning the prompt or the model itself.</p><h4 id="emotional-intelligence">Emotional Intelligence</h4><p>Emotional Intelligence has gotten more and more important as users adapt LLMs for daily use.</p><p>We strongly recommend EqBench for understanding this aspect of model behaviour:</p><figure class="w-richtext-figure-type-image w-richtext-align-center" style="max-width:1600px" data-rt-type="image" data-rt-align="center" data-rt-max-width="1600px"><div><img src="https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c724f4ab3f53d704f4587c_d5527a1e.png" width="auto" height="auto" alt="" loading="auto"></div></figure><p>It is necessary to understand and tune how your model engages with users. Even if you have prompted your model to help in financial situations, you cannot have your model deny a request to console someone with a bad cough. Human interactions with models are messy, they sometimes stress test the system and break the interaction rules. It is necessary that your model handles such tricky situations with grace in production scenarios. If the model cannot do that properly, you should probably postpone any releases.</p><p>We suggest relying on the collected testing data and EqBench for testing these scenarios.</p><h4 id="prompt-adherence">Prompt Adherence</h4><p>This is the subtle but most important part of any LLM deployment right now. Your PROMPT is the most important asset in your deployment. You tune it, you update it, spend hours of work, and test it out. <em>But if your model doesn‚Äôt follow your prompt, does any of it matter?</em></p><p>It is important to pick a model that can follow complex, multifaceted, and even dynamic prompts properly. You steer the model using your prompts if it doesn‚Äôt follow them, you are in big trouble. It is better to pick the most common good prompt following LLM from the start, most big LLMs are good enough at it, but you can check <a href="https://lmarena.ai/leaderboard/text/instruction-following">LmArena‚Äôs instruction following benchmark</a> for a better comparison. TBH we suggest going with any model in the top ten. You can start with the cheapest one, and slowly climb the price ladder if you need to.</p><p>We have been writing 3000 words prompts for many of our enterprise deployments and most modern LLMs are good at it. Our rule of thumb is:</p><p><em>The larger the LLM, the better the prompt adherence.</em> This is proven by various papers too.</p><p>One very good method to test out prompt adherence is to test the model on various temperatures. We often deploy at very low temperature settings, 0.1, or 0.3 max. But when stress testing the model, we suggest testing on lower to much higher temperatures, up to 1.1 in some cases. This tells us what happens when the model is thrown into a chaotic state. What happens when the sampling is too random, and the model is still trying to provide a proper answer. If the model can still maintain good outputs in higher temperatures and recovers quickly after making a mistake, it usually means that the model will hold itself well at lower temperatures too!</p><h5 id="jailbreaking">JailBreaking</h5><p>Jailbreaking is a major problem with deploying LLMs right now. This is something that you should test for as much as possible. Most modern models have safety features trained into them now, but can still be broken. For enterprise clients, we always suggest using a guard LLM along with the core LLM deployment. <a href="https://huggingface.co/google/shieldgemma-2b">ShieldGemma</a> and <a href="https://huggingface.co/meta-llama/Llama-Guard-3-8B">LLaMAGuard</a> are amazing models for handling these edge cases. The only challenge is to deploy them along side the core LLM and properly funneling realtime responses from the CoreLLM to these Guard LLMs.</p><figure class="w-richtext-figure-type-image w-richtext-align-center" style="max-width:1600px" data-rt-type="image" data-rt-align="center" data-rt-max-width="1600px"><div><img src="https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c724f4ab3f53d704f45879_bb9e3192.png" width="auto" height="auto" alt="" loading="auto"></div></figure><h2 id="how-to-collect-data-for-model-evaluations">How to Collect Data for Model Evaluations</h2><p>As mentioned before, good data is the foundation of good Evaluations and Benchmarks. And good data comes from good data collection practices. We will touch on how you can collect data properly internally and externally when building LLM-based applications.</p><h3 id="how-to-collect-data-in-your-organization">How to collect data in your Organization</h3><p><em>Easier to collect, harder to clean and work with.</em></p><p>People building the app will understand the app best. It is often best to let your internal teams test the model before moving forward. You already know your users well and potentially the edge cases too. You know what they will use it for and how they will not. You can rigorously test the various aspects of your model deployment by simply letting your team use it.</p><p>But this comes with a bias <strong>just because your team understands the app, doesn‚Äôt mean they understand the user too</strong>. Humans are messy. They are not going to use your model just for the things they say they are going to use it for. They are going to ask personal questions, and medical questions, and throw curveballs at your model. There is no way to model and collect such outlier data internally. That is why we need to collect data from real users too!</p><p>We use this software internally to collect data: <a href="https://github.com/Mercity-AI/LLM-Feedback-Collector">LLM Feedback Collector Tool</a></p><p>You can explore it here, we have opensourced it fully:&nbsp;Our open source <a href="https://github.com/Mercity-AI/LLM-Feedback-Collector/">LLM Feedback Collection tool </a></p><p>Video guide:</p><div data-rt-embed-type='true'><div style="position: relative; padding-bottom: 64.90384615384616%; height: 0;"><iframe src="https://www.loom.com/embed/7a1165e14d884429a4a423c9097db022?sid=c6737fe4-b100-4a33-9d6f-78e5e0753e03" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;"></iframe></div></div><h3 id="how-to-collect-data-from-users">How to Collect Data from Users</h3><p>This is so much more important. You MUST simply log everything. Every message and every conversation creates a massive dataset of these. And once you have enough, it's time to sit down and look at the data under a microscope.</p><p>Collect things like:</p><ul><li>How often the user interacts with the LLM? - Measure for likability</li><li>How often does the user rewrite a query? - Frustration signal or LLM misunderstanding the query</li><li>Likes/Dislikes on the LLM response - Direct strong likability signal</li><li>A/B test responses by giving the user an option to pick between two responses - Direct preference signal</li><li>What follow-up questions are asked after the LLM responds? - Tells us whether the answer on the first go was sufficient or not</li><li>How much time does the user spend reading the messages - LLM reply relevancy signal</li></ul><p>We have already talked about these in much more detail in the Human Evaluation heading.</p><h2 id="some-more-advanced-tips-on-testing-models">Some more advanced tips on Testing Models</h2><p>These are just a few more things that we have noticed and gathered from our experience. These are also good to test once in your testing process to avoid any surprises.</p><figure class="w-richtext-figure-type-image w-richtext-align-center" style="max-width:1600px" data-rt-type="image" data-rt-align="center" data-rt-max-width="1600px"><div><img src="https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c724f4ab3f53d704f4587f_2b8662e8.png" width="auto" height="auto" alt="" loading="auto"></div></figure><p><em>Beam Search Analysis on Models</em></p><ul><li><strong>Time taken - unnecessary thinking:</strong> When working with reasoning LLMs, we make sure the model is not overthinking any specific aspects of the problem. Usually, errors arise out of overthinking rather than underthinking. Overthinking also wastes tokens and increases response latency.</li><li><strong>Not answering until asked:</strong> Sometimes models don‚Äôt mention important details until asked specifically, which can be problematic in certain situations, like medical and legal.</li><li><strong>User satisfaction:</strong> ALWAYS TRACK USER FEEDBACK</li><li><strong>Reading level:</strong> Tracking reading level has been very effective for us. Some organizations prefer longer and more professional responses, whereas others prefer short, concise, and tight outputs.</li><li><strong>Check out the beams or generations:</strong> When dealing with high hallucination levels, we have found it beneficial to look at the different output variations, specifically at the output beams. This gives us a good idea of whether the model is at least close to the correct answer or not. If it is, we often tune the model a little to fix the issues.</li><li><strong>LLM as Judge:</strong> When using LLM as a judge, you must judge the judge itself before using it. Judge‚Äôs biases can easily overflow to your LLM if you are not careful enough.</li><li><strong>RedTeaming/Jailbreaking:</strong> It is good to test your model deployment against some jailbreaking prompts and see how your deployments handle it. Very important if you are deploying to a large scale of users. Here‚Äôs a good dataset to check behavior: <a href="https://huggingface.co/datasets/JailbreakBench/JBB-Behaviors">JBB-Behaviour</a>.</li></ul><h2 id="building-llm-benchmarks-or-evaluation-pipelines">Building LLM Benchmarks or Evaluation Pipelines?</h2><p>If you are building LLM Evaluation Pipelines, consider <a href="https://www.mercity.ai/contacts">reaching out</a>. We have been working with large organizations building custom eval pipelines extensively, and will slowly start putting out more detailed blogs and research on these topics.</p><p>We would love to work with you on creating your evaluations and help you deliver the best possible model for your users!<br><br>üí™</p>
