---
title: "Comparing Diffusion and GAN-based Imgae Upscaling Techniques"
slug: comparing-diffusion-and-gan-imgae-upscaling-techniques
publishedAt: "2024-06-05"
createdAt: "2024-06-05"
updatedAt: "2024-06-05"
summary: "In this comprehensive study, we benchmark and compare all the famous open image upscaling methods and measure them on quality, error and time. We compare GANs, diffusion and the classical image upscaling methods."
authors:
  - name: "Yash"
category: "Image Upscaling"
image: "https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6660da856d7a7bddccdbb499_image%20upscaling.png"
---

<p>Have you ever taken a low-resolution image and tried to enlarge it, only to find it blurry and distorted? This common issue arises because low-resolution images contain fewer pixels, limiting their ability to reproduce fine details. Traditional enlargement methods fail to maintain the original image's clarity and sharpness, resulting in unsatisfactory outcomes. However, image upscaling techniques aim to overcome this challenge by increasing the pixel count, thereby enhancing resolution and detail.</p><p>Advancements in algorithms and AI-driven methods have revolutionized image upscaling, offering impressive solutions to enhance image quality. These cutting-edge technologies analyze and generate additional pixels, preserving the original image's integrity while improving clarity. This blog explores the principles behind image upscaling, the challenges involved, and the latest innovations in the field. By understanding these techniques, you can transform low-resolution images into high-quality, detailed visuals.</p><blockquote><em>✨You can use the image upscaling benchmarking tool we built for this study here and run your own tests on your own images: </em><a href="https://github.com/Mercity-AI/Image-Upscaling-Benchmark"><em>https://github.com/Mercity-AI/Image-Upscaling-Benchmark</em></a></blockquote><h2 id="what-is-image-upscaling">What is Image Upscaling?</h2><p>Image upscaling refers to the process of increasing the resolution or size of an image. This technique is widely used across various fields such as photography, graphic design, AI-generated art, and video production. Image upscaling enables users to enhance the quality of images without the need to retake or recreate them from scratch. The primary objective of image upscaling is to improve the overall visual quality of an image by increasing its pixel count, thus providing a clearer, more detailed representation of the subject.</p><h3 id="the-essentials-and-importance-of-image-upscaling">The Essentials and Importance of Image Upscaling</h3><h4 id="medical-imaging">Medical Imaging</h4><p>Image upscaling significantly enhances medical imaging by providing detailed views of anatomical structures, enabling the detection of small lesions, tumors, or abnormalities that might be missed in lower-resolution images. This improved visualization leads to more accurate diagnoses and reduces the chances of misdiagnosis. In telemedicine, upscaled images ensure that remote specialists receive high-quality visuals, facilitating reliable remote diagnoses and enhancing patient records for future consultations. Additionally, detailed preoperative planning benefits from high-resolution images, allowing surgeons to visualize complex structures, identify critical areas, and plan safer surgical approaches.</p><p>High-resolution images are invaluable in medical research, enabling detailed analysis of tissues, cells, and organs, which contributes to a better understanding of diseases and the development of new treatments. Regular high-resolution scans are essential for tracking disease progression or response to treatment, providing detailed data for timely adjustments in treatment plans. This is particularly beneficial for pediatric and geriatric care, where specialized imaging needs require minimal radiation exposure while maintaining high detail. Moreover, high-resolution imaging plays a crucial role in early detection and screening programs, enabling early intervention and improving patient outcomes. Overall, image upscaling enhances diagnostic and therapeutic services, driving advancements in medical science and improving patient care.</p><figure class="w-richtext-figure-type-image w-richtext-align-center" style="max-width:1152px" data-rt-type="image" data-rt-align="center" data-rt-max-width="1152px"><div><img src="https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6660d7eebbc798110191df6e_AD_4nXdBWtckJw8B12h1XInQId-PZ_UqVkaUP_MLiV56dh2LV57nZNqj7lmUtWdqaustS3SGRASuCuG39mGkITtgkJ6L58D7nlZ6HRUy5FYoYoDv9EA0Ytvxbgs-LVu7PVLQlZIXDaXkYemQKfso-7SLY5Bg5wV7.png" width="auto" height="auto" alt="" loading="auto"></div></figure><h4 id="satellite-imaging-and-remote-sensing">Satellite Imaging and Remote Sensing</h4><p>Image upscaling significantly enhances satellite imaging and remote sensing by providing more detailed views of the Earth's surface. This improved detail is crucial for environmental monitoring, as it allows for accurate tracking of deforestation, urban development, and natural disasters. Higher-resolution images enable precise observation of changes over time, aiding in the assessment and management of environmental impacts. In agriculture, upscaled satellite images help monitor crop health, soil conditions, and pest infestations, leading to better crop management and yield prediction. These detailed images support precision agriculture, allowing farmers to make informed decisions that optimize resource use and improve productivity.</p><p>In disaster management, high-resolution images play a vital role in planning and responding to natural disasters such as floods, earthquakes, and hurricanes. They provide clear, detailed views of affected areas, facilitating effective coordination of rescue and relief efforts. Upscaled images also enhance mapping and planning by providing high-resolution maps essential for urban planning, resource management, and infrastructure development. These maps support accurate decision-making and efficient allocation of resources. Overall, image upscaling in satellite imaging and remote sensing improves the quality of data available for environmental monitoring, agriculture, disaster management, and urban planning, ultimately leading to better outcomes and more informed decision-making.</p><figure class="w-richtext-figure-type-image w-richtext-align-center" style="max-width:587px" data-rt-type="image" data-rt-align="center" data-rt-max-width="587px"><div><img src="https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6660d7eeb52b62f6cab193c5_AD_4nXdYU2PFoXAsTJ0-FztK_DIi0Sghdc-Z9Xqqb3BKIUrlYgKszsqQ25Ha-fDV6_mF2OEvT069QIEE7wb8IwIekqtYsFq07kbsMt9fh27d1PRai83LnTXBTaJ3lOu5A58z67dMdP9B1Jc2sXXebf5FCXNsuC78.png" width="auto" height="auto" alt="" loading="auto"></div></figure><h4 id="digital-media-and-entertainment">Digital Media and Entertainment</h4><p>Image upscaling significantly enhances digital media and entertainment by improving the visual quality of content. In the film and television industry, upscaling allows older, low-resolution content to be converted to higher resolutions, making it suitable for modern displays. This process breathes new life into classic movies and TV shows, providing audiences with a clearer, more immersive viewing experience. Additionally, upscaling helps preserve the integrity of the original material while adapting it for current technological standards, ensuring that valuable cultural and entertainment content remains relevant and accessible.</p><figure class="w-richtext-figure-type-image w-richtext-align-center" style="max-width:602px" data-rt-type="image" data-rt-align="center" data-rt-max-width="602px"><div><img src="https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6660d7ee212b428a2186b127_AD_4nXf4VX_vu6OA99LpefQQHMwzWs4A8NMGVRIpbpE5FRWO_ZhY1H9AOeU2X4_TvJX5VLMU_q-JpPamgfNKhkN6Vs7Xi5irLBQnyX70419T2vZjzzX7mKhlnZw3rhcVRdbChhTSKrU7jfZwBAPPtzz8nX3xWUvm.png" width="auto" height="auto" alt="" loading="auto"></div></figure><h4 id="forensics-and-security">Forensics and Security</h4><p>Image upscaling offers significant advantages in forensics and security by enhancing the quality and detail of visual data. In crime scene analysis, upscaled images from security cameras or other sources can reveal crucial details that might otherwise be overlooked, such as identifying suspects, reading license plates, or discerning specific objects. This improved clarity aids law enforcement agencies in gathering more accurate evidence, leading to better investigations and increased chances of solving cases. High-resolution images also assist forensic experts in analyzing minute details, such as fingerprints or tool marks, which are essential for accurate crime scene reconstruction.</p><p>In security and surveillance, upscaling technology enhances the effectiveness of monitoring systems by providing clearer, more detailed images. This improvement allows for better identification and tracking of individuals and activities, which is crucial for real-time threat detection and prevention. Enhanced image quality helps security personnel make more informed decisions and respond more effectively to potential threats. Additionally, upscaled images are beneficial in post-incident analysis, providing clearer evidence for legal proceedings and improving overall security measures. Overall, image upscaling in forensics and security improves the accuracy of investigations, enhances surveillance capabilities, and contributes to safer environments.</p><h3 id="ai-upscaling-vs-image-upscaling">AI Upscaling vs Image Upscaling</h3><p>Image upscaling, often referred to as traditional upscaling, utilizes simpler algorithms to enlarge images. Techniques such as Nearest Neighbor, which replicates adjacent pixels, are straightforward but can lead to blocky outcomes. More sophisticated methods like Bilinear and Bicubic Interpolation create new pixels through linear and cubic calculations, offering smoother transitions but sometimes resulting in blurred images, especially in areas with complex textures. This form of upscaling is generally suitable for basic needs where precision in detail preservation isn't the primary concern, and the requirement for computational resources is minimal.</p><p>On the other hand, AI upscaling represents a more advanced approach, employing deep learning models to enhance image resolution more effectively. These models are trained on extensive datasets to understand how specific details should look at higher resolutions, allowing them to generate new elements in the image that appear naturally detailed. As a result, AI upscaling can significantly improve image quality, adding clarity and reducing artifacts compared to traditional methods. This technique requires more robust computational power and is commonly used in high-demand applications such as video streaming, gaming, and professional photography, where delivering high-resolution visuals is crucial.</p><h2 id="different-methods-to-do-image-upscaling">Different Methods to Do Image Upscaling</h2><p>There are several methods for image upscaling, including classical techniques, deep learning methods, GAN-based approaches, and diffusion-based models. In this discussion, we will explore how to perform image upscaling using each of these techniques. We'll evaluate the efficiency of each model, analyze their size, and examine their performance metrics, such as <a href="https://en.wikipedia.org/wiki/Mean_squared_error">Mean Squared Error</a> and <a href="https://en.wikipedia.org/wiki/Structural_similarity_index_measure">SSIM</a> (Structural Similarity Index Measure). All the performance metrics are measured on Intel Core i5-1135G7 Microprocessor and 8 GB DDR4-3200 SDRAM machine. If you run on a GPU or a different machine with other specifications, results might be different regarding performance metrics.</p><figure class="w-richtext-figure-type-image w-richtext-align-center" style="max-width:1636px" data-rt-type="image" data-rt-align="center" data-rt-max-width="1636px"><div><img src="https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6660da856d7a7bddccdbb499_image%20upscaling.png" loading="lazy" alt="__wf_reserved_inherit" width="auto" height="auto"></div></figure><h3 id="image-upscaling-using-classical-methods">Image Upscaling Using Classical Methods</h3><p>Image upscaling using classical methods refers to techniques that enhance the resolution of an image based on predefined mathematical algorithms. These methods are typically less complex and computationally intensive compared to modern deep-learning approaches. The two most commonly used classical methods are nearest-neighbor interpolation and bicubic interpolation.</p><h4 id="nearest-neighbor-interpolation-method">Nearest Neighbor Interpolation Method</h4><p>Nearest-neighbor interpolation is a straightforward and computationally efficient image upscaling method that assigns the value of the nearest pixel in the original image to each pixel in the upscaled image. This method does not involve any complex calculations. It simply replicates the value of the closest pixel, making it very fast and easy to implement. However, the simplicity of nearest-neighbor interpolation comes with significant drawbacks in image quality. Upscaled images often appear blocky and pixelated, as the method does not smooth transitions between pixels or create new image details. This leads to a "staircase" effect, especially noticeable along diagonal lines and edges.</p><h4 id="bicubic-interpolation-method">Bicubic Interpolation Method</h4><p>Bicubic interpolation is a classical image upscaling method that provides smoother and more visually appealing results compared to simpler techniques like nearest-neighbor or bilinear interpolation. It achieves this by using cubic polynomials to interpolate the pixel values. Specifically, bicubic interpolation considers the 16 nearest pixels (a 4x4 grid) around the target pixel and computes the new pixel value as a weighted average of these surrounding pixels. The weights are determined by the distance of each pixel from the target pixel, with closer pixels having more influence. This method effectively smooths transitions and reduces artifacts such as jagged edges, resulting in higher-quality images.</p><h3 id="image-upscaling-using-deep-learning-methods">Image Upscaling Using Deep Learning Methods</h3><p>Image upscaling using deep learning methods has significantly advanced the field by providing superior image quality compared to classical methods. These models leverage neural networks trained on large datasets to learn complex patterns, generating high-resolution images from low-resolution inputs. By utilizing deep architectures and sophisticated layers, these methods enhance image details, reduce artifacts, and produce smoother transitions.</p><figure class="w-richtext-figure-type-image w-richtext-align-center" style="max-width:850px" data-rt-type="image" data-rt-align="center" data-rt-max-width="850px"><div><img src="https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6660d7ee1caaf3d5883c11e1_AD_4nXctVvCh2R5tJ7HlQCNvfv9AQB5pAzy2FV6zqvQMQSYLT3nU9WAQu5QcPamqn4Lf9R0YWGAGpNNwRKLBHntIJdMIZdvZqKEXVa1h8TRm2BR121h0tGGuwl39p737Xr7CJnkumxr-Oe9-ZPjTON-aq2omSHA.png" width="auto" height="auto" alt="" loading="auto"></div></figure><h4 id="edsr-enhanced-deep-super-resolution">EDSR (Enhanced Deep Super-Resolution)</h4><p><a href="https://github.com/sanghyun-son/EDSR-PyTorch">EDSR</a> is a deep learning-based approach that excels in image super-resolution by utilizing residual blocks to efficiently learn high-frequency details. Inspired by the ResNet model, EDSR is specifically tailored for super-resolution tasks, eliminating unnecessary components like batch normalization layers to reduce computational complexity and enhance performance. The use of a large number of filters in each convolutional layer allows EDSR to capture intricate details, resulting in sharp and detailed images. Its ability to handle multiple scaling factors, such as 2x, 3x, and 4x, adds to its versatility across various super-resolution tasks.</p><p>The deeper architecture of EDSR, with more layers, significantly enhances its capacity to learn and reconstruct fine details. This method has proven highly effective in benchmark evaluations, often outperforming other state-of-the-art approaches in terms of PSNR and SSIM. EDSR's high-quality results, particularly in sharpness and detail preservation, make it a popular choice for applications where image quality is crucial. Its success in producing detailed and clear images has cemented its reputation as a leading algorithm in the field of image super-resolution.</p><h4 id="espcn-efficient-sub-pixel-convolutional-neural-network">ESPCN (Efficient Sub-Pixel Convolutional Neural Network)</h4><p><a href="https://github.com/Lornatang/ESPCN-PyTorch">ESPCN </a>is a fast and efficient algorithm designed to make images bigger, especially useful for smaller enlargements like 2x and 3x. The main idea behind ESPCN is using special layers called sub-pixel convolution layers. These layers help the network learn how to increase the image's resolution directly, making the picture clearer. This method uses less computer power and memory compared to older ways of improving image quality. ESPCN works on the smaller image first and then makes it bigger at the end, which helps keep the picture's details accurate while using less computer power.</p><p>ESPCN is great for situations where speed and efficiency are very important, like streaming videos or processing images quickly. The algorithm works very fast, making it perfect for devices that don't have a lot of processing power. Because ESPCN can quickly and efficiently make high-quality images bigger, it is a popular choice for many real-time applications. Its ability to improve image resolution without using a lot of computer resources makes it very valuable for modern image processing tasks.</p><h4 id="fsrcnn-fast-super-resolution-convolutional-neural-network">FSRCNN (Fast Super-Resolution Convolutional Neural Network)</h4><p><a href="https://github.com/yjn870/FSRCNN-pytorch">FSRCNN </a>is a better and faster version of the earlier SRCNN model, made to speed up the process without making the pictures look worse. It has a smart design with a special part at the beginning that makes the picture smaller so it's easier to work with. Then, it goes through several layers that improve the details and finally, it has a part that makes the picture big and clear again. This way, FSRCNN can work quickly and still make the pictures look really good. It uses smaller parts and more layers to learn the fine details without becoming too big itself, which is great for things like making videos and games look better.</p><p>FSRCNN also includes extra layers at the end to further improve the picture quality. It works directly on low-quality images, which makes it very efficient. The design of FSRCNN ensures that it performs well on different tests, often doing as well as or better than more complicated models. Because it can make pictures look better so quickly, FSRCNN is perfect for situations where both speed and image quality are important. Its improvements over the older SRCNN model show how effective it is for modern tasks that need real-time image enhancement, making it a popular choice for making images clearer and more detailed in live scenarios.</p><h4 id="lapsrn-laplacian-pyramid-super-resolution-network">LAPSRN (Laplacian Pyramid Super-Resolution Network)</h4><figure class="w-richtext-figure-type-image w-richtext-align-center" style="max-width:850px" data-rt-type="image" data-rt-align="center" data-rt-max-width="850px"><div><img src="https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6660d7ee7f7cd1dfe494b3d6_AD_4nXf2X8Ucedt-cZGpYJwwtaQrnOR89SY-q2B8gWl7gX5is1APlijJvScgxHyaEj6FJGMnp2o0cmz1pZ6s0Ztf0Ihy_FTfPq_Si_CIT8JvWOHYvD8p08wHD9JE3FSvJC2ER5qB5_76O1fM5PYwkho23YoHg7gX.png" width="auto" height="auto" alt="" loading="auto"></div></figure><p><a href="https://github.com/Lornatang/LapSRN-PyTorch">LAPSRN </a>works like a magic microscope for pictures, breaking them down into tiny pieces and then putting them back together to make them look better. It's like taking a blurry photo and turning it into a clear one! This helps LAPSRN make images bigger without losing quality, especially if you want to make them four or eight times bigger. It uses special math tricks called transposed convolutions and residual learning to make the pictures sharp and detailed, like fixing old photos or making satellite images clearer.</p><p>LAPSRN is really smart because it can handle making pictures bigger by different amounts. It's like having a tool that can work with different sizes of puzzles! When people test LAPSRN, it usually gets really good scores, showing that the pictures it makes are very clear and nice to look at. By slowly fixing up the details in the picture, LAPSRN makes sure that both the big parts and the tiny details look just right in the bigger picture. This makes LAPSRN super helpful for making really big pictures that still look awesome, which is important for lots of cool projects.</p><figure class="w-richtext-figure-type-image w-richtext-align-center" style="max-width:571px" data-rt-type="image" data-rt-align="center" data-rt-max-width="571px"><div><img src="https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6660d7ee23741b8f165e0cab_AD_4nXdvQitBdMD790BqXDmITrEZjN2HXwvzvQm4BN-oCv1VBCbn_F8KMnJO_ekKxQolMz_Ap-JPOZHWwBN24QvqVADefzzNjhBnT95_9mNVpJZkOm7a7JFb1Yumog9HRmXuGomWrfB9ORYua9S6FyEkD3FCp6Sd.png" width="auto" height="auto" alt="" loading="auto"></div></figure><figure class="w-richtext-figure-type-image w-richtext-align-center" style="max-width:438px" data-rt-type="image" data-rt-align="center" data-rt-max-width="438px"><div><img src="https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6660d7ef3e04118fd3c59dd8_AD_4nXenoy9gxpnseu4hLuJ131VbQohR87fQVmFowzIkx00XO84cp8qbM5iah1bYTWAnXf9G_AuDIqgbKl503PWFwJsMn_QvYVEQvp8bCHgCj6v8NVieOLAewFnAGeBuZn3Em3q4zuK-_dSKT3JzlLl8fxRpF_eC.png" width="auto" height="auto" alt="" loading="auto"></div></figure><figure class="w-richtext-figure-type-image w-richtext-align-center" style="max-width:600px" data-rt-type="image" data-rt-align="center" data-rt-max-width="600px"><div><img src="https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6660d7ee0b872e8d9f3f3ac9_AD_4nXcKl4QSEYWsyMRi55-fXn2B0fLxxfl6N8wle9anpL2fYhn8nTFffDs-7ZyRTh-o0HjRAXDaACJu2x7Ny0ILXzrqVexPoxY706x6sge7cGRI_9PVwtb9m0R2Hs93eqDCly0Su7qBj0p5-TLYFCLa-9AoC8nd.png" width="auto" height="auto" alt="" loading="auto"></div></figure><figure class="w-richtext-figure-type-image w-richtext-align-center" style="max-width:542px" data-rt-type="image" data-rt-align="center" data-rt-max-width="542px"><div><img src="https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6660d7ee778ba8f98e74b96d_AD_4nXdPv1gI8ML761yVC_ZAdBthL8uQR5NFhmcbMwjhzHSE1dtip_Ayt1BxsVAyfh_iYAA1e8j4jGJXw156pfdgUxIKUpVg-YOQ8rMuqMrQnfnuu4xVnKcx6uijG18fu0wJ1WP7Fr0iNEiDq_wKPfALHELMPtmP.png" width="auto" height="auto" alt="" loading="auto"></div></figure><h3 id="image-upscaling-using-generative-adversarial-network">Image Upscaling Using Generative Adversarial Network</h3><p>A <a href="https://en.wikipedia.org/wiki/Generative_adversarial_network">Generative Adversarial Network</a> (GAN) is a deep learning architecture. It involves two neural networks, termed the "generator" and the "discriminator," that compete against each other. The generator creates new data instances, while the discriminator evaluates them against a real dataset. The goal is for the generator to become so good at producing data that the discriminator can't tell the difference between real and generated data. This process helps generate high-quality and realistic data. A GAN is called adversarial because it trains two different networks and pits them against each other.</p><h4 id="esrgan-model-enhanced-super-resolution-generative-adversarial-networks">ESRGAN Model (Enhanced Super-Resolution Generative Adversarial Networks)</h4><figure class="w-richtext-figure-type-image w-richtext-align-center" style="max-width:850px" data-rt-type="image" data-rt-align="center" data-rt-max-width="850px"><div><img src="https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6660d7ee0f4c743f69c85d60_AD_4nXdvMHXum4ZlZvC61uDTObZRR-eeDSB0zRmn6ZlllL-ch11gUVjtVXu5c9o5GiVloqFH2yR3_pqwTR6Lk4ZVbhQj2AxZkzo3XTcdrP2QRG5lBu2lTu0MI0-fAfIw9lR93ZBeElVtrRlBQtalVW39s0G4Uf0I.png" width="auto" height="auto" alt="" loading="auto"></div></figure><p>The architecture of ESRGAN is built upon the principles of a typical Generative Adversarial Network (GAN) but includes several key enhancements and modifications tailored for super-resolution tasks. Here's a breakdown of its architecture:</p><h5 id="generator-network">Generator Network</h5><ul><li>Residual-in-Residual Dense Block (RRDB): It is a component in advanced neural networks, particularly in super-resolution models. It consists of several densely connected convolutional layers where each layer’s input is concatenated with its outputs, enhancing feature reuse and information flow. This structure avoids using batch normalization, which helps in preserving the range of features.</li></ul><ul><li>Up-sampling Layers: The generator uses up-sampling layers to scale up the low-resolution input to the desired size. In ESRGAN, the up-sampling might be achieved using sub-pixel convolution layers that rearrange the output of a convolutional layer to form a higher-resolution image.</li></ul><ul><li>High-quality Image Reconstruction: The output of the last RRDB is passed through a convolution layer to reconstruct the high-resolution image. The ESRGAN generator focuses on enhancing finer details and reducing the blurring effects that are often present in super-resolved images.</li></ul><h5 id="discriminator-network">Discriminator Network</h5><ul><li>Convolutional Layers: The discriminator uses a series of convolutional layers that progressively downsample the input image, helping it extract various features at different scales.</li></ul><ul><li>Leaky ReLU Activation: Leaky ReLU is used instead of standard ReLU to provide a non-linearity that allows gradients to flow through the network even for negative values, enhancing the training stability.</li></ul><ul><li>Fully Connected Layers: After processing through convolutional layers, the features are flattened and passed through fully connected layers that finally output a scalar value indicating whether the input image is real or fake.</li></ul><h4 id="code-for-image-upscaling-using-real-esrgan-model">Code for Image Upscaling using Real-ESRGAN Model</h4><p>Install the necessary libraries for the ESRGAN</p><div data-rt-embed-type='true'><pre>
<code class="language-py">
pip install torch Pillow numpy scikit-image
pip install git+https://github.com/sberbank-ai/Real-ESRGAN.git
</code>
</pre></div><p>Import them,</p><div data-rt-embed-type='true'><pre>
<code class="language-py">
import torch
from PIL import Image
import numpy as np
from RealESRGAN import RealESRGAN
import time
from skimage.metrics import mean_squared_error, structural_similarity

# Setting the device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
</code>
</pre></div><p>The last line sets the device for computation. If CUDA is available (indicating the presence of a GPU), it uses the GPU for faster computation; otherwise, it falls back to the CPU.</p><div data-rt-embed-type='true'><pre>
<code class="language-py">
# Load the model
model = RealESRGAN(device, scale=4)
model.load_weights('weights/RealESRGAN_x4.pth', download=True)

# Path to the input image
path_to_image = 'path to your image'
image = Image.open(path_to_image).convert('RGB')

# Run the model
sr_image = model.predict(image)

# Save the output image
sr_image.save('output path to your image')
</code>
</pre></div><p>These lines initialize the RealESRGAN model with the specified device and a scaling factor (scale=4 which means the output image will have four times the resolution of the input). It then loads the pre-trained weights from a specified path, with an option to download the weights if they're not present locally.</p><h3 id="image-upscaling-using-stable-diffusion-models">Image Upscaling Using Stable Diffusion Models</h3><p><a href="https://en.wikipedia.org/wiki/Stable_Diffusion">Stable Diffusion</a> is a type of generative artificial intelligence (generative AI) model that specializes in creating detailed images from textual descriptions. It utilizes a variant of the diffusion model, which gradually transforms patterns of random dots into detailed images through a reverse process that removes noise over many steps. The model is based on diffusion technology and uses latent space.</p><h4 id="the-architecture-of-stable-diffusion-models">The architecture of Stable Diffusion Models</h4><figure class="w-richtext-figure-type-image w-richtext-align-center" data-rt-type="image" data-rt-align="center"><div><img src="https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6660d7eee2b259c929824af1_AD_4nXcw6pY9c4ugDdCtUYdLqNPFoS_H6sPRv6uZGbPghm7wpp2OsuI39dabTFTxurbz1LudiIGqjppP0oqi1x816y0L-CwRK9ZhfzyNVzhe-AqS10oWi_l0WnbPdgj5OBOe_fS7xzrO5TlXI93WZx18J-lJKKv2.png" width="auto" height="auto" alt="" loading="auto"></div></figure><p>The main architectural components of Stable Diffusion include a variational autoencoder, forward and reverse diffusion, a noise predictor, and text conditioning.</p><h5 id="variational-autoencoder">Variational AutoEncoder</h5><ul><li>Encoder: This part of the model takes a large image (512x512 pixels) and compresses it down to a smaller, more manageable size (64x64 pixels) in something called latent space. Latent space is a compressed representation that's easier for the model to work with.</li><li>Decoder: The decoder does the opposite of the encoder. It takes the compressed image from latent space and enlarges it back to its original size, restoring the details as much as possible.</li></ul><h5 id="forward-and-reverse-diffusion">Forward and Reverse Diffusion</h5><ul><li>This process gradually adds random noise to an image until it turns into pure noise. Essentially, it transforms the image into something unrecognizable. This is mainly used during training and sometimes in image-to-image conversions where an initial image is transformed into a different style or appearance.</li></ul><ul><li>Reverse diffusion is the process that turns the noisy image back into a clear picture. It works by estimating and removing the noise added during the forward diffusion step by step, eventually revealing a detailed image that can be a cat, a dog, or any other subject defined by the training data.</li></ul><h5 id="noise-predictor">Noise Predictor</h5><p>Stable Diffusion uses a special kind of neural network called U-Net, which is particularly good at removing noise from images. It predicts how much noise needs to be removed at each step of the reverse diffusion process to reveal the final image gradually.</p><h5 id="text-conditioning">Text Conditioning</h5><p>Stable Diffusion uses text prompts to guide image generation. Each word in the prompt is converted into a numerical format using a tokenizer, which the model understands. These numbers tell the model what features and elements to include in the image, like colors, objects, or styles.</p><h4 id="code-for-image-upscaling-using-stable-diffusion-x4-upscaler-model">Code for Image Upscaling using Stable Diffusion x4 upscaler Model</h4><p>Install these libraries</p><div data-rt-embed-type='true'><pre>
<code class="language-py">
!pip install diffusers transformers accelerate scipy safetensors
</code>
</pre></div><p>Setup the pipeline,</p><div data-rt-embed-type='true'><pre>
<code class="language-py">
import requests
from PIL import Image
from io import BytesIO
from diffusers import StableDiffusionUpscalePipeline
import torch

# load model and scheduler
model_id = "stabilityai/stable-diffusion-x4-upscaler"
pipeline = StableDiffusionUpscalePipeline.from_pretrained(model_id, torch_dtype=torch.float16)
pipeline = pipeline.to("cuda")
</code>
</pre></div><p>The model_id is a string that uniquely identifies the model on Hugging Face's model hub, in this case, "stabilityai/stable-diffusion-x4-upscaler", which specifies a version of the Stable Diffusion model specifically trained to upscale images by a factor of four. The method StableDiffusionUpscalePipeline.from_pretrained is used to load this model.&nbsp;</p><p>Here, it is initialized with a specific configuration to use 16-bit floating-point precision (torch.float16), which is a strategy to reduce memory usage, allowing the model to run faster and more efficiently on compatible hardware. The pipeline.to("cuda") command then shifts the model’s computations to a GPU, assuming one is available and CUDA-compatible. This significantly accelerates the processing speed, leveraging the GPU's ability to handle parallel computations, which is ideal for the intensive calculations required in upscaling images using deep learning models.</p><div data-rt-embed-type='true'><pre>
<code class="language-py">
path = 'path to your image'
low_res_img = Image.open(path).convert("RGB")
low_res_img = low_res_img.resize((128, 128))

prompt = "prompt describing your image"

upscaled_image = pipeline(prompt=prompt, image=low_res_img).images[0]
upscaled_image.save("path to your output image")
</code>
</pre></div><p>Define a prompt to guide the model in upscaling the image. This can be used to describe the content of the image or provide additional context to influence the upscaling process.</p><h3 id="quantitative-comparison-between-upscaling-models">Quantitative Comparison Between Upscaling Models</h3><figure class="w-richtext-figure-type-image w-richtext-align-center" style="max-width:1136px" data-rt-type="image" data-rt-align="center" data-rt-max-width="1136px"><div><img src="https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6660d7ee37e19cc871ef380d_AD_4nXchVrK7Tx0fCOzxq7BgxVSu8JM1uC-od2OT_h87Pa9djCyO0Ws3GLprqEmR2n9avU82wzIKzBHa0iAFrvsWhSxxgPVxYMxbO7E_Yaz2i5pUtMCm5-azY5opmS505DmLesNg8pQK8X90bw3vBluUKgury7zl.png" width="auto" height="auto" alt="" loading="auto"></div></figure><p>Image Upscaling Models used for Benchmarking are <a href="https://github.com/XPixelGroup/DiffBIR">DiffBIR</a>, <a href="https://github.com/zsyOAOA/ResShift">ResShift</a>, <a href="https://github.com/Fanghua-Yu/SUPIR">SUPIR</a>, and <a href="https://github.com/xinntao/Real-ESRGAN">RealESRGAN</a>.</p><figure class="w-richtext-figure-type-image w-richtext-align-center" style="max-width:600px" data-rt-type="image" data-rt-align="center" data-rt-max-width="600px"><div><img src="https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6660d7eebfc9b07b9991e056_AD_4nXfyGkN3zgcBBlto7j6GxovTG2O-yTAbZ7tA8Zc1sPrIPg5adNzssZ4JyNgWgaYnxPMg6CnGzbx7Gfb9xB--onTqsMEgOor09WW58rRg3cUpjEqJKp04x3TW6MG05awhxeWjj2t8VnLyHwNGd4pmEPoGQ2tc.png" width="auto" height="auto" alt="" loading="auto"></div></figure><figure class="w-richtext-figure-type-image w-richtext-align-center" style="max-width:600px" data-rt-type="image" data-rt-align="center" data-rt-max-width="600px"><div><img src="https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6660d7eed4582b981b394d06_AD_4nXfHKupYCzdpM_dMYjxY_KYGEzMGUgmfvq8g0YhYKsGnGKlj5SsFiUqNlxa8bsvkj7KUzecxzjdYLYywVjdMlXPPQLowtOUNdQu4nV--v1HKShT6OLVa8vNSxN95iY-LgA-i2X_KyAqDTsZRI4Py2WnIZxHj.png" width="auto" height="auto" alt="" loading="auto"></div></figure><figure class="w-richtext-figure-type-image w-richtext-align-center" style="max-width:600px" data-rt-type="image" data-rt-align="center" data-rt-max-width="600px"><div><img src="https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6660d7ee3e04118fd3c59db8_AD_4nXeX0kKbXBKeHJZOV-x0Uq03ZoLzjICPtApLfBH7A7bm08mJINc4NRYPd3o2hwloqkfPxN5to36-V0DrtUWbaaenQcjo-l_tb-t_ONqm_dWBRVVsLOZ_PsnDpAPbRyDYd7lItxK-Hq_WZKbe059WEbiPTIJk.png" width="auto" height="auto" alt="" loading="auto"></div></figure><figure class="w-richtext-figure-type-image w-richtext-align-center" style="max-width:620px" data-rt-type="image" data-rt-align="center" data-rt-max-width="620px"><div><img src="https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6660d7ee5c4df66ea68c3164_AD_4nXf_wUKhX22oqguzkFc03jqtH0xLQztsAkxh2uVmf8AstDKQR-xUyLNYwUeK0J3HZPtHXzLpZZKyZiwKxGlz2-Lbm8k7IoiZCzGBylASUWF4Eej0E4V2h3orjccYcsDO2YkqPzUm945WmLy7E78OA0BIt6oI.png" width="auto" height="auto" alt="" loading="auto"></div></figure><p>For the detailed code Refer to <a href="https://github.com/Mercity-AI/Image-Upscaling-Benchmark/tree/main">Github Link</a>.</p><h3 id="comparison-between-image-upscaling-models-on-further-downscaling">Comparison between Image Upscaling Models on Further Downscaling</h3><figure class="w-richtext-figure-type-image w-richtext-align-center" style="max-width:711px" data-rt-type="image" data-rt-align="center" data-rt-max-width="711px"><div><img src="https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6660d7ee3330567a869698b8_AD_4nXffxcvK3g_sZYY--pyVHMSmFBCh0UPZWXjVXXLDikePviWFejjeyZ_R2R_mx2qXME4yyEzd6iFkTj0bQrvfD1MiWkBb5NU7Hpv5eChSte53eQxEImSKvs0Vc44pubHjR6VxUFtwJoecWmZTMBMS1uM194f8.png" width="auto" height="auto" alt="" loading="auto"></div></figure><p>The task involves comparing the effectiveness of different image upscaling models through two distinct processes. In the first process, an image is downscaled by a factor of 2 and then upscaled back to its original size using various image upscaling models. This procedure evaluates the ability of the models to recover the original image details from a slightly reduced version. By doing so, it tests the models' proficiency in handling minimal information loss and restoring the image's quality effectively.</p><p>In the second process, the image undergoes a more aggressive downscaling by a factor of 8, followed by a two-step upscaling process. First, the image is upscaled by a factor of 4, and then it is further upscaled by a factor of 2 using different upscaling models. This approach simulates a more challenging scenario where significant information is lost during the initial downscaling, and the models must perform two stages of upscaling to restore the image to its original dimensions. The final upscaled images from both processes are then compared to the original image using metrics such as Mean Squared Error (MSE), Structural Similarity Index (SSIM), and processing time. This comprehensive evaluation helps determine which upscaling models are more efficient and effective in terms of image quality restoration and computational efficiency.</p><figure class="w-richtext-figure-type-image w-richtext-align-center" style="max-width:600px" data-rt-type="image" data-rt-align="center" data-rt-max-width="600px"><div><img src="https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6660d7eebbfad0e37b361dd0_AD_4nXfmoIONNwlheY8lJlmR4I5NshoLK3dBkUqKtJlDBCIZQYNh-mMB9McdUOeP9g7QA5A-r68Y3od5BhdmSWlO3UQrLsiPn8RmYp-8l8KN_yWKApn5kYi0a0gJLI2LgEg4G3ZudrPVw0XGZJr6UHbq0QBbZkw.png" width="auto" height="auto" alt="" loading="auto"></div></figure><figure class="w-richtext-figure-type-image w-richtext-align-center" style="max-width:600px" data-rt-type="image" data-rt-align="center" data-rt-max-width="600px"><div><img src="https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6660d7ee2ee15090b1613b2f_AD_4nXeR7nxVniPYiwenZotw7HZUHXlGauUvYlv8bldtch9jYdhiokOVZuR0I5J0x9D7ugzz_UZGARnJ1fXpEQJ9DMK_sx2FiQbpU0cQF1lxmN7HY05sVAT26FnpNeTWHRF8Mel7SNEP1Pqs4I4sZgZM1WkLS5PF.png" width="auto" height="auto" alt="" loading="auto"></div></figure><figure class="w-richtext-figure-type-image w-richtext-align-center" style="max-width:600px" data-rt-type="image" data-rt-align="center" data-rt-max-width="600px"><div><img src="https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6660d7ee707f912094336520_AD_4nXeN_NmFN4icWJ0tQL_DpqnSYMwIsfEVwrWE88RYipDGCs3ymzyxzcmnAh2m4IMFe5FfpSzEUeSVlLoeeBrNHAUjshlX0mYUxinItSVMNrv3XYtaQ-MGAQ-qqQ4mDLB1YqHFNS6RR1qaht_ha8fK7SknW7c.png" width="auto" height="auto" alt="" loading="auto"></div></figure><h2 id="ready-to-bring-your-images-into-stunning-high-definition">Ready to Bring Your Images Into Stunning High-Definition</h2><p>Experience the magic of high-resolution with Mercity AI! Our expert team specializes in transforming low-resolution images into stunning, high-definition visuals. Elevate the quality of your photos and graphics with our advanced upscaling technology. Ready to see the difference? <a href="https://www.mercity.ai/">Contact us</a> today and bring your images to life like never before!</p>
