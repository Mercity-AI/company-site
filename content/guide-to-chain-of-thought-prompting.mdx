---
title: Comprehensive Guide to Chain-of-Thought Prompting
slug: guide-to-chain-of-thought-prompting
publishedAt: '2025-09-13'
createdAt: '2023-09-01'
updatedAt: '2025-09-12'
summary: >-
  Learn how you can use Chain of Thought (CoT) prompting to boost LLM
  performance at reasoning tasks.
authors:
  - name: Maithili Badhan
category: Prompt Engineering
image: >-
  https://blog-cdn.mercity.ai/blog/guide-to-chain-of-thought-prompting/64f22a88aad6914b12dacbb4_chain-of-thought.jpg
---

<div class="rich-text w-richtext"><h1 id="how-chain-of-thought-cot-prompting-makes-llms-better-at-reasoning">How Chain-of-Thought (CoT) Prompting Makes LLMs Better At Reasoning?</h1><p>Scaling up large language models (LLMs) has shown good results in sentiment analysis and machine translation, even without any examples. However, they fail in complex multi-step problems such as arithmetic and commonsense reasoning. To address this, LLMs can either be fine-tuned for a particular task or taught with few-shot prompting. However, both of these methods have limitations. Fine-tuning is costly for creating high-quality reasoning, while only few-shot prompting is not effective enough for the task.</p><p>Chain-of-Thought (CoT) prompting can address both of these problems. In this article, we will explore CoT prompting and how implementing it can upskill your business.</p><h2 id="what-is-prompt-engineering">What is Prompt Engineering?</h2><p>Prompt engineering is the practice of writing well-structured and carefully crafted prompts that can be better interpreted by a generative AI model. A prompt tells the LLM what task to perform and what kind of output to generate. It can contain instruction, context, input data, and output indicators. Using prompt engineering, we can use LLMs to carry out various tasks, from simple question answering to complex creative text generation. It is based on an emergent property, in-context learning, allowing LLMs to learn from prompts. Prompt engineering improves the performance of LLMs on the task at hand. It uses zero-shot, few-shot, active and CoT prompting as discussed ahead.</p><p>We also have a blog on <a href="https://www.mercity.ai/blog-post/advanced-prompt-engineering-techniques#want-to-write-high-quality-prompts-for-llms">advanced prompt engineering</a>.</p><h3 id="zero-shot-prompting">Zero-shot Prompting</h3><p>In zero-shot prompting, we provide the LLM a prompt that describes the task, but the prompt does not provide any examples for the task. The LLM is then asked to generate a response to the prompt. It improves the flexibility and generalization of LLMs. It can be used to train LLMs on several tasks, without having to collect training data for each task. For example, ChatGPT can write a poem on prompt engineering without giving any examples of how to write a poem. However, zero-shot prompting is limited for complex tasks.</p><figure class="w-richtext-align-center w-richtext-figure-type-image" style="max-width:1021pxpx"><div><img alt="" loading="lazy" src="https://blog-cdn.mercity.ai/blog/guide-to-chain-of-thought-prompting/68c4a6be268c58e672f2c732_ea4a7243.jpg"/></div></figure><h3 id="few-shot-prompting">Few-shot Prompting</h3><p>Few-shot prompting can provide demonstrations to steer the model to better performance. It is a technique for providing LLMs with a few examples of the desired output, in addition to the prompt. The examples help the model to better understand the task and to generate more accurate and informative responses. We should provide vast and different examples to the model, instead of multiple similar examples. It ensures the model learns as much as possible about the task. Standard few-shot prompting is a good technique for many tasks, but not reliable for complex reasoning tasks. Therefore, more advanced prompting techniques, such as chain-of-thought, active prompting, and fine-tuning, are needed.</p><figure class="w-richtext-align-center w-richtext-figure-type-image" style="max-width:936pxpx"><div><img alt="" loading="lazy" src="https://blog-cdn.mercity.ai/blog/guide-to-chain-of-thought-prompting/68c4a6be268c58e672f2c735_b7362b56.jpg"/></div></figure><h3 id="active-prompting">Active Prompting</h3><p><a href="https://github.com/shizhediao/active-prompt">Active prompting</a> improves the performance of LLMs on complex tasks by iteratively providing them with feedback on their responses. This feedback can help the LLMs to learn from their mistakes and to generate more accurate and informative responses. It provides the LLM with a prompt and a few examples of the desired output. The LLM then generates a response. The response is then evaluated by a human evaluator, who provides feedback to the LLM on the accuracy and informativeness of the response. The LLM then uses this feedback to improve its response generation capabilities. This process repeats until the LLM can generate responses that are accurate and informative enough to satisfy the human evaluator.</p><p><a href="https://arxiv.org/pdf/2302.12246.pdf">Active prompting</a></p><p>is important for CoT prompting as it identifies important questions for annotation, minimizes human annotation efforts, and improves the accuracy and informativeness of CoT prompts. The following figure shows active prompting with CoT to improve performance. It is a four-stage process that involves estimating the uncertainty of a question by querying an LLM multiple times, selecting the most uncertain questions for annotation by ranking them, annotating them with detailed feedback from human evaluators, inferring the answers to new questions by using the LLM to generate answers and the feedback from the annotation step to improve the quality.</p><figure class="w-richtext-align-center w-richtext-figure-type-image" style="max-width:939pxpx"><div><img alt="" loading="lazy" src="https://blog-cdn.mercity.ai/blog/guide-to-chain-of-thought-prompting/68c4a6be268c58e672f2c738_38f81e06.jpg"/></div></figure><h2 id="what-is-chain-of-thought-prompting">What is Chain-of-Thought Prompting?</h2><p><a href="https://arxiv.org/pdf/2201.11903.pdf">Chain-of-Thought prompting</a> is a prompt engineering technique through which we force LLMs to output a sequence of intermediate steps that lead to the desired answer. It improves the reasoning abilities of LLMs. It is beneficial because it allows the model to focus on solving one step at a time, rather than having to consider the entire problem all at once. It can be especially helpful for complex problems that would be difficult or impossible to solve in a single step. It provides an interpretable window into the behavior of the model. We can see how the model arrived at its answer by following the sequence of steps that it took.</p><figure class="w-richtext-align-center w-richtext-figure-type-image" style="max-width:862pxpx"><div><img alt="" loading="lazy" src="https://blog-cdn.mercity.ai/blog/guide-to-chain-of-thought-prompting/68c4a6be268c58e672f2c741_3402878e.jpg"/></div></figure><p>CoT prompting can be used with LLMs with a large set of parameters (~100 B parameters) for several reasoning tasks, including math word problems, commonsense reasoning, and symbolic manipulation. For example, using <a href="https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html">CoT prompting</a> in the PaLM model instead of standard few-shots, improved the performance in the GSM8K benchmark from 17.9% to 58.1%. CoT prompting can be readily elicited in sufficiently large language models without any special training or fine-tuning of the model. It makes CoT prompting a scalable and accessible technique.</p><figure class="w-richtext-align-center w-richtext-figure-type-image" style="max-width:411pxpx"><div><img alt="" loading="lazy" src="https://blog-cdn.mercity.ai/blog/guide-to-chain-of-thought-prompting/68c4a6be268c58e672f2c744_7e42e126.jpg"/></div></figure><h3 id="few-shot-cot">Few-Shot CoT</h3><p>Few-shot prompting prompts the LLM with a question and the answer. Then, the LLM is provided with a few examples of how to solve similar problems. The examples are presented in a way that encourages the LLM to reason about the problem and come up with a chain of thought that leads to the answer.</p><figure class="w-richtext-align-center w-richtext-figure-type-image"><div><img alt="" loading="lazy" src="https://blog-cdn.mercity.ai/blog/guide-to-chain-of-thought-prompting/68c4a6be268c58e672f2c73b_d69047d3.jpg"/></div></figure><p><a href="https://arxiv.org/abs/2305.14045">Few-shot CoT</a> is a more effective technique for improving the reasoning abilities of LLMs than the few-shot baseline because it provides LLMs with examples of similar problems. It can be more complex to implement than a few-shot baseline because it requires the creation of example prompts. However, the benefits of few-shot CoT outweigh the additional complexity.</p><figure class="w-richtext-align-center w-richtext-figure-type-image" style="max-width:994pxpx"><div><img alt="" loading="lazy" src="https://blog-cdn.mercity.ai/blog/guide-to-chain-of-thought-prompting/68c4a6be268c58e672f2c73e_04c2ed6b.jpg"/></div></figure><h3 id="zero-shot-cot">Zero-Shot CoT</h3><p><a href="https://arxiv.org/abs/2205.11916">Zero-shot CoT</a> involves adding "Let's think step by step" to the original prompt. It extracts reasoning and answers using two prompts.</p><ul role="list"><li>Reasoning extraction: In this step, the language model thinks about the question and comes up with a chain of reasoning that leads to the answer. For this, we give the language model a prompt that includes the question and a trigger sentence "<strong>Let's think step by step</strong>." The language model will then generate a sentence that explains how it arrived at the answer.</li></ul><ul role="list"><li>Answer extraction: In the second step, we extract the final answer from the language model's response. We concatenate the prompt, the generated sentence, and a trigger sentence, "<strong>The answer is</strong>". It tells the language model to give us the answer. The language model will then generate a sentence that contains the answer to the question.</li></ul><figure class="w-richtext-align-center w-richtext-figure-type-image" style="max-width:852pxpx"><div><img alt="" loading="lazy" src="https://blog-cdn.mercity.ai/blog/guide-to-chain-of-thought-prompting/68c4a732f6c79420bbf1232c_f20c0952.jpg"/></div></figure><p>In contrast to this, the zero-shot baseline uses prompts like "The answer is" for answer extraction. Few-shot prompting, whether standard or CoT, avoids the need for such answer-extraction prompts by designing example answers to end in the correct formats.</p><figure class="w-richtext-align-center w-richtext-figure-type-image" style="max-width:802pxpx"><div><img alt="" loading="lazy" src="https://blog-cdn.mercity.ai/blog/guide-to-chain-of-thought-prompting/68c4a732f6c79420bbf1232f_15f147a5.jpg"/></div></figure><p>On comparing zero-shot CoT to two other methods for evaluating the zero-shot reasoning abilities of LLMs, researchers found that zero-shot-CoT outperforms the other methods on various reasoning tasks. If you are looking for a smaller model trained on CoT prompting, consider the Flan-T5 model. It can be used for zero-shot NLP tasks including text summarization, natural language inference, translation, and common sense reasoning.</p><figure class="w-richtext-align-center w-richtext-figure-type-image" style="max-width:712pxpx"><div><img alt="" loading="lazy" src="https://blog-cdn.mercity.ai/blog/guide-to-chain-of-thought-prompting/68c4a732f6c79420bbf12332_4ec38025.jpg"/></div></figure><h2 id="when-does-cot-emerge">When Does CoT Emerge?</h2><p>CoT reasoning is an <a href="https://web.stanford.edu/class/cs224v/lectures/jason-wei-emergence-talk-stanford.pdf">emergent ability</a> of LLMs that may arise due to <a href="https://arxiv.org/abs/2210.11416">scaling models</a> over 100 billion parameters. It does not positively impact performance for smaller LLMs and only yields performance gains when used with models of this size. There are two reasons for it. Firstly, smaller LLMs are not able to generate long chains of thought that are both fluent and logical. This leads to lower performance than standard prompting. Secondly, CoT reasoning is more effective for more complicated problems. It requires the LLM to be able to identify the key steps involved in solving a problem and then generate a chain of thoughts that leads to the solution. Smaller LLMs may not be able to do this as effectively as larger LLMs.</p><figure class="w-richtext-align-center w-richtext-figure-type-image" style="max-width:802pxpx"><div><img alt="" loading="lazy" src="https://blog-cdn.mercity.ai/blog/guide-to-chain-of-thought-prompting/68c4a732f6c79420bbf12335_ba0ffc6d.jpg"/></div></figure><p>Another reason for the emergence of CoT reasoning in large LLMs may be due to their pre-training data. Larger LLMs are typically trained on massive datasets that include step-by-step reasoning, which could help them to develop the ability to reason in a chain-of-thought fashion. Instruction-following does not seem to be necessary for CoT capabilities, as zero-shot and few-shot CoT reasoning was shown using LLMs that were not fine-tuned to follow instructions. However, instruction-following could possibly improve the quality of CoT reasoning. Ultimately, more research is needed to determine the exact cause of the emergence of CoT reasoning in large LLMs.</p><h2 id="how-to-perform-cot-prompting">How To Perform CoT Prompting?</h2><p>To perform Chain of Thought prompting you just need to append “<strong>Let’s think step by step</strong>” at the end of your prompt. This forces the model to think in steps and break down the problem in steps or smaller parts. Here’s an example of what happens when don’t use and do use Chain of Thought prompting:</p><figure class="w-richtext-align-center w-richtext-figure-type-image" style="max-width:1600pxpx"><div><img alt="" loading="lazy" src="https://blog-cdn.mercity.ai/blog/guide-to-chain-of-thought-prompting/68c4a7419d734a87bb467ac5_1ca195eb.jpg"/></div></figure><figure class="w-richtext-align-center w-richtext-figure-type-image" style="max-width:1598pxpx"><div><img alt="" loading="lazy" src="https://blog-cdn.mercity.ai/blog/guide-to-chain-of-thought-prompting/68c4a7419d734a87bb467ac8_3bb2d506.jpg"/></div></figure><p>Here you can see how using Chain of thought makes LLM return a better more sophisticated and correct output. The prompt without thinking in steps immediately results in a wrong answer.</p><p>If you have a rather strict problem that you know can only be solved with a specific set of reasoning patterns, that’s where you would use Few Shot COT. You can provide some examples of reasoning steps required for your specific set of problems and then the LLM will attempt to solve the given problem using similar steps. Or you can use this technique to solve the problem in a specific method for your users. For example, if students are going to be using your app, you might want to use few-shot-CoT to solve problems in a fun, simple and easy to understand way.</p><figure class="w-richtext-align-center w-richtext-figure-type-image" style="max-width:1030pxpx"><div><img alt="" loading="lazy" src="https://blog-cdn.mercity.ai/blog/guide-to-chain-of-thought-prompting/68c4a7419d734a87bb467acb_3973fe94.jpg"/></div></figure><p>These few shot examples should showcase the intermediate steps and the final solution. Once you have developed the chain of thought prompts and examples, you can incorporate them into the model. Finally, test the model and iterate on the chain of thought prompts and examples until the model's performance is satisfactory.</p><h2 id="key-aspects-of-cot-prompting">Key Aspects of CoT Prompting</h2><p>In this section, we will explore crucial dimensions of CoT prompting impacting its performance and reliability in large language models. We will delve into how sensitivity, self-consistency, robustness, and coherence play pivotal roles in shaping the effectiveness of CoT prompting technique.</p><h3 id="self-consistency">Self-consistency</h3><p><a href="https://arxiv.org/pdf/2203.11171.pdf">Self-consistency</a> is a technique for improving the performance of language models on tasks that require multi-step reasoning. In the context of chain-of-thought prompting, self-consistency can be used to improve the performance of the model by sampling multiple, diverse chains of thought for the same problem. The model can then be trained to select the most consistent answer from these chains of thought.</p><figure class="w-richtext-align-center w-richtext-figure-type-image" style="max-width:841pxpx"><div><img alt="" loading="lazy" src="https://blog-cdn.mercity.ai/blog/guide-to-chain-of-thought-prompting/68c4a7419d734a87bb467ace_02b580bb.jpg"/></div></figure><p>Self-consistency significantly boosts the performance of CoT prompting on many popular arithmetic and commonsense reasoning benchmarks. For example, on the GSM8K benchmark, self-consistency increased the performance of CoT prompting by 17.9%. On the SVAMP benchmark, by 11.0%. And on the AQuA benchmark, by 12.2%. It is an entirely unsupervised technique that works off-the-shelf with pre-trained language models. It requires no additional human annotation and avoids any other training, models, or fine-tuning. It is robust to sampling strategies and parameters. On varying T in temperature sampling, k in top-k sampling, and p in nucleus sampling strategies over PaLM-540B, self-consistency consistently improved performance.</p><figure class="w-richtext-align-center w-richtext-figure-type-image" style="max-width:754pxpx"><div><img alt="" loading="lazy" src="https://blog-cdn.mercity.ai/blog/guide-to-chain-of-thought-prompting/68c4a7419d734a87bb467ad1_edccafa6.jpg"/></div></figure><h3 id="robustness">Robustness</h3><p>The researchers conducted experiments with three different sets of chain-of-thought annotations, each written by a different annotator. They found that CoT prompting consistently outperformed the standard baseline, regardless of the annotator. This suggests that CoT prompting is not dependent on a particular linguistic style. The researchers also conducted experiments with exemplars randomly sampled from the GSM8K training set, an independent source. They found that CoT prompting with these exemplars performed comparably to CoT prompting with manually written exemplars. This suggests that CoT prompting is not dependent on the specific exemplars that are used.</p><figure class="w-richtext-align-center w-richtext-figure-type-image" style="max-width:657pxpx"><div><img alt="" loading="lazy" src="https://blog-cdn.mercity.ai/blog/guide-to-chain-of-thought-prompting/68c4a7429d734a87bb467ad5_2ba7207c.jpg"/></div></figure><p>The researchers also conducted experiments with varying numbers of exemplars. They found that CoT prompting remained robust to varying numbers of exemplars. This suggests that CoT prompting does not require a large number of exemplars to be effective. The researchers conducted experiments with a variety of language models, including LaMDA 137B. They found that CoT prompting was effective with all of these language models. This suggests that CoT prompting is not dependent on the specific language model that is used. Overall, the results of these experiments suggest that CoT prompting is a robust technique for improving the performance of language models on a variety of tasks. It is not dependent on a particular linguistic style, annotator, set of exemplars, or language model.</p><h3 id="sensitivity">Sensitivity</h3><p><a href="https://arxiv.org/pdf/2104.08786.pdf">Sensitivity</a> in CoT prompting refers to the extent to which the performance of the model is affected by the design of the prompts. If the prompts are not well-designed, then the model's performance may deteriorate. The prompts should be clear, concise, and easy for the model to understand. Avoid using jargon or technical terms that the model may not be familiar with. The prompts should be matched to the specific task that the model is trying to solve. If the prompts are not matched to the task, then the model may not be able to generate the correct answer. The more complex the task, the more sensitive the model may be to the design of the prompts.</p><p>The performance of few-shot CoT deteriorated when the prompt example question types and task question types were unmatched. This suggests that few-shot CoT is highly sensitive to the design of the prompts and that the prompts need to be carefully matched to the specific task to achieve good performance.</p><h3 id="coherence">Coherence</h3><p><a href="https://aclanthology.org/2023.acl-long.153.pdf">Coherence</a> refers to the extent to which the steps of a CoT rationale are in the correct order. This means that later steps should not be preconditions for earlier steps, and earlier steps should not be based on later steps. For example, a rationale where "32 + 42 = 74" appears before the introduction of "32" or "42", would not have coherence. This is because the equation "32 + 42 = 74" is a later step that depends on the earlier steps of introducing the numbers "32" and "42."</p><p>The researchers designed a set of ablation settings to examine the impact of coherence on different components of a CoT-like rationale. Ablation settings are a way of testing the importance of different parts of a system by removing them and observing the impact on the system's performance. It was found that coherence was important for all components of a CoT-like rationale. When coherence was removed, the performance of the system deteriorated.</p><figure class="w-richtext-align-center w-richtext-figure-type-image" style="max-width:828pxpx"><div><img alt="" loading="lazy" src="https://blog-cdn.mercity.ai/blog/guide-to-chain-of-thought-prompting/68c4a7429d734a87bb467ad8_53d6eee7.jpg"/></div></figure><p>The researchers also found that the coherence of language templates is particularly important for the performance of CoT prompting. Language templates are the phrases that are used to connect the different steps of a CoT rationale. If the language templates are not coherent, then the model may not be able to understand the rationale and generate the correct answer.</p><h2 id="types-of-chain-of-thought-prompting">Types of Chain-of-Thought Prompting</h2><p>Within the realm of chain-of-thought (CoT) prompting, two notable variations emerge as impactful strategies: multimodal CoT and least to most prompting. Let us explore these techniques in detail.</p><h3 id="multi-modal-cot">Multi-modal CoT</h3><p>Traditional CoT focuses on the language modality, which means that it only uses text to provide the model with a context for reasoning. <a href="https://arxiv.org/pdf/2302.00923.pdf">Multimodal CoT</a> incorporates text and vision into a two-stage framework. The first step involves rationale generation based on multimodal information. This means that the model is provided with both text and images, and it is then asked to generate a rationale that explains how the text and images are related. The second phase of the framework is answer inference. This is where the model uses the informative rationale that it generated in the first step to infer the correct answer to the question.</p><figure class="w-richtext-align-center w-richtext-figure-type-image" style="max-width:999pxpx"><div><img alt="" loading="lazy" src="https://blog-cdn.mercity.ai/blog/guide-to-chain-of-thought-prompting/68c4a75bd66134431539f318_c06330ee.jpg"/></div></figure><p>1B multimodal CoT outperforms GPT-3.5 by 16 percentage points (75.17% to 91.68% accuracy) and surpasses human performance on the ScienceQA benchmark. Among the 8 question classes, our model improved performance from 67.43% to 88.80% for questions with paired images. Methods such as UnifiedQA and GPT-3.5, use image captions to understand what the image shows, however, using image features was more effective. Future studies could improve CoT reasoning by using better image features, adding common sense knowledge, and filtering out irrelevant information.</p><figure class="w-richtext-align-center w-richtext-figure-type-image" style="max-width:961pxpx"><div><img alt="" loading="lazy" src="https://blog-cdn.mercity.ai/blog/guide-to-chain-of-thought-prompting/68c4a75bd66134431539f31b_93c3ddb5.jpg"/></div></figure><p>The only thing that you might need to consider when also using images is computational cost, as using CoT with images will increase the number of tokens your model is consuming AND the number of tokens the model ends up generating. You can check out this <a href="https://arxiv.org/abs/2503.12605">detailed survey</a> if you want to learn more about applications of CoT with Image Inputs.</p><p>Another very interesting and important parallel to CoT for images is how it can be used to focus on different parts of the images. You can simply prompt for it, but if your requirements are more advanced, we suggest checking out this paper: <a href="https://arxiv.org/pdf/2411.16044v4">ZoomEye</a>. This method enhances Multimodal LLMs by using tree or chain-based zooming-in methods and gives amazing results. Worth checking out if you are strongly dependent on CoT for your image based prompts.</p><h3 id="least-to-most-prompting">Least-to-Most Prompting</h3><p>Chain-of-thought prompting is a powerful technique for natural language reasoning, but it can struggle with tasks that require solving problems that are harder than the examples shown in the prompts. To address this challenge, we propose a novel prompting strategy called least-to-most prompting.</p><p><a href="https://arxiv.org/pdf/2205.10625.pdf">Least-to-most prompting</a> works by breaking down a complex problem into a series of simpler subproblems, and then solving them in sequence. Each subproblem is facilitated by the answers to the previous subproblems. For example, to solve a math word problem, we might first query the language model to decompose the problem into subproblems, such as "What is the cost of the first item?" and "What is the total cost?" We would then query the language model to sequentially solve the subproblems, using the answers to the previous subproblems to inform our queries.</p><figure class="w-richtext-align-center w-richtext-figure-type-image" style="max-width:826pxpx"><div><img alt="" loading="lazy" src="https://blog-cdn.mercity.ai/blog/guide-to-chain-of-thought-prompting/68c4a75cd66134431539f339_1081a457.jpg"/></div></figure><p>Least-to-most prompting generalizes to more difficult problems on symbolic manipulation, compositional generalization, and math reasoning tasks. GPT-3 code-davinci-002 with least-to-most prompting can solve SCAN with 99% accuracy using 14 exemplars, while chain-of-thought prompting only gets 16% accuracy. The table below shows the accuracies of different prompting methods on the subset of GSM8K and DROP benchmarks containing only numerical problems. The base language model is code-davinci-002.</p><figure class="w-richtext-align-center w-richtext-figure-type-image" style="max-width:658pxpx"><div><img alt="" loading="lazy" src="https://blog-cdn.mercity.ai/blog/guide-to-chain-of-thought-prompting/68c4a75cd66134431539f33c_474af1b7.jpg"/></div></figure><p>The table below shows the accuracies of different prompting methods on the last-letter-concatenation task.</p><figure class="w-richtext-align-center w-richtext-figure-type-image" style="max-width:598pxpx"><div><img alt="" loading="lazy" src="https://blog-cdn.mercity.ai/blog/guide-to-chain-of-thought-prompting/68c4a75cd66134431539f345_4a92f15d.jpg"/></div></figure><h2 id="auto-cot">Auto-CoT</h2><p><a href="https://arxiv.org/abs/2210.03493">Auto-CoT</a> is a way to automatically create demonstrations with questions and reasoning chains. It uses large language models to generate reasoning chains for each demonstration, using the prompt "Let's think step by step." Auto-CoT has two main steps. First, it partitions the questions in a given dataset into a few clusters. Then, it selects a representative question from each group and uses Zero-Shot-CoT with simple heuristics to generate a reasoning chain. The diversity of the demonstration questions is important for reducing the number of mistakes that Zero-Shot-CoT makes in the reasoning chain. By clustering the questions into a few groups, Auto-CoT can ensure that each demonstration is representative of a different type of question. This helps to reduce the chances that Zero-Shot-CoT will make mistakes in the reasoning chain.</p><figure class="w-richtext-align-center w-richtext-figure-type-image" style="max-width:960pxpx"><div><img alt="" loading="lazy" src="https://blog-cdn.mercity.ai/blog/guide-to-chain-of-thought-prompting/68c4a75cd66134431539f33f_2083e010.jpg"/></div></figure><p><a href="https://github.com/amazon-science/auto-cot">Auto-CoT</a> was tested on 10 reasoning tasks, including arithmetic reasoning (MultiArith, GSM8K, AQUA-RAT, SVAMP), commonsense reasoning (CSQA, StrategyQA), and symbolic reasoning (Last Letter Concatenation, Coin Flip). Auto-CoT consistently matched or exceeded the performance of Manual-CoT in GPT-3.</p><figure class="w-richtext-align-center w-richtext-figure-type-image" style="max-width:927pxpx"><div><img alt="" loading="lazy" src="https://blog-cdn.mercity.ai/blog/guide-to-chain-of-thought-prompting/68c4a75cd66134431539f342_7ecb233e.jpg"/></div></figure><p>Here is a comparison of Auto-CoT with four baseline methods: Zero-Shot, Zero-Shot-CoT, Few-Shot, and Manual-CoT.</p><figure class="w-richtext-align-center w-richtext-figure-type-image" style="max-width:927pxpx"><div><img alt="" loading="lazy" src="https://blog-cdn.mercity.ai/blog/guide-to-chain-of-thought-prompting/68c4a75cd66134431539f342_7ecb233e.jpg"/></div></figure><h2 id="applications-of-cot">Applications of CoT</h2><p>Applications of CoT are in various domains, including arithmetic, commonsense, symbolic reasoning, natural language inference, and question answering. CoT prompts offer capabilities to LLMs to address complex problems across these areas.</p><h3 id="arithmetic-reasoning">Arithmetic Reasoning</h3><p>Chain of thought (CoT) prompting, when used with a 540B parameter language model, has comparable performance with task-specific fine tuned models on various tasks, including arithmetic reasoning. Solving math word problems is a challenging task for language models. To evaluate LLMs on the ability to solve math problems, two benchmarks, MultiArith and GSM8K, are used. Standard prompting shows relatively flat scaling curves for these benchmarks, meaning increasing model size does not substantially improve performance. However, when using CoT prompting, increasing model scale significantly improves performance, especially for large model sizes.</p><figure class="w-richtext-align-center w-richtext-figure-type-image" style="max-width:804pxpx"><div><img alt="" loading="lazy" src="https://blog-cdn.mercity.ai/blog/guide-to-chain-of-thought-prompting/68c4a7d56a829551a9eccbd4_80b4f84d.jpg"/></div></figure><p>PaLM, a 540B parameter language model, combined with CoT prompting, achieves a state-of-the-art performance of 58% on the GSM8K benchmark. Self-consistency techniques further improve CoT prompting performance, reaching 74% accuracy on GSM8K. CoT prompting results in a state of the art in math word problem-solving, surpassing fine-tuned GPT-3 baselines.</p><figure class="w-richtext-align-center w-richtext-figure-type-image" style="max-width:1176pxpx"><div><img alt="" loading="lazy" src="https://blog-cdn.mercity.ai/blog/guide-to-chain-of-thought-prompting/68c4a7d56a829551a9eccbda_500c0b5b.jpg"/></div></figure><h3 id="commonsense-reasoning">Commonsense Reasoning</h3><p>Chain-of-thought prompting can also be used for commonsense reasoning tasks. Such tasks require reasoning about physical and human interactions based on general knowledge. Commonsense reasoning is challenging for current natural language understanding systems. CoT prompting is evaluated on commonsense reasoning benchmarks such as CommonsenseQA, StrategyQA, date understanding, and sports understanding. Performance on these tasks generally improves with an increase in model size. CoT prompting provides small improvements over it. CoT prompting is most effective in improving performance on sports understanding tasks.</p><p>PaLM 540B with CoT outperformed an unaided sports enthusiast with a score of 95% vs. 84% and the prior state-of-the-art on StrategyQA with a score of 75.6% vs. 69.4% and sports understanding with 95.4% vs. 84%. But, minimal improvement is seen in CommonsenseQA (CSQA).</p><figure class="w-richtext-align-center w-richtext-figure-type-image" style="max-width:924pxpx"><div><img alt="" loading="lazy" src="https://blog-cdn.mercity.ai/blog/guide-to-chain-of-thought-prompting/68c4a7d56a829551a9eccbd1_a076e14f.jpg"/></div></figure><h3 id="symbolic-reasoning">Symbolic reasoning</h3><p>Chain-of-thought prompting enables language models to perform symbolic reasoning tasks that are difficult with standard prompting. It also supports length generalization, allowing models to handle inference-time inputs longer than those seen in few-shot exemplars. During research, to test CoT prompting, two toy tasks were used for evaluation. The first was the last letter concatenation, where the model concatenates the last letters of words in a name. And the second was coin flip, where the model determines if a coin remains heads up after people flip it or not.</p><p>In-domain and out-of-domain test sets were used to evaluate the performance of PaLM 540B with chain-of-thought prompting (CoT) and standard prompting on these two tasks. For in-domain evaluations, the examples had the same number of steps as the training/few-shot exemplars. For out-of-domain evaluations, the examples had more steps than those in the exemplars.</p><figure class="w-richtext-align-center w-richtext-figure-type-image" style="max-width:324pxpx"><div><img alt="" loading="lazy" src="https://blog-cdn.mercity.ai/blog/guide-to-chain-of-thought-prompting/68c4a7d56a829551a9eccbd7_45ca8d61.jpg"/></div></figure><p>PaLM 540B with CoT achieved almost 100% solve rates for in-domain evaluations. Standard prompting failed for both tasks in both in-domain and out-of-domain evaluations. CoT prompting resulted in improved performance, but it was lower than in in-domain evaluations.</p><h3 id="question-answering">Question Answering</h3><p>CoT prompting improves question answering (QA) by decomposing complex questions or prompts into a sequence of simpler, logical steps. This approach helps the language model understand the structure of the question and the relationships between its components. Each step focuses on a specific aspect of the question, helping the model to identify relevant information more effectively. CoT encourages the model to perform multi-hop reasoning, where it iteratively gathers and combines information from different sources or documents. This enables the model to perform improved inference and connect separate pieces of knowledge to arrive at an accurate answer. By explicitly specifying reasoning steps, CoT prompts can help prevent common errors or biases that language models might introduce when answering complex questions. Additionally, CoT prompts allows users to understand how the model arrived at a particular response.</p><figure class="w-richtext-align-center w-richtext-figure-type-image" style="max-width:631pxpx"><div><img alt="" loading="lazy" src="https://blog-cdn.mercity.ai/blog/guide-to-chain-of-thought-prompting/68c4a7d56a829551a9eccbe5_30133278.jpg"/></div></figure><h2 id="advanced-cot-variants">Advanced CoT Variants</h2><p>After the core CoT technique, many sophisticated variants improving upon it have come out over the years. Some optimize for faster problem solving, some for more accurate and more in-depth. Let’s look at a few of these advanced Chain of Thought prompting techniques that can be used to boost accuracy and speed, too!</p><p>In our experience, these are very strong and niche techniques that you should explore only when dealing with very specific issues. CoT and simply running the same problem through multiple different types of prompts often solves many issues. It is recommended to explore the low hanging fruits before diving deeper into these sophisticated techniques.</p><h3 id="tree-of-thoughts">Tree of Thoughts</h3><p>CoT follows a linear approach where each new word or idea is linked directly to the one before it, forming a chain. It represents a sequential thought organization. <a href="https://arxiv.org/abs/2305.08291">Tree of Thoughts (ToT)</a>, on the other hand, adopts a hierarchical approach. Ideas are organized into a tree-like structure, with each idea branching off into multiple related ideas. It represents a more complex and branching thought organization.</p><figure class="w-richtext-align-center w-richtext-figure-type-image" style="max-width:918pxpx"><div><img alt="" loading="lazy" src="https://blog-cdn.mercity.ai/blog/guide-to-chain-of-thought-prompting/68c4a7d56a829551a9eccbe8_dd9f16ae.jpg"/></div></figure><p>CoT models, like GPT-3, are generally good at generating coherent and contextually relevant text over short spans. ToT models, such as Transformer models, are often better at maintaining coherence over longer texts and can keep track of multiple related ideas at once. CoT models are simpler in structure and are computationally less intensive compared to ToT models because of the latter’s hierarchical nature. Also, ToT introduces the concept of a "ToT Controller" trained through reinforcement learning (RL). This controller can potentially learn from new data or self-play, allowing the ToT system to evolve and acquire new knowledge even with a fixed language model.</p><p>CoT-SC (Self-consistency with a Chain of Thoughts) uses a simple prompting technique. It doesn't explicitly mention the use of search algorithms. ToT employs search algorithms like breadth-first search (BFS) and depth-first search (DFS) to enable systematic exploration of thoughts. It uses these algorithms in conjunction with the tree structure for problem-solving. Hence, ToT outperforms other methods significantly.</p><figure class="w-richtext-align-center w-richtext-figure-type-image" style="max-width:964pxpx"><div><img alt="" loading="lazy" src="https://blog-cdn.mercity.ai/blog/guide-to-chain-of-thought-prompting/68c4a7d56a829551a9eccbde_62a08931.jpg"/></div></figure><p>You can choose CoT for simpler, shorter texts, and ToT can be more appropriate for complex, longer texts and problem-solving tasks.</p><h3 id="graph-of-thoughts">Graph of Thoughts</h3><figure class="w-richtext-align-center w-richtext-figure-type-image" style="max-width:1600pxpx"><div><img alt="" loading="lazy" src="https://blog-cdn.mercity.ai/blog/guide-to-chain-of-thought-prompting/68c4a81f77d8b9e652c31502_b1ef4946.jpg"/></div></figure><p><a href="https://arxiv.org/abs/2308.09687">Graph of Thoughts</a> goes beyond ToT's tree structure by letting LLM thoughts connect in a web instead of a tree. In ToT, thoughts can only branch out from a single parent (like a family tree). GoT allows thoughts to have multiple parents, meaning several different reasoning paths can combine into one new thought. This lets the LLM cover more ground quickly and perform more sophisticated reasoning operations by following different paths. GoT works through three main operations.</p><ul role="list"><li><strong>Aggregation:</strong> Multiple thoughts merge into one (k→1 transformation)</li></ul><ul role="list"><li><strong>Generation:</strong> Single thought branches into multiple (1→k transformation)</li></ul><ul role="list"><li><strong>Refinement:</strong> Thought loops back on itself for iterative improvement (1→1 with self-edge)</li></ul><p>These operations are controlled by a Graph of Operations (GoO), which is basically a recipe that tells the system what operations to perform and in what order.</p><figure class="w-richtext-align-center w-richtext-figure-type-image" style="max-width:1600pxpx"><div><img alt="" loading="lazy" src="https://blog-cdn.mercity.ai/blog/guide-to-chain-of-thought-prompting/68c4a81f77d8b9e652c314ff_af108bcc.jpg"/></div></figure><p>For sorting 128 numbers, <strong>GoT reduces errors by 62% compared to ToT while costing 31% less</strong>. It achieves this by breaking the list into smaller chunks, sorting each chunk separately, then merging them back together level by level, like organizing papers by first sorting them into piles, then combining those piles systematically. The whole process takes about 55 steps across 7 merging levels. In set intersection tasks (finding common elements between sets), GoT gets perfect accuracy while ToT makes 4 or more errors.</p><p>The key technical advantage is the latency-volume tradeoff. Volume means how many previous thoughts contributed to the final answer. GoT keeps latency at O(log k N) while maintaining volume N, meaning all intermediate thoughts can potentially influence the final result. ToT only achieves O(log k N) for both, limiting the amount of information that flows to the final answer. But both algorithms are highly parallel and are fine in terms of how much time they take to run in our experience.</p><p>GoT works best for problems that naturally split into parts that need to be solved separately, then combined. However, GoT adds unnecessary complexity for straightforward problems that don't need multiple paths or merging. It also costs more than simpler methods due to multiple LLM calls. One good application of GoT we can identify is Contract evaluation and analysis, especially for complex ones. We have worked with these, and we tend to run multiple prompts to analyze the contract from different aspects like technical, sales, IP, etc. That is good, but GoT can do that out of the box without multiple prompts and sophisticated architectures.</p><h3 id="layer-of-thoughts">Layer of Thoughts</h3><figure class="w-richtext-align-center w-richtext-figure-type-image" style="max-width:1494pxpx"><div><img alt="" loading="lazy" src="https://blog-cdn.mercity.ai/blog/guide-to-chain-of-thought-prompting/68c4a82077d8b9e652c3152b_cf8da515.jpg"/></div></figure><p>LoT builds on Graph of Thoughts by organizing the reasoning process into distinct layers. Instead of letting thoughts connect freely like in GoT, LoT structures them hierarchically. Thoughts in layer 1 must be completed before layer 2 begins, and layer 2's output feeds into layer 3. Each layer contains two types of thoughts: layer thoughts that manage the overall process for that level, and option thoughts that explore different solutions within that layer. Each layer acts as a filter with specific criteria. Layer 1 might filter by basic keywords, layer 2 by semantic meaning, and layer 3 by final validation.</p><p>LoT uses several aggregation methods to combine results within a layer. The "all" metric requires documents to pass every criterion (like requiring both a password AND fingerprint). The "at-least-k" metric is more flexible—documents need to pass at least k criteria out of the total. The "locally-better" metric keeps documents that aren't beaten on every criterion by another document. The "max-count" metric simply counts how many criteria each document passes and ranks them accordingly.</p><figure class="w-richtext-align-center w-richtext-figure-type-image" style="max-width:1600pxpx"><div><img alt="" loading="lazy" src="https://blog-cdn.mercity.ai/blog/guide-to-chain-of-thought-prompting/68c4a82077d8b9e652c31506_8545694c.jpg"/></div></figure><p>In testing on Japanese Civil Law retrieval, LoT achieved an F2 score of 0.835, beating the best systems from COLIEE 2024 competition. It maintained precision at 0.838 while keeping recall at 0.839—a balance that simpler approaches couldn't achieve. When they removed the semantic filtering layer, recall jumped to 0.885 but precision crashed to 0.432. One extreme case had the LLM marking 400 articles as relevant when only 2 actually were. The layered filtering prevented these false positives.</p><p>LoT works best when you have clear filtering criteria that can be arranged from broad to specific. Legal document retrieval is perfect first filter by relevant keywords, then by legal concepts, then confirm relevance to the specific query. It struggles when criteria don't have a natural hierarchy or when the problem needs thoughts to influence each other across layers (which GoT handles better through arbitrary connections).</p><h2 id="cot-vs-other-methods">CoT vs. Other Methods</h2><p>In this section, we go into a detailed comparison of CoT prompting with other methods, specifically Standard and Tree of Thought Prompting. Evaluating their strengths and limitations offers valuable insights into selecting the most suitable approach for your business applications.</p><h3 id="cot-vs-standard-prompting">CoT vs. Standard Prompting</h3><p>Standard Prompting uses input-output pairs as examples. The pairs are formatted as questions and answers. The model predicts answers based on these pairs. It is limited in handling multi-step reasoning tasks effectively. But is suitable for straightforward tasks, such as single-turn questions. It demands fewer computational resources. It commonly uses single-shot prompts for training and tends to require more data to fine-tune for complex tasks. Standard prompting may not exhibit significant performance improvements with the model scale.</p><p><a href="https://github.com/FranxYao/chain-of-thought-hub">CoT prompting</a> involves generating intermediate reasoning steps. These steps precede providing a final answer. It excels at complex reasoning, enabling models to think step by step. It is versatile and applicable to a wide range of tasks requiring intricate reasoning. It requires training on sequences of prompts and efficiently utilizes data for multi-step reasoning. It demonstrates enhanced performance with larger models and, thus, requires more computational power. It excels in complex reasoning benchmarks and tasks that demand multi-step problem-solving.</p><p><strong>Comparison on MAWPS Benchmark:</strong></p><figure class="w-richtext-align-center w-richtext-figure-type-image" style="max-width:636pxpx"><div><img alt="" loading="lazy" src="https://blog-cdn.mercity.ai/blog/guide-to-chain-of-thought-prompting/68c4a82077d8b9e652c3150c_cbb81dd8.jpg"/></div></figure><p><strong>Comparison on Length Generalization Task:</strong></p><figure class="w-richtext-align-center w-richtext-figure-type-image" style="max-width:781pxpx"><div><img alt="" loading="lazy" src="https://blog-cdn.mercity.ai/blog/guide-to-chain-of-thought-prompting/68c4a82077d8b9e652c3150f_26414907.jpg"/></div></figure><p>You can choose Standard Prompting for straightforward tasks and CoT Prompting, as the superior choice, for applications requiring deep, multi-step reasoning and interpretability. An open-source repository of data and tools related to CoT reasoning is available on <a href="https://github.com/OpenBioLink/ThoughtSource">GitHub</a>. It has datasets on various tasks such as math problems and common sense reasoning. It also has a community forum for discussions.</p><h3 id="cot-vs-react-prompting">CoT vs. ReAct Prompting</h3><p>CoT forces the model to generate an internal, sequential reasoning trace before providing a final answer. <a href="https://arxiv.org/abs/2210.03629">ReAct</a>, on the other hand, synergizes reasoning and acting by generating interleaved reasoning traces and task-specific actions.</p><p>The fundamental difference lies in their interaction with information. CoT operates as a static chain, where it thinks a lot and then gives an output. This makes it highly susceptible to fact hallucination and error propagation, as its reasoning is not grounded. ReAct works differently. The <strong>Action</strong> step allows the LLM to interface with external tools. The subsequent <strong>Observation</strong> grounds the model's next <strong>Thought</strong>, allowing it to create, maintain, and adjust plans based on real-world feedback.</p><p>CoT excels at tasks requiring purely internal, sequential logic with a fixed context, such as multi-step arithmetic. Its linear structure is efficient for problems that do not require external validation.</p><figure class="w-richtext-align-center w-richtext-figure-type-image" style="max-width:1600pxpx"><div><img alt="" loading="lazy" src="https://blog-cdn.mercity.ai/blog/guide-to-chain-of-thought-prompting/68c4a82077d8b9e652c31518_f9f2809f.jpg"/></div></figure><p>ReAct is designed for dynamic tasks where the environment is interactive or information is incomplete. On knowledge-intensive tasks like HotpotQA, ReAct overcomes CoT's hallucination issues. On decision-making benchmarks like ALFWorld, ReAct outperforms imitation learning methods by an absolute success rate of 34% with only one or two in-context examples, demonstrating its ability to handle long-horizon tasks.</p><p><em>Use CoT for self-contained problems solvable with internal knowledge and linear logic. Use ReAct for dynamic tasks that require grounding in external information, tool use, or interaction with an environment.</em></p><p>You can read our <a href="https://www.mercity.ai/blog-post/react-prompting-and-react-based-agentic-systems">Extended Report on React prompting</a> to learn more about it.</p><h3 id="cot-vs-reflexion">CoT vs Reflexion</h3><figure class="w-richtext-align-center w-richtext-figure-type-image" style="max-width:1600pxpx"><div><img alt="" loading="lazy" src="https://blog-cdn.mercity.ai/blog/guide-to-chain-of-thought-prompting/68c4a82077d8b9e652c31512_1a0c45f5.jpg"/></div></figure><p>Chain of Thought is a static, single-turn reasoning process. <a href="https://arxiv.org/abs/2303.11366">Reflexion</a>, on the other hand, is a dynamic, multi-trial framework for agent improvement through verbal reinforcement. While CoT generates a linear sequence of thoughts to solve a problem in one attempt, Reflexion uses a learning loop. This loop involves three models: an Actor that generates actions, an Evaluator that scores the outcome, and a Self-Reflection model that converts task feedback into natural language summaries.</p><p>The main improvement is the learning mechanism. CoT has no way to learn from failure, a wrong answer in one attempt does not inform the next. Reflexion uses failures by storing feedback in a memory buffer, reinforcing the agent for upcoming trials. This converts environmental feedback scalar values, compiler errors, etc.</p><figure class="w-richtext-align-center w-richtext-figure-type-image" style="max-width:1600pxpx"><div><img alt="" loading="lazy" src="https://blog-cdn.mercity.ai/blog/guide-to-chain-of-thought-prompting/68c4a82077d8b9e652c31515_e5959e48.jpg"/></div></figure><p>Reflexion significantly outperforms static approaches on tasks requiring iterative refinement. On the HumanEval coding benchmark, a Reflexion agent achieves 91% pass@1 accuracy, surpassing a strong GPT-4 baseline (80%). For the AlfWorld sequential decision-making task, it improves success rates by an absolute 22% over a ReAct baseline across 12 trials. On the HotPotQA reasoning task, it boosts accuracy by 20% by learning from initial failures. <em>An ablation study shows that simply providing episodic memory of past trajectories is less effective than the full self-reflection loop, demonstrating that focused, verbal reinforcement is the key contributor.</em></p><p><strong>It is also very important to note that Reflexion can simply be combined with CoT or ReACT to boost the performance of your preexisting prompts.</strong></p><p>Use CoT for problems where a correct line of reasoning can be generated in a single pass. Reflexion is designed for complex tasks that benefit from trial-and-error, allowing an agent to systematically learn from its mistakes across multiple attempts in coding, reasoning, or decision-making.</p><h2 id="want-to-write-high-quality-production-grade-prompts-for-your-llms">Want to write high-quality production-grade prompts for your LLMs?</h2><p>If you are looking to leverage prompt engineering for your business needs, we can help. We are a team of AI engineers with extensive experience in prompt engineering techniques like Chain-of-Thought, ReAct, etc. <a href="https://www.mercity.ai/contacts">Contact us</a> today and let us apply CoT applications to elevate your business.</p></div>
