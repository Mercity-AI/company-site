---
title: "In-Depth Guide to Visual Language Models"
slug: advanced-guide-to-visual-language-models
publishedAt: "2024-06-02"
createdAt: "2024-06-02"
updatedAt: "2024-06-02"
summary: "In this blog we explain how Visual Language Models work from scratch, we explain CLIP, image embeddings, and necessary topics. We also explain how to train a visual language model from scratch too."
authors:
  - name: "Mathavan"
category: "Visual Language Models"
image: "https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/665cc28f93bb7a343ed2a36e_vlm-structure.png"
---

<blockquote><em>❗ To learn how to use Visual Language Models for Medical applications, checkout our blog on </em><a href="https://www.mercity.ai/blog-post/building-medical-ai-assistants-with-visual-llms"><em>Building Medical AI assistants with Visual LLMs</em></a></blockquote><p>The evolution of Visual Large Language Models (VLLMs) began with the Transformer architecture introduced in the “<a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a>” paper by Vaswani et al., which revolutionized natural language processing by enabling efficient self-attention mechanisms. Building on this, OpenAI's CLIP model combined visual and textual data, demonstrating strong zero-shot learning capabilities. Models like <a href="https://arxiv.org/abs/1908.02265">VilBERT</a> and <a href="https://arxiv.org/abs/1908.03557">VisualBERT</a> extended this by integrating visual and linguistic inputs, improving multimodal interactions. Recent advancements include <a href="https://llava-vl.github.io">LLaVA</a> and <a href="https://huggingface.co/docs/transformers/en/model_doc/siglip">SigCLIP</a>, which enhance visual grounding and fine-grained visual concept recognition. Techniques like fine-grained reward modeling in the <a href="https://github.com/Jeff-Zilence/VIGOR">ViGoR</a> framework and benchmarks such as <a href="https://arxiv.org/abs/2402.13607">CODIS</a> further enhance VLLMs' real-world applicability. This blog will explore these developments in detail, highlighting their impact and potential applications.</p><h2 id="foundational-concepts">Foundational Concepts</h2><h3 id="convolutional-neural-networkcnn">Convolutional Neural Network(CNN)</h3><p><a href="https://arxiv.org/abs/1511.08458">Convolutional Neural Networks (CNNs)</a> have revolutionized image processing and computer vision tasks. They are specifically designed to process and interpret visual data by mimicking the way humans perceive images. A CNN consists of multiple layers, including convolutional layers, pooling layers, and fully connected layers. The convolutional layers apply filters to the input image to create feature maps that highlight various aspects of the image, such as edges, textures, and shapes. Pooling layers then reduce the spatial dimensions of these feature maps, retaining the most important information while reducing computational complexity. Fully connected layers at the end of the network combine these features to make predictions.</p><figure class="w-richtext-figure-type-image w-richtext-align-center" style="max-width:1280px" data-rt-type="image" data-rt-align="center" data-rt-max-width="1280px"><div><img src="https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/665cbfcf56e78026235fa7e5_uecVWNK5mbUypp7OgvGjndP4r7Fe9VwVKBbUbYe-tL7lnEp1KvhE6cgdOGAx4tZ_aVTXP7wV9T0XgdbznNbMIo8dw9PQ6fpL0aNFlB_4PquM6boIMBZvmJeqIzpigAqRe2Tc9H5FDv8AXcNYDXu-dJg.png" width="auto" height="auto" alt="" loading="auto"></div></figure><h3 id="vision-transformers-vits">Vision Transformers (ViTs)</h3><p><a href="https://arxiv.org/abs/2010.11929">Vision Transformers</a> are a recent innovation in image processing that adapt the transformer architecture, originally designed for natural language processing, to handle visual data. Unlike CNNs, which process images through local receptive fields, ViTs divide an image into fixed-size patches and treat each patch as a token, similar to words in a sentence. These tokens are then processed by a standard transformer encoder, which captures global context through self-attention mechanisms. ViTs have shown state-of-the-art performance on various image classification tasks, particularly when trained on large datasets.</p><figure class="w-richtext-figure-type-image w-richtext-align-center" style="max-width:1600px" data-rt-type="image" data-rt-align="center" data-rt-max-width="1600px"><div><img src="https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/665cbfd09e06cf0c12932956_6esuU4q3cR_XdfjzrOeYWqpGDH0bJbuoz1v3MrO1MzTVj_bO_SOI_ulC9ci9zQve1tBnrkoQ6lxinpm-sunwvGrkI0aIRFWlLwwi3xzvoVYYPrnljqjWyYfOvfnEMHvLa9I_ssHlpBQZSpjT_NBt1MQ.png" width="auto" height="auto" alt="" loading="auto"></div></figure><h4 id="advantage-of-using-vision-transformer-vits-over-cnn">Advantage of using Vision Transformer (ViT’s)&nbsp; over&nbsp; CNN</h4><p>Choosing Vision Transformer (ViT) over Convolutional Neural Networks (CNNs) for image processing tasks depends on the specific requirements and characteristics of the application. ViT is advantageous due to its ability to capture long-range dependencies and contextual information across the entire image through self-attention mechanisms, which are often more challenging for CNNs that rely on local receptive fields. This capability allows ViT to excel in tasks requiring a holistic understanding of the image, such as image classification, object detection, and segmentation. Additionally, ViT has shown impressive performance on large datasets and benefits significantly from extensive pre-training on vast amounts of data.</p><p>However, CNNs remain highly efficient for many real-time applications due to their optimized convolution operations, which are well-suited for current hardware accelerators like GPUs. CNNs' hierarchical feature extraction is particularly effective for tasks involving spatial hierarchies and local patterns, making them ideal for applications such as real-time video processing, edge computing, and mobile devices where computational efficiency and lower latency are critical.</p><h2 id="technical-concepts">Technical Concepts</h2><p>Before we get into the heart of working we need to understand the technical terms and concepts in the Visual Large Language model architecture. Let’s understand why these layers or concepts are important and what is their role in the architecture.</p><h3 id="multi-modal-embeddings">&nbsp;Multi-Modal Embeddings</h3><figure class="w-richtext-figure-type-image w-richtext-align-center" style="max-width:1316px" data-rt-type="image" data-rt-align="center" data-rt-max-width="1316px"><div><img src="https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/665cbfd1a1eedf962921dd10_DIPZUktNSudg08kNKgluQBkVWW0vWIM982zRGjNIMwy8HS4k1cv5uJwqPKxtI4WKDM3AJ0Reyl6PnU7dFNKSwXC-o5i6smd5cDYe5Q7lRzPGYayM0N_INFgT3k-dzSSq3d6D0WyHrB6UJKbxdfuxQqs.png" width="auto" height="auto" alt="" loading="auto"></div></figure><p>Multi-modal embeddings involve representing data from different modalities (e.g., text and images) in a unified embedding space. This allows the model to understand and relate information from multiple modalities simultaneously, leading to more accurate and efficient information retrieval and analysis across various domains. An implementation of this concept is the <a href="https://arxiv.org/pdf/2305.05665">ImageBind model</a>, which maps diverse data types like text, images, audio, and even sensor data into a single embedding space. This unified approach enhances the model's ability to perform tasks that require cross-modal understanding and integration, making it particularly powerful for applications such as cross-modal search, multimodal content generation, and comprehensive data analysis.</p><h4 id="techniques-for-aligning-visual-and-textual-data">Techniques for Aligning Visual and Textual Data</h4><h5 id="joint-embedding-space">Joint Embedding Space</h5><p>Techniques like CLIP (Contrastive Language-Image Pre-training) project both visual and textual data into a common embedding space. This is achieved by training the model on pairs of images and corresponding text descriptions. Joint embedding space is a more efficient and flexible option compared to a cross-modal transformer for several reasons. First, it provides a unified representation where both visual and textual data are projected into a common space, enabling straightforward comparison and retrieval tasks. This approach simplifies the architecture and reduces computational complexity since it avoids the need for separate, intricate attention mechanisms for each modality as required in cross-modal transformers. Additionally, joint embedding spaces are highly effective in scenarios like image-text matching, where the goal is to find correspondences between different types of data. They facilitate quick and efficient retrieval of relevant information by leveraging learned associations in a shared latent space. This can lead to faster inference times and lower resource consumption, making joint embedding spaces more suitable for real-time applications and large-scale deployments.</p><figure class="w-richtext-figure-type-image w-richtext-align-center" style="max-width:600px" data-rt-type="image" data-rt-align="center" data-rt-max-width="600px"><div><img src="https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/665cbfcf0e6a31f1efa4942c_k3sAwapi6GobVCbYUMWvSdGkf6iGPD2FARDRx9h_o2sGa-YXPoPcb5bdeHwNE5oyJiooS5MqbIkacAq9VrEU5ToYy7GlhGk1kBOtKC6Itl_b8f5Vm0luhU3T1bQD-V2Uss731jzAKvx2I2tOQEUSVuU.png" width="auto" height="auto" alt="" loading="auto"></div></figure><h5 id="cross-modal-transformers">Cross-Modal Transformers</h5><p>Models such as <a href="https://arxiv.org/abs/1908.02265">ViLBERT</a> and <a href="https://arxiv.org/abs/1908.07490">LXMERT</a> use separate encoders for each modality and align the representations using transformer layers that allow for cross-modal interactions. The model consists of two parallel streams for visual (green) and linguistic (purple) processing that interact through novel co-attentional transformer layers. This structure allows for variable depths for each modality and enables sparse interaction through co-attention. Dashed boxes with multiplier subscripts denote repeated blocks of layers.</p><figure class="w-richtext-figure-type-image w-richtext-align-center" style="max-width:1600px" data-rt-type="image" data-rt-align="center" data-rt-max-width="1600px"><div><img src="https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/665cbfd09d6820883ee0b09e_mI4Tq_to7X0hL0RzqsZHa1AlGEAJsFtSJNkjHSiLEnVv4fh4YSuxn-2OpeP7lt8tdkUonUHvyMzOXkt51TZivgj87J0pKevcL0gWTVvqK-4StbIfdB195c88DMHzO9lM-UpP3IGU5Yi2odi44ThvoMg.png" width="auto" height="auto" alt="" loading="auto"></div></figure><h4 id="why-embedding-space-is-considered-important">Why embedding space is considered important&nbsp;</h4><p>Embedding spaces enable VLLMs to perform zero-shot learning&nbsp; where the model recognizes and categorizes new, unseen data based on its relationship to known data. It also improves cross-modal retrieval&nbsp; and facilitates fine-grained understanding where retrieving&nbsp; relevant information across different modalities, enhancing applications like image captioning and visual question answering and enables nuanced and detailed understanding of visual concepts through fine-grained embeddings.</p><h3 id="attention-mechanisms-in-vllms">Attention Mechanisms in VLLMs</h3><h4 id="self-attention">Self-Attention</h4><p>Allows the model to weigh the importance of different parts of a single input (e.g., different words in a sentence or different regions in an image) relative to each other. This mechanism helps in capturing long-range dependencies and contextual relationships within the same modality. In transformer-based models like BERT and Vision Transformers (ViT), self-attention helps in understanding the contextual relevance of different tokens (words or image patches).</p><p>The self-attention mechanism in Transformers allows the model to dynamically determine the relevance of each word in a sentence by assigning attention scores, which are learned during training. Each word in the input sequence is first converted into an embedding and combined with positional encoding to incorporate information about the word's position. The self-attention mechanism then computes three vectors for each word: Query, Key, and Value. The attention score between each pair of words is calculated using the dot product of their Query and Key vectors, normalized via a softmax function to produce attention weights.</p><figure class="w-richtext-figure-type-image w-richtext-align-center" style="max-width:437px" data-rt-type="image" data-rt-align="center" data-rt-max-width="437px"><div><img src="https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/665cbfcf653bd153c34df20c_KM8h8TYwBydLgfng4qXTAWiqxtaFumQGtnCWVa_2JNTaaRRi3D2AS2E1tJgU3yHi0nmcWqx5mH2G5FNpp5NMVQtiCfK3gU_8wmf_zghhOpaqFhEAmYPax-OhDh0oNpYBcq40FqkSZ5-gyR6fOYIPwao.png" width="auto" height="auto" alt="" loading="auto"></div></figure><p>These weights dictate the focus each word should have on other words in the sequence. Multiple attention heads are used to capture different aspects of the relationships between words. The refined representations produced by these heads are then passed through several layers, allowing the model to understand complex word relationships and contexts better. For example, in the sentence "The animal didn't cross the street because it was too tired," the self-attention mechanism would likely assign higher attention scores between "it" and "animal" rather than "it" and "street," based on their contextual relevance. This iterative refinement through multiple layers enables the model to perform tasks like coreference resolution effectively, focusing more on contextually relevant words to minimize the loss and produce accurate outputs.</p><h4 id="cross-attention">Cross-Attention</h4><p>Enables the model to align and integrate information from two different modalities. It helps in establishing correspondences between visual features and textual descriptions. In models like ViLBERT, cross-attention layers allow for the fusion of visual and textual information, enabling the model to generate coherent and contextually relevant outputs.</p><p>In cross-attention, the Query (Q) vectors are derived from one modality, such as text, while the Key (K) and Value (V) vectors come from another modality, such as images. For the sentence "The animal didn't cross the street because it was too tired," the word "it" would generate a Query vector from its textual embedding. The image associated with the sentence is divided into regions, each represented by feature vectors serving as the Key and Value vectors. The cross-attention mechanism calculates attention scores by taking the dot product of the Query vector (text) and the Key vectors (image regions), which is then scaled and passed through a softmax function to produce normalized attention weights.&nbsp;</p><figure class="w-richtext-figure-type-image w-richtext-align-center" style="max-width:650px" data-rt-type="image" data-rt-align="center" data-rt-max-width="650px"><div><img src="https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/665cbfcfe7c709241becf3a2_lKxg_oMwXFMg-1YJTqO1HgDg3Lc3MXjePajR-sTuhbbyVbISutmGGbeoiLGjD459FHeUWQQK1rKHqNpRl_YIbJ7ui6AwdSz4GuQFDQ58caRLB_0LtC8kWEiRXXe7osQk4STiog60ZLEWvXqIvydkLsg.png" width="auto" height="auto" alt="" loading="auto"></div></figure><p>These weights indicate the relevance of each image region to the word "it." The Value vectors are then weighted by these attention scores and summed, producing a context vector that integrates relevant visual features into the textual context. This process helps the model correctly associate "it" with "animal" rather than "street" by focusing on the visual regions related to the animal. The output from the cross-attention block is a combined representation that enhances the model's understanding of the sentence by incorporating both visual and textual information, allowing for better performance on tasks like visual question answering and image captioning.</p><h3 id="contrastive-learning">Contrastive Learning</h3><figure class="w-richtext-figure-type-image w-richtext-align-center" style="max-width:2085px" data-rt-type="image" data-rt-align="center" data-rt-max-width="2085px"><div><img src="https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/665cc0ce653bd153c34eada8_Screenshot%202024-06-03%20002746.png" loading="lazy" alt="__wf_reserved_inherit" width="auto" height="auto"></div></figure><p>Contrastive learning is a self-supervised learning technique that focuses on learning representations by distinguishing between similar (positive) and dissimilar (negative) pairs. This method is particularly effective in scenarios where it is crucial to differentiate between closely related data points. In models like CLIP (Contrastive Language-Image Pre-training) developed by OpenAI, contrastive learning is applied to large datasets of image-text pairs. CLIP uses separate encoders for images and text, projecting both into a shared embedding space. The training objective, driven by a contrastive loss function, aims to maximize the similarity between the embeddings of positive pairs (correct image-text pairs) while minimizing the similarity between negative pairs (incorrect image-text pairs). This approach enables the model to perform well on various tasks without task-specific fine-tuning, demonstrating strong zero-shot learning capabilities in the image below compared to other state of the art deep learning models in different datasets.</p><figure class="w-richtext-figure-type-image w-richtext-align-center" style="max-width:1484px" data-rt-type="image" data-rt-align="center" data-rt-max-width="1484px"><div><img src="https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/665cbfcf7e1168ddfa0871e9_9ujum91yM3KFngnnJw3KdZClBBnPMbFEdaTyOEke8lxTO8Bd4XiGc2gLZBqEHyV0FTJJ4qvJQi8xbEzRVzJZz-9mMBn4iLbizSh_9i2zbmO4PkWZPPR5m0juanmOjWB1mFQ1p8VKqfYPxpXUQDjY4vM.png" width="auto" height="auto" alt="" loading="auto"></div></figure><p>The benefits of contrastive learning in VLLMs include the development of robust and generalizable representations, the ability to handle zero-shot learning scenarios, and scalability with large datasets. However, it also presents challenges such as the need for efficient negative sampling, high computational costs, and the reliance on high-quality data. The implementation of contrastive learning in CLIP, as detailed in the paper "Learning Transferable Visual Models From Natural Language Supervision" by Alec Radford et al., highlights these strengths and challenges, showcasing the potential and limitations of this powerful technique. For more detailed insights, refer to the <a href="https://arxiv.org/abs/2103.00020">original CLIP paper</a>.</p><h2 id="how-does-a-visual-large-language-model-work">How does a Visual Large Language Model Work</h2><figure class="w-richtext-figure-type-image w-richtext-align-center" style="max-width:1226px" data-rt-type="image" data-rt-align="center" data-rt-max-width="1226px"><div><img src="https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/665cbfcfa41944613ae685a0_i9pMvG_1xkmpQ7hJRxhf8ME5aUOPl6V7M5FRl6l6d78d4m5BmmtLBwh3R1zC_w-hLxRcbabZOQPPr3QOy1d3rILxOOeU3TExqJdtZZEgTWRfA1jOSiSA3glvOA23hv4fKLmrAAJ1VJio6CxPIJ8DNL8.png" width="auto" height="auto" alt="" loading="auto"></div></figure><h3 id="backbone">Backbone</h3><p>The Backbone in a Visual Large Language Model (VLLM) is the fundamental neural network tasked with extracting essential features from input images. Typically, this backbone is a sophisticated convolutional neural network (CNN) like ResNet or a Vision Transformer (ViT). These networks process the image, converting it into a high-dimensional tensor of visual features (Fv), which encapsulates critical spatial and semantic details. These extracted features form the basis for subsequent stages, facilitating the model's ability to interpret and manipulate the visual data effectively for diverse tasks.</p><h3 id="language-guided-image-tokenizer">Language-Guided Image Tokenizer</h3><p>The Language-Guided Image Tokenizer&nbsp; is crucial for integrating visual and textual information within the VLLM framework. This component operates by initially receiving visual features (Fv) from the Backbone and textual features ( Ft ) from a text encoder, often a Transformer model like BERT. Using a cross-attention mechanism, it aligns and combines these modalities, producing language-guided image tokens ( T ). These tokens are enriched with both visual and contextual data, enabling the model to understand and respond accurately to the tasks specified by the accompanying language instructions.</p><p><br></p><h3 id="random-query">Random Query</h3><p>The Random Query component represents the VLLM's capability to handle a wide array of tasks flexibly. This feature allows the model to process various vision-only and vision-language tasks dynamically. By introducing randomness, the model can adapt to different inputs and instructions, showcasing its robustness and versatility in generating appropriate outputs. This adaptability is key to the model's performance across diverse applications, enabling it to handle novel and unexpected scenarios effectively.</p><h3 id="language-instructions-lttextgt">Language Instructions (\&lt;text\&gt;)</h3><p>Language Instructions are the natural language prompts that guide the VLLM on what specific tasks to perform. These instructions provide detailed descriptions of the tasks, such as "Describe the image &lt;image&gt; in detail" for vision-language tasks or "For each object in the image &lt;image&gt; that belongs to the class set &lt;class&gt;, output a tuple with the class label and coordinates" for vision-only tasks. The instructions are parsed into a machine-readable format, directing the model on how to interpret the visual data and generate the required outputs.</p><h3 id="open-ended-task-decoder-with-llm">Open-Ended Task Decoder with LLM</h3><p>The Open-Ended Task Decoder with LLM&nbsp; is the component that interprets the language-guided image tokens ( T ) and generates the final output based on the provided instructions. This decoder utilizes the capabilities of large language models (LLMs) like GPT to process integrated tokens and leverage its extensive language understanding to produce meaningful results. Whether classifying tokens for object detection or generating sequences for tasks like image captioning, this decoder can adapt its outputs to the specified formats, ensuring flexibility and accuracy in addressing a variety of vision-centric tasks.</p><h3 id="desired-output">Desired Output</h3><p>The Desired Output is the end result produced by the VLLM, tailored to the task defined by the language instructions. This output can take various forms depending on the task, such as class labels and bounding box coordinates for object detection, descriptive text for image captioning, or text-based answers for visual question answering. The ability to generate such a wide range of outputs demonstrates the VLLM's versatility and effectiveness in integrating and processing both visual and textual information to meet diverse application needs.</p><h2 id="how-to-train-a-visual-llm">How to Train a Visual LLM</h2><p>Meta released an amazing <a href="https://arxiv.org/pdf/2405.17247v1">guide</a> on training Visual Language Models.</p><p>Training a Vision-Language Large Model (VLLM) involves several crucial steps to ensure the model effectively associates textual descriptions with visual elements (grounding) while managing computational resources efficiently. Grounding can be enhanced by using bounding box annotations to teach the model where objects are located in the images, employing contrastive learning techniques with negative captioning to distinguish between correct and incorrect text-image pairs, and ensuring high-quality, diverse datasets. Optimizing data quality by pruning low-quality or duplicate entries and improving caption quality with synthetic data generation techniques are also essential steps.</p><p>Managing GPU resources is critical due to the significant computational requirements for training VLLMs. High-quality datasets reduce the need for extensive compute power, and efficient training techniques like masking and optimized data loading can speed up the process. Leveraging pre-trained models for fine-tuning instead of training from scratch can also help manage costs. Libraries like <a href="https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html">torch.compile</a> and <a href="https://github.com/facebookresearch/xformers">xformers</a> optimize attention mechanisms, while Fast Forward Computer Vision (FFCV) helps in creating faster-loading data files.</p><figure class="w-richtext-figure-type-image w-richtext-align-center" data-rt-type="image" data-rt-align="center"><div><img src="https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/665cbfcfe9e4579125284116_PRhWeD29HWsslKnnF7dH-fw4p91dDFn9Svpax1a2xXpxS2wcMaF0lh0he7j8vMH8_EIC2kqGDoxBRQAZuRoC3lrvBturtjMV0jjWq5RIVWH7V8jWtH3qh8Bjt7RH-FzCOSeOuiJ-8Xak_lbbVoxbCIM.png" width="auto" height="auto" alt="" loading="auto"></div></figure><p>Important considerations to keep in mind when training VLMs. Data is one of the most important aspects of training VLMs. Having a diverse and balanced dataset is important for learning good world models that can span enough concepts. It is also important to remove duplicates which occur a lot within large-scale datasets, this will save a lot of compute time and mitigate the risks of memorization. In addition, pruning the data is also an important component since we want to be sure that the captions are indeed related to the image content. Lastly, improving the caption quality is crucial to enhance VLMs performance. Grounding VLMs is another important step to ensure that the VLMs correctly associate words with specific concepts. Two common grounding methods leverage either bounding boxes or negative captions. Lastly, alignment is a much-needed step to ensure that the model is producing answers that are expected from a human point of view.</p><h3 id="how-many-gpus-are-required-for-training">How many GPU’s are required for Training</h3><p>The compute resources required for training a VLLM significantly influence the budget needed for such projects. Models like CLIP and OpenCLIP have utilized more than 500 GPUs, which equates to costs in the hundreds of thousands of dollars—often inaccessible for most companies or academic labs. However, by using high-quality datasets and leveraging efficient techniques like masking strategies, training a contrastive model like CLIP on hundreds of millions of images from scratch can be done with as few as 64 GPUs, costing around $10K in compute. If the VLM leverages existing pre-trained image or text encoders, or LLMs, the cost of learning a mapping should be much lower.</p><h3 id="steps-for-training-vllm">Steps for Training VLLM</h3><figure class="w-richtext-figure-type-image w-richtext-align-center" style="max-width:2167px" data-rt-type="image" data-rt-align="center" data-rt-max-width="2167px"><div><img src="https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/665cc1410e6d0daa66457198_Screenshot%202024-06-03%20002937.png" loading="lazy" alt="__wf_reserved_inherit" width="auto" height="auto"></div></figure><h4 id="data-preparation">Data Preparation</h4><p>Collect and preprocess a diverse set of image-text pairs, ensuring that the dataset is both extensive and varied to cover a wide range of concepts. This includes ensuring high-quality captions using synthetic data generation techniques if necessary. Removing duplicates and low-quality samples from the dataset is crucial to save computational resources and prevent the model from memorizing redundant information, which can degrade its performance and efficiency. Proper data curation and preparation form the foundation for successful VLLM training.</p><h4 id="model-architecture">Model Architecture:&nbsp;</h4><p>Choose an appropriate model architecture based on the specific requirements of your task, whether it is contrastive, masking, or generative models. For effective grounding, consider models that leverage bounding boxes to explicitly indicate object locations or those that use negative samples to teach the model to distinguish between correct and incorrect text-image pairs. The choice of architecture should align with the end goals of the VLLM, such as image retrieval, caption generation, or both.</p><h4 id="training-process">Training Process:&nbsp;</h4><p>Implement contrastive learning techniques to align text and image representations effectively. This involves training the model to push the representations of matching image-text pairs closer together while pushing non-matching pairs further apart. Additionally, implement masking strategies to improve training efficiency and model performance by randomly masking parts of the input data and training the model to predict the masked content. Fine-tune pre-trained models to reduce computational costs, leveraging existing knowledge to expedite the training process and achieve better initial performance.</p><h4 id="optimization-techniques">Optimization Techniques:&nbsp;</h4><p>Apply efficient attention mechanisms and data loading optimizations to ensure that the training process is as fast and effective as possible. Utilize libraries like <a href="https://pytorch.org/docs/stable/generated/torch.compile.html">torch.compile</a> and <a href="https://huggingface.co/docs/diffusers/en/optimization/xformers">xformers</a>, which offer significant speed improvements for model training. Regularly evaluate the model's performance and adjust hyperparameters as needed to ensure optimal results. Optimizing these aspects can greatly reduce the overall training time and computational costs while maintaining or improving model performance.</p><h4 id="fine-tuning-and-evaluation">Fine-Tuning and Evaluation:</h4><p>Fine-tune the model on specific downstream tasks to ensure it performs well in practical applications. This involves adjusting the model parameters based on specific task requirements, such as image classification, caption generation, or retrieval tasks. Evaluate the model using benchmarks like zero-shot and retrieval tasks to ensure it generalizes well across different scenarios. Regular performance evaluations help in identifying and addressing potential issues early, ensuring the model is robust and reliable for real-world applications.</p><h3 id="improving-grounding">Improving Grounding</h3><p>Grounding in a Visual Large Language Model (VLLM) refers to the process of associating textual descriptions with specific visual elements within an image. This involves identifying and linking parts of the text to corresponding objects or regions in the visual data, enabling the model to understand and interpret images in the context of the provided language. Grounding helps the model make accurate predictions and generate relevant outputs by ensuring that the visual and textual components are meaningfully connected, enhancing tasks like object detection, image captioning, and visual question answering.</p><figure class="w-richtext-figure-type-image w-richtext-align-center" style="max-width:977px" data-rt-type="image" data-rt-align="center" data-rt-max-width="977px"><div><img src="https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/665cbfd0826846a47adbd810_-rDUZbSkRx9XgGVPWK-jyksbgMlnuElDh8ff-dpoIuLIag6NarB8BkVnukpM-oDTHBAOKUZCOcIj6ucGE-hK0rwla6pP7rmDysNyYoAz-iC4mRN1zmOI_OOhyE8vFibyiE_lCR9QPdsmnp38svlZKrA.png" width="auto" height="auto" alt="" loading="auto"></div></figure><p>Grounding is a significant challenge in the VLM and generative model literature. It primarily addresses the issue of models not fully understanding text prompts, which can result in ignoring parts of the prompt or generating hallucinated content not present in the prompt. Challenges in grounding include understanding spatial relations (e.g., left or right of an object), handling negations, counting, and recognizing attributes like colors or textures. Although no single method can completely solve grounding issues, several techniques can improve grounding performance.You can also try out the model in this <a href="https://huggingface.co/spaces/merve/Grounding_DINO_demo">link</a>.</p><h4 id="using-bounding-box-annotations">Using Bounding Box Annotations:</h4><p>Models like X-VLM leverage bounding box annotations, incorporating box regression and Intersection over Union (IoU) loss to accurately locate and align visual concepts with their corresponding textual descriptions. By knowing where objects are in images and the associated captions, the model can better associate text with visual clues, improving grounding. X-VLM is trained on datasets like COCO, Visual Genome, SBU, and Conceptual Captions, using up to 16 million images. This extensive training data with bounding box annotations enables X-VLM to excel in tasks like image-text retrieval, visual reasoning, visual grounding, and image captioning.</p><h4 id="negative-captioning">Negative Captioning:</h4><p>Contrastive objectives use negative samples to mitigate collapse, enhance generalization, and improve discriminative feature learning. By contrasting positive pairs (similar or related samples) with negative pairs (dissimilar or unrelated samples), models develop a nuanced understanding of data, grasping underlying patterns that distinguish different classes or categories. Recent works have shown that using negative samples can mitigate various problems in VLMs. For instance, the ARO benchmark evaluates VLMs on their ability to correctly associate images with captions, using negative samples to test the model's understanding of incorrect pairings. This approach has shown that VLMs significantly benefit from the differentiation capabilities fostered by exposure to negative samples, leading to more accurate and contextually aware models.&nbsp;</p><h2 id="recent-research-in-visual-llm">Recent Research in Visual LLM&nbsp;</h2><h3 id="llama-3-v">LLama 3-v</h3><figure class="w-richtext-figure-type-image w-richtext-align-center" style="max-width:1600px" data-rt-type="image" data-rt-align="center" data-rt-max-width="1600px"><div><img src="https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/665cbfcfee118657b7e00285_lTceOhxHHRjkNwHVB-Rbr0DaySb744RDHdWoKZzJlWsA0fusdPswlBJdl3IjU9eUINE8uqbGG_pePPul2NZ5yteCY4Amu4GtoeREAqZh5l1ZL7lDyV0eTFFt5nYKChh37qcnCn4Q3GVd_MtFYOqlD2o.png" width="auto" height="auto" alt="" loading="auto"></div></figure><p><a href="https://github.com/mustafaaljadery/llama3v">Llama 3-V</a> leverages the SigLIP model to embed visual information, distinguishing itself by employing a pairwise sigmoid loss instead of a contrastive loss. Input images are transformed into patch embeddings, which are aligned with textual tokens via a projection block using self-attention mechanisms. This joint representation, combining visual and textual tokens, is processed through Llama3. Unlike models such as Llava that utilize a single linear layer for image embeddings, Llama 3-V's dual self-attention blocks capture intricate patterns in the data, enabling superior multimodal understanding and performance on various benchmarks. This architecture is particularly optimized for cost-effective training and inference, maintaining high performance with significantly lower computational resources.</p><figure class="w-richtext-figure-type-image w-richtext-align-center" style="max-width:1085px" data-rt-type="image" data-rt-align="center" data-rt-max-width="1085px"><div><img src="https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/665cbfcf254938203309c68a_T2C9Ha5xLRmSWkqibQHftRPzOGrZqyti_K7i6stCnyG2Uz8sdVHqx3LLG7oclaqe9ahjgPXItedELdyN3gNoWRqTnSX3dc2LWBZ_tQKIwmJkk5OWAKrLz-7s1eop_sVar2o2w9cE5R1hBk8plK56_KE.png" width="auto" height="auto" alt="" loading="auto"></div></figure><h3 id="visual-bert">Visual-Bert</h3><figure class="w-richtext-figure-type-image w-richtext-align-center" style="max-width:1600px" data-rt-type="image" data-rt-align="center" data-rt-max-width="1600px"><div><img src="https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/665cbfd037e731458becfdba_qlE25By-veWC2QAVdaqZ4Q8K3Nz41wyhX8tSKVFZ_dOB5uinCcLH8yiY0463LUEyFM39APpRaFz6WptA4f2pnsfFvsaG0GF4W0Loi46w2qq4ms03Ap_63I0kipE9WM8ta1F02958atS3KtECpt-Inrg.png" width="auto" height="auto" alt="" loading="auto"></div></figure><p><a href="https://arxiv.org/pdf/1908.03557">VisualBERT</a> processes image regions, extracted using an object detector, and treats these regions as visual tokens alongside text tokens. Both text and visual features are embedded into a shared space using token, segment, and position embeddings. These embeddings are then passed through multiple Transformer layers, allowing self-attention mechanisms to align elements of text with corresponding image regions. The model is pre-trained with masked language modeling, predicting masked words using visual context, and sentence-image prediction, determining if a text matches an image. This joint processing enables VisualBERT to capture rich interactions between visual and textual data, making it effective for tasks like Visual Question Answering and Visual Commonsense Reasoning. Unlike CLIP, which uses separate encoders for images and text aligned through contrastive learning, VisualBERT uses a single Transformer for richer interaction and grounding of visual and textual information.</p><h3 id="llava">LLava</h3><figure class="w-richtext-figure-type-image w-richtext-align-center" style="max-width:1188px" data-rt-type="image" data-rt-align="center" data-rt-max-width="1188px"><div><img src="https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/665cbfcf2207d5ba4c1e79f7_ZgTyfnIri9CL6T79PPyxmjwEALdoBGnxuSVSuOpfwHPbOoDGNNiuOaQM0-fHpzG2eQrilMM4_PxqST-dr0mIRxUOAYd8vUa7OD0HHRQBrPdh1LT_KuSkITViAFPyrPqFyEnEWf1BfkWu-AZYmaoaCjk.png" width="auto" height="auto" alt="" loading="auto"></div></figure><p><a href="https://arxiv.org/pdf/2304.08485">The LLaVA (Large Language and Vision Assistant) model</a> combines a pre-trained language model (Vicuna) with a visual encoder (CLIP's ViT-L/14). The visual encoder processes an input image to generate visual features, which are then projected into the language embedding space using a trainable linear layer. These visual tokens are combined with language instruction tokens and fed into the language model to generate responses. Unlike CLIP, which aligns visual and textual representations using contrastive learning, LLaVA directly integrates visual features into the language model's embedding space for end-to-end vision-language tasks.</p><h3 id="idefics-2">Idefics 2</h3><p>The <a href="https://blog.paperspace.com/idefics2/">Idefics 2</a> model integrates visual and language processing to generate contextually informed text responses. The architecture comprises three main components: a Vision Encoder, a Vision-Language Connector, and an LLM (Large Language Model) Decoder. The Vision Encoder processes input images to extract high-dimensional visual features. These features are then transformed by the Vision-Language Connector into language embedding tokens that align with the LLM’s word embedding space. This transformation allows the visual data to be seamlessly integrated with textual information. The LLM Decoder takes these integrated tokens, along with any language instructions, to generate coherent text responses.</p><figure class="w-richtext-figure-type-image w-richtext-align-center" style="max-width:498px" data-rt-type="image" data-rt-align="center" data-rt-max-width="498px"><div><img src="https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/665cbfd0bfdc2a2b6bd3cd46_-Fz7f0uRlibbfefXuL8wZIt22E0vtEH4oo196UxcX-bQSoIBedox9MBl4Xr48VW4UzYuClhTvGwqAsTO3UqIOY832YTx0Qz4jqLLUJm4WE8nbdzAN8rnfsJ4P1zL-KlESxl8rZD5Fr4d1A0f9kilcAk.png" width="auto" height="auto" alt="" loading="auto"></div></figure><p>This approach differs from models like CLIP, which uses separate encoders for images and text to align their representations in a shared embedding space primarily for retrieval tasks. In contrast, Idefics 2 focuses on generating language responses informed by visual data, leveraging a direct transformation layer to bridge the visual and textual modalities effectively. This enables tasks such as describing images, answering visual questions, and generating narratives based on visual inputs.</p><figure class="w-richtext-figure-type-image w-richtext-align-center" style="max-width:1878px" data-rt-type="image" data-rt-align="center" data-rt-max-width="1878px"><div><img src="https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/665cc25573fcce1bda73c2f9_Screenshot%202024-06-03%20003356.png" loading="lazy" alt="__wf_reserved_inherit" width="auto" height="auto"></div></figure><p>Idefics 2 is an 8 billion parameter vision-language model designed to excel in Optical Character Recognition (OCR) and document data extraction, such as reading bills and invoices. It outperforms larger models with its efficient architecture, supporting image resolutions up to 980 × 980 pixels. The model has been pre-trained on over 6TB of OCR data, enhancing its ability to accurately extract text from images. This makes it particularly effective for automating data entry and managing document workflows, thanks to its improved visual reasoning and document understanding capabilities. You can also try out the model in this <a href="https://huggingface.co/spaces/HuggingFaceM4/idefics-8b?ref=blog.paperspace.com">link</a>.</p><h2 id="benchmarking-and-evaluation-of-vision-language-large-models-vllms">Benchmarking and Evaluation of Vision-Language Large Models (VLLMs)</h2><h3 id="common-datasets">Common Datasets</h3><p>Standard benchmarks for evaluating Vision-Language Large Models (VLLMs) often utilize widely recognized datasets such as COCO (Common Objects in Context) and Visual Genome.&nbsp;</p><p>COCO dataset includes over 200,000 labeled images with annotations for object detection, segmentation, and captioning. It is extensively used for evaluating image captioning, object detection, and segmentation tasks. The dataset is detailed in "<a href="https://arxiv.org/abs/1405.0312">Microsoft COCO: Common Objects in Contex</a>t" by Lin et al. (2014) .</p><p>Visual Genome&nbsp; dataset contains over 100,000 images with dense annotations of objects, attributes, and relationships, making it suitable for tasks requiring detailed scene understanding. "<a href="https://arxiv.org/abs/1602.07332">Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations</a>" by Krishna et al. (2017) describes this dataset .</p><p>The main evaluation metrics are accuracy, f1 score, exact match.Accuracy measures the proportion of correct predictions made by the model and is commonly used in classification tasks. F1 Score is the harmonic mean of precision and recall, providing a balance between the two metrics, making it particularly useful for imbalanced datasets. Exact Match (EM) Score measures the percentage of predictions that match the ground truth exactly, often used in tasks like question answering and retrieval.</p><h3 id="recent-benchmarks">Recent Benchmarks</h3><p>CODIS evaluates a model's ability to disambiguate images based on contextual information. This benchmark assesses how well models can understand and interpret images in context, rather than in isolation. "<a href="https://arxiv.org/html/2402.13607v1">CODIS: A Benchmark for Context-Dependent Image Disambiguation</a>" by Peng et al. (2022) provides a comprehensive overview of this benchmark .</p><p>Fine-Grained Visual Concept Recognition benchmark involves recognizing detailed and specific visual concepts within images, often requiring models to differentiate between subtle differences. It tests the model's ability to understand fine-grained details and nuances in visual data. Relevant research includes "<a href="https://arxiv.org/abs/2111.06119">Fine-Grained Recognition: A Survey</a>" by Wei et al. (2019) .</p><p>By leveraging these benchmarks and evaluation metrics, researchers can systematically assess the performance of VLLMs, identify areas for improvement, and ensure that models are robust and effective across a variety of tasks and datasets.</p><h2 id="transform-your-visual-data-with-custom-vllms">Transform Your Visual Data with Custom VLLMs</h2><p>If you're looking to leverage the power of Visual Large Language Models (VLLMs) for your business or research needs, <a href="http://mercity.ai">Mercity.ai</a> can help you build a custom VLLM tailored to your specific requirements. Whether it's enhancing image classification, improving visual question answering, or integrating sophisticated visual and textual data analysis into your applications, our team of experts is here to assist you every step of the way. Contact Mercity.ai today to learn how we can transform your vision into reality with cutting-edge VLLM technology. <a href="https://www.mercity.ai/contacts">Contact us</a> now!</p>
