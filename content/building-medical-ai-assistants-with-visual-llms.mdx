---
title: "Building Medical AI assistants with Visual LLMs"
slug: building-medical-ai-assistants-with-visual-llms
publishedAt: "2024-05-17"
createdAt: "2024-05-17"
updatedAt: "2024-05-17"
summary: "LLMs for medical industry are very helpful, but medical images like CT scans, MRIs etc cannot be ignored. Hence we propose a pipeline with Visual LLMs which can process medical images and extract insights from them."
authors:
  - name: "Mathavan"
category: "AI in Medical Industry"
image: "https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/664760258cfa5fd5c1cdca4f_medical%20vllm.png"
---

<p>Medical image analysis has become an integral part of modern healthcare, enabling clinicians to make informed decisions and improve patient outcomes. However, accurately segmenting medical images presents several challenges, including the costly and time-consuming task of manually annotating datasets for training deep learning models. Additionally, while large language models (LLMs) have shown promise in various domains, they often lack the specialisation required for precise medical image analysis, leading to suboptimal performance in critical tasks like tumour detection.</p><p>Existing medical LLMs, such as <a href="https://sites.research.google/med-palm/">MedPaLM</a> and <a href="https://github.com/epfLLM/meditron">MediTron</a>, have demonstrated promising results in various medical domains. However, these models often lack the specialised architecture required for precise medical image segmentation. The proposed solution, which combines the strengths of U-Net and visual foundation models, has the potential to outperform existing medical LLMs in tasks like tumour detection, where precision is paramount.</p><p>In this article, we'll show you how to build a personalised Med VLLM for your particular use case and dataset.</p><h2 id="what-are-visual-large-language-models-vllms">What are Visual Large Language Models (VLLMs)?</h2><p>Visual Large Language Models (VLLMs) are a class of deep learning models that excel at processing and understanding visual data, such as images and videos. These models are trained on vast amounts of visual data and can extract high-level features and semantic information from images. VLLMs have shown promising results in various computer vision tasks, including image classification, object detection, and image segmentation. The approach of Visual LLMs are inspired from the <a href="https://github.com/RustamyF/clip-multimodal-ml">CLIP (Contrastive Language-Image Pretraining)</a> model developed by OpenAI in 2021.</p><figure class="w-richtext-figure-type-image w-richtext-align-center" style="max-width:1400px" data-rt-type="image" data-rt-align="center" data-rt-max-width="1400px"><div><img src="https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/66475ecfce5605b468a616bf_dKs_4hZNrfFTu_J_9lmfTOZPSDCeOB7zlJ4shOuTfviGCcT_INWcVjonUyV0ordRBxvtA8ouYKa2b7iCAUTzC9nvaFX-IPJ5i4MNeZ6og5pNyKWCyH0gJKJfJVbfwhfcn9ynb0Wg1m0H6diYxibq77w.png" width="auto" height="auto" alt="" loading="auto"></div></figure><p>In the field of medicine,Visual LLMs can show promising results in tasks like diagnostic imaging, surgical assistance. However, their effectiveness can be hindered by challenges in accurately interpreting medical images, especially without proper image segmentation. This limitation often results in decreased accuracy in critical tasks such as disease identification and surgical planning.Therefore, it led us to a solution from the U-Net Model.Let’s understand what U-net is and how well it can tackle the difficulties faced in Visual LLM approach in further below.</p><h2 id="what-is-u-net">What is U-NET ?</h2><p>The U-Net architecture is known for its effectiveness in semantic segmentation tasks in medical imaging analysis. In the context of tumour detection, the encoder component of U-Net acts as a feature extractor, capturing intricate details within the medical images that may indicate the presence of tumours. The decoder reconstructs the segmented regions based on these features, enabling precise delineation of tumour boundaries and shapes, thus aiding in accurate tumour detection and analysis.</p><p>The incorporation of skip connections in the U-Net architecture enhances image processing by facilitating the flow of high-resolution information between encoder and decoder layers, preserving fine-grained details and subtle features of tumours throughout the segmentation process.</p><figure class="w-richtext-figure-type-image w-richtext-align-center" style="max-width:1600px" data-rt-type="image" data-rt-align="center" data-rt-max-width="1600px"><div><img src="https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/66475ecfae56940bf7fa2557_wo9w12VFOTBAL36-rG_2j0HHlPib29a9UmFKWMKGWci557aI-QSREid5MOedkB2Qs3vOynCawnpd9FEXagHhLncb3O7UiJL6nYV8_gmQ5J6zDpUISaRQX5F9sDxqYo5FpR1Bes3PFaVmexLkZfwiNrs.png" width="auto" height="auto" alt="" loading="auto"></div></figure><h2 id="why-use-u-net-model-in-medical-imaging">Why use U-Net Model in Medical Imaging?</h2><p>Recent studies have shown that integrating VLLMs and U-Net can lead to significant improvements in medical image segmentation accuracy. For example, the Mamba-UNet architecture, which combines the strengths of U-Net with the long-range dependency modelling capabilities of the Mamba architecture, has achieved state-of-the-art results on the ACDC MRI Cardiac segmentation dataset and the Synapse CT Abdomen segmentation dataset. This architecture has been recognized for its ability to capture intricate details and broader semantic contexts within medical images, making it highly suitable for our proposed project.</p><p>The table below provides the exact numerical values, further demonstrating the superior performance of&nbsp; the U-Net architecture achieves higher accuracy compared to other models like DeepLabV3 across various metrics such as Dice Similarity Coefficient (DSC), F1-score, and Intersection over Union (IoU) from a <a href="https://www.researchgate.net/figure/Accuracy-comparison-of-the-proposed-BLSNet-model-with-UNet-and-DeepLabV3_tbl2_351396518">research paper</a>:</p><figure class="w-richtext-figure-type-image w-richtext-align-center" style="max-width:1108px" data-rt-type="image" data-rt-align="center" data-rt-max-width="1108px"><div><img src="https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/66475ecf770a7b1df1b225f2_OerEshZjakFDEp_EycCO5Lv-jOpZ0A4anj-nLN68gwZJoo1d9qVU-AmiHJcE-HmszmN4CR7PKFFTh-sP2NtlERpTPOwP7T5pR_YbPdc8huXyP6JJwcyyReyQRPhmpx_xKiWKXfkCzOp9seReOV3rNP8.png" width="auto" height="auto" alt="" loading="auto"></div></figure><p>By leveraging the power of VLLMs and U-Net, our project aims to develop an efficient and accurate medical image segmentation framework that addresses the challenges and limitations of existing approaches. The integration of these cutting-edge technologies, combined with the innovative approach, has the potential to revolutionise the field of medical image analysis and significantly improve patient outcomes.</p><h2 id="using-visual-large-language-models-vllms-in-medical-imaging-and-healthcare">Using Visual Large Language Models (VLLMs) in Medical Imaging and Healthcare</h2><p>Visual Large Language Models (VLLMs) have the potential to revolutionise medical imaging and healthcare by seamlessly integrating advanced language processing capabilities with visual data analysis. These cutting-edge AI models, such as <a href="https://arxiv.org/html/2303.00915v2">BiomedCLIP</a> and ChatGPT-4, <a href="https://sites.research.google/med-palm/">Med-Palm 2</a> ,<a href="https://www.semanticscholar.org/paper/Med-Flamingo%3A-a-Multimodal-Medical-Few-shot-Learner-Moor-Huang/c9dbdae8146b9f97e254f5d26fd6efde96eaa703">Med Flamingo</a> have demonstrated remarkable performance in various medical imaging tasks, including diagnostic analysis, image segmentation, and report generation. Let’s see how VLLMs can help doctors and patients.</p><h3 id="generating-comprehensive-personalised-medical-reports-with-radiologydata">Generating Comprehensive Personalised Medical Reports with RadiologyData</h3><p>One of the key advantages of VLLMs in medical imaging is their ability to process and understand multimodal data efficiently. These models can analyse a wide range of medical images, such as X-rays, CT scans, MRIs, and histopathological slides, and extract relevant information to aid in diagnosis and treatment planning which was significantly proved in the <a href="https://www.medrxiv.org/content/10.1101/2023.12.21.23300146v3.full">research paper</a>. By leveraging the power of language models, VLLMs can also incorporate patient history, medication records, and other relevant textual data to provide a more comprehensive and personalised assessment.The below graph taken from the research paper shows a comparison of human and LLM.In this paper they have used Flamingo-80B&nbsp; model where Flamingo-80B is quite less than the human accuracy where the approach of integrating&nbsp; with U-Net Model&nbsp; can eventually improve the existing metrics.</p><figure class="w-richtext-figure-type-image w-richtext-align-center" style="max-width:842px" data-rt-type="image" data-rt-align="center" data-rt-max-width="842px"><div><img src="https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/66475ecf3aedb1f3483fdf6c_ZVU640bauu_3knJRrrVLYOZYSY7-zo3gVPlO50W-Kn3wGjhao-eNX7cXI38SAiQYmsE2ee9flgBas3sSHnn7NsujMAHCeH5B3j7drlPjTWQ0jep5XwAO049GQLF_wo6h0JOFeTD_J2KQe0MEWf9NCwk.png" width="auto" height="auto" alt="" loading="auto"></div></figure><h3 id="more-efficient-and-faster-diagnostic-processes">More Efficient and Faster Diagnostic Processes</h3><p>VLLMs can significantly streamline diagnostic processes by enabling doctors to quickly analyse medical images and generate detailed reports. Using specific prompts, doctors can direct the model to focus on specific anatomical regions, structures, or tissues, allowing for targeted and efficient analysis. This interactive approach enhances the accuracy of image interpretation and reduces the time required for diagnosis, ultimately leading to faster treatment initiation and improved patient outcomes.</p><p>VLLMs can also assist doctors in differential diagnostics and creating a clinical plan for the patient. Because VLLMs can process a lot of medical history of a patient, they can be much better at informing the doctor of the current status of the patient and a bullet point summary of previous history of medications and health problems faced by the patient and then with a longer context understanding the doctor can proceed with next steps with suitable treatment.</p><h3 id="automating-medical-reporting-and-integrating-patient-history">Automating Medical Reporting and Integrating Patient History</h3><p>One of the most promising applications of VLLMs in medical imaging is the generation of comprehensive medical reports. These models can analyse a set of medical images, such as X-rays, CT scans, or MRIs, and generate detailed reports summarising the observed abnormalities, their locations, and potential implications for diagnosis or treatment. By automating the report generation process, VLLMs can significantly reduce the workload of radiologists and other medical professionals, allowing them to focus on more critical tasks.To read more about the specific work please read the <a href="https://arxiv.org/html/2403.02469v1">research paper</a>.</p><p>VLLMs can also incorporate patient history and medication records to provide a more holistic assessment of the patient's health. By combining visual data from medical images with textual information from patient records, VLLMs can identify potential correlations, flag potential drug interactions, and provide personalised treatment recommendations. This integration of multimodal data can lead to more accurate diagnoses and more effective treatment plans.</p><h2 id="how-to-build-a-custom-medical-visual-llm">How to build a Custom Medical Visual LLM</h2><figure class="w-richtext-figure-type-image w-richtext-align-center" style="max-width:1600px" data-rt-type="image" data-rt-align="center" data-rt-max-width="1600px"><div><img src="https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/66475ecfedcbfb5d7f77949e_1bD3mDmcTgaU9FbUA-oNEEx5wS9VDO2hJCKycfXkio1H66hrhJF4r0dt1eeD0FgNLZyP32hdPfP3GYOBFAukn65vaHSGcjgFIRPNEbOaM9WDuG-ilIEG7tUZNo5agIt15pw3R8-cpxFsfCuO1Jajv6I.png" width="auto" height="auto" alt="" loading="auto"></div></figure><h3 id="chat-interface">Chat-Interface</h3><p>The integration of a chat interface,allowing medical practitioners to easily interact with the system using both text and images. By leveraging the multimodal capabilities of large language models (LLMs), the approach can effectively process and analyse both textual queries and medical images, such as MRI scans.</p><p>When a medical practitioner obtains an MRI image, they can simply upload it to the chat interface. The approach then proceeds with the necessary preprocessing steps, including normalisation, augmentation, and noise reduction, to ensure optimal performance of the U-Net architecture. The chat interface provides a user-friendly and accessible way for doctors to interact with the system, enabling them to input specific prompts or questions related to the uploaded MRI image.&nbsp;</p><p>This interactive approach allows medical professionals to direct the model's focus to particular anatomical regions, structures, or tissues of interest, facilitating targeted and efficient analysis.</p><figure class="w-richtext-figure-type-image w-richtext-align-center" style="max-width:728px" data-rt-type="image" data-rt-align="center" data-rt-max-width="728px"><div><img src="https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/66475ecf4c2c12d45d83830a_BJpZkfgAE7g1Mp6bnRgIQbb2TRE1P-ZkmnP5uOitpluXyezLnNhXzuWBGr1apI_eIwMQTQUj4qWVsA0hPtKHVEg2MVUiqcKFZ-UUeVhSeAudmW3gaEeBpA3g7hiFdgBKRi-d8rzatWzoJ3Dl6BPfwaA.png" width="auto" height="auto" alt="" loading="auto"></div></figure><p>By combining the chat interface with the powerful capabilities of VLLMs and U-Net, The approach generates detailed medical reports that summarise the observed abnormalities, their locations, and potential implications for diagnosis or treatment. These reports are easily accessible through the chat interface, allowing doctors to quickly review the findings and make informed decisions about patient care.</p><h3 id="u-net-model">U-Net Model</h3><p>The U-Net architecture is particularly well-suited for the task of tumour segmentation in medical images. Its unique encoder-decoder structure and skip connections enable precise localization and accurate delineation of tumour regions.</p><figure class="w-richtext-figure-type-image w-richtext-align-center" style="max-width:685px" data-rt-type="image" data-rt-align="center" data-rt-max-width="685px"><div><img src="https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/66475ecfa84d888b44916f60_oD_3Xpke7xltjNOfn9o_L_tR82yVxi5sIkK2lm5fRe4iBRBT3hlkDa4kODlidNy9KRYko1HO-elIf5Aj1WKTrFQqKQffK9_4S-cTD91mWrkGljjVQ_qkybl9GfSK19RuOhVAbA_Fdc6-_Yb5yM8ZvLc.png" width="auto" height="auto" alt="" loading="auto"></div></figure><h4 id="encoder-capturing-tumour-context">Encoder: Capturing Tumour Context</h4><p>The encoder component of U-Net acts as a feature extractor, processing the input image and capturing relevant contextual information about the tumour. It utilises a series of convolutional and pooling layers to extract hierarchical features at different scales. These features include tumour texture, shape, location, and surrounding anatomical structures.</p><p>As the encoder progresses through the layers, it gradually reduces the spatial dimensions of the feature maps while increasing the number of feature channels. This allows the model to capture broader contextual information about the tumour and its relationship with the surrounding tissues.</p><h4 id="decoder-precise-tumour-localization">Decoder: Precise Tumour Localization</h4><p>The decoder part of U-Net is responsible for precisely localising the tumour region within the image. It takes the encoded features from the encoder and progressively samples them to restore the original spatial dimensions. At each decoding stage, the decoder combines the upsampled features with the corresponding high-resolution features from the encoder via skip connections.</p><p>This combination of upsampled features and skip-connected features enables the decoder to precisely localise the tumour boundaries and reconstruct the segmented tumour region. The decoder's ability to precisely localise the tumour is crucial for accurate tumour volume measurement, delineation for targeted therapy, and classification based on tumour appearance and location.</p><h4 id="skip-connections-preserving-tumour-details">Skip Connections: Preserving Tumour Details</h4><p>Skip connections play a vital role in preserving fine-grained tumour details throughout the U-Net architecture. These connections directly link the encoder and decoder layers, allowing the decoder to access high-resolution spatial information from the corresponding encoder layers.</p><p>By retaining these details, skip connections ensure that critical tumour characteristics, such as irregular shapes, heterogeneous textures, and subtle boundaries, are not lost during the encoding and decoding process. This preservation of tumour details is essential for accurate tumour segmentation and subsequent analysis.</p><p>The combination of the encoder's contextual feature extraction, the decoder's precise localization capabilities, and the skip connections' preservation of tumour details makes the U-Net architecture a powerful tool for tumour segmentation in medical imaging. By leveraging these strengths, researchers have developed highly accurate and robust tumour segmentation models that significantly aid in diagnosis, treatment planning, and monitoring.</p><h3 id="visual-language-model-vlm">Visual Language Model (VLM)</h3><figure class="w-richtext-figure-type-image w-richtext-align-center" style="max-width:1600px" data-rt-type="image" data-rt-align="center" data-rt-max-width="1600px"><div><img src="https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/66475ecf341fca86aaca9567_wrOXc7_erwNFhgNtxgk0UCOYhy4ZY-Km8ioGuyS5KLg3cqIRtcENfY1pxSnAZhgXzsSu4n9y4H1lZS7oOyEqOEmj_V1fWyBZRc72nnNBKIyNOznAh_6QAZiCvNtdPhorHERpy3SRdS5AWNKW9YuzYq8.png" width="auto" height="auto" alt="" loading="auto"></div></figure><p>In this multimodal model architecture, we are focusing on utilising the system for medical purposes, specifically for detecting tumours in MRI images.</p><p>The architecture begins with the vision encoder, which in this case is the CLIP ViT-L/336px model. This vision encoder processes the input MRI image, extracting high-level visual features. These features are then passed through the vision-language connector, a Multi-Layer Perceptron (MLP), which bridges the visual features with the language model.</p><p>The language model, represented by Vicuna v1.5 with 13 billion parameters, interprets these visual features in the context of medical knowledge. The tokenizer and embedding components convert the visual features into a numerical format that the language model can process effectively.</p><p>When an MRI image is input into the system, the vision encoder captures detailed information about the image, identifying potential areas of concern. The vision-language connector then translates these visual details into a form that the language model can understand and analyse.</p><p>Upon receiving a user query such as, "What is unusual about this MRI image?" The system utilises the combined capabilities of the vision encoder and language model to detect abnormalities, such as tumours. The language model analyses the encoded features and generates a detailed response, pinpointing the exact location and nature of the tumour within the MRI image. This integrated approach leverages the strengths of both visual and textual data processing, resulting in precise and efficient tumour detection in medical imaging.</p><h2 id="u-net-compared-totraditional-methods-in-terms-of-speed-and-accuracy">U-Net compared toTraditional Methods in terms of Speed and Accuracy</h2><p>Traditional medical approaches primarily focus on a person's disability, condition, and limitations, often overlooking their personal and psycho-social needs. These methods rely on symptom analysis and diagnostic tests to establish a diagnosis, aiming to address and treat underlying conditions. In contrast, traditional computer vision methods, such as those employing algorithms like SIFT, SURF, and BRIEF, rely on human-engineered feature extraction, which can be limited by the predefined features chosen for each image which would be really time consuming and time would be a barrier for a quick response to the medical situation.</p><p>The medical vision approach, particularly with the integration of U-Net, presents a prominent solution for medical image analysis. U-Net, with its encoder-decoder architecture and skip connections, excels in semantic segmentation tasks, allowing the model to capture intricate details within medical images. This U-Net-based methodology has demonstrated groundbreaking advancements in medical image analysis, offering improved performance indicators and structural characteristics.</p><h2 id="advancements-and-potential-of-vllms-in-medical-imaging">Advancements and Potential of VLLMs in Medical Imaging</h2><p>Research teams are actively exploring the use of LLMs in medical imaging. For instance, <a href="https://www.medrxiv.org/content/10.1101/2023.12.21.23300146v3.full">a study by Huang et al</a>. found that LLMs can outperform specialised, task-specific models in certain contexts. Another <a href="https://arxiv.org/html/2403.02469v1">study by Li et al</a>. introduced a cross-modal clinical graph transformer for ophthalmic report generation, showcasing the potential of VLLMs in specialised medical domains.</p><p>As research progresses, more sophisticated and specialised VLLMs tailored to specific medical imaging tasks will likely emerge. Collaborating with medical professionals ensures these models meet the unique needs and challenges of healthcare.</p><p>In summary, Visual Large Language Models can significantly enhance medical imaging by streamlining diagnostic processes, generating comprehensive reports, and integrating patient history and medication records. Despite existing challenges, the improved accuracy, efficiency, and personalised care offered by VLLMs make them a promising tool for the future of healthcare.</p><h2 id="ready-to-revolutionise-medical-imaging">Ready to Revolutionise Medical Imaging?</h2><p>Experience the future of medical image analysis with our advanced vision solutions. Let us guide you through the process of building a personalised medical visual LLM tailored to your specific use case and dataset. <a href="https://www.mercity.ai/contacts">Contact us </a>&nbsp;today to embark on your journey towards precise and efficient medical image analysis. We can help you transform the landscape of healthcare and improve patient outcomes.</p>
