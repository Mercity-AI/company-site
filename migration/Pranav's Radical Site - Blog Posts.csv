Name,Slug,Collection ID,Item ID,Archived,Draft,Created On,Updated On,Published On,Blog Post Content,Blog Post Cover,Author,Post category,Meta description,Featured?,scraped_content
In-Depth Guide to Visual Language Models,advanced-guide-to-visual-language-models,640f56f76d313b2faa631c11,665cc2db4d1e0817a5af1d16,False,False,Sun Jun 02 2024 19:07:07 GMT+0000 (Coordinated Universal Time),Sun Jun 02 2024 19:09:10 GMT+0000 (Coordinated Universal Time),Sun Jun 02 2024 19:09:10 GMT+0000 (Coordinated Universal Time),"<blockquote id=""""><em>❗ To learn how to use Visual Language Models for Medical applications, checkout our blog on </em><a href=""https://www.mercity.ai/blog-post/building-medical-ai-assistants-with-visual-llms"" id=""""><em>Building Medical AI assistants with Visual LLMs</em></a></blockquote><p id=""""><em>‍</em></p><p id="""">The evolution of Visual Large Language Models (VLLMs) began with the Transformer architecture introduced in the “<a href=""https://arxiv.org/abs/1706.03762"" id="""">Attention Is All You Need</a>” paper by Vaswani et al., which revolutionized natural language processing by enabling efficient self-attention mechanisms. Building on this, OpenAI's CLIP model combined visual and textual data, demonstrating strong zero-shot learning capabilities. Models like <a href=""https://arxiv.org/abs/1908.02265"" id="""">VilBERT</a> and <a href=""https://arxiv.org/abs/1908.03557"" id="""">VisualBERT</a> extended this by integrating visual and linguistic inputs, improving multimodal interactions. Recent advancements include <a href=""https://llava-vl.github.io"" id="""">LLaVA</a> and <a href=""https://huggingface.co/docs/transformers/en/model_doc/siglip"" id="""">SigCLIP</a>, which enhance visual grounding and fine-grained visual concept recognition. Techniques like fine-grained reward modeling in the <a href=""https://github.com/Jeff-Zilence/VIGOR"" id="""">ViGoR</a> framework and benchmarks such as <a href=""https://arxiv.org/abs/2402.13607"" id="""">CODIS</a> further enhance VLLMs' real-world applicability. This blog will explore these developments in detail, highlighting their impact and potential applications.</p><h2 id="""">Foundational Concepts</h2><h3 id="""">Convolutional Neural Network(CNN)</h3><p id="""">‍</p><p id=""""><a href=""https://arxiv.org/abs/1511.08458"" id="""">Convolutional Neural Networks (CNNs)</a> have revolutionized image processing and computer vision tasks. They are specifically designed to process and interpret visual data by mimicking the way humans perceive images. A CNN consists of multiple layers, including convolutional layers, pooling layers, and fully connected layers. The convolutional layers apply filters to the input image to create feature maps that highlight various aspects of the image, such as edges, textures, and shapes. Pooling layers then reduce the spatial dimensions of these feature maps, retaining the most important information while reducing computational complexity. Fully connected layers at the end of the network combine these features to make predictions.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1280px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1280px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/665cbfcf56e78026235fa7e5_uecVWNK5mbUypp7OgvGjndP4r7Fe9VwVKBbUbYe-tL7lnEp1KvhE6cgdOGAx4tZ_aVTXP7wV9T0XgdbznNbMIo8dw9PQ6fpL0aNFlB_4PquM6boIMBZvmJeqIzpigAqRe2Tc9H5FDv8AXcNYDXu-dJg.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><h3 id="""">Vision Transformers (ViTs)</h3><p id="""">‍</p><p id=""""><a href=""https://arxiv.org/abs/2010.11929"" id="""">Vision Transformers</a> are a recent innovation in image processing that adapt the transformer architecture, originally designed for natural language processing, to handle visual data. Unlike CNNs, which process images through local receptive fields, ViTs divide an image into fixed-size patches and treat each patch as a token, similar to words in a sentence. These tokens are then processed by a standard transformer encoder, which captures global context through self-attention mechanisms. ViTs have shown state-of-the-art performance on various image classification tasks, particularly when trained on large datasets.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/665cbfd09e06cf0c12932956_6esuU4q3cR_XdfjzrOeYWqpGDH0bJbuoz1v3MrO1MzTVj_bO_SOI_ulC9ci9zQve1tBnrkoQ6lxinpm-sunwvGrkI0aIRFWlLwwi3xzvoVYYPrnljqjWyYfOvfnEMHvLa9I_ssHlpBQZSpjT_NBt1MQ.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><h4 id="""">Advantage of using Vision Transformer (ViT’s)&nbsp; over&nbsp; CNN</h4><p id="""">‍</p><p id="""">Choosing Vision Transformer (ViT) over Convolutional Neural Networks (CNNs) for image processing tasks depends on the specific requirements and characteristics of the application. ViT is advantageous due to its ability to capture long-range dependencies and contextual information across the entire image through self-attention mechanisms, which are often more challenging for CNNs that rely on local receptive fields. This capability allows ViT to excel in tasks requiring a holistic understanding of the image, such as image classification, object detection, and segmentation. Additionally, ViT has shown impressive performance on large datasets and benefits significantly from extensive pre-training on vast amounts of data.</p><p id="""">‍</p><p id="""">However, CNNs remain highly efficient for many real-time applications due to their optimized convolution operations, which are well-suited for current hardware accelerators like GPUs. CNNs' hierarchical feature extraction is particularly effective for tasks involving spatial hierarchies and local patterns, making them ideal for applications such as real-time video processing, edge computing, and mobile devices where computational efficiency and lower latency are critical.</p><p id="""">‍</p><h2 id="""">Technical Concepts</h2><p id="""">‍</p><p id="""">Before we get into the heart of working we need to understand the technical terms and concepts in the Visual Large Language model architecture. Let’s understand why these layers or concepts are important and what is their role in the architecture.</p><p id="""">‍</p><h3 id="""">&nbsp;Multi-Modal Embeddings</h3><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1316px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1316px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/665cbfd1a1eedf962921dd10_DIPZUktNSudg08kNKgluQBkVWW0vWIM982zRGjNIMwy8HS4k1cv5uJwqPKxtI4WKDM3AJ0Reyl6PnU7dFNKSwXC-o5i6smd5cDYe5Q7lRzPGYayM0N_INFgT3k-dzSSq3d6D0WyHrB6UJKbxdfuxQqs.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><p id="""">Multi-modal embeddings involve representing data from different modalities (e.g., text and images) in a unified embedding space. This allows the model to understand and relate information from multiple modalities simultaneously, leading to more accurate and efficient information retrieval and analysis across various domains. An implementation of this concept is the <a href=""https://arxiv.org/pdf/2305.05665"" id="""">ImageBind model</a>, which maps diverse data types like text, images, audio, and even sensor data into a single embedding space. This unified approach enhances the model's ability to perform tasks that require cross-modal understanding and integration, making it particularly powerful for applications such as cross-modal search, multimodal content generation, and comprehensive data analysis.</p><p id="""">‍</p><h4 id="""">Techniques for Aligning Visual and Textual Data</h4><h5 id="""">Joint Embedding Space</h5><p id="""">‍</p><p id="""">Techniques like CLIP (Contrastive Language-Image Pre-training) project both visual and textual data into a common embedding space. This is achieved by training the model on pairs of images and corresponding text descriptions. Joint embedding space is a more efficient and flexible option compared to a cross-modal transformer for several reasons. First, it provides a unified representation where both visual and textual data are projected into a common space, enabling straightforward comparison and retrieval tasks. This approach simplifies the architecture and reduces computational complexity since it avoids the need for separate, intricate attention mechanisms for each modality as required in cross-modal transformers. Additionally, joint embedding spaces are highly effective in scenarios like image-text matching, where the goal is to find correspondences between different types of data. They facilitate quick and efficient retrieval of relevant information by leveraging learned associations in a shared latent space. This can lead to faster inference times and lower resource consumption, making joint embedding spaces more suitable for real-time applications and large-scale deployments.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""600px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/665cbfcf0e6a31f1efa4942c_k3sAwapi6GobVCbYUMWvSdGkf6iGPD2FARDRx9h_o2sGa-YXPoPcb5bdeHwNE5oyJiooS5MqbIkacAq9VrEU5ToYy7GlhGk1kBOtKC6Itl_b8f5Vm0luhU3T1bQD-V2Uss731jzAKvx2I2tOQEUSVuU.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><h5 id="""">Cross-Modal Transformers</h5><p id="""">‍</p><p id="""">Models such as <a href=""https://arxiv.org/abs/1908.02265"" id="""">ViLBERT</a> and <a href=""https://arxiv.org/abs/1908.07490"" id="""">LXMERT</a> use separate encoders for each modality and align the representations using transformer layers that allow for cross-modal interactions. The model consists of two parallel streams for visual (green) and linguistic (purple) processing that interact through novel co-attentional transformer layers. This structure allows for variable depths for each modality and enables sparse interaction through co-attention. Dashed boxes with multiplier subscripts denote repeated blocks of layers.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/665cbfd09d6820883ee0b09e_mI4Tq_to7X0hL0RzqsZHa1AlGEAJsFtSJNkjHSiLEnVv4fh4YSuxn-2OpeP7lt8tdkUonUHvyMzOXkt51TZivgj87J0pKevcL0gWTVvqK-4StbIfdB195c88DMHzO9lM-UpP3IGU5Yi2odi44ThvoMg.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><h4 id="""">Why embedding space is considered important&nbsp;</h4><p id="""">‍</p><p id="""">Embedding spaces enable VLLMs to perform zero-shot learning&nbsp; where the model recognizes and categorizes new, unseen data based on its relationship to known data. It also improves cross-modal retrieval&nbsp; and facilitates fine-grained understanding where retrieving&nbsp; relevant information across different modalities, enhancing applications like image captioning and visual question answering and enables nuanced and detailed understanding of visual concepts through fine-grained embeddings.</p><p id="""">‍</p><h3 id="""">Attention Mechanisms in VLLMs</h3><h4 id="""">Self-Attention</h4><p id="""">‍</p><p id="""">Allows the model to weigh the importance of different parts of a single input (e.g., different words in a sentence or different regions in an image) relative to each other. This mechanism helps in capturing long-range dependencies and contextual relationships within the same modality. In transformer-based models like BERT and Vision Transformers (ViT), self-attention helps in understanding the contextual relevance of different tokens (words or image patches).</p><p id="""">‍</p><p id="""">The self-attention mechanism in Transformers allows the model to dynamically determine the relevance of each word in a sentence by assigning attention scores, which are learned during training. Each word in the input sequence is first converted into an embedding and combined with positional encoding to incorporate information about the word's position. The self-attention mechanism then computes three vectors for each word: Query, Key, and Value. The attention score between each pair of words is calculated using the dot product of their Query and Key vectors, normalized via a softmax function to produce attention weights.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:437px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""437px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/665cbfcf653bd153c34df20c_KM8h8TYwBydLgfng4qXTAWiqxtaFumQGtnCWVa_2JNTaaRRi3D2AS2E1tJgU3yHi0nmcWqx5mH2G5FNpp5NMVQtiCfK3gU_8wmf_zghhOpaqFhEAmYPax-OhDh0oNpYBcq40FqkSZ5-gyR6fOYIPwao.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><p id="""">These weights dictate the focus each word should have on other words in the sequence. Multiple attention heads are used to capture different aspects of the relationships between words. The refined representations produced by these heads are then passed through several layers, allowing the model to understand complex word relationships and contexts better. For example, in the sentence ""The animal didn't cross the street because it was too tired,"" the self-attention mechanism would likely assign higher attention scores between ""it"" and ""animal"" rather than ""it"" and ""street,"" based on their contextual relevance. This iterative refinement through multiple layers enables the model to perform tasks like coreference resolution effectively, focusing more on contextually relevant words to minimize the loss and produce accurate outputs.</p><p id="""">‍</p><h4 id="""">Cross-Attention</h4><p id="""">Enables the model to align and integrate information from two different modalities. It helps in establishing correspondences between visual features and textual descriptions. In models like ViLBERT, cross-attention layers allow for the fusion of visual and textual information, enabling the model to generate coherent and contextually relevant outputs.</p><p id="""">‍</p><p id="""">In cross-attention, the Query (Q) vectors are derived from one modality, such as text, while the Key (K) and Value (V) vectors come from another modality, such as images. For the sentence ""The animal didn't cross the street because it was too tired,"" the word ""it"" would generate a Query vector from its textual embedding. The image associated with the sentence is divided into regions, each represented by feature vectors serving as the Key and Value vectors. The cross-attention mechanism calculates attention scores by taking the dot product of the Query vector (text) and the Key vectors (image regions), which is then scaled and passed through a softmax function to produce normalized attention weights.&nbsp;</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:650px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""650px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/665cbfcfe7c709241becf3a2_lKxg_oMwXFMg-1YJTqO1HgDg3Lc3MXjePajR-sTuhbbyVbISutmGGbeoiLGjD459FHeUWQQK1rKHqNpRl_YIbJ7ui6AwdSz4GuQFDQ58caRLB_0LtC8kWEiRXXe7osQk4STiog60ZLEWvXqIvydkLsg.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><p id="""">These weights indicate the relevance of each image region to the word ""it."" The Value vectors are then weighted by these attention scores and summed, producing a context vector that integrates relevant visual features into the textual context. This process helps the model correctly associate ""it"" with ""animal"" rather than ""street"" by focusing on the visual regions related to the animal. The output from the cross-attention block is a combined representation that enhances the model's understanding of the sentence by incorporating both visual and textual information, allowing for better performance on tasks like visual question answering and image captioning.</p><p id="""">‍</p><h3 id="""">Contrastive Learning</h3><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:2085px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""2085px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/665cc0ce653bd153c34eada8_Screenshot%202024-06-03%20002746.png"" loading=""lazy"" alt=""__wf_reserved_inherit"" width=""auto"" height=""auto"" id=""""></div></figure><p id="""">‍</p><p id="""">Contrastive learning is a self-supervised learning technique that focuses on learning representations by distinguishing between similar (positive) and dissimilar (negative) pairs. This method is particularly effective in scenarios where it is crucial to differentiate between closely related data points. In models like CLIP (Contrastive Language-Image Pre-training) developed by OpenAI, contrastive learning is applied to large datasets of image-text pairs. CLIP uses separate encoders for images and text, projecting both into a shared embedding space. The training objective, driven by a contrastive loss function, aims to maximize the similarity between the embeddings of positive pairs (correct image-text pairs) while minimizing the similarity between negative pairs (incorrect image-text pairs). This approach enables the model to perform well on various tasks without task-specific fine-tuning, demonstrating strong zero-shot learning capabilities in the image below compared to other state of the art deep learning models in different datasets.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1484px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1484px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/665cbfcf7e1168ddfa0871e9_9ujum91yM3KFngnnJw3KdZClBBnPMbFEdaTyOEke8lxTO8Bd4XiGc2gLZBqEHyV0FTJJ4qvJQi8xbEzRVzJZz-9mMBn4iLbizSh_9i2zbmO4PkWZPPR5m0juanmOjWB1mFQ1p8VKqfYPxpXUQDjY4vM.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><p id="""">The benefits of contrastive learning in VLLMs include the development of robust and generalizable representations, the ability to handle zero-shot learning scenarios, and scalability with large datasets. However, it also presents challenges such as the need for efficient negative sampling, high computational costs, and the reliance on high-quality data. The implementation of contrastive learning in CLIP, as detailed in the paper ""Learning Transferable Visual Models From Natural Language Supervision"" by Alec Radford et al., highlights these strengths and challenges, showcasing the potential and limitations of this powerful technique. For more detailed insights, refer to the <a href=""https://arxiv.org/abs/2103.00020"" id="""">original CLIP paper</a>.</p><h2 id="""">How does a Visual Large Language Model Work</h2><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1226px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1226px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/665cbfcfa41944613ae685a0_i9pMvG_1xkmpQ7hJRxhf8ME5aUOPl6V7M5FRl6l6d78d4m5BmmtLBwh3R1zC_w-hLxRcbabZOQPPr3QOy1d3rILxOOeU3TExqJdtZZEgTWRfA1jOSiSA3glvOA23hv4fKLmrAAJ1VJio6CxPIJ8DNL8.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><h3 id="""">Backbone</h3><p id="""">‍</p><p id="""">The Backbone in a Visual Large Language Model (VLLM) is the fundamental neural network tasked with extracting essential features from input images. Typically, this backbone is a sophisticated convolutional neural network (CNN) like ResNet or a Vision Transformer (ViT). These networks process the image, converting it into a high-dimensional tensor of visual features (Fv), which encapsulates critical spatial and semantic details. These extracted features form the basis for subsequent stages, facilitating the model's ability to interpret and manipulate the visual data effectively for diverse tasks.</p><p id="""">‍</p><h3 id="""">Language-Guided Image Tokenizer</h3><p id="""">‍</p><p id="""">The Language-Guided Image Tokenizer&nbsp; is crucial for integrating visual and textual information within the VLLM framework. This component operates by initially receiving visual features (Fv) from the Backbone and textual features ( Ft ) from a text encoder, often a Transformer model like BERT. Using a cross-attention mechanism, it aligns and combines these modalities, producing language-guided image tokens ( T ). These tokens are enriched with both visual and contextual data, enabling the model to understand and respond accurately to the tasks specified by the accompanying language instructions.</p><p id="""">‍<br></p><h3 id="""">Random Query</h3><p id="""">‍</p><p id="""">The Random Query component represents the VLLM's capability to handle a wide array of tasks flexibly. This feature allows the model to process various vision-only and vision-language tasks dynamically. By introducing randomness, the model can adapt to different inputs and instructions, showcasing its robustness and versatility in generating appropriate outputs. This adaptability is key to the model's performance across diverse applications, enabling it to handle novel and unexpected scenarios effectively.</p><p id="""">‍</p><h3 id="""">Language Instructions (\&lt;text\&gt;)</h3><p id="""">‍</p><p id="""">Language Instructions are the natural language prompts that guide the VLLM on what specific tasks to perform. These instructions provide detailed descriptions of the tasks, such as ""Describe the image &lt;image&gt; in detail"" for vision-language tasks or ""For each object in the image &lt;image&gt; that belongs to the class set &lt;class&gt;, output a tuple with the class label and coordinates"" for vision-only tasks. The instructions are parsed into a machine-readable format, directing the model on how to interpret the visual data and generate the required outputs.</p><p id="""">‍</p><h3 id="""">Open-Ended Task Decoder with LLM</h3><p id="""">‍</p><p id="""">The Open-Ended Task Decoder with LLM&nbsp; is the component that interprets the language-guided image tokens ( T ) and generates the final output based on the provided instructions. This decoder utilizes the capabilities of large language models (LLMs) like GPT to process integrated tokens and leverage its extensive language understanding to produce meaningful results. Whether classifying tokens for object detection or generating sequences for tasks like image captioning, this decoder can adapt its outputs to the specified formats, ensuring flexibility and accuracy in addressing a variety of vision-centric tasks.</p><p id="""">‍</p><h3 id="""">Desired Output</h3><p id="""">‍</p><p id="""">The Desired Output is the end result produced by the VLLM, tailored to the task defined by the language instructions. This output can take various forms depending on the task, such as class labels and bounding box coordinates for object detection, descriptive text for image captioning, or text-based answers for visual question answering. The ability to generate such a wide range of outputs demonstrates the VLLM's versatility and effectiveness in integrating and processing both visual and textual information to meet diverse application needs.</p><p id="""">‍</p><h2 id="""">How to Train a Visual LLM</h2><p id="""">Meta released an amazing <a href=""https://arxiv.org/pdf/2405.17247v1"" id="""">guide</a> on training Visual Language Models.</p><p id="""">‍</p><p id="""">Training a Vision-Language Large Model (VLLM) involves several crucial steps to ensure the model effectively associates textual descriptions with visual elements (grounding) while managing computational resources efficiently. Grounding can be enhanced by using bounding box annotations to teach the model where objects are located in the images, employing contrastive learning techniques with negative captioning to distinguish between correct and incorrect text-image pairs, and ensuring high-quality, diverse datasets. Optimizing data quality by pruning low-quality or duplicate entries and improving caption quality with synthetic data generation techniques are also essential steps.</p><p id="""">‍</p><p id="""">Managing GPU resources is critical due to the significant computational requirements for training VLLMs. High-quality datasets reduce the need for extensive compute power, and efficient training techniques like masking and optimized data loading can speed up the process. Leveraging pre-trained models for fine-tuning instead of training from scratch can also help manage costs. Libraries like <a href=""https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html"" id="""">torch.compile</a> and <a href=""https://github.com/facebookresearch/xformers"" id="""">xformers</a> optimize attention mechanisms, while Fast Forward Computer Vision (FFCV) helps in creating faster-loading data files.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/665cbfcfe9e4579125284116_PRhWeD29HWsslKnnF7dH-fw4p91dDFn9Svpax1a2xXpxS2wcMaF0lh0he7j8vMH8_EIC2kqGDoxBRQAZuRoC3lrvBturtjMV0jjWq5RIVWH7V8jWtH3qh8Bjt7RH-FzCOSeOuiJ-8Xak_lbbVoxbCIM.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><p id="""">Important considerations to keep in mind when training VLMs. Data is one of the most important aspects of training VLMs. Having a diverse and balanced dataset is important for learning good world models that can span enough concepts. It is also important to remove duplicates which occur a lot within large-scale datasets, this will save a lot of compute time and mitigate the risks of memorization. In addition, pruning the data is also an important component since we want to be sure that the captions are indeed related to the image content. Lastly, improving the caption quality is crucial to enhance VLMs performance. Grounding VLMs is another important step to ensure that the VLMs correctly associate words with specific concepts. Two common grounding methods leverage either bounding boxes or negative captions. Lastly, alignment is a much-needed step to ensure that the model is producing answers that are expected from a human point of view.</p><h3 id="""">How many GPU’s are required for Training</h3><p id="""">‍</p><p id="""">The compute resources required for training a VLLM significantly influence the budget needed for such projects. Models like CLIP and OpenCLIP have utilized more than 500 GPUs, which equates to costs in the hundreds of thousands of dollars—often inaccessible for most companies or academic labs. However, by using high-quality datasets and leveraging efficient techniques like masking strategies, training a contrastive model like CLIP on hundreds of millions of images from scratch can be done with as few as 64 GPUs, costing around $10K in compute. If the VLM leverages existing pre-trained image or text encoders, or LLMs, the cost of learning a mapping should be much lower.</p><h3 id="""">Steps for Training VLLM</h3><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:2167px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""2167px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/665cc1410e6d0daa66457198_Screenshot%202024-06-03%20002937.png"" loading=""lazy"" alt=""__wf_reserved_inherit"" width=""auto"" height=""auto"" id=""""></div></figure><h4 id="""">Data Preparation</h4><p id="""">Collect and preprocess a diverse set of image-text pairs, ensuring that the dataset is both extensive and varied to cover a wide range of concepts. This includes ensuring high-quality captions using synthetic data generation techniques if necessary. Removing duplicates and low-quality samples from the dataset is crucial to save computational resources and prevent the model from memorizing redundant information, which can degrade its performance and efficiency. Proper data curation and preparation form the foundation for successful VLLM training.</p><p id="""">‍</p><h4 id="""">Model Architecture:&nbsp;</h4><p id="""">Choose an appropriate model architecture based on the specific requirements of your task, whether it is contrastive, masking, or generative models. For effective grounding, consider models that leverage bounding boxes to explicitly indicate object locations or those that use negative samples to teach the model to distinguish between correct and incorrect text-image pairs. The choice of architecture should align with the end goals of the VLLM, such as image retrieval, caption generation, or both.</p><h4 id="""">Training Process:&nbsp;</h4><p id="""">Implement contrastive learning techniques to align text and image representations effectively. This involves training the model to push the representations of matching image-text pairs closer together while pushing non-matching pairs further apart. Additionally, implement masking strategies to improve training efficiency and model performance by randomly masking parts of the input data and training the model to predict the masked content. Fine-tune pre-trained models to reduce computational costs, leveraging existing knowledge to expedite the training process and achieve better initial performance.</p><p id="""">‍</p><h4 id="""">Optimization Techniques:&nbsp;</h4><p id="""">‍</p><p id="""">Apply efficient attention mechanisms and data loading optimizations to ensure that the training process is as fast and effective as possible. Utilize libraries like <a href=""https://pytorch.org/docs/stable/generated/torch.compile.html"" id="""">torch.compile</a> and <a href=""https://huggingface.co/docs/diffusers/en/optimization/xformers"" id="""">xformers</a>, which offer significant speed improvements for model training. Regularly evaluate the model's performance and adjust hyperparameters as needed to ensure optimal results. Optimizing these aspects can greatly reduce the overall training time and computational costs while maintaining or improving model performance.</p><p id="""">‍</p><h4 id="""">Fine-Tuning and Evaluation:</h4><p id="""">‍</p><p id="""">Fine-tune the model on specific downstream tasks to ensure it performs well in practical applications. This involves adjusting the model parameters based on specific task requirements, such as image classification, caption generation, or retrieval tasks. Evaluate the model using benchmarks like zero-shot and retrieval tasks to ensure it generalizes well across different scenarios. Regular performance evaluations help in identifying and addressing potential issues early, ensuring the model is robust and reliable for real-world applications.</p><p id="""">‍</p><h3 id="""">Improving Grounding</h3><p id="""">‍</p><p id="""">Grounding in a Visual Large Language Model (VLLM) refers to the process of associating textual descriptions with specific visual elements within an image. This involves identifying and linking parts of the text to corresponding objects or regions in the visual data, enabling the model to understand and interpret images in the context of the provided language. Grounding helps the model make accurate predictions and generate relevant outputs by ensuring that the visual and textual components are meaningfully connected, enhancing tasks like object detection, image captioning, and visual question answering.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:977px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""977px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/665cbfd0826846a47adbd810_-rDUZbSkRx9XgGVPWK-jyksbgMlnuElDh8ff-dpoIuLIag6NarB8BkVnukpM-oDTHBAOKUZCOcIj6ucGE-hK0rwla6pP7rmDysNyYoAz-iC4mRN1zmOI_OOhyE8vFibyiE_lCR9QPdsmnp38svlZKrA.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><p id="""">Grounding is a significant challenge in the VLM and generative model literature. It primarily addresses the issue of models not fully understanding text prompts, which can result in ignoring parts of the prompt or generating hallucinated content not present in the prompt. Challenges in grounding include understanding spatial relations (e.g., left or right of an object), handling negations, counting, and recognizing attributes like colors or textures. Although no single method can completely solve grounding issues, several techniques can improve grounding performance.You can also try out the model in this <a href=""https://huggingface.co/spaces/merve/Grounding_DINO_demo"" id="""">link</a>.</p><h4 id="""">Using Bounding Box Annotations:</h4><p id="""">‍</p><p id="""">Models like X-VLM leverage bounding box annotations, incorporating box regression and Intersection over Union (IoU) loss to accurately locate and align visual concepts with their corresponding textual descriptions. By knowing where objects are in images and the associated captions, the model can better associate text with visual clues, improving grounding. X-VLM is trained on datasets like COCO, Visual Genome, SBU, and Conceptual Captions, using up to 16 million images. This extensive training data with bounding box annotations enables X-VLM to excel in tasks like image-text retrieval, visual reasoning, visual grounding, and image captioning.</p><p id="""">‍</p><h4 id="""">Negative Captioning:</h4><p id="""">‍</p><p id="""">Contrastive objectives use negative samples to mitigate collapse, enhance generalization, and improve discriminative feature learning. By contrasting positive pairs (similar or related samples) with negative pairs (dissimilar or unrelated samples), models develop a nuanced understanding of data, grasping underlying patterns that distinguish different classes or categories. Recent works have shown that using negative samples can mitigate various problems in VLMs. For instance, the ARO benchmark evaluates VLMs on their ability to correctly associate images with captions, using negative samples to test the model's understanding of incorrect pairings. This approach has shown that VLMs significantly benefit from the differentiation capabilities fostered by exposure to negative samples, leading to more accurate and contextually aware models.&nbsp;</p><p id="""">‍</p><h2 id="""">Recent Research in Visual LLM&nbsp;</h2><h3 id="""">LLama 3-v</h3><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/665cbfcfee118657b7e00285_lTceOhxHHRjkNwHVB-Rbr0DaySb744RDHdWoKZzJlWsA0fusdPswlBJdl3IjU9eUINE8uqbGG_pePPul2NZ5yteCY4Amu4GtoeREAqZh5l1ZL7lDyV0eTFFt5nYKChh37qcnCn4Q3GVd_MtFYOqlD2o.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><p id=""""><a href=""https://github.com/mustafaaljadery/llama3v"" id="""">Llama 3-V</a> leverages the SigLIP model to embed visual information, distinguishing itself by employing a pairwise sigmoid loss instead of a contrastive loss. Input images are transformed into patch embeddings, which are aligned with textual tokens via a projection block using self-attention mechanisms. This joint representation, combining visual and textual tokens, is processed through Llama3. Unlike models such as Llava that utilize a single linear layer for image embeddings, Llama 3-V's dual self-attention blocks capture intricate patterns in the data, enabling superior multimodal understanding and performance on various benchmarks. This architecture is particularly optimized for cost-effective training and inference, maintaining high performance with significantly lower computational resources.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1085px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1085px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/665cbfcf254938203309c68a_T2C9Ha5xLRmSWkqibQHftRPzOGrZqyti_K7i6stCnyG2Uz8sdVHqx3LLG7oclaqe9ahjgPXItedELdyN3gNoWRqTnSX3dc2LWBZ_tQKIwmJkk5OWAKrLz-7s1eop_sVar2o2w9cE5R1hBk8plK56_KE.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><h3 id="""">Visual-Bert</h3><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/665cbfd037e731458becfdba_qlE25By-veWC2QAVdaqZ4Q8K3Nz41wyhX8tSKVFZ_dOB5uinCcLH8yiY0463LUEyFM39APpRaFz6WptA4f2pnsfFvsaG0GF4W0Loi46w2qq4ms03Ap_63I0kipE9WM8ta1F02958atS3KtECpt-Inrg.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><p id=""""><a href=""https://arxiv.org/pdf/1908.03557"" id="""">VisualBERT</a> processes image regions, extracted using an object detector, and treats these regions as visual tokens alongside text tokens. Both text and visual features are embedded into a shared space using token, segment, and position embeddings. These embeddings are then passed through multiple Transformer layers, allowing self-attention mechanisms to align elements of text with corresponding image regions. The model is pre-trained with masked language modeling, predicting masked words using visual context, and sentence-image prediction, determining if a text matches an image. This joint processing enables VisualBERT to capture rich interactions between visual and textual data, making it effective for tasks like Visual Question Answering and Visual Commonsense Reasoning. Unlike CLIP, which uses separate encoders for images and text aligned through contrastive learning, VisualBERT uses a single Transformer for richer interaction and grounding of visual and textual information.</p><p id="""">‍</p><h3 id="""">LLava</h3><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1188px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1188px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/665cbfcf2207d5ba4c1e79f7_ZgTyfnIri9CL6T79PPyxmjwEALdoBGnxuSVSuOpfwHPbOoDGNNiuOaQM0-fHpzG2eQrilMM4_PxqST-dr0mIRxUOAYd8vUa7OD0HHRQBrPdh1LT_KuSkITViAFPyrPqFyEnEWf1BfkWu-AZYmaoaCjk.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><p id=""""><a href=""https://arxiv.org/pdf/2304.08485"" id="""">The LLaVA (Large Language and Vision Assistant) model</a> combines a pre-trained language model (Vicuna) with a visual encoder (CLIP's ViT-L/14). The visual encoder processes an input image to generate visual features, which are then projected into the language embedding space using a trainable linear layer. These visual tokens are combined with language instruction tokens and fed into the language model to generate responses. Unlike CLIP, which aligns visual and textual representations using contrastive learning, LLaVA directly integrates visual features into the language model's embedding space for end-to-end vision-language tasks.</p><p id="""">‍</p><h3 id="""">Idefics 2</h3><p id="""">‍</p><p id="""">The <a href=""https://blog.paperspace.com/idefics2/"" id="""">Idefics 2</a> model integrates visual and language processing to generate contextually informed text responses. The architecture comprises three main components: a Vision Encoder, a Vision-Language Connector, and an LLM (Large Language Model) Decoder. The Vision Encoder processes input images to extract high-dimensional visual features. These features are then transformed by the Vision-Language Connector into language embedding tokens that align with the LLM’s word embedding space. This transformation allows the visual data to be seamlessly integrated with textual information. The LLM Decoder takes these integrated tokens, along with any language instructions, to generate coherent text responses.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:498px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""498px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/665cbfd0bfdc2a2b6bd3cd46_-Fz7f0uRlibbfefXuL8wZIt22E0vtEH4oo196UxcX-bQSoIBedox9MBl4Xr48VW4UzYuClhTvGwqAsTO3UqIOY832YTx0Qz4jqLLUJm4WE8nbdzAN8rnfsJ4P1zL-KlESxl8rZD5Fr4d1A0f9kilcAk.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><p id="""">This approach differs from models like CLIP, which uses separate encoders for images and text to align their representations in a shared embedding space primarily for retrieval tasks. In contrast, Idefics 2 focuses on generating language responses informed by visual data, leveraging a direct transformation layer to bridge the visual and textual modalities effectively. This enables tasks such as describing images, answering visual questions, and generating narratives based on visual inputs.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1878px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1878px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/665cc25573fcce1bda73c2f9_Screenshot%202024-06-03%20003356.png"" loading=""lazy"" alt=""__wf_reserved_inherit"" width=""auto"" height=""auto"" id=""""></div></figure><p id="""">Idefics 2 is an 8 billion parameter vision-language model designed to excel in Optical Character Recognition (OCR) and document data extraction, such as reading bills and invoices. It outperforms larger models with its efficient architecture, supporting image resolutions up to 980 × 980 pixels. The model has been pre-trained on over 6TB of OCR data, enhancing its ability to accurately extract text from images. This makes it particularly effective for automating data entry and managing document workflows, thanks to its improved visual reasoning and document understanding capabilities. You can also try out the model in this <a href=""https://huggingface.co/spaces/HuggingFaceM4/idefics-8b?ref=blog.paperspace.com"" id="""">link</a>.</p><p id="""">‍</p><h2 id="""">Benchmarking and Evaluation of Vision-Language Large Models (VLLMs)</h2><h3 id="""">Common Datasets</h3><p id="""">Standard benchmarks for evaluating Vision-Language Large Models (VLLMs) often utilize widely recognized datasets such as COCO (Common Objects in Context) and Visual Genome.&nbsp;</p><p id="""">‍</p><p id="""">COCO dataset includes over 200,000 labeled images with annotations for object detection, segmentation, and captioning. It is extensively used for evaluating image captioning, object detection, and segmentation tasks. The dataset is detailed in ""<a href=""https://arxiv.org/abs/1405.0312"" id="""">Microsoft COCO: Common Objects in Contex</a>t"" by Lin et al. (2014) .</p><p id="""">‍</p><p id="""">Visual Genome&nbsp; dataset contains over 100,000 images with dense annotations of objects, attributes, and relationships, making it suitable for tasks requiring detailed scene understanding. ""<a href=""https://arxiv.org/abs/1602.07332"" id="""">Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations</a>"" by Krishna et al. (2017) describes this dataset .</p><p id="""">‍</p><p id="""">The main evaluation metrics are accuracy, f1 score, exact match.Accuracy measures the proportion of correct predictions made by the model and is commonly used in classification tasks. F1 Score is the harmonic mean of precision and recall, providing a balance between the two metrics, making it particularly useful for imbalanced datasets. Exact Match (EM) Score measures the percentage of predictions that match the ground truth exactly, often used in tasks like question answering and retrieval.</p><h3 id="""">Recent Benchmarks</h3><p id="""">‍</p><p id="""">CODIS evaluates a model's ability to disambiguate images based on contextual information. This benchmark assesses how well models can understand and interpret images in context, rather than in isolation. ""<a href=""https://arxiv.org/html/2402.13607v1"" id="""">CODIS: A Benchmark for Context-Dependent Image Disambiguation</a>"" by Peng et al. (2022) provides a comprehensive overview of this benchmark .</p><p id="""">‍</p><p id="""">Fine-Grained Visual Concept Recognition benchmark involves recognizing detailed and specific visual concepts within images, often requiring models to differentiate between subtle differences. It tests the model's ability to understand fine-grained details and nuances in visual data. Relevant research includes ""<a href=""https://arxiv.org/abs/2111.06119"" id="""">Fine-Grained Recognition: A Survey</a>"" by Wei et al. (2019) .</p><p id="""">‍</p><p id="""">By leveraging these benchmarks and evaluation metrics, researchers can systematically assess the performance of VLLMs, identify areas for improvement, and ensure that models are robust and effective across a variety of tasks and datasets.</p><h2 id="""">Transform Your Visual Data with Custom VLLMs</h2><p id="""">If you're looking to leverage the power of Visual Large Language Models (VLLMs) for your business or research needs, <a href=""http://mercity.ai"" id="""">Mercity.ai</a> can help you build a custom VLLM tailored to your specific requirements. Whether it's enhancing image classification, improving visual question answering, or integrating sophisticated visual and textual data analysis into your applications, our team of experts is here to assist you every step of the way. Contact Mercity.ai today to learn how we can transform your vision into reality with cutting-edge VLLM technology. <a href=""https://www.mercity.ai/contacts"" id="""">Contact us</a> now!</p><p id="""">‍</p>",https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/665cc28f93bb7a343ed2a36e_vlm-structure.png,Mathavan,Visual Language Models,"In this blog we explain how Visual Language Models work from scratch, we explain CLIP, image embeddings, and necessary topics. We also explain how to train a visual language model from scratch too.",False,"<div class=""rich-text w-richtext""><blockquote><em>❗ To learn how to use Visual Language Models for Medical applications, checkout our blog on </em><a href=""https://www.mercity.ai/blog-post/building-medical-ai-assistants-with-visual-llms""><em>Building Medical AI assistants with Visual LLMs</em></a></blockquote><p><em>‍</em></p><p>The evolution of Visual Large Language Models (VLLMs) began with the Transformer architecture introduced in the “<a href=""https://arxiv.org/abs/1706.03762"">Attention Is All You Need</a>” paper by Vaswani et al., which revolutionized natural language processing by enabling efficient self-attention mechanisms. Building on this, OpenAI's CLIP model combined visual and textual data, demonstrating strong zero-shot learning capabilities. Models like <a href=""https://arxiv.org/abs/1908.02265"">VilBERT</a> and <a href=""https://arxiv.org/abs/1908.03557"">VisualBERT</a> extended this by integrating visual and linguistic inputs, improving multimodal interactions. Recent advancements include <a href=""https://llava-vl.github.io"">LLaVA</a> and <a href=""https://huggingface.co/docs/transformers/en/model_doc/siglip"">SigCLIP</a>, which enhance visual grounding and fine-grained visual concept recognition. Techniques like fine-grained reward modeling in the <a href=""https://github.com/Jeff-Zilence/VIGOR"">ViGoR</a> framework and benchmarks such as <a href=""https://arxiv.org/abs/2402.13607"">CODIS</a> further enhance VLLMs' real-world applicability. This blog will explore these developments in detail, highlighting their impact and potential applications.</p><h2>Foundational Concepts</h2><h3>Convolutional Neural Network(CNN)</h3><p>‍</p><p><a href=""https://arxiv.org/abs/1511.08458"">Convolutional Neural Networks (CNNs)</a> have revolutionized image processing and computer vision tasks. They are specifically designed to process and interpret visual data by mimicking the way humans perceive images. A CNN consists of multiple layers, including convolutional layers, pooling layers, and fully connected layers. The convolutional layers apply filters to the input image to create feature maps that highlight various aspects of the image, such as edges, textures, and shapes. Pooling layers then reduce the spatial dimensions of these feature maps, retaining the most important information while reducing computational complexity. Fully connected layers at the end of the network combine these features to make predictions.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1280pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/665cbfcf56e78026235fa7e5_uecVWNK5mbUypp7OgvGjndP4r7Fe9VwVKBbUbYe-tL7lnEp1KvhE6cgdOGAx4tZ_aVTXP7wV9T0XgdbznNbMIo8dw9PQ6fpL0aNFlB_4PquM6boIMBZvmJeqIzpigAqRe2Tc9H5FDv8AXcNYDXu-dJg.png""/></div></figure><p>‍</p><h3>Vision Transformers (ViTs)</h3><p>‍</p><p><a href=""https://arxiv.org/abs/2010.11929"">Vision Transformers</a> are a recent innovation in image processing that adapt the transformer architecture, originally designed for natural language processing, to handle visual data. Unlike CNNs, which process images through local receptive fields, ViTs divide an image into fixed-size patches and treat each patch as a token, similar to words in a sentence. These tokens are then processed by a standard transformer encoder, which captures global context through self-attention mechanisms. ViTs have shown state-of-the-art performance on various image classification tasks, particularly when trained on large datasets.</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/665cbfd09e06cf0c12932956_6esuU4q3cR_XdfjzrOeYWqpGDH0bJbuoz1v3MrO1MzTVj_bO_SOI_ulC9ci9zQve1tBnrkoQ6lxinpm-sunwvGrkI0aIRFWlLwwi3xzvoVYYPrnljqjWyYfOvfnEMHvLa9I_ssHlpBQZSpjT_NBt1MQ.png""/></div></figure><h4>Advantage of using Vision Transformer (ViT’s)  over  CNN</h4><p>‍</p><p>Choosing Vision Transformer (ViT) over Convolutional Neural Networks (CNNs) for image processing tasks depends on the specific requirements and characteristics of the application. ViT is advantageous due to its ability to capture long-range dependencies and contextual information across the entire image through self-attention mechanisms, which are often more challenging for CNNs that rely on local receptive fields. This capability allows ViT to excel in tasks requiring a holistic understanding of the image, such as image classification, object detection, and segmentation. Additionally, ViT has shown impressive performance on large datasets and benefits significantly from extensive pre-training on vast amounts of data.</p><p>‍</p><p>However, CNNs remain highly efficient for many real-time applications due to their optimized convolution operations, which are well-suited for current hardware accelerators like GPUs. CNNs' hierarchical feature extraction is particularly effective for tasks involving spatial hierarchies and local patterns, making them ideal for applications such as real-time video processing, edge computing, and mobile devices where computational efficiency and lower latency are critical.</p><p>‍</p><h2>Technical Concepts</h2><p>‍</p><p>Before we get into the heart of working we need to understand the technical terms and concepts in the Visual Large Language model architecture. Let’s understand why these layers or concepts are important and what is their role in the architecture.</p><p>‍</p><h3> Multi-Modal Embeddings</h3><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1316pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/665cbfd1a1eedf962921dd10_DIPZUktNSudg08kNKgluQBkVWW0vWIM982zRGjNIMwy8HS4k1cv5uJwqPKxtI4WKDM3AJ0Reyl6PnU7dFNKSwXC-o5i6smd5cDYe5Q7lRzPGYayM0N_INFgT3k-dzSSq3d6D0WyHrB6UJKbxdfuxQqs.png""/></div></figure><p>‍</p><p>Multi-modal embeddings involve representing data from different modalities (e.g., text and images) in a unified embedding space. This allows the model to understand and relate information from multiple modalities simultaneously, leading to more accurate and efficient information retrieval and analysis across various domains. An implementation of this concept is the <a href=""https://arxiv.org/pdf/2305.05665"">ImageBind model</a>, which maps diverse data types like text, images, audio, and even sensor data into a single embedding space. This unified approach enhances the model's ability to perform tasks that require cross-modal understanding and integration, making it particularly powerful for applications such as cross-modal search, multimodal content generation, and comprehensive data analysis.</p><p>‍</p><h4>Techniques for Aligning Visual and Textual Data</h4><h5>Joint Embedding Space</h5><p>‍</p><p>Techniques like CLIP (Contrastive Language-Image Pre-training) project both visual and textual data into a common embedding space. This is achieved by training the model on pairs of images and corresponding text descriptions. Joint embedding space is a more efficient and flexible option compared to a cross-modal transformer for several reasons. First, it provides a unified representation where both visual and textual data are projected into a common space, enabling straightforward comparison and retrieval tasks. This approach simplifies the architecture and reduces computational complexity since it avoids the need for separate, intricate attention mechanisms for each modality as required in cross-modal transformers. Additionally, joint embedding spaces are highly effective in scenarios like image-text matching, where the goal is to find correspondences between different types of data. They facilitate quick and efficient retrieval of relevant information by leveraging learned associations in a shared latent space. This can lead to faster inference times and lower resource consumption, making joint embedding spaces more suitable for real-time applications and large-scale deployments.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/665cbfcf0e6a31f1efa4942c_k3sAwapi6GobVCbYUMWvSdGkf6iGPD2FARDRx9h_o2sGa-YXPoPcb5bdeHwNE5oyJiooS5MqbIkacAq9VrEU5ToYy7GlhGk1kBOtKC6Itl_b8f5Vm0luhU3T1bQD-V2Uss731jzAKvx2I2tOQEUSVuU.png""/></div></figure><p>‍</p><h5>Cross-Modal Transformers</h5><p>‍</p><p>Models such as <a href=""https://arxiv.org/abs/1908.02265"">ViLBERT</a> and <a href=""https://arxiv.org/abs/1908.07490"">LXMERT</a> use separate encoders for each modality and align the representations using transformer layers that allow for cross-modal interactions. The model consists of two parallel streams for visual (green) and linguistic (purple) processing that interact through novel co-attentional transformer layers. This structure allows for variable depths for each modality and enables sparse interaction through co-attention. Dashed boxes with multiplier subscripts denote repeated blocks of layers.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/665cbfd09d6820883ee0b09e_mI4Tq_to7X0hL0RzqsZHa1AlGEAJsFtSJNkjHSiLEnVv4fh4YSuxn-2OpeP7lt8tdkUonUHvyMzOXkt51TZivgj87J0pKevcL0gWTVvqK-4StbIfdB195c88DMHzO9lM-UpP3IGU5Yi2odi44ThvoMg.png""/></div></figure><p>‍</p><h4>Why embedding space is considered important </h4><p>‍</p><p>Embedding spaces enable VLLMs to perform zero-shot learning  where the model recognizes and categorizes new, unseen data based on its relationship to known data. It also improves cross-modal retrieval  and facilitates fine-grained understanding where retrieving  relevant information across different modalities, enhancing applications like image captioning and visual question answering and enables nuanced and detailed understanding of visual concepts through fine-grained embeddings.</p><p>‍</p><h3>Attention Mechanisms in VLLMs</h3><h4>Self-Attention</h4><p>‍</p><p>Allows the model to weigh the importance of different parts of a single input (e.g., different words in a sentence or different regions in an image) relative to each other. This mechanism helps in capturing long-range dependencies and contextual relationships within the same modality. In transformer-based models like BERT and Vision Transformers (ViT), self-attention helps in understanding the contextual relevance of different tokens (words or image patches).</p><p>‍</p><p>The self-attention mechanism in Transformers allows the model to dynamically determine the relevance of each word in a sentence by assigning attention scores, which are learned during training. Each word in the input sequence is first converted into an embedding and combined with positional encoding to incorporate information about the word's position. The self-attention mechanism then computes three vectors for each word: Query, Key, and Value. The attention score between each pair of words is calculated using the dot product of their Query and Key vectors, normalized via a softmax function to produce attention weights.</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:437pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/665cbfcf653bd153c34df20c_KM8h8TYwBydLgfng4qXTAWiqxtaFumQGtnCWVa_2JNTaaRRi3D2AS2E1tJgU3yHi0nmcWqx5mH2G5FNpp5NMVQtiCfK3gU_8wmf_zghhOpaqFhEAmYPax-OhDh0oNpYBcq40FqkSZ5-gyR6fOYIPwao.png""/></div></figure><p>‍</p><p>These weights dictate the focus each word should have on other words in the sequence. Multiple attention heads are used to capture different aspects of the relationships between words. The refined representations produced by these heads are then passed through several layers, allowing the model to understand complex word relationships and contexts better. For example, in the sentence ""The animal didn't cross the street because it was too tired,"" the self-attention mechanism would likely assign higher attention scores between ""it"" and ""animal"" rather than ""it"" and ""street,"" based on their contextual relevance. This iterative refinement through multiple layers enables the model to perform tasks like coreference resolution effectively, focusing more on contextually relevant words to minimize the loss and produce accurate outputs.</p><p>‍</p><h4>Cross-Attention</h4><p>Enables the model to align and integrate information from two different modalities. It helps in establishing correspondences between visual features and textual descriptions. In models like ViLBERT, cross-attention layers allow for the fusion of visual and textual information, enabling the model to generate coherent and contextually relevant outputs.</p><p>‍</p><p>In cross-attention, the Query (Q) vectors are derived from one modality, such as text, while the Key (K) and Value (V) vectors come from another modality, such as images. For the sentence ""The animal didn't cross the street because it was too tired,"" the word ""it"" would generate a Query vector from its textual embedding. The image associated with the sentence is divided into regions, each represented by feature vectors serving as the Key and Value vectors. The cross-attention mechanism calculates attention scores by taking the dot product of the Query vector (text) and the Key vectors (image regions), which is then scaled and passed through a softmax function to produce normalized attention weights. </p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:650pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/665cbfcfe7c709241becf3a2_lKxg_oMwXFMg-1YJTqO1HgDg3Lc3MXjePajR-sTuhbbyVbISutmGGbeoiLGjD459FHeUWQQK1rKHqNpRl_YIbJ7ui6AwdSz4GuQFDQ58caRLB_0LtC8kWEiRXXe7osQk4STiog60ZLEWvXqIvydkLsg.png""/></div></figure><p>‍</p><p>These weights indicate the relevance of each image region to the word ""it."" The Value vectors are then weighted by these attention scores and summed, producing a context vector that integrates relevant visual features into the textual context. This process helps the model correctly associate ""it"" with ""animal"" rather than ""street"" by focusing on the visual regions related to the animal. The output from the cross-attention block is a combined representation that enhances the model's understanding of the sentence by incorporating both visual and textual information, allowing for better performance on tasks like visual question answering and image captioning.</p><p>‍</p><h3>Contrastive Learning</h3><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:2085pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/665cc0ce653bd153c34eada8_Screenshot%202024-06-03%20002746.png""/></div></figure><p>‍</p><p>Contrastive learning is a self-supervised learning technique that focuses on learning representations by distinguishing between similar (positive) and dissimilar (negative) pairs. This method is particularly effective in scenarios where it is crucial to differentiate between closely related data points. In models like CLIP (Contrastive Language-Image Pre-training) developed by OpenAI, contrastive learning is applied to large datasets of image-text pairs. CLIP uses separate encoders for images and text, projecting both into a shared embedding space. The training objective, driven by a contrastive loss function, aims to maximize the similarity between the embeddings of positive pairs (correct image-text pairs) while minimizing the similarity between negative pairs (incorrect image-text pairs). This approach enables the model to perform well on various tasks without task-specific fine-tuning, demonstrating strong zero-shot learning capabilities in the image below compared to other state of the art deep learning models in different datasets.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1484pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/665cbfcf7e1168ddfa0871e9_9ujum91yM3KFngnnJw3KdZClBBnPMbFEdaTyOEke8lxTO8Bd4XiGc2gLZBqEHyV0FTJJ4qvJQi8xbEzRVzJZz-9mMBn4iLbizSh_9i2zbmO4PkWZPPR5m0juanmOjWB1mFQ1p8VKqfYPxpXUQDjY4vM.png""/></div></figure><p>‍</p><p>The benefits of contrastive learning in VLLMs include the development of robust and generalizable representations, the ability to handle zero-shot learning scenarios, and scalability with large datasets. However, it also presents challenges such as the need for efficient negative sampling, high computational costs, and the reliance on high-quality data. The implementation of contrastive learning in CLIP, as detailed in the paper ""Learning Transferable Visual Models From Natural Language Supervision"" by Alec Radford et al., highlights these strengths and challenges, showcasing the potential and limitations of this powerful technique. For more detailed insights, refer to the <a href=""https://arxiv.org/abs/2103.00020"">original CLIP paper</a>.</p><h2>How does a Visual Large Language Model Work</h2><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1226pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/665cbfcfa41944613ae685a0_i9pMvG_1xkmpQ7hJRxhf8ME5aUOPl6V7M5FRl6l6d78d4m5BmmtLBwh3R1zC_w-hLxRcbabZOQPPr3QOy1d3rILxOOeU3TExqJdtZZEgTWRfA1jOSiSA3glvOA23hv4fKLmrAAJ1VJio6CxPIJ8DNL8.png""/></div></figure><p>‍</p><h3>Backbone</h3><p>‍</p><p>The Backbone in a Visual Large Language Model (VLLM) is the fundamental neural network tasked with extracting essential features from input images. Typically, this backbone is a sophisticated convolutional neural network (CNN) like ResNet or a Vision Transformer (ViT). These networks process the image, converting it into a high-dimensional tensor of visual features (Fv), which encapsulates critical spatial and semantic details. These extracted features form the basis for subsequent stages, facilitating the model's ability to interpret and manipulate the visual data effectively for diverse tasks.</p><p>‍</p><h3>Language-Guided Image Tokenizer</h3><p>‍</p><p>The Language-Guided Image Tokenizer  is crucial for integrating visual and textual information within the VLLM framework. This component operates by initially receiving visual features (Fv) from the Backbone and textual features ( Ft ) from a text encoder, often a Transformer model like BERT. Using a cross-attention mechanism, it aligns and combines these modalities, producing language-guided image tokens ( T ). These tokens are enriched with both visual and contextual data, enabling the model to understand and respond accurately to the tasks specified by the accompanying language instructions.</p><p>‍<br/></p><h3>Random Query</h3><p>‍</p><p>The Random Query component represents the VLLM's capability to handle a wide array of tasks flexibly. This feature allows the model to process various vision-only and vision-language tasks dynamically. By introducing randomness, the model can adapt to different inputs and instructions, showcasing its robustness and versatility in generating appropriate outputs. This adaptability is key to the model's performance across diverse applications, enabling it to handle novel and unexpected scenarios effectively.</p><p>‍</p><h3>Language Instructions (\&lt;text\&gt;)</h3><p>‍</p><p>Language Instructions are the natural language prompts that guide the VLLM on what specific tasks to perform. These instructions provide detailed descriptions of the tasks, such as ""Describe the image &lt;image&gt; in detail"" for vision-language tasks or ""For each object in the image &lt;image&gt; that belongs to the class set &lt;class&gt;, output a tuple with the class label and coordinates"" for vision-only tasks. The instructions are parsed into a machine-readable format, directing the model on how to interpret the visual data and generate the required outputs.</p><p>‍</p><h3>Open-Ended Task Decoder with LLM</h3><p>‍</p><p>The Open-Ended Task Decoder with LLM  is the component that interprets the language-guided image tokens ( T ) and generates the final output based on the provided instructions. This decoder utilizes the capabilities of large language models (LLMs) like GPT to process integrated tokens and leverage its extensive language understanding to produce meaningful results. Whether classifying tokens for object detection or generating sequences for tasks like image captioning, this decoder can adapt its outputs to the specified formats, ensuring flexibility and accuracy in addressing a variety of vision-centric tasks.</p><p>‍</p><h3>Desired Output</h3><p>‍</p><p>The Desired Output is the end result produced by the VLLM, tailored to the task defined by the language instructions. This output can take various forms depending on the task, such as class labels and bounding box coordinates for object detection, descriptive text for image captioning, or text-based answers for visual question answering. The ability to generate such a wide range of outputs demonstrates the VLLM's versatility and effectiveness in integrating and processing both visual and textual information to meet diverse application needs.</p><p>‍</p><h2>How to Train a Visual LLM</h2><p>Meta released an amazing <a href=""https://arxiv.org/pdf/2405.17247v1"">guide</a> on training Visual Language Models.</p><p>‍</p><p>Training a Vision-Language Large Model (VLLM) involves several crucial steps to ensure the model effectively associates textual descriptions with visual elements (grounding) while managing computational resources efficiently. Grounding can be enhanced by using bounding box annotations to teach the model where objects are located in the images, employing contrastive learning techniques with negative captioning to distinguish between correct and incorrect text-image pairs, and ensuring high-quality, diverse datasets. Optimizing data quality by pruning low-quality or duplicate entries and improving caption quality with synthetic data generation techniques are also essential steps.</p><p>‍</p><p>Managing GPU resources is critical due to the significant computational requirements for training VLLMs. High-quality datasets reduce the need for extensive compute power, and efficient training techniques like masking and optimized data loading can speed up the process. Leveraging pre-trained models for fine-tuning instead of training from scratch can also help manage costs. Libraries like <a href=""https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html"">torch.compile</a> and <a href=""https://github.com/facebookresearch/xformers"">xformers</a> optimize attention mechanisms, while Fast Forward Computer Vision (FFCV) helps in creating faster-loading data files.</p><p>‍</p><figure class=""w-richtext-align-center w-richtext-figure-type-image""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/665cbfcfe9e4579125284116_PRhWeD29HWsslKnnF7dH-fw4p91dDFn9Svpax1a2xXpxS2wcMaF0lh0he7j8vMH8_EIC2kqGDoxBRQAZuRoC3lrvBturtjMV0jjWq5RIVWH7V8jWtH3qh8Bjt7RH-FzCOSeOuiJ-8Xak_lbbVoxbCIM.png""/></div></figure><p>‍</p><p>Important considerations to keep in mind when training VLMs. Data is one of the most important aspects of training VLMs. Having a diverse and balanced dataset is important for learning good world models that can span enough concepts. It is also important to remove duplicates which occur a lot within large-scale datasets, this will save a lot of compute time and mitigate the risks of memorization. In addition, pruning the data is also an important component since we want to be sure that the captions are indeed related to the image content. Lastly, improving the caption quality is crucial to enhance VLMs performance. Grounding VLMs is another important step to ensure that the VLMs correctly associate words with specific concepts. Two common grounding methods leverage either bounding boxes or negative captions. Lastly, alignment is a much-needed step to ensure that the model is producing answers that are expected from a human point of view.</p><h3>How many GPU’s are required for Training</h3><p>‍</p><p>The compute resources required for training a VLLM significantly influence the budget needed for such projects. Models like CLIP and OpenCLIP have utilized more than 500 GPUs, which equates to costs in the hundreds of thousands of dollars—often inaccessible for most companies or academic labs. However, by using high-quality datasets and leveraging efficient techniques like masking strategies, training a contrastive model like CLIP on hundreds of millions of images from scratch can be done with as few as 64 GPUs, costing around $10K in compute. If the VLM leverages existing pre-trained image or text encoders, or LLMs, the cost of learning a mapping should be much lower.</p><h3>Steps for Training VLLM</h3><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:2167pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/665cc1410e6d0daa66457198_Screenshot%202024-06-03%20002937.png""/></div></figure><h4>Data Preparation</h4><p>Collect and preprocess a diverse set of image-text pairs, ensuring that the dataset is both extensive and varied to cover a wide range of concepts. This includes ensuring high-quality captions using synthetic data generation techniques if necessary. Removing duplicates and low-quality samples from the dataset is crucial to save computational resources and prevent the model from memorizing redundant information, which can degrade its performance and efficiency. Proper data curation and preparation form the foundation for successful VLLM training.</p><p>‍</p><h4>Model Architecture: </h4><p>Choose an appropriate model architecture based on the specific requirements of your task, whether it is contrastive, masking, or generative models. For effective grounding, consider models that leverage bounding boxes to explicitly indicate object locations or those that use negative samples to teach the model to distinguish between correct and incorrect text-image pairs. The choice of architecture should align with the end goals of the VLLM, such as image retrieval, caption generation, or both.</p><h4>Training Process: </h4><p>Implement contrastive learning techniques to align text and image representations effectively. This involves training the model to push the representations of matching image-text pairs closer together while pushing non-matching pairs further apart. Additionally, implement masking strategies to improve training efficiency and model performance by randomly masking parts of the input data and training the model to predict the masked content. Fine-tune pre-trained models to reduce computational costs, leveraging existing knowledge to expedite the training process and achieve better initial performance.</p><p>‍</p><h4>Optimization Techniques: </h4><p>‍</p><p>Apply efficient attention mechanisms and data loading optimizations to ensure that the training process is as fast and effective as possible. Utilize libraries like <a href=""https://pytorch.org/docs/stable/generated/torch.compile.html"">torch.compile</a> and <a href=""https://huggingface.co/docs/diffusers/en/optimization/xformers"">xformers</a>, which offer significant speed improvements for model training. Regularly evaluate the model's performance and adjust hyperparameters as needed to ensure optimal results. Optimizing these aspects can greatly reduce the overall training time and computational costs while maintaining or improving model performance.</p><p>‍</p><h4>Fine-Tuning and Evaluation:</h4><p>‍</p><p>Fine-tune the model on specific downstream tasks to ensure it performs well in practical applications. This involves adjusting the model parameters based on specific task requirements, such as image classification, caption generation, or retrieval tasks. Evaluate the model using benchmarks like zero-shot and retrieval tasks to ensure it generalizes well across different scenarios. Regular performance evaluations help in identifying and addressing potential issues early, ensuring the model is robust and reliable for real-world applications.</p><p>‍</p><h3>Improving Grounding</h3><p>‍</p><p>Grounding in a Visual Large Language Model (VLLM) refers to the process of associating textual descriptions with specific visual elements within an image. This involves identifying and linking parts of the text to corresponding objects or regions in the visual data, enabling the model to understand and interpret images in the context of the provided language. Grounding helps the model make accurate predictions and generate relevant outputs by ensuring that the visual and textual components are meaningfully connected, enhancing tasks like object detection, image captioning, and visual question answering.</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:977pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/665cbfd0826846a47adbd810_-rDUZbSkRx9XgGVPWK-jyksbgMlnuElDh8ff-dpoIuLIag6NarB8BkVnukpM-oDTHBAOKUZCOcIj6ucGE-hK0rwla6pP7rmDysNyYoAz-iC4mRN1zmOI_OOhyE8vFibyiE_lCR9QPdsmnp38svlZKrA.png""/></div></figure><p>‍</p><p>Grounding is a significant challenge in the VLM and generative model literature. It primarily addresses the issue of models not fully understanding text prompts, which can result in ignoring parts of the prompt or generating hallucinated content not present in the prompt. Challenges in grounding include understanding spatial relations (e.g., left or right of an object), handling negations, counting, and recognizing attributes like colors or textures. Although no single method can completely solve grounding issues, several techniques can improve grounding performance.You can also try out the model in this <a href=""https://huggingface.co/spaces/merve/Grounding_DINO_demo"">link</a>.</p><h4>Using Bounding Box Annotations:</h4><p>‍</p><p>Models like X-VLM leverage bounding box annotations, incorporating box regression and Intersection over Union (IoU) loss to accurately locate and align visual concepts with their corresponding textual descriptions. By knowing where objects are in images and the associated captions, the model can better associate text with visual clues, improving grounding. X-VLM is trained on datasets like COCO, Visual Genome, SBU, and Conceptual Captions, using up to 16 million images. This extensive training data with bounding box annotations enables X-VLM to excel in tasks like image-text retrieval, visual reasoning, visual grounding, and image captioning.</p><p>‍</p><h4>Negative Captioning:</h4><p>‍</p><p>Contrastive objectives use negative samples to mitigate collapse, enhance generalization, and improve discriminative feature learning. By contrasting positive pairs (similar or related samples) with negative pairs (dissimilar or unrelated samples), models develop a nuanced understanding of data, grasping underlying patterns that distinguish different classes or categories. Recent works have shown that using negative samples can mitigate various problems in VLMs. For instance, the ARO benchmark evaluates VLMs on their ability to correctly associate images with captions, using negative samples to test the model's understanding of incorrect pairings. This approach has shown that VLMs significantly benefit from the differentiation capabilities fostered by exposure to negative samples, leading to more accurate and contextually aware models. </p><p>‍</p><h2>Recent Research in Visual LLM </h2><h3>LLama 3-v</h3><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/665cbfcfee118657b7e00285_lTceOhxHHRjkNwHVB-Rbr0DaySb744RDHdWoKZzJlWsA0fusdPswlBJdl3IjU9eUINE8uqbGG_pePPul2NZ5yteCY4Amu4GtoeREAqZh5l1ZL7lDyV0eTFFt5nYKChh37qcnCn4Q3GVd_MtFYOqlD2o.png""/></div></figure><p>‍</p><p><a href=""https://github.com/mustafaaljadery/llama3v"">Llama 3-V</a> leverages the SigLIP model to embed visual information, distinguishing itself by employing a pairwise sigmoid loss instead of a contrastive loss. Input images are transformed into patch embeddings, which are aligned with textual tokens via a projection block using self-attention mechanisms. This joint representation, combining visual and textual tokens, is processed through Llama3. Unlike models such as Llava that utilize a single linear layer for image embeddings, Llama 3-V's dual self-attention blocks capture intricate patterns in the data, enabling superior multimodal understanding and performance on various benchmarks. This architecture is particularly optimized for cost-effective training and inference, maintaining high performance with significantly lower computational resources.</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1085pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/665cbfcf254938203309c68a_T2C9Ha5xLRmSWkqibQHftRPzOGrZqyti_K7i6stCnyG2Uz8sdVHqx3LLG7oclaqe9ahjgPXItedELdyN3gNoWRqTnSX3dc2LWBZ_tQKIwmJkk5OWAKrLz-7s1eop_sVar2o2w9cE5R1hBk8plK56_KE.png""/></div></figure><p>‍</p><h3>Visual-Bert</h3><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/665cbfd037e731458becfdba_qlE25By-veWC2QAVdaqZ4Q8K3Nz41wyhX8tSKVFZ_dOB5uinCcLH8yiY0463LUEyFM39APpRaFz6WptA4f2pnsfFvsaG0GF4W0Loi46w2qq4ms03Ap_63I0kipE9WM8ta1F02958atS3KtECpt-Inrg.png""/></div></figure><p>‍</p><p><a href=""https://arxiv.org/pdf/1908.03557"">VisualBERT</a> processes image regions, extracted using an object detector, and treats these regions as visual tokens alongside text tokens. Both text and visual features are embedded into a shared space using token, segment, and position embeddings. These embeddings are then passed through multiple Transformer layers, allowing self-attention mechanisms to align elements of text with corresponding image regions. The model is pre-trained with masked language modeling, predicting masked words using visual context, and sentence-image prediction, determining if a text matches an image. This joint processing enables VisualBERT to capture rich interactions between visual and textual data, making it effective for tasks like Visual Question Answering and Visual Commonsense Reasoning. Unlike CLIP, which uses separate encoders for images and text aligned through contrastive learning, VisualBERT uses a single Transformer for richer interaction and grounding of visual and textual information.</p><p>‍</p><h3>LLava</h3><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1188pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/665cbfcf2207d5ba4c1e79f7_ZgTyfnIri9CL6T79PPyxmjwEALdoBGnxuSVSuOpfwHPbOoDGNNiuOaQM0-fHpzG2eQrilMM4_PxqST-dr0mIRxUOAYd8vUa7OD0HHRQBrPdh1LT_KuSkITViAFPyrPqFyEnEWf1BfkWu-AZYmaoaCjk.png""/></div></figure><p>‍</p><p><a href=""https://arxiv.org/pdf/2304.08485"">The LLaVA (Large Language and Vision Assistant) model</a> combines a pre-trained language model (Vicuna) with a visual encoder (CLIP's ViT-L/14). The visual encoder processes an input image to generate visual features, which are then projected into the language embedding space using a trainable linear layer. These visual tokens are combined with language instruction tokens and fed into the language model to generate responses. Unlike CLIP, which aligns visual and textual representations using contrastive learning, LLaVA directly integrates visual features into the language model's embedding space for end-to-end vision-language tasks.</p><p>‍</p><h3>Idefics 2</h3><p>‍</p><p>The <a href=""https://blog.paperspace.com/idefics2/"">Idefics 2</a> model integrates visual and language processing to generate contextually informed text responses. The architecture comprises three main components: a Vision Encoder, a Vision-Language Connector, and an LLM (Large Language Model) Decoder. The Vision Encoder processes input images to extract high-dimensional visual features. These features are then transformed by the Vision-Language Connector into language embedding tokens that align with the LLM’s word embedding space. This transformation allows the visual data to be seamlessly integrated with textual information. The LLM Decoder takes these integrated tokens, along with any language instructions, to generate coherent text responses.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:498pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/665cbfd0bfdc2a2b6bd3cd46_-Fz7f0uRlibbfefXuL8wZIt22E0vtEH4oo196UxcX-bQSoIBedox9MBl4Xr48VW4UzYuClhTvGwqAsTO3UqIOY832YTx0Qz4jqLLUJm4WE8nbdzAN8rnfsJ4P1zL-KlESxl8rZD5Fr4d1A0f9kilcAk.png""/></div></figure><p>‍</p><p>This approach differs from models like CLIP, which uses separate encoders for images and text to align their representations in a shared embedding space primarily for retrieval tasks. In contrast, Idefics 2 focuses on generating language responses informed by visual data, leveraging a direct transformation layer to bridge the visual and textual modalities effectively. This enables tasks such as describing images, answering visual questions, and generating narratives based on visual inputs.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1878pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/665cc25573fcce1bda73c2f9_Screenshot%202024-06-03%20003356.png""/></div></figure><p>Idefics 2 is an 8 billion parameter vision-language model designed to excel in Optical Character Recognition (OCR) and document data extraction, such as reading bills and invoices. It outperforms larger models with its efficient architecture, supporting image resolutions up to 980 × 980 pixels. The model has been pre-trained on over 6TB of OCR data, enhancing its ability to accurately extract text from images. This makes it particularly effective for automating data entry and managing document workflows, thanks to its improved visual reasoning and document understanding capabilities. You can also try out the model in this <a href=""https://huggingface.co/spaces/HuggingFaceM4/idefics-8b?ref=blog.paperspace.com"">link</a>.</p><p>‍</p><h2>Benchmarking and Evaluation of Vision-Language Large Models (VLLMs)</h2><h3>Common Datasets</h3><p>Standard benchmarks for evaluating Vision-Language Large Models (VLLMs) often utilize widely recognized datasets such as COCO (Common Objects in Context) and Visual Genome. </p><p>‍</p><p>COCO dataset includes over 200,000 labeled images with annotations for object detection, segmentation, and captioning. It is extensively used for evaluating image captioning, object detection, and segmentation tasks. The dataset is detailed in ""<a href=""https://arxiv.org/abs/1405.0312"">Microsoft COCO: Common Objects in Contex</a>t"" by Lin et al. (2014) .</p><p>‍</p><p>Visual Genome  dataset contains over 100,000 images with dense annotations of objects, attributes, and relationships, making it suitable for tasks requiring detailed scene understanding. ""<a href=""https://arxiv.org/abs/1602.07332"">Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations</a>"" by Krishna et al. (2017) describes this dataset .</p><p>‍</p><p>The main evaluation metrics are accuracy, f1 score, exact match.Accuracy measures the proportion of correct predictions made by the model and is commonly used in classification tasks. F1 Score is the harmonic mean of precision and recall, providing a balance between the two metrics, making it particularly useful for imbalanced datasets. Exact Match (EM) Score measures the percentage of predictions that match the ground truth exactly, often used in tasks like question answering and retrieval.</p><h3>Recent Benchmarks</h3><p>‍</p><p>CODIS evaluates a model's ability to disambiguate images based on contextual information. This benchmark assesses how well models can understand and interpret images in context, rather than in isolation. ""<a href=""https://arxiv.org/html/2402.13607v1"">CODIS: A Benchmark for Context-Dependent Image Disambiguation</a>"" by Peng et al. (2022) provides a comprehensive overview of this benchmark .</p><p>‍</p><p>Fine-Grained Visual Concept Recognition benchmark involves recognizing detailed and specific visual concepts within images, often requiring models to differentiate between subtle differences. It tests the model's ability to understand fine-grained details and nuances in visual data. Relevant research includes ""<a href=""https://arxiv.org/abs/2111.06119"">Fine-Grained Recognition: A Survey</a>"" by Wei et al. (2019) .</p><p>‍</p><p>By leveraging these benchmarks and evaluation metrics, researchers can systematically assess the performance of VLLMs, identify areas for improvement, and ensure that models are robust and effective across a variety of tasks and datasets.</p><h2>Transform Your Visual Data with Custom VLLMs</h2><p>If you're looking to leverage the power of Visual Large Language Models (VLLMs) for your business or research needs, <a href=""http://mercity.ai"">Mercity.ai</a> can help you build a custom VLLM tailored to your specific requirements. Whether it's enhancing image classification, improving visual question answering, or integrating sophisticated visual and textual data analysis into your applications, our team of experts is here to assist you every step of the way. Contact Mercity.ai today to learn how we can transform your vision into reality with cutting-edge VLLM technology. <a href=""https://www.mercity.ai/contacts"">Contact us</a> now!</p><p>‍</p></div>"
Advanced Prompt Engineering Techniques,advanced-prompt-engineering-techniques,640f56f76d313b2faa631c11,64e9061e315024ba31ede90b,False,False,Fri Aug 25 2023 19:50:54 GMT+0000 (Coordinated Universal Time),Sun Apr 20 2025 21:40:19 GMT+0000 (Coordinated Universal Time),Sun Apr 20 2025 21:42:08 GMT+0000 (Coordinated Universal Time),"<p id="""">Large Language Models (LLMs) can handle complex tasks like math problems and commonsense reasoning with the help of prompt engineering. LLMs are not inherently capable of performing such complicated tasks. They require guidance and optimization to extend their capabilities and broaden the range of tasks they can perform effectively. It can be achieved through the use of prompts. Prompts can specify the desired output format, provide prior knowledge, or guide the LLM through a complex task. Using advanced prompting techniques like Chain-of-Thought (CoT) prompting can significantly improve problem-solving rates in LLMs.&nbsp;</p><p id="""">‍</p><p id="""">In this article, we will explore the advanced prompt engineering techniques that will help your business gain a competitive advantage.</p><h2 id="""">What is Prompt Design?</h2><p id="""">Prompt design is creating the most effective prompt for a LLM with a clear objective. Crafting a successful prompt requires a deeper understanding. Different LLMs may interpret the same prompt differently, and some may have specific keywords with particular meanings. Also, depending on the task, domain-specific knowledge is crucial in prompt creation. Finding the perfect prompt often involves a trial-and-error process.&nbsp;</p><p id="""">‍</p><p id="""">A prompt has three main types of content: input, context, and examples. The former specifies the information for which the model needs to generate a response. Inputs can take various forms, such as questions, tasks, or entities. The latter two are optional parts of a prompt. Context provides instructions on the model’s behavior. Examples are input-output pairs in the prompt to demonstrate the expected response. They customize the response format and behavior of the model.</p><p id="""">‍</p><p id="""">Common prompt design strategies improve LLMs performance significantly. Include clear and concise instructions to guide the model's behavior effectively. Use examples for desired response patterns to the model to improve results, depending on the model's complexity. Show desired patterns rather than showing what to avoid. Provide partial content as it allows the model to generate the rest, considering examples and context. Include instructions and information to aid the model in problem-solving. Add prefixes to the input or output to provide semantic cues or formatting guidance to the model.</p><h2 id="""">Advanced Prompt Engineering Techniques</h2><p id="""">Advanced Prompt Engineering Techniques are a set of methods for improving the performance of large language models on complex tasks. These techniques involve providing the LLM with more informative and structured prompts, as well as using prior knowledge and logical reasoning to guide the LLM's responses.</p><h3 id="""">Chain-of-Thought (CoT) Prompting</h3><p id=""""><a href=""https://www.mercity.ai/blog-post/guide-to-chain-of-thought-prompting"">Chain-of-Thought prompting (CoT)</a> is a technique that provides the LLM with a sequence of intermediate steps that lead to the desired answer. It improves the reasoning abilities of large language models (LLMs). It allows the model to focus on solving one step at a time, rather than having to consider the entire problem all at once. It can be used for several reasoning tasks, including math word problems, commonsense reasoning, and symbolic manipulation. It can be readily implemented in sufficiently large language models without any special training or fine-tuning of the model. For example, CoT prompting in the PaLM model significantly enhanced performance in the GSM8K benchmark, improving it from 17.9% to 58.1%.</p><p id="""">‍</p><p id="""">Few-shot CoT prompts LLMs with examples of similar problems to improve reasoning abilities. It is more effective than a few-shot baseline but can be more complex to implement.&nbsp; Zero-shot CoT involves adding ""<strong id="""">Let's think step by step</strong>"" to the original prompt. This prompts the LLM to think about the question and come up with a chain of reasoning that leads to the answer. The reasoning is extracted from the LLM's response using a second prompt, “The answer is.” Zero-shot CoT has been shown to outperform other methods for evaluating the zero-shot reasoning abilities of LLMs.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:807px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""807px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64e905906c79e1c629741888_m4kic4K_LHnm9fAit67UUt12NIdx9Th2-9lxXf8HDwyFFYgmyixsPX1xkNbEoMGYQjaHI_W1m4GL6by-J_ckSmRI8AUANe_AMBTG7p0FcyQRvFcOA0LhsFw9KoeXJ7EDkHhpzDLjyCqvmBLEcbSWZlY.png"" id="""" width=""auto"" height=""auto"" loading=""auto"" alt=""""></div></figure><p id="""">‍</p><p id="""">CoT reasoning emerges in LLMs exceeding 100 billion parameters. This ability may stem from large LLMs' training on extensive datasets that include step-by-step reasoning. While instruction-following isn't essential for CoT, it might enhance its quality. Further research is required to fully understand the origins and potential of CoT reasoning in large LLMs. The researchers found that CoT prompting consistently outperformed standard baseline prompting across various linguistic styles, annotators, examples, and language models. It shows its robustness and effectiveness in enhancing language models' performance on diverse tasks. Sensitivity in CoT prompting pertains to how prompt design influences model performance. Well-matched, clear prompts are crucial, especially for complex tasks. Coherence in CoT ensures that reasoning steps follow a logical order. Later steps shouldn't depend on earlier ones, and vice versa. Removing coherence negatively affected system performance.</p><h4 id="""">Self Consistency</h4><p id="""">Self-consistency is a technique for generating multiple diverse chains of thought for the same problem and then training the model to select the most consistent answer among these chains.</p><p id="""">It is used to enhance the performance of language models, especially in tasks requiring multi-step reasoning, like chain-of-thought prompting.&nbsp;</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:841px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""841px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64e905906e1426612ed41eaf_64xSDd7GfXCBj_LMgTbokmgeV-q8axJbJRtbPX9Pr5IuPZkRjF62P1iwmA2eUiQhg3osfG7iwSNy3IoaSStgZIklGZWNq1Z302faCdYRBlqZfEeCMxb6Id1f6MXz3EfdPIzpx6rWEgXXT3OBJLPHS2w.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto"" alt=""""></div></figure><p id="""">‍</p><p id="""">It improves performance of CoT prompting across various benchmarks, such as GSM8K by 17.9%, SVAMP by 11.0%, and AQuA by 12.2%.It's an unsupervised technique that is compatible with pre-trained language models, requiring no extra human annotation, training, fine-tuning, or model changes. It remains robust across different sampling strategies and parameters, consistently enhancing performance. The benefits of self-consistency become more significant as language model scale increases. For example, it contributes up to +23% accuracy improvement for larger models like LaMDA137B and GPT-3. Even for large models that already perform well, self-consistency consistently offers additional gains, such as +12%-18% accuracy improvement on tasks like AQuA and GSM8K over PaLM-540B.</p><p id="""">‍</p><h3 id="""">Tree-of-Thoughts (ToT) Prompting</h3><p id="""">Tree of Thoughts (ToT) is a new framework that extends the Chain-of-Thought approach by allowing language models to explore coherent units of text (""thoughts"") as intermediate steps towards problem solving. ToT enables LMs to make deliberate decisions, consider multiple reasoning paths, and self-evaluate choices. It also allows LMs to look ahead or backtrack when necessary for making global decisions.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:662px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""662px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64e9059119d687d600525557_1VBYRxJjQaXvRsh78hEhK-kGjcE0D1rq1KDgXh7Y5d8TlVvnMvz07rTj2J1MlJMQ6uqLxMRlZExDgwbgt161IA7kolBs-u7D7dQZRRv6ncsFeWeLdlpGtoifLCixGXRzv2NAo6-5Wqh67GqBo2LcBzU.png"" id="""" width=""auto"" height=""auto"" loading=""auto"" alt=""""></div></figure><p id="""">‍</p><p id="""">Tree of Thoughts enhances language models' problem-solving abilities on tasks like Game of 24, Creative Writing, and Mini Crosswords.&nbsp;</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64e905915c6637bdeef57c6a_D1Jk-Dnc7mweEDtlQo5gFN_odit2bZEZoAn58uYRzjSOrJ3l9wKblNWAzxbdWS0bKLst7qU4oqvvS7jiTOh9rdVVZg8onIsgBYgFWmZ_5a068XokovIuelpr19ay1dXoakY_pSV7oo27o3guE1MM5R4.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto"" alt=""""></div></figure><p id="""">‍</p><p id="""">For example, IO, CoT, and CoT-SC perform poorly on the task of solving Game of 24, achieving only 7.3%, 4.0%, and 9.0% success rates, respectively. ToT achieves much better results on this task. ToT with a breadth of b = 1 (meaning that it considers one possible solution at a time) already achieves a success rate of 45%, while b = 5 (meaning that it considers five possible solutions at a time) achieves 74%.&nbsp;</p><p id="""">‍</p><p id="""">ToT is effective in tasks that require non-trivial planning or search. In the average GPT-4 scores for the three methods (ToT, IO, and CoT) across 100 tasks, ToT has the highest average score (7.56), followed by IO (6.19) and CoT (6.93). ToT is able to generate more coherent passages than IO and CoT on average.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:817px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""817px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64e90591bca1079f95b9ad2f_KNwb-blOKBkk3L4s0IvDxUpTa3fbQIN64ZRJJZqpzSTiCTGDV8MIMzB-WlpbJlw7IE_WGs-WCyn0FLPBu25uAQnRPksDmCtAeNBJ52WorziTAhScrzupit4Sfw9ibuvhQiw9geqpKfa10kgZvHy-4ok.png"" id="""" width=""auto"" height=""auto"" loading=""auto"" alt=""""></div></figure><h3 id="""">Active Prompting</h3><p id="""">Active prompting uses uncertainty-based active learning for adapting large language models (LLMs) to different tasks. It works in four stages. The first stage is uncertainty estimation. In this stage, the LLM is queried k times to generate possible answers with intermediate steps for a set of training questions. The uncertainty of each question is then calculated based on the k answers with a method called disagreement. Disagreement measures how much the k answers disagree with each other. The second stage is selection. The most uncertain questions are selected for annotation. The algorithm starts with the most uncertain question and then selects the next most uncertain question that is not already selected. The third stage is annotation. Humans annotate the selected questions with human-designed CoT reasoning. The CoT reasoning provides the LLM with additional information about how to answer the questions. The fourth stage is inference. The LLM is used to infer the answers to the questions. The LLM uses the new annotated exemplars to improve its performance on the questions.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:939px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""939px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64e905916a56211220925f22_qAPsN9NAHmarCiBv3uX56Q2Q1b--X0RrvcnUBquWLYo7mLfHrLqNXzRpfqATdXf8epIy6_QY423aIv_uLVLBVlqz6YhX2sdwHe8IBA_psNPkziHNAoLFxSrVJMvuMARCFYa_q0S_JRfZAzdmeA4tDus.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto"" alt=""""></div></figure><p id="""">‍</p><p id="""">Active prompt achieves the best performance compared with all baseline models. It is the most effective method for improving the performance of large language models (LLMs) on a variety of reasoning tasks.&nbsp;</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1069px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1069px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64e905917ee01a3082efdd7a_46dsLaSyx_8UZLJdv_X33OuR7GZ1rPlOHcV81HmInurNA9Pe2A-3VDFZdfCQcXoxO6mJUjCQw28Lcoz2tpWoO6kxs11nyKhRMWHak7FHlJyRZtFzJ56O-pZV4KjKQyhQkTmeBv62RC_lGO3ZbdI4tdE.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto"" alt=""""></div></figure><p id="""">‍</p><p id="""">It outperforms self-consistency by an average of 2.1% with code-davinci-002 and 7.2% with text-davinci-002. This suggests that Active-Prompt is a more effective way to improve the performance of LLMs than self-consistency, which is a previous method for training LLMs. The largest improvement is observed in GSM8K (4.2%) and AQuA (3.1%). This suggests that Active-Prompt is particularly effective for tasks that do not require the transferability of CoT prompts.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:822px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""822px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64e905924814b74a2bc2fe76_XCS1BqeLyff1dYXU4w4v4HLegmtRNb96kRnPunn-wcv9MTV4-5t9-sHWJuj4tMhPYtfOVpV51e2yLBqPiu8kjwbA_ZyZEx_ktN7SOsXcPDBZIcfd_KDdQwcBL3C-EbwV6I5HUopwWzYRpJbIDyyV1cI.png"" id="""" width=""auto"" height=""auto"" loading=""auto"" alt=""""></div></figure><p id="""">‍</p><h3 id="""">Reasoning WithOut Observation (ReWOO)</h3><p id="""">ReWOO (Reasoning WithOut Observation) is a technique that detaches the reasoning process from external observations, such as the ability to access and process information from the real world. This detachment significantly reduces the amount of tokens that the LLM needs to consume, which in turn improves the efficiency of the LLM. ReWOO divided the workflow into three separate modules: <strong id="""">Planner</strong>, <strong id="""">Worker</strong>, and <strong id="""">Solver</strong>. The Planner takes a question as input and breaks it down into a sequence of steps. Each step is then formulated as a plan. The plans are interdependent, meaning that the output of one plan is used as the input to another plan. The Worker takes a plan as input and retrieves external knowledge from tools to provide evidence. The evidence can be anything from factual information to code snippets. The Solver takes the plans and evidence from the Worker module and synthesizes them to generate the ultimate answer to the initial question.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:775px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""775px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64e905926c79e1c629741a81_KCfgA2nz-kDIzO8zfLkTKwZz8ceaLEbCdsLf9z5-6vUfzRLBBM3P5OXmFmtGoyni27jTVbENwmbqx7K4DeUHzhWo8dwAl-oAkc_FEt8PpNO3IilOBdT3DRFUTOOD0zIcqtLkrtWSM9QwA2_Mlexy27M.png"" id="""" width=""auto"" height=""auto"" loading=""auto"" alt=""""></div></figure><p id="""">‍</p><p id="""">ReWOO was evaluated on six public NLP benchmarks and a curated dataset. It consistently outperformed the baseline methods on all of the benchmarks. For example, on HotpotQA, a multi-step reasoning benchmark, ReWOO achieved 5× token efficiency and 4% accuracy improvement. ReWOO also demonstrated robustness under tool-failure scenarios. It means that ReWOO is still able to perform well even when the external tools that it relies on are not available.&nbsp;</p><p id="""">‍</p><p id="""">ReWOO outperforms ReAct. ReWOO was able to reduce token usage by 64% with an absolute accuracy gain of 4.4%. It is able to elicit more reasoning capabilities from LLMs than ReAct. ReWOO was also found to be more robust to tool failures than ReAct. When tools malfunction and return errors, ReAct-like ALM systems are highly fragile. ReWOO, on the other hand, is less compromised. ReWOO also performed well on the curated dataset, SOTUQA. SOTUQA is a document QA dataset that is more closely aligned with real-world ALM applications than previous public NLP benchmarks.&nbsp;</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:624px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""624px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64e90592a514b40943a960c8_wRiBDHNq8IDdrcvmKuKYEBDWkZp3dBrO64c1eNvnq-oEtN4wztGGieDs-dd6x3ma_DmgLwsQdIyLCG63U76E8JhV-XhU8O03Ucsi4U11rBvwuyxcVhPeAUP0urr9C-LrXLj6WCo9uZeP-YHEQfwqAyQ.png"" id="""" width=""auto"" height=""auto"" loading=""auto"" alt=""""></div></figure><p id="""">‍</p><p id="""">ReWOO decouples parametric modules from nonparametric tool calls. It means that the LLM can be fine-tuned to offload some of its reasoning ability to smaller language models. This offloading can substantially reduce the number of parameters that the LLM needs to store, which further improves the efficiency of the LLM.&nbsp; ReWOO can offload reasoning ability from a 175B GPT3.5 model to a 7B LLaMA model. It has the potential to create truly efficient and scalable ALM systems.&nbsp;</p><h3 id="""">Reason and Act (ReAct)</h3><p id="""">ReAct is a technique that combines reasoning and acting with language models for solving various language reasoning and decision-making tasks. It prompts language models to generate both verbal reasoning traces and actions. It enables dynamic reasoning, high-level planning for acting, and interaction with external environments.&nbsp;</p><p id="""">‍</p><p id="""">Here is our <a href=""https://www.mercity.ai/blog-post/react-prompting-and-react-based-agentic-systems"" id="""">extensive guide on ReAct Prompting.</a></p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:637px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""637px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64e90592219b27430844e4e6_XxnqSuUVzTPkAfR8GjAEs6LuMCGu2nRxQ7pYq6Zx2f7PaPEXQ4S3CLIKgiAQbxH-jKM3bLq_-62cr4oNK_J6LOg0KZMOXDus8G6cSBrAkp5eS9JteJy7NY2TjkdP1b1GwoyrUgmIwEHryn4RhvtZzJE.png"" id="""" width=""auto"" height=""auto"" loading=""auto"" alt=""""></div></figure><p id="""">‍</p><p id="""">It is evaluated on four diverse benchmarks, including question answering (HotPotQA), fact verification (Fever), text-based games (ALFWorld), and web page navigation (WebShop). On HotpotQA and Fever, ReAct was able to overcome prevalent issues of hallucination and error propagation in chain-of-thought reasoning. It also outperformed imitation and reinforcement learning methods with an improved 34% and 10% on ALFWorld and WebShop. This is because ReAct is able to learn from human examples and apply that knowledge to new situations.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:805px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""805px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64e9059237d55fa07702fbfc_eQxMaqYdOAIHxGegYcDnNf180f1nzGIitpevWKD0JajlPrYq4-ATzELSaMUYY7YJax389iyer-PfF3sEM7BdKenShmhnDgxZR-c5wAGqrbf6JjKGuzH9LK_cXwyDTstE18OpbBdXamVYyo_oidsFLQU.png"" id="""" width=""auto"" height=""auto"" loading=""auto"" alt=""""></div></figure><p id="""">‍</p><p id="""">ReAct is designed to be intuitive, general, performant, and robust. It is applicable to diverse tasks, including question answering, fact verification, text games, and web navigation. It provides an interpretable decision-making and reasoning process, allowing humans to inspect reasoning, factual correctness, and even control or correct the agent's behavior during task execution.</p><h3 id="""">Reflection</h3><p id="""">Reflexion is a framework that uses linguistic feedback to reinforce language agents. Linguistic feedback is feedback that is expressed in natural language. Reflexion agents learn to reflect on task feedback signals, and then maintain their own reflective text in an episodic memory buffer. This reflective text is then used to induce better decision-making in subsequent trials.&nbsp; The Reflexion framework uses self-reflection. It generates verbal self-reflections to provide more informative feedback. These self-reflections are then stored in the agent's memory. The agent can then use this information to improve its performance on future trials.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:435px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""435px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64e9059219d687d60052562a_EBLL_IcwgJMdMmDRajNDBc2MLxpcIB63P9M4l2Skg6x49qsVkZTJKVJil7uIHTHAE9HOVCSRp0QS8voiFbFfQRZLTtJ8Tt7voUJAgO7ne1vQluOy3q1r3dVuToelCxdAhQfHBT_qHPCpHUYH_eGjNzo.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto"" alt=""""></div></figure><p id="""">Reflexion is flexible enough to incorporate various types and sources of feedback signals. For example, feedback signals can be scalar values (such as rewards or punishments), or they can be free-form language. Feedback signals can also be external (provided by a human or another agent), or they can be internally simulated (generated by the agent itself).</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:825px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""825px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64e905936c79e1c629741ad9_wTeI4ICjWtjoufPc7LuNjEZmp5Lf26cMbRs_ajVMMh4FcUfmN9KWJT7ijCUdjWDarIbb3e8OBcKyq1SDKXoj-08gQ9XHtkG_y1dLYu5a_a_EH6igpTZpKpFJtDwgFlTPVMVuYp1JG6TD8skYPle97Ck.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto"" alt=""""></div></figure><p id="""">‍</p><p id="""">Reflexion agents outperforms strong baseline approaches in decision-making tasks, reasoning tasks, and programming tasks . In decision-making tasks (AlfWorld), Reflexion agents improve by 22% over 12 iterative learning steps. In reasoning questions (HotPotQA), Reflexion agents show a 20% improvement. In Python programming tasks (HumanEval), Reflexion agents achieve an improvement of up to 11%. It achieves a 91% pass@1 accuracy on the HumanEval, surpassing the previous state-of-the-art GPT-4 that achieves 80%.</p><p id="""">‍</p><h3 id="""">Expert Prompting</h3><p id="""">Expert Prompting is an augmented strategy for instructing Large Language Models (LLMs). It envisions a distinguished expert agent tailored to each specific instruction. LLMs are asked to answer instructions conditioned on the identity of the envisioned expert. It is an automatic prompting method. Expert identities are generated using In-Context Learning. It requires writing several instruction-expert pair exemplars. The generated expert identities are found to be satisfactory.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:517px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""517px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64e90593e818b44181366f49_XN06W861ViSc3LoJ3a50snCTmW-InnZqgWlxfjsDC1Bz02lQf75iUT9fwEMqs5ugFmZrsDX35JvMEVmTt6T6g5BySKCd_MQN6dFyivY8ErS7qISX02HKMA7nfJ8n8Ad5HC_tqmAv4HOBkyDZxhgJiPg.png"" id="""" width=""auto"" height=""auto"" loading=""auto"" alt=""""></div></figure><p id="""">‍</p><p id="""">Expert Prompting is a generalized prompting method. Expert identities are defined with detailed and elaborate descriptions. It can match instructions in various domains or genres. It's adaptable to different areas, such as nutrition or physics. It is simple to implement. It doesn't require complex crafting of prompt templates or iterative processes. Writing good expert identity is critical. It should be specialized, detailed, and comprehensive for each instruction. The descriptions must be automatically generated to be practical and efficient.&nbsp;</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:736px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""736px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64e90593fae4f49204b2b610_KUBueS5mV1XOV70kmD9FQiD7vGNrJS0mbQLZHldjADXrraG2w0W-1BloBvPRcC5nVhZ8GB0HxHmX6xYOBuoWvhoz5l0FKDtYMVMuMd0WgPca1ApvIVYXhDhzIK6snDm4rJinLPghbzBv6ml3sEZzQ2I.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto"" alt=""""></div></figure><h2 id="""">Automatic Prompt Engineering (APE)</h2><p id="""">APE is a technique that treats the instruction as the “program,” and it optimizes the instruction by searching over a pool of instruction candidates proposed by an LLM. The LLM candidates are scored using a chosen score function, and the instruction with the highest score is selected. APE is inspired by classical program synthesis and the human approach to prompt engineering. Program synthesis is the task of automatically generating code from a natural language description of the desired behavior. The human approach is the process of manually crafting instructions effective at using LLMs to produce desired outputs.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1054px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1054px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64e9059337d55fa07702fc6f_9YNNrp_6W_08wnDcNFfhoFnYNbqSKZKAFUHCKR9jHhLbzMjbPzk1CrNhkN8q0PGP7mh87bSbMmSCB-P3c7KASgrgsci88rb9mK_BIC8iG2_MgMFlMZTkTmDsBxOyUi3e8EOeSuTV6ushGZRg8SACcns.png"" id="""" width=""auto"" height=""auto"" loading=""auto"" alt=""""></div></figure><p id="""">‍</p><p id="""">APE achieves human-level performance on zero-shot learning with model-generated instructions on 24/24 Instruction Induction and 17/21 Big-Bench tasks. It surpasses human performance with the InstructGPT model, obtaining an IQM of 0.810 compared to humans' 0.749. To achieve this, a dataset of questions and reasoning steps is generated using InstructGPT with the prompt ""Let's think step by step."" Then any data points that had incorrect answers were removed. Finally, APE was used to find a prompt starting with ""Let's"" that maximized the likelihood of these correct reasoning steps. APE produced the prompt ""Let's work this out in a step-by-step way to be sure we have the right answer."" This generated prompt further improved performance on two tasks: MultiArith from 78.7 to 82.0, and GSM8K from 40.7 to 43.0.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:927px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""927px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64e905935c6637bdeef5803b_7boon2MJdaMcF0lxz7ALQ4z5WzIT9uu2--DiZLnlaIkfj2aFu_fLPkCj81m3ouFwy52mOIdIfqq5UqEJn9r2ZrFqLMNDY1W_-xUpxsevtcg4DblsBXoJgipnO3ZXjbGVBgvCX5skxg71h2IAYbqrtkE.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto"" alt=""""></div></figure><p id="""">‍</p><h3 id="""">Auto-CoT</h3><p id="""">Auto-CoT is a process of automatically constructing demonstrations with questions and reasoning chains. It first clusters the questions in a dataset into a few clusters. Then, it selects a representative question from each cluster and generates its reasoning chain using Zero-Shot-CoT with simple heuristics. The Auto-CoT method has several advantages over other methods. It is automatic, scalable, and effective, which means that it generates demonstrations that are accurate and informative.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:960px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""960px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64e90593acc59c457830a0b9_-E0ZSyqcr1bkOFU47sI3ERrhUuqYRkJAy1JRvWQFIqiguA-jqZaM8A-HVTk6_RUnuEFbZmQNcsFHVxoJnwjdT_-xqmiUssbKzCiEi9oAJqSW3dKgugGyfGBU1Ucy4O3XzKGo5m38V4VXVfsbiA3LtXQ.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto"" alt=""""></div></figure><p id="""">‍</p><p id="""">On comparing the accuracy of Auto-CoT with the four baseline methods on ten datasets from three categories of reasoning tasks,&nbsp; Auto-CoT consistently matches or exceeds the performance of the CoT that requires manual designs of demonstrations. The reason for this is that Auto-CoT is able to generate demonstrations that are task-adaptive. It means that the demonstrations are tailored to the specific dataset and reasoning task. In contrast, Manual-CoT may use the same demonstrations for multiple datasets, which can lead to lower accuracy.&nbsp;</p><p id="""">‍</p><h3 id="""">Automatic Multi-step Reasoning and Tool-use (ART)</h3><p id="""">ART is a framework that uses large language models to automatically generate intermediate reasoning steps for a new task. The LLMs are frozen, which means that they are not updated during the reasoning process. It allows ART to be more efficient and scalable than frameworks that use trainable LLMs.&nbsp; ART selects demonstrations of multistep reasoning and tool use from a task library. A decomposition is a high-level description of the steps involved in solving a task. ART then selects and uses tools in the tool library alongside LLM generation to complete the intermediate reasoning steps. At test time, ART seamlessly pauses generation whenever external tools are called, and integrates their output before resuming generation. This allows ART to leverage the capabilities of external tools to solve complex tasks.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:426px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""426px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64e905947ee01a3082efdf2c_8V4rX8ymLZMkTMBahv7kuBiLRzAyC-fgM761YqerYUAQD_q7kmSZ0nUaCAPrjWpQkANpC8BQULEWzk4iNMfyOfyHcTBWkgYO1i6UNGf39zFWtZxO5bbAEDyTDY24HP4eDxydFYU78vXe7JJnxqFYhxQ.png"" id="""" width=""auto"" height=""auto"" loading=""auto"" alt=""""></div></figure><p id="""">‍</p><p id="""">ART has been shown to be effective on a variety of tasks, including natural language inference, question answering, and code generation. It outperforms previous approaches to few-shot reasoning and tool-use, and it is able to solve tasks that were previously thought to be impossible for LLMs. Humans can optionally edit decompositions to improve performance. For example, they can correct errors in code or incorporate new tools. ART is extensible, which means that it can be easily extended to include new tasks and tools.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:940px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""940px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64e905949975c2126a118627_GQlu6j17prD9WnCNRn4Cx7gDwirx1PuOmPSKT3hkqzaXClyM0YH-5FljiBo4TrLsPTUovkziwOH7s8ttlCqri458nITRaJ2yW3UG7JwvpUjTT2MsWXjwPU2al8CpnfJzVaww2Dk5YgGqGi_nEMbMkj8.png"" id="""" width=""auto"" height=""auto"" loading=""auto"" alt=""""></div></figure><p id="""">‍</p><p id="""">ART consistently matched or outperformed automatically generated CoT reasoning chains on 32 out of 34 BigBench tasks and all MMLU tasks. On average, it achieved an improvement of over 22 percentage points. The use of tools in ART significantly enhanced performance on test tasks, with an average improvement of over 12.3 percentage points compared to scenarios where no tools were allowed. ART also improved over direct few-shot prompting by an average of 10.8 percentage points across unseen BigBench and MMLU tasks. Its improvements were particularly remarkable in tasks requiring arithmetic and algorithmic reasoning, where it outperformed direct few-shot prompting by 12.5%. ART also surpassed previous best-known results for GPT3, which use supervision for decomposition and/or tool use, by 6.1 percentage points. ART allows for human intervention and performance improvement by updating the task and tool libraries with new demonstrations. With additional human feedback, ART surpassed the best-known results for GPT3 by an average of over 20 percentage points on 12 test tasks.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:741px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""741px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64e90594bca1079f95b9aec7_5JLbRJJ0HDgdZhmtSbSqt_sZZ6-eog6rvojESX99u3QdpXJCYYOXESsCtc-uAeFL0j3NkRSafjOTRA6-wt2fOC9qv1x-9lZhei8RXq5UJjhqr35T6rEycF8Dk3BYs1hNvvxUjlf7WDG54dsuQmOlaPU.png"" id="""" width=""auto"" height=""auto"" loading=""auto"" alt=""""></div></figure><p id="""">‍</p><h2 id="""">Advanced Prompt Engineering Strategies</h2><p id="""">You can enhance your prompts with some effective prompting strategies, such as temperature and token control, prompt chaining, multi-turn conversations, and more. Temperature and token control fine-tune language model behavior. Temperature adjusts randomness, with higher values promoting creativity. Lower temperature refines responses for precision. Token control sets response length, useful for brevity.&nbsp;</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1128px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1128px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64e90594760e2a6c3003546c_Q54lqzPNmrYVUz-0mzYG-Dx0KcO9NMp9DS2WQW4rFLHkUKunKeBFkQTJM2EfLaKZaLt4F_oifU7eYVT-NFOI-Z6ByB7ACNDevanvcjkZl_1TfuwqcMX6F2DUz66nRqdAyN3WdzI-HOGJl9Bp2_SKG_M.png"" id="""" width=""auto"" height=""auto"" loading=""auto"" alt=""""></div></figure><p id="""">‍</p><p id="""">Prompt chaining is the practice of connecting multiple prompts together to create a continuous flow of conversation by referencing previous inputs or the language model's previous responses in each prompt. Multi-turn conversations are conversations that consist of multiple exchanges between the user and the language model by the user providing multiple prompts, or by the language model providing multiple responses. Multi-turn conversations allow for a more detailed and nuanced conversation, as the user and the language model can build on each other's contributions. For example, to engage in a detailed discussion, users could chain prompts together to explore a topic in depth. The language model could then provide different perspectives on the topic, allowing for a more nuanced and informative discussion.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1069px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1069px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64e905946c79e1c629741c9b_vAKdwbozIcDMgKlldo2p43TE7ETWHoXsUe2f5pIGwynapDT0L19qtXhhgbHjQla3PRihLlYKSswg8KD1SmXJByjjEismrOEbUktgBV3XSrvXu4FCO9SfUgfBBv6b82C0j_MeJt5JQ7UNe8IoxFHAyzc.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto"" alt=""""></div></figure><p id="""">Also, tailoring prompts to specific industries or fields ensures relevant responses from LLMs, building user trust and encouraging future use. Domain-specific prompts enable better context understanding and accuracy, as LLMs are trained on domain-relevant text. This enhances the overall user experience, leading to greater satisfaction. The ability to handle unclear or contradicting user inputs can improve prompting in any LLM by ensuring that the model is able to understand the user's request and generate a relevant and informative response. It involves actively engaging with the user and seeking clarification, using natural language understanding to identify the user's intent, and generating multiple responses. For example, if the user asks ""I'm looking for a restaurant,"" the chatbot could generate responses that recommend restaurants based on the user's location, budget, or dietary restrictions.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1056px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1056px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64e905947ee01a3082efe07d_MiZDJqvGX_Nkb3K7u7yqBssQxISmNBhCj4r7glTh1iPuY4dObOJaOPcWx_0zH1Yb5dY2YyHU9j5-MrLmDoD84D0z_ocA4W2E8qwDCw0waQROOWZE7dmaSsI4Y4TnWJOs_DS6qKrUB26wTISWuWoN2cM.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto"" alt=""""></div></figure><h2 id="""">Tools For Implementing Prompting Techniques</h2><p id="""">Tools like Langchain, Guidance AI, Semantic Kernel, and Auto-GPT make it easier for us to interact with language models. These powerful tools offer innovative solutions for crafting more effective and context-aware prompts and enhancing the capabilities of language models.</p><h3 id="""">Langchain</h3><p id=""""><a href=""https://github.com/langchain-ai/langchain"" id="""">Langchain</a> is a versatile framework for building data-aware and agentic applications using language models. It was launched in October 2022 by Harrison Chase at Robust Intelligence. Langchain provides standard and extendable interfaces for modules like models, prompts, memory, indexes, chains, agents, and callbacks. This makes it easy to build applications that use language models for a wide range of tasks.</p><p id="""">‍</p><p id="""">Langchain integrates with a variety of tools and cloud storage systems, including cloud storage systems like Amazon, Google, and Microsoft Azure. It has API wrappers for news, movie information, and weather and Bash for summarization, syntax checking, and script execution.&nbsp; Langchain supports web scraping with multiple subsystems and templates. It facilitates few-shot learning prompt generation. It interacts with Google Drive documents, spreadsheets, and presentations. Langchain performs web searches using Google Search and Microsoft Bing. It integrates with OpenAI, Anthropic, and Hugging Face language models. Langchain generates, analyzes, and debugs Python and JavaScript code. It utilizes the Weaviate vector database for caching embedding and data objects. Langchain integrates with various databases, performs text mapping, and supports time zone conversions.</p><h3 id="""">Semantic Kernel</h3><p id=""""><a href=""https://github.com/microsoft/semantic-kernel"" id="""">Semantic Kernel</a> is an open-source SDK that makes it easy to integrate AI services like OpenAI, Azure OpenAI, and Hugging Face with traditional programming languages like C# and Python. It provides a set of connectors that make it easy to add memories and models to your apps, giving them a simulated ""brain."" Semantic Kernel also provides a set of AI plugins that allow your apps to interact with the real world via prompts and native functions. These plugins are like the ""body"" of your AI app.&nbsp;</p><p id="""">‍</p><p id="""">It focuses on avoiding software bloat. It employs a planner to break down tasks and interlock parts, turning user queries into desired outcomes. SK enables integration of LLMs with traditional programming by combining natural language semantics with code functions. SK uses embeddings-based memory for enhanced application capabilities, supporting prompt engineering, chaining, retrieval-augmented generation, and more. SK offers contextual and long-term vectorized memory, allowing access to external knowledge stores and proprietary data. SK incorporates design patterns from AI research for intelligent planning and reasoning.</p><h3 id="""">Guidance AI</h3><p id=""""><a href=""https://github.com/guidance-ai/guidance"" id="""">Guidance</a> by Microsoft is a templating language for controlling large language models (LLMs). It supports a variety of prompt engineering techniques and is well-suited for use with powerful LLMs like GPT-4. Guidance offers efficient and effective control of LLMs by integrating generation, prompting, and logical control in a continuous flow, which matches how LLMs process text. It provides a simple and intuitive syntax based on Handlebars templating. It can be used to create rich output structures with multiple generations, selections, conditionals, and tool use.&nbsp;</p><p id="""">‍</p><p id="""">It offers a playground-like streaming experience in Jupyter/VSCode Notebooks, making it easy to experiment with different prompts and parameters. Smart seed-based generation caching is supported for optimization, which can significantly speed up the generation process. Guidance is compatible with role-based chat models like ChatGPT, and it seamlessly integrates with Hugging Face models. Guidance offers a number of features that can improve the performance and usability of Hugging Face models, such as guidance acceleration, token healing, and regex pattern guides.</p><h3 id="""">Auto-GPT</h3><p id=""""><a href=""https://github.com/Significant-Gravitas/Auto-GPT"" id="""">Auto-GPT</a> is an experimental, open-source application that demonstrates the capabilities of the GPT-4 language model. It is a popular tool for designing LLM agents, chaining together LLM thoughts to autonomously achieve user-defined goals. Auto-GPT showcases the potential of GPT-4 to operate autonomously, with key features that include internet access for searches, long-term and short-term memory management, and the ability to use GPT-4 instances for text generation. Auto-GPT supports file storage and summarization using GPT-3.5. The application is extensible with plugins.</p><p id="""">‍</p><p id="""">Auto-GPT is an AI agent that can achieve goals set in natural language. It breaks down goals into sub-tasks and uses internet resources and tools to complete them. It can operate autonomously without requiring manual commands. Auto-GPT can create and revise its own prompts, and it can manage memory by reading from and writing to databases and files. However, Auto-GPT cannot modify the underlying GPT models, as they are proprietary, and it does not typically access its own base system code.</p><p id="""">‍</p><h2 id="""">Want to write high quality prompts for LLMs?</h2><p id="""">We are a team of researchers and engineers who have been working on AI for very long. We have written may prompts, some as long as 500 words. If you are looking to improve performance of your prompts or setup monitoring systems for your language models, <a href=""https://www.mercity.ai/contacts"" id="""">reach out </a>and we’ll be happy to help!</p><p id="""">‍</p>",https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64e90503bdf530d40305d0a6_prompt%20engineering.png,Maithili Badhan,Prompt Engineering,"In this post we review advanced prompt engineering techniques like chain of thought (CoT) prompting, self consistency, ReAct, ToT, etc for production grade prompts.",False,"<div class=""rich-text w-richtext""><p>Large Language Models (LLMs) can handle complex tasks like math problems and commonsense reasoning with the help of prompt engineering. LLMs are not inherently capable of performing such complicated tasks. They require guidance and optimization to extend their capabilities and broaden the range of tasks they can perform effectively. It can be achieved through the use of prompts. Prompts can specify the desired output format, provide prior knowledge, or guide the LLM through a complex task. Using advanced prompting techniques like Chain-of-Thought (CoT) prompting can significantly improve problem-solving rates in LLMs. </p><p>‍</p><p>In this article, we will explore the advanced prompt engineering techniques that will help your business gain a competitive advantage.</p><h2>What is Prompt Design?</h2><p>Prompt design is creating the most effective prompt for a LLM with a clear objective. Crafting a successful prompt requires a deeper understanding. Different LLMs may interpret the same prompt differently, and some may have specific keywords with particular meanings. Also, depending on the task, domain-specific knowledge is crucial in prompt creation. Finding the perfect prompt often involves a trial-and-error process. </p><p>‍</p><p>A prompt has three main types of content: input, context, and examples. The former specifies the information for which the model needs to generate a response. Inputs can take various forms, such as questions, tasks, or entities. The latter two are optional parts of a prompt. Context provides instructions on the model’s behavior. Examples are input-output pairs in the prompt to demonstrate the expected response. They customize the response format and behavior of the model.</p><p>‍</p><p>Common prompt design strategies improve LLMs performance significantly. Include clear and concise instructions to guide the model's behavior effectively. Use examples for desired response patterns to the model to improve results, depending on the model's complexity. Show desired patterns rather than showing what to avoid. Provide partial content as it allows the model to generate the rest, considering examples and context. Include instructions and information to aid the model in problem-solving. Add prefixes to the input or output to provide semantic cues or formatting guidance to the model.</p><h2>Advanced Prompt Engineering Techniques</h2><p>Advanced Prompt Engineering Techniques are a set of methods for improving the performance of large language models on complex tasks. These techniques involve providing the LLM with more informative and structured prompts, as well as using prior knowledge and logical reasoning to guide the LLM's responses.</p><h3>Chain-of-Thought (CoT) Prompting</h3><p><a href=""https://www.mercity.ai/blog-post/guide-to-chain-of-thought-prompting"">Chain-of-Thought prompting (CoT)</a> is a technique that provides the LLM with a sequence of intermediate steps that lead to the desired answer. It improves the reasoning abilities of large language models (LLMs). It allows the model to focus on solving one step at a time, rather than having to consider the entire problem all at once. It can be used for several reasoning tasks, including math word problems, commonsense reasoning, and symbolic manipulation. It can be readily implemented in sufficiently large language models without any special training or fine-tuning of the model. For example, CoT prompting in the PaLM model significantly enhanced performance in the GSM8K benchmark, improving it from 17.9% to 58.1%.</p><p>‍</p><p>Few-shot CoT prompts LLMs with examples of similar problems to improve reasoning abilities. It is more effective than a few-shot baseline but can be more complex to implement.  Zero-shot CoT involves adding ""<strong>Let's think step by step</strong>"" to the original prompt. This prompts the LLM to think about the question and come up with a chain of reasoning that leads to the answer. The reasoning is extracted from the LLM's response using a second prompt, “The answer is.” Zero-shot CoT has been shown to outperform other methods for evaluating the zero-shot reasoning abilities of LLMs.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:807pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64e905906c79e1c629741888_m4kic4K_LHnm9fAit67UUt12NIdx9Th2-9lxXf8HDwyFFYgmyixsPX1xkNbEoMGYQjaHI_W1m4GL6by-J_ckSmRI8AUANe_AMBTG7p0FcyQRvFcOA0LhsFw9KoeXJ7EDkHhpzDLjyCqvmBLEcbSWZlY.png""/></div></figure><p>‍</p><p>CoT reasoning emerges in LLMs exceeding 100 billion parameters. This ability may stem from large LLMs' training on extensive datasets that include step-by-step reasoning. While instruction-following isn't essential for CoT, it might enhance its quality. Further research is required to fully understand the origins and potential of CoT reasoning in large LLMs. The researchers found that CoT prompting consistently outperformed standard baseline prompting across various linguistic styles, annotators, examples, and language models. It shows its robustness and effectiveness in enhancing language models' performance on diverse tasks. Sensitivity in CoT prompting pertains to how prompt design influences model performance. Well-matched, clear prompts are crucial, especially for complex tasks. Coherence in CoT ensures that reasoning steps follow a logical order. Later steps shouldn't depend on earlier ones, and vice versa. Removing coherence negatively affected system performance.</p><h4>Self Consistency</h4><p>Self-consistency is a technique for generating multiple diverse chains of thought for the same problem and then training the model to select the most consistent answer among these chains.</p><p>It is used to enhance the performance of language models, especially in tasks requiring multi-step reasoning, like chain-of-thought prompting. </p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:841pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64e905906e1426612ed41eaf_64xSDd7GfXCBj_LMgTbokmgeV-q8axJbJRtbPX9Pr5IuPZkRjF62P1iwmA2eUiQhg3osfG7iwSNy3IoaSStgZIklGZWNq1Z302faCdYRBlqZfEeCMxb6Id1f6MXz3EfdPIzpx6rWEgXXT3OBJLPHS2w.jpeg""/></div></figure><p>‍</p><p>It improves performance of CoT prompting across various benchmarks, such as GSM8K by 17.9%, SVAMP by 11.0%, and AQuA by 12.2%.It's an unsupervised technique that is compatible with pre-trained language models, requiring no extra human annotation, training, fine-tuning, or model changes. It remains robust across different sampling strategies and parameters, consistently enhancing performance. The benefits of self-consistency become more significant as language model scale increases. For example, it contributes up to +23% accuracy improvement for larger models like LaMDA137B and GPT-3. Even for large models that already perform well, self-consistency consistently offers additional gains, such as +12%-18% accuracy improvement on tasks like AQuA and GSM8K over PaLM-540B.</p><p>‍</p><h3>Tree-of-Thoughts (ToT) Prompting</h3><p>Tree of Thoughts (ToT) is a new framework that extends the Chain-of-Thought approach by allowing language models to explore coherent units of text (""thoughts"") as intermediate steps towards problem solving. ToT enables LMs to make deliberate decisions, consider multiple reasoning paths, and self-evaluate choices. It also allows LMs to look ahead or backtrack when necessary for making global decisions.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:662pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64e9059119d687d600525557_1VBYRxJjQaXvRsh78hEhK-kGjcE0D1rq1KDgXh7Y5d8TlVvnMvz07rTj2J1MlJMQ6uqLxMRlZExDgwbgt161IA7kolBs-u7D7dQZRRv6ncsFeWeLdlpGtoifLCixGXRzv2NAo6-5Wqh67GqBo2LcBzU.png""/></div></figure><p>‍</p><p>Tree of Thoughts enhances language models' problem-solving abilities on tasks like Game of 24, Creative Writing, and Mini Crosswords. </p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64e905915c6637bdeef57c6a_D1Jk-Dnc7mweEDtlQo5gFN_odit2bZEZoAn58uYRzjSOrJ3l9wKblNWAzxbdWS0bKLst7qU4oqvvS7jiTOh9rdVVZg8onIsgBYgFWmZ_5a068XokovIuelpr19ay1dXoakY_pSV7oo27o3guE1MM5R4.jpeg""/></div></figure><p>‍</p><p>For example, IO, CoT, and CoT-SC perform poorly on the task of solving Game of 24, achieving only 7.3%, 4.0%, and 9.0% success rates, respectively. ToT achieves much better results on this task. ToT with a breadth of b = 1 (meaning that it considers one possible solution at a time) already achieves a success rate of 45%, while b = 5 (meaning that it considers five possible solutions at a time) achieves 74%. </p><p>‍</p><p>ToT is effective in tasks that require non-trivial planning or search. In the average GPT-4 scores for the three methods (ToT, IO, and CoT) across 100 tasks, ToT has the highest average score (7.56), followed by IO (6.19) and CoT (6.93). ToT is able to generate more coherent passages than IO and CoT on average.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:817pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64e90591bca1079f95b9ad2f_KNwb-blOKBkk3L4s0IvDxUpTa3fbQIN64ZRJJZqpzSTiCTGDV8MIMzB-WlpbJlw7IE_WGs-WCyn0FLPBu25uAQnRPksDmCtAeNBJ52WorziTAhScrzupit4Sfw9ibuvhQiw9geqpKfa10kgZvHy-4ok.png""/></div></figure><h3>Active Prompting</h3><p>Active prompting uses uncertainty-based active learning for adapting large language models (LLMs) to different tasks. It works in four stages. The first stage is uncertainty estimation. In this stage, the LLM is queried k times to generate possible answers with intermediate steps for a set of training questions. The uncertainty of each question is then calculated based on the k answers with a method called disagreement. Disagreement measures how much the k answers disagree with each other. The second stage is selection. The most uncertain questions are selected for annotation. The algorithm starts with the most uncertain question and then selects the next most uncertain question that is not already selected. The third stage is annotation. Humans annotate the selected questions with human-designed CoT reasoning. The CoT reasoning provides the LLM with additional information about how to answer the questions. The fourth stage is inference. The LLM is used to infer the answers to the questions. The LLM uses the new annotated exemplars to improve its performance on the questions.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:939pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64e905916a56211220925f22_qAPsN9NAHmarCiBv3uX56Q2Q1b--X0RrvcnUBquWLYo7mLfHrLqNXzRpfqATdXf8epIy6_QY423aIv_uLVLBVlqz6YhX2sdwHe8IBA_psNPkziHNAoLFxSrVJMvuMARCFYa_q0S_JRfZAzdmeA4tDus.jpeg""/></div></figure><p>‍</p><p>Active prompt achieves the best performance compared with all baseline models. It is the most effective method for improving the performance of large language models (LLMs) on a variety of reasoning tasks. </p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1069pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64e905917ee01a3082efdd7a_46dsLaSyx_8UZLJdv_X33OuR7GZ1rPlOHcV81HmInurNA9Pe2A-3VDFZdfCQcXoxO6mJUjCQw28Lcoz2tpWoO6kxs11nyKhRMWHak7FHlJyRZtFzJ56O-pZV4KjKQyhQkTmeBv62RC_lGO3ZbdI4tdE.jpeg""/></div></figure><p>‍</p><p>It outperforms self-consistency by an average of 2.1% with code-davinci-002 and 7.2% with text-davinci-002. This suggests that Active-Prompt is a more effective way to improve the performance of LLMs than self-consistency, which is a previous method for training LLMs. The largest improvement is observed in GSM8K (4.2%) and AQuA (3.1%). This suggests that Active-Prompt is particularly effective for tasks that do not require the transferability of CoT prompts.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:822pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64e905924814b74a2bc2fe76_XCS1BqeLyff1dYXU4w4v4HLegmtRNb96kRnPunn-wcv9MTV4-5t9-sHWJuj4tMhPYtfOVpV51e2yLBqPiu8kjwbA_ZyZEx_ktN7SOsXcPDBZIcfd_KDdQwcBL3C-EbwV6I5HUopwWzYRpJbIDyyV1cI.png""/></div></figure><p>‍</p><h3>Reasoning WithOut Observation (ReWOO)</h3><p>ReWOO (Reasoning WithOut Observation) is a technique that detaches the reasoning process from external observations, such as the ability to access and process information from the real world. This detachment significantly reduces the amount of tokens that the LLM needs to consume, which in turn improves the efficiency of the LLM. ReWOO divided the workflow into three separate modules: <strong>Planner</strong>, <strong>Worker</strong>, and <strong>Solver</strong>. The Planner takes a question as input and breaks it down into a sequence of steps. Each step is then formulated as a plan. The plans are interdependent, meaning that the output of one plan is used as the input to another plan. The Worker takes a plan as input and retrieves external knowledge from tools to provide evidence. The evidence can be anything from factual information to code snippets. The Solver takes the plans and evidence from the Worker module and synthesizes them to generate the ultimate answer to the initial question.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:775pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64e905926c79e1c629741a81_KCfgA2nz-kDIzO8zfLkTKwZz8ceaLEbCdsLf9z5-6vUfzRLBBM3P5OXmFmtGoyni27jTVbENwmbqx7K4DeUHzhWo8dwAl-oAkc_FEt8PpNO3IilOBdT3DRFUTOOD0zIcqtLkrtWSM9QwA2_Mlexy27M.png""/></div></figure><p>‍</p><p>ReWOO was evaluated on six public NLP benchmarks and a curated dataset. It consistently outperformed the baseline methods on all of the benchmarks. For example, on HotpotQA, a multi-step reasoning benchmark, ReWOO achieved 5× token efficiency and 4% accuracy improvement. ReWOO also demonstrated robustness under tool-failure scenarios. It means that ReWOO is still able to perform well even when the external tools that it relies on are not available. </p><p>‍</p><p>ReWOO outperforms ReAct. ReWOO was able to reduce token usage by 64% with an absolute accuracy gain of 4.4%. It is able to elicit more reasoning capabilities from LLMs than ReAct. ReWOO was also found to be more robust to tool failures than ReAct. When tools malfunction and return errors, ReAct-like ALM systems are highly fragile. ReWOO, on the other hand, is less compromised. ReWOO also performed well on the curated dataset, SOTUQA. SOTUQA is a document QA dataset that is more closely aligned with real-world ALM applications than previous public NLP benchmarks. </p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:624pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64e90592a514b40943a960c8_wRiBDHNq8IDdrcvmKuKYEBDWkZp3dBrO64c1eNvnq-oEtN4wztGGieDs-dd6x3ma_DmgLwsQdIyLCG63U76E8JhV-XhU8O03Ucsi4U11rBvwuyxcVhPeAUP0urr9C-LrXLj6WCo9uZeP-YHEQfwqAyQ.png""/></div></figure><p>‍</p><p>ReWOO decouples parametric modules from nonparametric tool calls. It means that the LLM can be fine-tuned to offload some of its reasoning ability to smaller language models. This offloading can substantially reduce the number of parameters that the LLM needs to store, which further improves the efficiency of the LLM.  ReWOO can offload reasoning ability from a 175B GPT3.5 model to a 7B LLaMA model. It has the potential to create truly efficient and scalable ALM systems. </p><h3>Reason and Act (ReAct)</h3><p>ReAct is a technique that combines reasoning and acting with language models for solving various language reasoning and decision-making tasks. It prompts language models to generate both verbal reasoning traces and actions. It enables dynamic reasoning, high-level planning for acting, and interaction with external environments. </p><p>‍</p><p>Here is our <a href=""https://www.mercity.ai/blog-post/react-prompting-and-react-based-agentic-systems"">extensive guide on ReAct Prompting.</a></p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:637pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64e90592219b27430844e4e6_XxnqSuUVzTPkAfR8GjAEs6LuMCGu2nRxQ7pYq6Zx2f7PaPEXQ4S3CLIKgiAQbxH-jKM3bLq_-62cr4oNK_J6LOg0KZMOXDus8G6cSBrAkp5eS9JteJy7NY2TjkdP1b1GwoyrUgmIwEHryn4RhvtZzJE.png""/></div></figure><p>‍</p><p>It is evaluated on four diverse benchmarks, including question answering (HotPotQA), fact verification (Fever), text-based games (ALFWorld), and web page navigation (WebShop). On HotpotQA and Fever, ReAct was able to overcome prevalent issues of hallucination and error propagation in chain-of-thought reasoning. It also outperformed imitation and reinforcement learning methods with an improved 34% and 10% on ALFWorld and WebShop. This is because ReAct is able to learn from human examples and apply that knowledge to new situations.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:805pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64e9059237d55fa07702fbfc_eQxMaqYdOAIHxGegYcDnNf180f1nzGIitpevWKD0JajlPrYq4-ATzELSaMUYY7YJax389iyer-PfF3sEM7BdKenShmhnDgxZR-c5wAGqrbf6JjKGuzH9LK_cXwyDTstE18OpbBdXamVYyo_oidsFLQU.png""/></div></figure><p>‍</p><p>ReAct is designed to be intuitive, general, performant, and robust. It is applicable to diverse tasks, including question answering, fact verification, text games, and web navigation. It provides an interpretable decision-making and reasoning process, allowing humans to inspect reasoning, factual correctness, and even control or correct the agent's behavior during task execution.</p><h3>Reflection</h3><p>Reflexion is a framework that uses linguistic feedback to reinforce language agents. Linguistic feedback is feedback that is expressed in natural language. Reflexion agents learn to reflect on task feedback signals, and then maintain their own reflective text in an episodic memory buffer. This reflective text is then used to induce better decision-making in subsequent trials.  The Reflexion framework uses self-reflection. It generates verbal self-reflections to provide more informative feedback. These self-reflections are then stored in the agent's memory. The agent can then use this information to improve its performance on future trials.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:435pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64e9059219d687d60052562a_EBLL_IcwgJMdMmDRajNDBc2MLxpcIB63P9M4l2Skg6x49qsVkZTJKVJil7uIHTHAE9HOVCSRp0QS8voiFbFfQRZLTtJ8Tt7voUJAgO7ne1vQluOy3q1r3dVuToelCxdAhQfHBT_qHPCpHUYH_eGjNzo.jpeg""/></div></figure><p>Reflexion is flexible enough to incorporate various types and sources of feedback signals. For example, feedback signals can be scalar values (such as rewards or punishments), or they can be free-form language. Feedback signals can also be external (provided by a human or another agent), or they can be internally simulated (generated by the agent itself).</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:825pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64e905936c79e1c629741ad9_wTeI4ICjWtjoufPc7LuNjEZmp5Lf26cMbRs_ajVMMh4FcUfmN9KWJT7ijCUdjWDarIbb3e8OBcKyq1SDKXoj-08gQ9XHtkG_y1dLYu5a_a_EH6igpTZpKpFJtDwgFlTPVMVuYp1JG6TD8skYPle97Ck.jpeg""/></div></figure><p>‍</p><p>Reflexion agents outperforms strong baseline approaches in decision-making tasks, reasoning tasks, and programming tasks . In decision-making tasks (AlfWorld), Reflexion agents improve by 22% over 12 iterative learning steps. In reasoning questions (HotPotQA), Reflexion agents show a 20% improvement. In Python programming tasks (HumanEval), Reflexion agents achieve an improvement of up to 11%. It achieves a 91% pass@1 accuracy on the HumanEval, surpassing the previous state-of-the-art GPT-4 that achieves 80%.</p><p>‍</p><h3>Expert Prompting</h3><p>Expert Prompting is an augmented strategy for instructing Large Language Models (LLMs). It envisions a distinguished expert agent tailored to each specific instruction. LLMs are asked to answer instructions conditioned on the identity of the envisioned expert. It is an automatic prompting method. Expert identities are generated using In-Context Learning. It requires writing several instruction-expert pair exemplars. The generated expert identities are found to be satisfactory.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:517pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64e90593e818b44181366f49_XN06W861ViSc3LoJ3a50snCTmW-InnZqgWlxfjsDC1Bz02lQf75iUT9fwEMqs5ugFmZrsDX35JvMEVmTt6T6g5BySKCd_MQN6dFyivY8ErS7qISX02HKMA7nfJ8n8Ad5HC_tqmAv4HOBkyDZxhgJiPg.png""/></div></figure><p>‍</p><p>Expert Prompting is a generalized prompting method. Expert identities are defined with detailed and elaborate descriptions. It can match instructions in various domains or genres. It's adaptable to different areas, such as nutrition or physics. It is simple to implement. It doesn't require complex crafting of prompt templates or iterative processes. Writing good expert identity is critical. It should be specialized, detailed, and comprehensive for each instruction. The descriptions must be automatically generated to be practical and efficient. </p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:736pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64e90593fae4f49204b2b610_KUBueS5mV1XOV70kmD9FQiD7vGNrJS0mbQLZHldjADXrraG2w0W-1BloBvPRcC5nVhZ8GB0HxHmX6xYOBuoWvhoz5l0FKDtYMVMuMd0WgPca1ApvIVYXhDhzIK6snDm4rJinLPghbzBv6ml3sEZzQ2I.jpeg""/></div></figure><h2>Automatic Prompt Engineering (APE)</h2><p>APE is a technique that treats the instruction as the “program,” and it optimizes the instruction by searching over a pool of instruction candidates proposed by an LLM. The LLM candidates are scored using a chosen score function, and the instruction with the highest score is selected. APE is inspired by classical program synthesis and the human approach to prompt engineering. Program synthesis is the task of automatically generating code from a natural language description of the desired behavior. The human approach is the process of manually crafting instructions effective at using LLMs to produce desired outputs.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1054pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64e9059337d55fa07702fc6f_9YNNrp_6W_08wnDcNFfhoFnYNbqSKZKAFUHCKR9jHhLbzMjbPzk1CrNhkN8q0PGP7mh87bSbMmSCB-P3c7KASgrgsci88rb9mK_BIC8iG2_MgMFlMZTkTmDsBxOyUi3e8EOeSuTV6ushGZRg8SACcns.png""/></div></figure><p>‍</p><p>APE achieves human-level performance on zero-shot learning with model-generated instructions on 24/24 Instruction Induction and 17/21 Big-Bench tasks. It surpasses human performance with the InstructGPT model, obtaining an IQM of 0.810 compared to humans' 0.749. To achieve this, a dataset of questions and reasoning steps is generated using InstructGPT with the prompt ""Let's think step by step."" Then any data points that had incorrect answers were removed. Finally, APE was used to find a prompt starting with ""Let's"" that maximized the likelihood of these correct reasoning steps. APE produced the prompt ""Let's work this out in a step-by-step way to be sure we have the right answer."" This generated prompt further improved performance on two tasks: MultiArith from 78.7 to 82.0, and GSM8K from 40.7 to 43.0.</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:927pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64e905935c6637bdeef5803b_7boon2MJdaMcF0lxz7ALQ4z5WzIT9uu2--DiZLnlaIkfj2aFu_fLPkCj81m3ouFwy52mOIdIfqq5UqEJn9r2ZrFqLMNDY1W_-xUpxsevtcg4DblsBXoJgipnO3ZXjbGVBgvCX5skxg71h2IAYbqrtkE.jpeg""/></div></figure><p>‍</p><h3>Auto-CoT</h3><p>Auto-CoT is a process of automatically constructing demonstrations with questions and reasoning chains. It first clusters the questions in a dataset into a few clusters. Then, it selects a representative question from each cluster and generates its reasoning chain using Zero-Shot-CoT with simple heuristics. The Auto-CoT method has several advantages over other methods. It is automatic, scalable, and effective, which means that it generates demonstrations that are accurate and informative.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:960pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64e90593acc59c457830a0b9_-E0ZSyqcr1bkOFU47sI3ERrhUuqYRkJAy1JRvWQFIqiguA-jqZaM8A-HVTk6_RUnuEFbZmQNcsFHVxoJnwjdT_-xqmiUssbKzCiEi9oAJqSW3dKgugGyfGBU1Ucy4O3XzKGo5m38V4VXVfsbiA3LtXQ.jpeg""/></div></figure><p>‍</p><p>On comparing the accuracy of Auto-CoT with the four baseline methods on ten datasets from three categories of reasoning tasks,  Auto-CoT consistently matches or exceeds the performance of the CoT that requires manual designs of demonstrations. The reason for this is that Auto-CoT is able to generate demonstrations that are task-adaptive. It means that the demonstrations are tailored to the specific dataset and reasoning task. In contrast, Manual-CoT may use the same demonstrations for multiple datasets, which can lead to lower accuracy. </p><p>‍</p><h3>Automatic Multi-step Reasoning and Tool-use (ART)</h3><p>ART is a framework that uses large language models to automatically generate intermediate reasoning steps for a new task. The LLMs are frozen, which means that they are not updated during the reasoning process. It allows ART to be more efficient and scalable than frameworks that use trainable LLMs.  ART selects demonstrations of multistep reasoning and tool use from a task library. A decomposition is a high-level description of the steps involved in solving a task. ART then selects and uses tools in the tool library alongside LLM generation to complete the intermediate reasoning steps. At test time, ART seamlessly pauses generation whenever external tools are called, and integrates their output before resuming generation. This allows ART to leverage the capabilities of external tools to solve complex tasks.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:426pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64e905947ee01a3082efdf2c_8V4rX8ymLZMkTMBahv7kuBiLRzAyC-fgM761YqerYUAQD_q7kmSZ0nUaCAPrjWpQkANpC8BQULEWzk4iNMfyOfyHcTBWkgYO1i6UNGf39zFWtZxO5bbAEDyTDY24HP4eDxydFYU78vXe7JJnxqFYhxQ.png""/></div></figure><p>‍</p><p>ART has been shown to be effective on a variety of tasks, including natural language inference, question answering, and code generation. It outperforms previous approaches to few-shot reasoning and tool-use, and it is able to solve tasks that were previously thought to be impossible for LLMs. Humans can optionally edit decompositions to improve performance. For example, they can correct errors in code or incorporate new tools. ART is extensible, which means that it can be easily extended to include new tasks and tools.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:940pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64e905949975c2126a118627_GQlu6j17prD9WnCNRn4Cx7gDwirx1PuOmPSKT3hkqzaXClyM0YH-5FljiBo4TrLsPTUovkziwOH7s8ttlCqri458nITRaJ2yW3UG7JwvpUjTT2MsWXjwPU2al8CpnfJzVaww2Dk5YgGqGi_nEMbMkj8.png""/></div></figure><p>‍</p><p>ART consistently matched or outperformed automatically generated CoT reasoning chains on 32 out of 34 BigBench tasks and all MMLU tasks. On average, it achieved an improvement of over 22 percentage points. The use of tools in ART significantly enhanced performance on test tasks, with an average improvement of over 12.3 percentage points compared to scenarios where no tools were allowed. ART also improved over direct few-shot prompting by an average of 10.8 percentage points across unseen BigBench and MMLU tasks. Its improvements were particularly remarkable in tasks requiring arithmetic and algorithmic reasoning, where it outperformed direct few-shot prompting by 12.5%. ART also surpassed previous best-known results for GPT3, which use supervision for decomposition and/or tool use, by 6.1 percentage points. ART allows for human intervention and performance improvement by updating the task and tool libraries with new demonstrations. With additional human feedback, ART surpassed the best-known results for GPT3 by an average of over 20 percentage points on 12 test tasks.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:741pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64e90594bca1079f95b9aec7_5JLbRJJ0HDgdZhmtSbSqt_sZZ6-eog6rvojESX99u3QdpXJCYYOXESsCtc-uAeFL0j3NkRSafjOTRA6-wt2fOC9qv1x-9lZhei8RXq5UJjhqr35T6rEycF8Dk3BYs1hNvvxUjlf7WDG54dsuQmOlaPU.png""/></div></figure><p>‍</p><h2>Advanced Prompt Engineering Strategies</h2><p>You can enhance your prompts with some effective prompting strategies, such as temperature and token control, prompt chaining, multi-turn conversations, and more. Temperature and token control fine-tune language model behavior. Temperature adjusts randomness, with higher values promoting creativity. Lower temperature refines responses for precision. Token control sets response length, useful for brevity. </p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1128pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64e90594760e2a6c3003546c_Q54lqzPNmrYVUz-0mzYG-Dx0KcO9NMp9DS2WQW4rFLHkUKunKeBFkQTJM2EfLaKZaLt4F_oifU7eYVT-NFOI-Z6ByB7ACNDevanvcjkZl_1TfuwqcMX6F2DUz66nRqdAyN3WdzI-HOGJl9Bp2_SKG_M.png""/></div></figure><p>‍</p><p>Prompt chaining is the practice of connecting multiple prompts together to create a continuous flow of conversation by referencing previous inputs or the language model's previous responses in each prompt. Multi-turn conversations are conversations that consist of multiple exchanges between the user and the language model by the user providing multiple prompts, or by the language model providing multiple responses. Multi-turn conversations allow for a more detailed and nuanced conversation, as the user and the language model can build on each other's contributions. For example, to engage in a detailed discussion, users could chain prompts together to explore a topic in depth. The language model could then provide different perspectives on the topic, allowing for a more nuanced and informative discussion.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1069pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64e905946c79e1c629741c9b_vAKdwbozIcDMgKlldo2p43TE7ETWHoXsUe2f5pIGwynapDT0L19qtXhhgbHjQla3PRihLlYKSswg8KD1SmXJByjjEismrOEbUktgBV3XSrvXu4FCO9SfUgfBBv6b82C0j_MeJt5JQ7UNe8IoxFHAyzc.jpeg""/></div></figure><p>Also, tailoring prompts to specific industries or fields ensures relevant responses from LLMs, building user trust and encouraging future use. Domain-specific prompts enable better context understanding and accuracy, as LLMs are trained on domain-relevant text. This enhances the overall user experience, leading to greater satisfaction. The ability to handle unclear or contradicting user inputs can improve prompting in any LLM by ensuring that the model is able to understand the user's request and generate a relevant and informative response. It involves actively engaging with the user and seeking clarification, using natural language understanding to identify the user's intent, and generating multiple responses. For example, if the user asks ""I'm looking for a restaurant,"" the chatbot could generate responses that recommend restaurants based on the user's location, budget, or dietary restrictions.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1056pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64e905947ee01a3082efe07d_MiZDJqvGX_Nkb3K7u7yqBssQxISmNBhCj4r7glTh1iPuY4dObOJaOPcWx_0zH1Yb5dY2YyHU9j5-MrLmDoD84D0z_ocA4W2E8qwDCw0waQROOWZE7dmaSsI4Y4TnWJOs_DS6qKrUB26wTISWuWoN2cM.jpeg""/></div></figure><h2>Tools For Implementing Prompting Techniques</h2><p>Tools like Langchain, Guidance AI, Semantic Kernel, and Auto-GPT make it easier for us to interact with language models. These powerful tools offer innovative solutions for crafting more effective and context-aware prompts and enhancing the capabilities of language models.</p><h3>Langchain</h3><p><a href=""https://github.com/langchain-ai/langchain"">Langchain</a> is a versatile framework for building data-aware and agentic applications using language models. It was launched in October 2022 by Harrison Chase at Robust Intelligence. Langchain provides standard and extendable interfaces for modules like models, prompts, memory, indexes, chains, agents, and callbacks. This makes it easy to build applications that use language models for a wide range of tasks.</p><p>‍</p><p>Langchain integrates with a variety of tools and cloud storage systems, including cloud storage systems like Amazon, Google, and Microsoft Azure. It has API wrappers for news, movie information, and weather and Bash for summarization, syntax checking, and script execution.  Langchain supports web scraping with multiple subsystems and templates. It facilitates few-shot learning prompt generation. It interacts with Google Drive documents, spreadsheets, and presentations. Langchain performs web searches using Google Search and Microsoft Bing. It integrates with OpenAI, Anthropic, and Hugging Face language models. Langchain generates, analyzes, and debugs Python and JavaScript code. It utilizes the Weaviate vector database for caching embedding and data objects. Langchain integrates with various databases, performs text mapping, and supports time zone conversions.</p><h3>Semantic Kernel</h3><p><a href=""https://github.com/microsoft/semantic-kernel"">Semantic Kernel</a> is an open-source SDK that makes it easy to integrate AI services like OpenAI, Azure OpenAI, and Hugging Face with traditional programming languages like C# and Python. It provides a set of connectors that make it easy to add memories and models to your apps, giving them a simulated ""brain."" Semantic Kernel also provides a set of AI plugins that allow your apps to interact with the real world via prompts and native functions. These plugins are like the ""body"" of your AI app. </p><p>‍</p><p>It focuses on avoiding software bloat. It employs a planner to break down tasks and interlock parts, turning user queries into desired outcomes. SK enables integration of LLMs with traditional programming by combining natural language semantics with code functions. SK uses embeddings-based memory for enhanced application capabilities, supporting prompt engineering, chaining, retrieval-augmented generation, and more. SK offers contextual and long-term vectorized memory, allowing access to external knowledge stores and proprietary data. SK incorporates design patterns from AI research for intelligent planning and reasoning.</p><h3>Guidance AI</h3><p><a href=""https://github.com/guidance-ai/guidance"">Guidance</a> by Microsoft is a templating language for controlling large language models (LLMs). It supports a variety of prompt engineering techniques and is well-suited for use with powerful LLMs like GPT-4. Guidance offers efficient and effective control of LLMs by integrating generation, prompting, and logical control in a continuous flow, which matches how LLMs process text. It provides a simple and intuitive syntax based on Handlebars templating. It can be used to create rich output structures with multiple generations, selections, conditionals, and tool use. </p><p>‍</p><p>It offers a playground-like streaming experience in Jupyter/VSCode Notebooks, making it easy to experiment with different prompts and parameters. Smart seed-based generation caching is supported for optimization, which can significantly speed up the generation process. Guidance is compatible with role-based chat models like ChatGPT, and it seamlessly integrates with Hugging Face models. Guidance offers a number of features that can improve the performance and usability of Hugging Face models, such as guidance acceleration, token healing, and regex pattern guides.</p><h3>Auto-GPT</h3><p><a href=""https://github.com/Significant-Gravitas/Auto-GPT"">Auto-GPT</a> is an experimental, open-source application that demonstrates the capabilities of the GPT-4 language model. It is a popular tool for designing LLM agents, chaining together LLM thoughts to autonomously achieve user-defined goals. Auto-GPT showcases the potential of GPT-4 to operate autonomously, with key features that include internet access for searches, long-term and short-term memory management, and the ability to use GPT-4 instances for text generation. Auto-GPT supports file storage and summarization using GPT-3.5. The application is extensible with plugins.</p><p>‍</p><p>Auto-GPT is an AI agent that can achieve goals set in natural language. It breaks down goals into sub-tasks and uses internet resources and tools to complete them. It can operate autonomously without requiring manual commands. Auto-GPT can create and revise its own prompts, and it can manage memory by reading from and writing to databases and files. However, Auto-GPT cannot modify the underlying GPT models, as they are proprietary, and it does not typically access its own base system code.</p><p>‍</p><h2>Want to write high quality prompts for LLMs?</h2><p>We are a team of researchers and engineers who have been working on AI for very long. We have written may prompts, some as long as 500 words. If you are looking to improve performance of your prompts or setup monitoring systems for your language models, <a href=""https://www.mercity.ai/contacts"">reach out </a>and we’ll be happy to help!</p><p>‍</p></div>"
Extensive study of AI applications in Virtual Reality,ai-in-virtual-reality,640f56f76d313b2faa631c11,64dd18c79b00701ba7e9c966,False,False,Wed Aug 16 2023 18:43:19 GMT+0000 (Coordinated Universal Time),Wed Aug 16 2023 19:33:06 GMT+0000 (Coordinated Universal Time),Wed Aug 16 2023 19:33:06 GMT+0000 (Coordinated Universal Time),"<p id="""">Blending artificial intelligence (AI) in virtual reality (VR) is changing how we interact with the world. AI can create more realistic and immersive VR experiences by generating 3D models, textures, and scenes. It can track and interact in VR by understanding the user's movements and facial expressions. Businesses can use this technology in training, customer service, and market research. Early adopters of AI-powered VR will be well-prepared to leverage this emerging technology.</p><p id="""">In this article, we will discuss how artificial intelligence is improving virtual reality. It will help you understand the use of this next-level technology in your business.</p><p id="""">‍</p><h2 id="""">How AI Can Be Used In VR?</h2><p id="""">Utilizing the below-mentioned techniques within VR environments has led to substantive outcomes. These technologies collectively enhance human-computer interactions, elevate visual fidelity, and improve object recognition accuracy. The intersection of AI and VR is reshaping how we experience and interact within virtual spaces, opening doors to a new era of immersive possibilities and practical applications.</p><p id="""">‍</p><h3 id="""">Natural Language Interaction in VR</h3><p id=""""><a href=""https://www.researchgate.net/publication/270337753_Development_of_an_Intelligent_Virtual_Environment_for_Augmenting_Natural_Language_Processing_in_Virtual_Reality_Systems"" id="""">Natural Language Processing (NLP)</a> helps computers to understand human language. It can be integrated with VR using five components: a text parser, a rule base, an NLP to VR interface, a library of CAD (Computer Aided Design) models in ASCII formats, and a renderer. The text parser first receives the input from the user. It then breaks the input into parts and extracts relevant information from it. It would then generate a <strong id="""">rule base</strong>, a set of rules that maintain the actions the renderer can perform. The NLP to VR interface fetches the required object from the library. It directs the extracted details and the rule base to the renderer. The renderer displays the model in the VR environment.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1158px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1158px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64dd147ba03216dc0e22bda3_iFIib04xG6y50LMjnAW0Sp8FY3uyRzwKXMWnbeREyH0_hrJVeBcIltva2XEQI8xbUog6mP__S0oY5VZX71Bd0h8NqhAJlqBaVaMVOxEM49GUlQh_3scc-id8ZQg0_fhD5BC_a1CTpgWL2zXAkcJCx6o.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">NLP techniques can assist voice-activated navigation, a more immersive and natural way to interact with VR environments. For example, <a href=""https://github.com/suno-ai/bark"" id="""">Bark</a> is a multilingual text-to-audio framework by Suno that can be used to generate realistic speech and sounds.</p><p id="""">‍</p><p id="""">It can also be used to create more realistic and engaging NPC interactions. An NPC could respond to a user's voice commands, or it could even hold a conversation with the user. One such recent example: A Unity/Unreal player tries to convince AI NPCs that they are in stimulation and do not exist beyond it.</p><p id="""">‍</p><div data-rt-embed-type='true'><blockquote class=""twitter-tweet tw-align-center"" data-theme=""dark""><p lang=""en"" dir=""ltr"">This is wild: watch this guy go around in the Matrix Awakens (!) game explaining to AI NPCs they’re in a simulation. <br><br>Remember, these are not scripted. They&#39;re not human voices. And this is not &quot;some day&quot;, this is in Unity/Unreal now. <br><br>Video games are about to be filled with AI… <a href=""https://t.co/2nvzJCY9vo"">https://t.co/2nvzJCY9vo</a> <a href=""https://t.co/5DyrmdkBX0"">pic.twitter.com/5DyrmdkBX0</a></p>&mdash; AI Notkilleveryoneism Memes (@AISafetyMemes) <a href=""https://twitter.com/AISafetyMemes/status/1683077335875035136?ref_src=twsrc%5Etfw"">July 23, 2023</a></blockquote> <script async src=""https://platform.twitter.com/widgets.js"" charset=""utf-8""></script></div><p id="""">‍</p><p id="""">Also, Stanford researchers developed computer programs called <a href=""https://arxiv.org/pdf/2304.03442.pdf"" id="""">Generative Agents</a> that can simulate authentic human behavior. The model learns from real-world data and generates realistic conversations, interactions, and decisions. The agents interact with each other using natural language.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64dd147b76c770c868cc98d9_Xn7cFS3mpdDA-_Z9gZEEWZFJVOSyyrHukKR4VNIG_EnsK4e3X0x4qzrb_5qwuW0CqkdsqFhevboMW-Krxs8fztUEHdUysRplLgaZ0CixtXjEOk3sd8oVKlu6TqCLOiNCI-G5CN7UPcGKh5nWFsp-GnY.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">Figure From <a href=""https://www.artisana.ai/articles/generative-agents-stanfords-groundbreaking-ai-study-simulates-authentic"" id=""""><em id="""">Artisana</em></a></p><h3 id="""">Image and Object Recognition</h3><p id="""">Image and <a href=""https://www.researchgate.net/publication/326164461_Object_Detection_with_Deep_Learning_for_a_Virtual_Reality_Based_Training_Simulator"" id="""">object detection in VR</a> uses computer vision techniques to identify objects in the virtual world. Deep learning techniques for object detection are one-stage, two-stage or transformer-based algorithms. One-stage algorithms generate positioning coordinates and classification probabilities of objects in an image, in a single shot. Two-stage networks on the other hand use a region proposal network to propose possible regions where an object can be found, then a detection network is used to classify the objects. One-stage networks are faster than two-staged ones because of no advance region proposal generation.</p><p id="""">‍</p><p id=""""><a href=""https://arxiv.org/pdf/2304.00501.pdf"" id="""">YOLO series</a> are a part of one-stages algorithms. <a href=""https://deci.ai/blog/yolo-nas-object-detection-foundation-model/"" id="""">YONO-NAS</a> is a neural architecture search (NAS) algorithm by Deci AI. It is one of the SOTA models that surpassed previous YOLO models. It finds the optimal network architecture for object detection. It searches a large space of possible network architectures and selects the one that achieves the best performance on a given dataset. Its components are backbone, neck, head, QSP, and QSI block.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64dd147b29a36f9097c3a920_vSJtnoTcUKHsEH6FbNL_ENIvBHz7znVGoLlAm2v0rhrhOhrDwRiICaYzbNPasJEPkJDtj8dL-xyvljaR2-d8DqveadyVcePoNOYQpDqXOEdAy52xjMJbWrT7DtLKmJrn7vYpz7Aj_A3VttNUWzeFVh4.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">Figure from <a href=""https://deci.ai/blog/yolo-nas-object-detection-foundation-model/"" id=""""><em id="""">Deci AI</em></a></p><p id="""">The backbone is responsible for extracting features from the input image. It is a convolutional neural network (CNN) that has been pre-trained on a large dataset of images. The neck connects the backbone to the head. It consists of a few convolutional layers to reduce the dimensionality of the features extracted by the backbone. The head generates the bounding boxes and class predictions for the objects in the image. It consists of a few convolutional layers to classify the objects in the image and to predict the bounding boxes for those objects. The QSP block is responsible for quantizing the features extracted by the backbone. It makes the features more efficient to process and allows YOLO-NAS to be used on devices with limited computational resources. The QSI block is responsible for dequantizing the features extracted by the QSP block. It allows YOLO-NAS to generate high-quality object detection results.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1558px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1558px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64dd147b0c076b55927e316c_eLRYUvVoW6ao9EcWCfUhgVx-cd-O-P3ohH84OYETofwox_mROcLzeWaA6D2aL0IT-eVsTnxnpvt9n_ckRLEcwrJWfRGFz3AgKNXgcdRboxPkdCk9aPJg9siZpwYaPcBLQEHkc5Kny9JEKm2tqwKkkSM.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">Figure From <a href=""https://www.makeuseof.com/yolo-nas-best-object-detection-model-in-yolo-series/"" id=""""><em id="""">MUO</em></a></p><p id="""">Object recognition has a major application in <a href=""https://www.researchgate.net/publication/343015573_A_brief_analysis_of_gesture_recognition_in_VR"" id="""">hand gesture recognition</a> that allows VR systems to track the movements of the user's hands and use this information to control objects in the VR environment. There are a variety of different hand gesture recognition techniques that can be used in VR, including optical tracking, inertial tracking, and depth sensing. Once the user's hand movements have been tracked, the VR system can use this information to control objects in the VR environment, such as grabbing and moving objects, or interacting with menus and buttons.</p><h3 id="""">NeRFs</h3><p id=""""><a href=""https://arxiv.org/abs/2212.01120"" id="""">Neural Radiance Fields (NeRFs)</a> is a technology to create photorealistic 3D models of objects from a collection of images. It is a powerful tool to create VR experiences that are more immersive and realistic. NeRF uses a neural network to learn the relationship between the 3D position of a point in space and the color of the light reflected from that point. This relationship is called a radiance field. The neural network is trained on a dataset of images taken from different viewpoints of the object. It then generates a 3D model of the object by taking a ray from the viewer's eye and tracing it through the radiance field. Then the neural network determines the color of the light reflected from the object. It is repeated for every pixel in the image.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:800px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""800px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64dd22a1aae0270819728241_nerf.png"" alt=""How Neural Radiance Fields (NeRF) and Instant Neural Graphics Primitives  work | AI Summer"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">‍</p><p id=""""><a href=""https://github.com/bmild/nerf"" id="""">NeRF model</a> editing and saving are aspects of using NeRF for VR. NeRF model editing is the process of changing the 3D model generated by NeRF. It improves the accuracy or realism of the model, or changes the object's appearance. NeRF model saving is the process of storing the 3D model generated by NeRF. Point clouds are a good choice for VR experiences that require high performance, while meshes are a good choice for VR experiences that require high accuracy.</p><p id="""">‍</p><p id="""">Recently, <a href=""https://research.nvidia.com/labs/dir/neuralangelo/"" id="""">NVIDIA </a>released <a href=""https://arxiv.org/abs/2306.03092"" id="""">Neuralangelo</a>, a new AI technology that can transform any video into a highly detailed 3D environment. Neuralangelo is based on Instant NeRF, but it improves the quality of the generated models by using numerical gradients and coarse-to-fine optimization. It takes a 2D video as input and analyzes it to extract details such as depth, size, and the shapes of objects. It then uses this information to create an initial 3D model of the scene.</p><p id="""">‍</p><div data-rt-embed-type='true'><blockquote class=""twitter-tweet tw-align-center"" data-theme=""dark""><p lang=""en"" dir=""ltr"">🔴 Finally! NVIDIA has finally made the code for Neuralangelo public!<br><br>It has the ability to transform any video into a highly detailed 3D environment, and it&#39;s a technology related to but DIFFERENT from NeRF.<br><br>💡 Here&#39;s how it works:<br>It takes a 2D video as input, showing an… <a href=""https://t.co/tS3dv7df61"">pic.twitter.com/tS3dv7df61</a></p>&mdash; Javi Lopez ⛩️ (@javilopen) <a href=""https://twitter.com/javilopen/status/1691120893638950912?ref_src=twsrc%5Etfw"">August 14, 2023</a></blockquote> <script async src=""https://platform.twitter.com/widgets.js"" charset=""utf-8""></script></div><p id="""">‍</p><h3 id="""">Gaussian Splatting</h3><p id=""""><a href=""https://arxiv.org/pdf/2308.04079.pdf"" id="""">Gaussian splatting</a> is a technique to render volumetric data. It can render a 3D scene from a set of images. The first step is to create a sparse point cloud from the images. It can estimate the position, covariance matrix, and opacity of a set of <a href=""https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/"" id="""">3D Gaussians</a>. The Gaussians are then used to render the scene.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1320px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1320px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64dd147cf7cf539ed23c38f2_Fwj3VWr13Ljo8_-BU88S5zbestwa-IFhBdAgNN5R6wZUICT0SOS0tJtp6kvDKOKFDevE4y8mlcpaiPXo92tz0dxGVnZL1SaH81OvGb4F3K2AWQyHnK-Ny73CKhXdeuthZDS-PQ-py-9jWCjjzhF1VlE.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">Figure From <a href=""https://arxiv.org/pdf/2308.04079.pdf"" id=""""><em id="""">3D Gaussian Splatting for Real-Time Radiance Field Rendering</em></a></p><p id="""">The volume is divided into a grid of voxels. For each voxel, the Gaussians are sampled at the voxel's center. The sampled values then create a smooth, continuous surface approximating the true volumetric data. This process repeats for all voxels in the volume. The resulting surface can then be rendered using many techniques, such as ray tracing or rasterization.</p><p id="""">‍</p><p id="""">Gaussian splatting is much better than NeRF AI for rendering 3D scenes from a single image because it is more efficient, robust to noise, and flexible. <a href=""https://github.com/graphdeco-inria/gaussian-splatting"" id="""">Gaussian splatting</a> is also a better choice for scenes with complex materials. It has competitive training times. This means that it can be trained quickly, even on large datasets. It can achieve high-quality results with only SfM points as input. This means that it does not require additional data, such as Multi-View Stereo (MVS) data, which can be time-consuming and expensive to collect.</p><p id="""">‍</p><p id="""">The <a href=""https://huggingface.co/papers/2308.04079"" id="""">Gaussian splatting</a> technique is a powerful tool that can render volumetric data in various applications. It is relatively simple and efficient, making it a good choice for real-time applications. It is a good choice for rendering dynamic scenes, such as those that contain moving objects or changing lighting conditions.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64dd147c60890e5fda0f0db9_fMWXQOpwQOmvKCXd8Urs0bIQfMy5aGGIXIAlx2-RRrdD-VPKQvO_gy0J4Wy-9MwkxbA-5J_ivkCtyZUOfARtvbr1SajKpugJbQKK6F9Q0uCYkV5JwaKReClpZZIryWuqUtFvcLa0EMjoLdncYBS-BQM.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h3 id="""">Stable Diffusion&nbsp;&nbsp;</h3><p id=""""><a href=""https://huggingface.co/CompVis/stable-diffusion-v-1-1-original"" id="""">Stable diffusion</a> is a new model used to generate high-quality images from text descriptions. The diffusion model has three main components: the <strong id="""">image encoder</strong>, the <strong id="""">image information creator</strong> and an <strong id="""">autoencoder decoder</strong>. The image encoder compresses the input image into a latent representation. This latent representation is a lower-dimensional representation of the image that contains the most important information about the image. The image information creator takes the latent representation from the image encoder and generates a sequence of numbers for pixels in the image. The autoencoder decoder takes the information from the image information creator and reconstructs the original image. The autoencoder decoder is trained to minimize the difference between the reconstructed image and the original image.</p><p id="""">‍</p><p id="""">To generate images,a random latent matrix is generated. Smaller the latent space faster the image generation process. The noise predictor estimates the noise in the latent matrix. The estimated noise is subtracted from the latent matrix. This process is repeated for multiple steps. With each step, some noise is removed from the latent matrix, and the given prompt is used for guidance resulting in a more accurate image that is closer to the prompt. The decoder then converts the latent matrix to the final image.&nbsp;</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64dd147c84411f6e7392307e_x3iLucKtE-LhQYGihZm2-mEENbOsCSxT38kx8_Oa3lUiWoEYetcuQHQ7JnlmLMaQT5hWYBOYJ_bXxtZ40Y0GumPZs-Iov6bzHldfdPDWLXk2kphfIWryx39JmGGrIDtnfdd4LMTHjY4pH6OfdojhTeU.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">Figure From <a href=""https://jalammar.github.io/illustrated-stable-diffusion/"" id=""""><em id="""">The Illustrated Stable Diffusion</em></a></p><p id="""">The text description guides the diffusion process so the generated image matches the description. The text prompt you provide the model gets converted into numbers relating to the individual words, called tokens. Each token gets converted to a 768-value vector known as embedding. These embeddings get processed and ready to be consumed by the noise predictor.</p><p id="""">‍</p><p id=""""><a href=""https://github.com/CompVis/stable-diffusion"" id="""">Stable diffusion</a> models can be used to create realistic and immersive environments, generate interactive objects, and empower creativity in VR. It can create various VR experiences, such as virtual worlds, museum exhibits, games, and fashion shows.</p><h4 id="""">ControlNet</h4><p id=""""><a href=""https://arxiv.org/abs/2302.05543"" id="""">ControlNet</a> is a neural network structure to control pre-trained large diffusion models to support additional input conditions. It creates two copies of the weights. The locked copy has frozen weights and preserves the original model. The trainable copy learns to manipulate the inputs of the network to control the overall behavior of the network. It allows <a href=""https://github.com/lllyasviel/ControlNet"" id="""">ControlNet</a> to be trained on small datasets of image pairs. It can be trained on personal devices, and it can scale to large amounts of data if powerful computation clusters are available.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1569px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1569px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64dd147c8f9b7865df8ff37a_kTd7rkb3qUdFlgv-PoacrJfq1EHO2vHSEJLaMR47KOgFHGH5AZtvJj-hBS_Ldl5drf-hhTpmY5g92r0u70I8oiBPVZsmEMIPnYw0WQPevmj1JogXrz2_imXv4kDT1yb6JHfCXgA-yU7NXZb4cx1FyRM.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">Figure From <a href=""https://www.cameralyze.co/models/controlnet-scribble"" id=""""><em id="""">Cameralyze</em></a></p><p id="""">‍</p><div data-rt-embed-type='true'><blockquote class=""twitter-tweet tw-align-center"" data-theme=""dark""><p lang=""en"" dir=""ltr""><a href=""https://twitter.com/hashtag/AIart?src=hash&amp;ref_src=twsrc%5Etfw"">#AIart</a> <a href=""https://twitter.com/hashtag/stablediffusion?src=hash&amp;ref_src=twsrc%5Etfw"">#stablediffusion</a> <br>Introducing (a WIP version of) TemporalNet, a ControlNet model trained for temporal consistency <a href=""https://t.co/p95UH8Yo3m"">pic.twitter.com/p95UH8Yo3m</a></p>&mdash; CiaraRowles (@CiaraRowles1) <a href=""https://twitter.com/CiaraRowles1/status/1637486561917906944?ref_src=twsrc%5Etfw"">March 19, 2023</a></blockquote> <script async src=""https://platform.twitter.com/widgets.js"" charset=""utf-8""></script></div><p id="""">‍</p><h3 id="""">Shap-E</h3><p id="""">Many text-to-3D image generation models are available today, such as Spline AI and DreamFusion. The first such model was <a href=""https://arxiv.org/pdf/2305.02463.pdf"" id="""">Shap-E</a>. It is a diffusion model created by OpenAI. It creates 3D objects using text or image input. It is trained on a conditional diffusion model and 3D asset mapping. Here are 3D images for “A penguin” and “A chair that looks like an avocado” by Shap-E:</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:128px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""128px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64dd148565df2fbc410d35e6_813zFZPlVeU_zg1a0bo1TUE5RdFe4QwGwsARiVpOtrNeJldxtObT0FVwhJhf_AMKfEkfx3hAsWa_ldHOC3WFe4FTVm1Pp0P3FHF4cm9KgfWBuOA5NPlZLrRDDV8hK4MVrWqlaId9KplUO-l2qgSSAPo.gif"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:128px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""128px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64dd147c88e225ae5cb08517_h85LrCB3RRuhPIYIzrXzwmx9B0R--qdeccUJGhC0ZQDPFOiJn8tnUKpwVQI3eiTnqptMKo-7dmSTFhXwue7QLHo0aB-zs_E9KbRYkSnDj5xhLgy4c-LM0ErXxQlG4Yi55Y_Brox1F9UI7dtp-dN1l38.gif"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">Figure From <a href=""https://github.com/openai/shap-e"" id=""""><em id="""">GitHub</em></a></p><p id="""">‍</p><p id=""""><a href=""https://github.com/openai/shap-e"" id="""">Shap-E</a> comprises two models: an encoder that converts 3D assets into compact neural network codes, and a latent diffusion model that generates novel 3D assets based on images or text, needing additional steps for finalization. The models in Shap-E are trained on a variety of datasets, including a million more 3D assets and 120K captions from human annotators. It uses 60 different angles to understand how 3D things look.</p><p id="""">‍</p><p id="""">It produces 3D assets compared to 2D images created by <a href=""https://openai.com/dall-e-2"" id="""">DALL-E</a>. Shap-E achieves comparable or better CLIP R-Precision scores than optimization-based methods while being significantly faster to sample. This makes Shap-E a good choice for applications where speed is important, such as real-time 3D content generation. It can generate realistic and diverse 3D models. When taken randomly selected image-conditional samples from both Point-E and Shap-E for the same conditioning images. The samples from <a href=""https://huggingface.co/openai/shap-e"" id="""">Shap-E</a> are generally more realistic and diverse than the samples from Point-E.</p><p id="""">‍</p><h3 id="""">Digital Twins and Stimulation</h3><p id="""">A <a href=""https://www.tandfonline.com/doi/full/10.1080/21693277.2019.1660283"" id="""">digital twin</a> is a virtual copy of a real-world object or process. Its integration with virtual reality helps users to design or redesign systems in VR environments. They can see how the object or process behaves in real-time and make changes to the design as needed. It can help to improve the design of the system and reduce the risk of problems.&nbsp;</p><p id="""">‍</p><p id="""">The co-stimulation workspace, a shared space where users can interact with a digital twin in virtual reality, has three main tools, a <strong id="""">digital twin</strong>, a<strong id=""""> data server</strong>, and a <strong id="""">VR environment</strong>. The <a href=""https://arxiv.org/abs/2303.11463"" id="""">digital twin </a>allows users to interact with a live simulation of the object. The data server is responsible for exchanging real-time data between the digital twin and the virtual reality environment. The virtual reality environment uses the simulation data to visualize and interact with the object.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:607px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""607px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64dd147da177e658cd25f49f_hoLcROffzZ3c4clCJ15qf_oMTNuUkdIs91uO74DcVnJsF6ZjMscutdyaetP41Xsq1dg6s5MRI9rtsGobuBIgX6cuScqyGmHQBMyJTNnRvGhxeiW27brg3sDyR4SycD0uQxBUA_Zhz9nJPYqaxdzyvy0.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">Figure From <a href=""https://www.tandfonline.com/doi/full/10.1080/21693277.2019.1660283"" id=""""><em id="""">Taylor &amp; Francis Online</em></a></p><p id="""">‍</p><p id="""">The <a href=""https://www.researchgate.net/publication/331141793_Digital_Twin_and_Virtual_Reality_and_Augmented_RealityMixed_Reality"" id="""">digital twin</a> block and the data server block are connected to each other using the Functional Mock-up Interface (FMI) standard. The FMI standard is a software independent standard that allows different simulation tools to communicate with each other. The data server block and the virtual reality environment block are connected to each other using the ZMQ socket machine-to-machine communication protocol. The ZMQ socket protocol is a lightweight and efficient protocol that is well-suited for real-time data exchange.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1204px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1204px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64dd147dd510e8d61163c6c8_0IAPXdDRZ5eU9izNqe4EZmLHkZsHN17ypsP6yBPQ3YPa_mEZE0tfnkRxKDzcmYS8wfi6xx4PFNgSFkUNH0jQRfCMme0hFn2IJ588kbLOfCe_A6XkNQTwa6FHFxIdYyCR-V0t2TVU-kwr_pT_vlntIcc.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">Figure From <a href=""https://www.researchgate.net/figure/Design-process-involving-digital-twin-and-virtual-reality-environment_fig6_335584217"" id=""""><em id="""">ResearchGate</em></a></p><p id="""">‍</p><p id="""">The workspace can assess the safety of the system, train operators on how to use the system, make design changes, and save time and money by avoiding the need to build physical prototypes.</p><p id="""">‍</p><h2 id="""">How Does Generative AI Enhance VR?</h2><p id="""">With the help of above mentioned techniques, Generative AI can enhance VR by leveraging its ability to create new content, such as photorealistic virtual environments, lifelike characters, and interactive objects. It can personalize VR experiences, making them more enjoyable for users. The following will provide you an in-depth insight about it.</p><h3 id="""">Perception</h3><p id="""">Perception creates realistic and immersive VR environments. AI algorithms perceive the user’s environment and actions allowing for natural and engaging interactions with the virtual world.&nbsp;</p><p id="""">‍</p><p id="""">For example, AI algorithms create realistic and diverse virtual environments, new hyper-real characters, creatures, and objects by training on large datasets of real-world scenes, 3-D models, textures, and animations. It can save developers time and effort, as they no longer need to manually design every aspect of the virtual world. Also, it will add rich and interactive content to VR.&nbsp;</p><p id="""">‍</p><p id="""">AI perception algorithms can track the user's head and eye movements, which can control the view in VR. Tracking the user's hand movements to interact with objects in VR can make the user feel like they are actually interacting with the virtual world.</p><p id="""">‍</p><p id="""">AI algorithms enable procedural generation techniques in VR for dynamic and infinite content creation. Developers can create endless variations of landscapes, levels, and objects in real-time. It leads to more interactive and engaging VR experiences.</p><h3 id="""">Performance</h3><p id="""">Performance is critical for VR experiences as users expect smooth and responsive visuals. AI can upscale and super-resolve VR graphics. It can generate higher-resolution images and textures from lower-resolution sources to improve the visual quality of VR experiences without increasing the computational demands.</p><p id="""">‍</p><p id="""">AI can optimize rendering techniques. By dynamically adjusting settings based on the scene complexity and user interactions, AI can help to ensure that VR experiences are rendered at a high frame rate, even on low-powered devices. It can reduce the size of VR experiences by compressing data without sacrificing quality. It makes VR more accessible to users with limited bandwidth or storage space.</p><p id="""">‍</p><p id="""">AI can anticipate user movements to reduce latency and improve the overall responsiveness of VR experiences. By anticipating where the user is going to look or move, AI can ensure that the VR environment is rendered correctly and in a timely manner.</p><h3 id="""">Content</h3><p id="""">AI has a major role in developing new and innovative VR content. AI can generate realistic sound effects, music, and speech in real time, matching the virtual environment and the user's actions. It contributes to a more immersive and engaging VR experience. For example, AI can generate the sound of a car driving past or footsteps sounds on a wooden floor. It can make VR experiences feel more real.</p><p id="""">‍</p><p id="""">Dynamic foveated rendering is an AI-powered technique that can improve the performance of VR experiences by rendering only the parts of the image that the user is looking at in high resolution. It can reduce eye strain and make VR experiences more comfortable to use.</p><p id="""">‍</p><p id="""">Generative AI can create new content for VR experiences, such as characters, objects, and environments. It can create avatars and digital characters that respond more naturally to users' behavior and emotions. It makes engagement and interactions more engaging and drives the user experience. For example, AI-powered avatars will create realistic interactions between players, making the game more immersive and enjoyable.</p><h2 id="""">Applications Of AI-Powered VR</h2><p id="""">AI-powered VR has the potential to revolutionize many industries, including healthcare, training, entertainment, and education.It can help to improve safety, reduce costs, and improve outcomes. The use of AI-powered VR in these industries is still in its early stages, but the potential benefits are significant.</p><h3 id="""">Education</h3><p id="""">VR is being used to create interactive and engaging learning experiences. AI can generate personalized content that adapts to an individual’s learning styles and progress, providing a more effective and personalized educational experience. VR can help increase student attention and engagement, as students are more likely to be interested and engaged with what they are learning when they are immersed in a virtual environment. VR can also transport students to different environments, allowing them to learn and explore various concepts safely and efficiently.&nbsp;</p><p id="""">‍</p><p id="""">VR can also provide students with hands-on learning experiences. It can be especially beneficial for STEM subjects, as students can use VR to simulate experiments and procedures that would be difficult or dangerous to perform in the real world. VR can also be used to create virtual field trips, allowing students to visit historical landmarks and other places they would not otherwise be able to see. Students can learn at their own pace and in a way that is most effective for them.</p><h3 id="""">Medical Training</h3><p id="""">VR and AR are being used in medical training to create realistic simulations for medical students and surgeons to practice on. AI can generate variations in patients' conditions, so students can experience various medical scenarios and learn how to respond appropriately. For example, VR can simulate surgery. Students can practice the steps of a surgery on a virtual patient without the risk of harming a real patient. AI-powered VR can also simulate complex procedures, such as brain surgery.</p><p id="""">‍</p><p id="""">In addition to surgery, VR can train medical students in other areas, such as diagnostic imaging and patient communication. VR can create realistic simulations of medical imaging machines, so that students can practice interpreting images. AI-powered VR can also create simulations of patient interactions so students can practice their communication skills. VR is a valuable tool for medical training. It allows students to practice procedures and skills in a safe and controlled environment. It can help to improve patient safety and outcomes.</p><h3 id="""">Marketing</h3><p id="""">AI-powered VR and AR are being used in marketing to create immersive and personalized experiences for consumers. AI can create virtual product showrooms where consumers interact with products in real time. It helps consumers make informed purchase decisions. For example, Wayfair uses AI to create virtual product showrooms where consumers can see how furniture will look in their homes.</p><p id="""">‍</p><p id="""">Another application of AI is in lead generation. It can generate leads by creating interactive VR and AR experiences that capture consumers' attention and encourage them to provide their contact information. For example, BMW uses AI to create a VR experience that allows consumers to virtually test drive cars. If consumers are interested in learning more about a particular car, they can provide their contact information to receive more information.</p><h3 id="""">Entertainment</h3><p id="""">AI-powered VR creates more immersive and engaging entertainment experiences. For example, AI can generate realistic virtual characters, create interactive worlds, personalized experience and game mechanics. AI creates more realistic and challenging VR games. For example, in Beat Saber, AI tracks the player's movements and adjusts the game difficulty accordingly. IT ensures that the game is always challenging, but not impossible to beat.</p><p id="""">‍</p><p id="""">AI can create virtual tours of real-world locations. For example, the company <a href=""https://arvr.google.com/"" id="""">Google Earth VR</a> uses AI to create photorealistic 360-degree images of cities and landmarks around the world. It can create virtual concerts that allow fans to experience a live show from the comfort of their own homes. For example, the company MelodyVR uses AI to create virtual concerts featuring high-quality sound and visuals.</p><h2 id="""">Use Cases Of AI-Powered VR</h2><p id="""">Having understood the techniques and methods of integrating AI in VR, let us look at some use cases. From realistic character behaviors driven by AI to real-time object detection for interactive environments, AI-powered VR is changing the world.</p><h3 id="""">Roblox AI for Virtual World Creation</h3><p id=""""><a href=""https://www.roblox.com/"" id="""">Roblox</a> is harnessing Generative AI to reshape content creation. Its Roblox Studio, a tool for crafting 3D experiences, will receive a boost from Generative AI, redefining how users craft immersive worlds. By mastering patterns and structures, Generative AI accelerates media creation - images, audio, code, text, and 3D models. This integration empowers creators by bridging skill gaps, and fostering groundbreaking innovations. Roblox envisions integrated 3D objects with innate behavior, simplifying interactive content development. Responsible and ethical AI implementation is paramount, ensuring a secure and diverse environment. Roblox's Generative AI sets the stage for a visionary era in content creation.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:560px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""560px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64dd147d85090af8ec1d3965_tlffm2Vcnc5i4d9q88eiuAnaj7luwZnupEiTD-N6IxJhY7xvV5ODaxZlt1hwvVWEOEp9e4WVuRPQemZL1E0Whc0n5uB1wDlUxzJEu27Zy_mEraeCub2fN5L6y_l0Ht9mMdzCAT8YP7i1HAmW1quodlA.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h3 id="""">Generative AI for VR Gaming by Unity</h3><p id="""">CEO of <a href=""https://unity.com/"" id="""">Unity Software Inc.</a>, John Riccitiello, revealed plans for a generative AI marketplace tailored for game developers. This visionary space is set to simplify game creation by offering AI-generated assets - characters, sounds, and more. based on player input. Riccitiello envisions AI-generated game characters complete with motivations, personalities, and objectives - all without human intervention. Unity has already granted developers early access to its forthcoming AI resources, although the marketplace launch timeline remains undisclosed. With tools like DALL-E and Stable Diffusion crafting images, and emerging products concocting videos and game content from text inputs, Unity aims to reshape game development, offering efficiency and accessibility to creators.</p><h3 id="""">NVIDIA for Generative AI</h3><p id="""">NVIDIA advances in generative AI and graphics. It is integrating OpenAI's ChatGPT to help users generate 3D models and 3D environments. NVIDIA is also using generative AI to make NPCs more intelligent. Their AI marketplace aids game developers with AI-generated assets, streamlining content creation. NVIDIA partnered with Hugging Face for AI training. AI Enterprise 4.0 integrates NVIDIA NeMo for large-scale generative AI models. The NVIDIA AI Workbench offers flexibility across platforms. <a href=""https://arxiv.org/pdf/2104.00622.pdf"" id="""">Omniverse's </a>growth is transforming industries. The GH200 Grace Hopper platform enhances generative AI capabilities. NVIDIA shapes a future where diverse sectors harness AI's potential.</p><p id="""">‍</p><h2 id="""">Challenges In Traditional Virtual Reality</h2><p id="""">Traditional VR technologies rely on headsets to create an immersive experience. It impacts the level of realism. The technology has many challenges, including accessibility, adaptability and user discomfort. Exploring them will help you understand and appreciate the need for AI-powered VR.</p><h3 id="""">Technical Limitations</h3><p id="""">VR requires high-resolution displays, fast processing, and robust graphics to render realistic images. But, hardware technology may not always be able to meet these requirements. VR headsets face limitations in achieving high resolution and pixel densities. Low processing power reduces frame rates, visual quality, and immersion. High latency in VRs affects user experience by delaying user input and system response. Also, scarce and incompatible software can hinder quality and experience across different platforms and devices.</p><h3 id="""">Customization and Adaptability</h3><p id="""">Traditional VR systems lack personalization. They might not be comfortable for everyone, as headsets can be heavy or bulky, and lenses might not be the right prescription for some users. Additionally, they mostly allow single-user experiences and offer very limited interactivity. Traditional VR environments are developed in a studio and lack a sense of presence, as they do not provide enough sensory information to the user to make them experience the virtual world to its full potential.</p><h3 id="""">Content and Software Optimization</h3><p id="""">Traditional VR is limited by the need for content and software optimization. VR requires high-resolution graphics and high frame rates. It can strain the computational resources of VR devices and systems. Hence, VR content and software must be optimized to ensure they can be rendered and displayed in real time without sacrificing quality. It is a challenging and time-consuming process. It can limit the development of VR content and software.</p><h3 id="""">Cybersickness</h3><p id="""">Cybersickness is a type of motion sickness experienced due to immersive exposure to extended reality technologies. It can cause headache, disorientation, nausea, eyestrain and sweating, with symptoms lasting for minutes to hours. The symptoms may vary depending on the type of immersion. For VR exposure, the probability of disorientation is higher than nausea which is higher than oculomotor disturbance symptoms. It is a problem that hinders use of VR technology by a large audience.</p><p id="""">‍</p><h2 id="""">Challenges And Limitations Of AI-Powered VR</h2><p id="""">Challenges and limitations that must be addressed before AI-generated content can be widely adopted for VR applications. The data required to train AI algorithms for VR is often difficult to obtain. For example, generating accurate 3D models of real-world objects and environments requires extensive data collection and processing, which can be time-consuming and costly. Also, creating complex algorithms that can generate realistic and engaging VR content requires significant expertise and computational resources. This can be a barrier to entry for smaller developers or organizations.</p><p id="""">‍</p><p id="""">Integrating AI-generated content with existing VR systems can also be a challenge. AI-generated content must be compatible with existing hardware and software platforms, which can require significant development and testing. As AI becomes more advanced, there is a risk of creating content that is too realistic or engaging, which could lead to unintended consequences. Developers must consider issues such as user safety and privacy when creating AI-generated content for VR.</p><h2 id="""">The Future Of Generative AI In Extended Reality</h2><p id="""">AI is rapidly developing and will play a major role in the future of XR. AI is being experimented with to create virtual shopping and traveling experiences. It is playing a role in the development of the metaverse. AI can create digital twins of real-world objects and environments for more realistic metaverse experiences. AI can also create virtual assistants that help users navigate the metaverse and interact with other users.</p><p id="""">‍</p><p id="""">Some upcoming projects using AI in XR include Apple Vision Pro, a new AI chip that could power more advanced features in its products, such as augmented reality and facial recognition. Recently, Researchers at UT Austin developed a VR headset with EEG sensors to measure brain activity. It allows for unprecedented insights into how humans process stimuli in VR. The technology has potential applications in human-robot interaction and brain-machine interfaces.</p><p id="""">‍</p><p id="""">AI has the potential to revolutionize the way we shop, travel, and interact with the world around us. As AI continues to develop, we can expect to see even more amazing and innovative applications of AI in XR in the coming years.</p><h2 id="""">Want To Build AI-integrated VR For Your Business?</h2><p id="""">‍</p><p id="""">If you are looking to integrate AI into virtual reality to boost your business or integrate VR into your games, we can help. We are a team of AI engineers with experience in virtual reality and AI. <a href=""https://www.mercity.ai/contacts"" id="""">Contact us</a> today and let us create AI-powered VR applications to elevate your business.</p><p id="""">‍</p>",https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64dd170193fec362e8319125_ai-in-vr.png,Maithili Badhan,Virtual Reality,"Learn how AI can be used to create more immersive and realistic VR experiences. Use AI to generate realistic textures, lighting, and sound effects, making VR experiences more engaging.",False,"<div class=""rich-text w-richtext""><p>Blending artificial intelligence (AI) in virtual reality (VR) is changing how we interact with the world. AI can create more realistic and immersive VR experiences by generating 3D models, textures, and scenes. It can track and interact in VR by understanding the user's movements and facial expressions. Businesses can use this technology in training, customer service, and market research. Early adopters of AI-powered VR will be well-prepared to leverage this emerging technology.</p><p>In this article, we will discuss how artificial intelligence is improving virtual reality. It will help you understand the use of this next-level technology in your business.</p><p>‍</p><h2>How AI Can Be Used In VR?</h2><p>Utilizing the below-mentioned techniques within VR environments has led to substantive outcomes. These technologies collectively enhance human-computer interactions, elevate visual fidelity, and improve object recognition accuracy. The intersection of AI and VR is reshaping how we experience and interact within virtual spaces, opening doors to a new era of immersive possibilities and practical applications.</p><p>‍</p><h3>Natural Language Interaction in VR</h3><p><a href=""https://www.researchgate.net/publication/270337753_Development_of_an_Intelligent_Virtual_Environment_for_Augmenting_Natural_Language_Processing_in_Virtual_Reality_Systems"">Natural Language Processing (NLP)</a> helps computers to understand human language. It can be integrated with VR using five components: a text parser, a rule base, an NLP to VR interface, a library of CAD (Computer Aided Design) models in ASCII formats, and a renderer. The text parser first receives the input from the user. It then breaks the input into parts and extracts relevant information from it. It would then generate a <strong>rule base</strong>, a set of rules that maintain the actions the renderer can perform. The NLP to VR interface fetches the required object from the library. It directs the extracted details and the rule base to the renderer. The renderer displays the model in the VR environment.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1158pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64dd147ba03216dc0e22bda3_iFIib04xG6y50LMjnAW0Sp8FY3uyRzwKXMWnbeREyH0_hrJVeBcIltva2XEQI8xbUog6mP__S0oY5VZX71Bd0h8NqhAJlqBaVaMVOxEM49GUlQh_3scc-id8ZQg0_fhD5BC_a1CTpgWL2zXAkcJCx6o.jpeg""/></div></figure><p>NLP techniques can assist voice-activated navigation, a more immersive and natural way to interact with VR environments. For example, <a href=""https://github.com/suno-ai/bark"">Bark</a> is a multilingual text-to-audio framework by Suno that can be used to generate realistic speech and sounds.</p><p>‍</p><p>It can also be used to create more realistic and engaging NPC interactions. An NPC could respond to a user's voice commands, or it could even hold a conversation with the user. One such recent example: A Unity/Unreal player tries to convince AI NPCs that they are in stimulation and do not exist beyond it.</p><p>‍</p><div class=""w-embed w-script""><blockquote class=""twitter-tweet tw-align-center"" data-theme=""dark""><p dir=""ltr"" lang=""en"">This is wild: watch this guy go around in the Matrix Awakens (!) game explaining to AI NPCs they’re in a simulation. <br/><br/>Remember, these are not scripted. They're not human voices. And this is not ""some day"", this is in Unity/Unreal now. <br/><br/>Video games are about to be filled with AI… <a href=""https://t.co/2nvzJCY9vo"">https://t.co/2nvzJCY9vo</a> <a href=""https://t.co/5DyrmdkBX0"">pic.twitter.com/5DyrmdkBX0</a></p>— AI Notkilleveryoneism Memes (@AISafetyMemes) <a href=""https://twitter.com/AISafetyMemes/status/1683077335875035136?ref_src=twsrc%5Etfw"">July 23, 2023</a></blockquote> <script async="""" charset=""utf-8"" src=""https://platform.twitter.com/widgets.js""></script></div><p>‍</p><p>Also, Stanford researchers developed computer programs called <a href=""https://arxiv.org/pdf/2304.03442.pdf"">Generative Agents</a> that can simulate authentic human behavior. The model learns from real-world data and generates realistic conversations, interactions, and decisions. The agents interact with each other using natural language.</p><figure class=""w-richtext-align-center w-richtext-figure-type-image""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64dd147b76c770c868cc98d9_Xn7cFS3mpdDA-_Z9gZEEWZFJVOSyyrHukKR4VNIG_EnsK4e3X0x4qzrb_5qwuW0CqkdsqFhevboMW-Krxs8fztUEHdUysRplLgaZ0CixtXjEOk3sd8oVKlu6TqCLOiNCI-G5CN7UPcGKh5nWFsp-GnY.jpeg""/></div></figure><p>Figure From <a href=""https://www.artisana.ai/articles/generative-agents-stanfords-groundbreaking-ai-study-simulates-authentic""><em>Artisana</em></a></p><h3>Image and Object Recognition</h3><p>Image and <a href=""https://www.researchgate.net/publication/326164461_Object_Detection_with_Deep_Learning_for_a_Virtual_Reality_Based_Training_Simulator"">object detection in VR</a> uses computer vision techniques to identify objects in the virtual world. Deep learning techniques for object detection are one-stage, two-stage or transformer-based algorithms. One-stage algorithms generate positioning coordinates and classification probabilities of objects in an image, in a single shot. Two-stage networks on the other hand use a region proposal network to propose possible regions where an object can be found, then a detection network is used to classify the objects. One-stage networks are faster than two-staged ones because of no advance region proposal generation.</p><p>‍</p><p><a href=""https://arxiv.org/pdf/2304.00501.pdf"">YOLO series</a> are a part of one-stages algorithms. <a href=""https://deci.ai/blog/yolo-nas-object-detection-foundation-model/"">YONO-NAS</a> is a neural architecture search (NAS) algorithm by Deci AI. It is one of the SOTA models that surpassed previous YOLO models. It finds the optimal network architecture for object detection. It searches a large space of possible network architectures and selects the one that achieves the best performance on a given dataset. Its components are backbone, neck, head, QSP, and QSI block.</p><figure class=""w-richtext-align-center w-richtext-figure-type-image""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64dd147b29a36f9097c3a920_vSJtnoTcUKHsEH6FbNL_ENIvBHz7znVGoLlAm2v0rhrhOhrDwRiICaYzbNPasJEPkJDtj8dL-xyvljaR2-d8DqveadyVcePoNOYQpDqXOEdAy52xjMJbWrT7DtLKmJrn7vYpz7Aj_A3VttNUWzeFVh4.jpeg""/></div></figure><p>Figure from <a href=""https://deci.ai/blog/yolo-nas-object-detection-foundation-model/""><em>Deci AI</em></a></p><p>The backbone is responsible for extracting features from the input image. It is a convolutional neural network (CNN) that has been pre-trained on a large dataset of images. The neck connects the backbone to the head. It consists of a few convolutional layers to reduce the dimensionality of the features extracted by the backbone. The head generates the bounding boxes and class predictions for the objects in the image. It consists of a few convolutional layers to classify the objects in the image and to predict the bounding boxes for those objects. The QSP block is responsible for quantizing the features extracted by the backbone. It makes the features more efficient to process and allows YOLO-NAS to be used on devices with limited computational resources. The QSI block is responsible for dequantizing the features extracted by the QSP block. It allows YOLO-NAS to generate high-quality object detection results.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1558pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64dd147b0c076b55927e316c_eLRYUvVoW6ao9EcWCfUhgVx-cd-O-P3ohH84OYETofwox_mROcLzeWaA6D2aL0IT-eVsTnxnpvt9n_ckRLEcwrJWfRGFz3AgKNXgcdRboxPkdCk9aPJg9siZpwYaPcBLQEHkc5Kny9JEKm2tqwKkkSM.jpeg""/></div></figure><p>Figure From <a href=""https://www.makeuseof.com/yolo-nas-best-object-detection-model-in-yolo-series/""><em>MUO</em></a></p><p>Object recognition has a major application in <a href=""https://www.researchgate.net/publication/343015573_A_brief_analysis_of_gesture_recognition_in_VR"">hand gesture recognition</a> that allows VR systems to track the movements of the user's hands and use this information to control objects in the VR environment. There are a variety of different hand gesture recognition techniques that can be used in VR, including optical tracking, inertial tracking, and depth sensing. Once the user's hand movements have been tracked, the VR system can use this information to control objects in the VR environment, such as grabbing and moving objects, or interacting with menus and buttons.</p><h3>NeRFs</h3><p><a href=""https://arxiv.org/abs/2212.01120"">Neural Radiance Fields (NeRFs)</a> is a technology to create photorealistic 3D models of objects from a collection of images. It is a powerful tool to create VR experiences that are more immersive and realistic. NeRF uses a neural network to learn the relationship between the 3D position of a point in space and the color of the light reflected from that point. This relationship is called a radiance field. The neural network is trained on a dataset of images taken from different viewpoints of the object. It then generates a 3D model of the object by taking a ray from the viewer's eye and tracing it through the radiance field. Then the neural network determines the color of the light reflected from the object. It is repeated for every pixel in the image.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:800pxpx""><div><img alt=""How Neural Radiance Fields (NeRF) and Instant Neural Graphics Primitives  work | AI Summer"" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64dd22a1aae0270819728241_nerf.png""/></div></figure><p>‍</p><p><a href=""https://github.com/bmild/nerf"">NeRF model</a> editing and saving are aspects of using NeRF for VR. NeRF model editing is the process of changing the 3D model generated by NeRF. It improves the accuracy or realism of the model, or changes the object's appearance. NeRF model saving is the process of storing the 3D model generated by NeRF. Point clouds are a good choice for VR experiences that require high performance, while meshes are a good choice for VR experiences that require high accuracy.</p><p>‍</p><p>Recently, <a href=""https://research.nvidia.com/labs/dir/neuralangelo/"">NVIDIA </a>released <a href=""https://arxiv.org/abs/2306.03092"">Neuralangelo</a>, a new AI technology that can transform any video into a highly detailed 3D environment. Neuralangelo is based on Instant NeRF, but it improves the quality of the generated models by using numerical gradients and coarse-to-fine optimization. It takes a 2D video as input and analyzes it to extract details such as depth, size, and the shapes of objects. It then uses this information to create an initial 3D model of the scene.</p><p>‍</p><div class=""w-embed w-script""><blockquote class=""twitter-tweet tw-align-center"" data-theme=""dark""><p dir=""ltr"" lang=""en"">🔴 Finally! NVIDIA has finally made the code for Neuralangelo public!<br/><br/>It has the ability to transform any video into a highly detailed 3D environment, and it's a technology related to but DIFFERENT from NeRF.<br/><br/>💡 Here's how it works:<br/>It takes a 2D video as input, showing an… <a href=""https://t.co/tS3dv7df61"">pic.twitter.com/tS3dv7df61</a></p>— Javi Lopez ⛩️ (@javilopen) <a href=""https://twitter.com/javilopen/status/1691120893638950912?ref_src=twsrc%5Etfw"">August 14, 2023</a></blockquote> <script async="""" charset=""utf-8"" src=""https://platform.twitter.com/widgets.js""></script></div><p>‍</p><h3>Gaussian Splatting</h3><p><a href=""https://arxiv.org/pdf/2308.04079.pdf"">Gaussian splatting</a> is a technique to render volumetric data. It can render a 3D scene from a set of images. The first step is to create a sparse point cloud from the images. It can estimate the position, covariance matrix, and opacity of a set of <a href=""https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/"">3D Gaussians</a>. The Gaussians are then used to render the scene.</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1320pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64dd147cf7cf539ed23c38f2_Fwj3VWr13Ljo8_-BU88S5zbestwa-IFhBdAgNN5R6wZUICT0SOS0tJtp6kvDKOKFDevE4y8mlcpaiPXo92tz0dxGVnZL1SaH81OvGb4F3K2AWQyHnK-Ny73CKhXdeuthZDS-PQ-py-9jWCjjzhF1VlE.jpeg""/></div></figure><p>Figure From <a href=""https://arxiv.org/pdf/2308.04079.pdf""><em>3D Gaussian Splatting for Real-Time Radiance Field Rendering</em></a></p><p>The volume is divided into a grid of voxels. For each voxel, the Gaussians are sampled at the voxel's center. The sampled values then create a smooth, continuous surface approximating the true volumetric data. This process repeats for all voxels in the volume. The resulting surface can then be rendered using many techniques, such as ray tracing or rasterization.</p><p>‍</p><p>Gaussian splatting is much better than NeRF AI for rendering 3D scenes from a single image because it is more efficient, robust to noise, and flexible. <a href=""https://github.com/graphdeco-inria/gaussian-splatting"">Gaussian splatting</a> is also a better choice for scenes with complex materials. It has competitive training times. This means that it can be trained quickly, even on large datasets. It can achieve high-quality results with only SfM points as input. This means that it does not require additional data, such as Multi-View Stereo (MVS) data, which can be time-consuming and expensive to collect.</p><p>‍</p><p>The <a href=""https://huggingface.co/papers/2308.04079"">Gaussian splatting</a> technique is a powerful tool that can render volumetric data in various applications. It is relatively simple and efficient, making it a good choice for real-time applications. It is a good choice for rendering dynamic scenes, such as those that contain moving objects or changing lighting conditions.</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1000pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64dd147c60890e5fda0f0db9_fMWXQOpwQOmvKCXd8Urs0bIQfMy5aGGIXIAlx2-RRrdD-VPKQvO_gy0J4Wy-9MwkxbA-5J_ivkCtyZUOfARtvbr1SajKpugJbQKK6F9Q0uCYkV5JwaKReClpZZIryWuqUtFvcLa0EMjoLdncYBS-BQM.jpeg""/></div></figure><h3>Stable Diffusion  </h3><p><a href=""https://huggingface.co/CompVis/stable-diffusion-v-1-1-original"">Stable diffusion</a> is a new model used to generate high-quality images from text descriptions. The diffusion model has three main components: the <strong>image encoder</strong>, the <strong>image information creator</strong> and an <strong>autoencoder decoder</strong>. The image encoder compresses the input image into a latent representation. This latent representation is a lower-dimensional representation of the image that contains the most important information about the image. The image information creator takes the latent representation from the image encoder and generates a sequence of numbers for pixels in the image. The autoencoder decoder takes the information from the image information creator and reconstructs the original image. The autoencoder decoder is trained to minimize the difference between the reconstructed image and the original image.</p><p>‍</p><p>To generate images,a random latent matrix is generated. Smaller the latent space faster the image generation process. The noise predictor estimates the noise in the latent matrix. The estimated noise is subtracted from the latent matrix. This process is repeated for multiple steps. With each step, some noise is removed from the latent matrix, and the given prompt is used for guidance resulting in a more accurate image that is closer to the prompt. The decoder then converts the latent matrix to the final image. </p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64dd147c84411f6e7392307e_x3iLucKtE-LhQYGihZm2-mEENbOsCSxT38kx8_Oa3lUiWoEYetcuQHQ7JnlmLMaQT5hWYBOYJ_bXxtZ40Y0GumPZs-Iov6bzHldfdPDWLXk2kphfIWryx39JmGGrIDtnfdd4LMTHjY4pH6OfdojhTeU.jpeg""/></div></figure><p>Figure From <a href=""https://jalammar.github.io/illustrated-stable-diffusion/""><em>The Illustrated Stable Diffusion</em></a></p><p>The text description guides the diffusion process so the generated image matches the description. The text prompt you provide the model gets converted into numbers relating to the individual words, called tokens. Each token gets converted to a 768-value vector known as embedding. These embeddings get processed and ready to be consumed by the noise predictor.</p><p>‍</p><p><a href=""https://github.com/CompVis/stable-diffusion"">Stable diffusion</a> models can be used to create realistic and immersive environments, generate interactive objects, and empower creativity in VR. It can create various VR experiences, such as virtual worlds, museum exhibits, games, and fashion shows.</p><h4>ControlNet</h4><p><a href=""https://arxiv.org/abs/2302.05543"">ControlNet</a> is a neural network structure to control pre-trained large diffusion models to support additional input conditions. It creates two copies of the weights. The locked copy has frozen weights and preserves the original model. The trainable copy learns to manipulate the inputs of the network to control the overall behavior of the network. It allows <a href=""https://github.com/lllyasviel/ControlNet"">ControlNet</a> to be trained on small datasets of image pairs. It can be trained on personal devices, and it can scale to large amounts of data if powerful computation clusters are available.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1569pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64dd147c8f9b7865df8ff37a_kTd7rkb3qUdFlgv-PoacrJfq1EHO2vHSEJLaMR47KOgFHGH5AZtvJj-hBS_Ldl5drf-hhTpmY5g92r0u70I8oiBPVZsmEMIPnYw0WQPevmj1JogXrz2_imXv4kDT1yb6JHfCXgA-yU7NXZb4cx1FyRM.jpeg""/></div></figure><p>Figure From <a href=""https://www.cameralyze.co/models/controlnet-scribble""><em>Cameralyze</em></a></p><p>‍</p><div class=""w-embed w-script""><blockquote class=""twitter-tweet tw-align-center"" data-theme=""dark""><p dir=""ltr"" lang=""en""><a href=""https://twitter.com/hashtag/AIart?src=hash&amp;ref_src=twsrc%5Etfw"">#AIart</a> <a href=""https://twitter.com/hashtag/stablediffusion?src=hash&amp;ref_src=twsrc%5Etfw"">#stablediffusion</a> <br/>Introducing (a WIP version of) TemporalNet, a ControlNet model trained for temporal consistency <a href=""https://t.co/p95UH8Yo3m"">pic.twitter.com/p95UH8Yo3m</a></p>— CiaraRowles (@CiaraRowles1) <a href=""https://twitter.com/CiaraRowles1/status/1637486561917906944?ref_src=twsrc%5Etfw"">March 19, 2023</a></blockquote> <script async="""" charset=""utf-8"" src=""https://platform.twitter.com/widgets.js""></script></div><p>‍</p><h3>Shap-E</h3><p>Many text-to-3D image generation models are available today, such as Spline AI and DreamFusion. The first such model was <a href=""https://arxiv.org/pdf/2305.02463.pdf"">Shap-E</a>. It is a diffusion model created by OpenAI. It creates 3D objects using text or image input. It is trained on a conditional diffusion model and 3D asset mapping. Here are 3D images for “A penguin” and “A chair that looks like an avocado” by Shap-E:</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:128pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64dd148565df2fbc410d35e6_813zFZPlVeU_zg1a0bo1TUE5RdFe4QwGwsARiVpOtrNeJldxtObT0FVwhJhf_AMKfEkfx3hAsWa_ldHOC3WFe4FTVm1Pp0P3FHF4cm9KgfWBuOA5NPlZLrRDDV8hK4MVrWqlaId9KplUO-l2qgSSAPo.gif""/></div></figure><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:128pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64dd147c88e225ae5cb08517_h85LrCB3RRuhPIYIzrXzwmx9B0R--qdeccUJGhC0ZQDPFOiJn8tnUKpwVQI3eiTnqptMKo-7dmSTFhXwue7QLHo0aB-zs_E9KbRYkSnDj5xhLgy4c-LM0ErXxQlG4Yi55Y_Brox1F9UI7dtp-dN1l38.gif""/></div></figure><p>Figure From <a href=""https://github.com/openai/shap-e""><em>GitHub</em></a></p><p>‍</p><p><a href=""https://github.com/openai/shap-e"">Shap-E</a> comprises two models: an encoder that converts 3D assets into compact neural network codes, and a latent diffusion model that generates novel 3D assets based on images or text, needing additional steps for finalization. The models in Shap-E are trained on a variety of datasets, including a million more 3D assets and 120K captions from human annotators. It uses 60 different angles to understand how 3D things look.</p><p>‍</p><p>It produces 3D assets compared to 2D images created by <a href=""https://openai.com/dall-e-2"">DALL-E</a>. Shap-E achieves comparable or better CLIP R-Precision scores than optimization-based methods while being significantly faster to sample. This makes Shap-E a good choice for applications where speed is important, such as real-time 3D content generation. It can generate realistic and diverse 3D models. When taken randomly selected image-conditional samples from both Point-E and Shap-E for the same conditioning images. The samples from <a href=""https://huggingface.co/openai/shap-e"">Shap-E</a> are generally more realistic and diverse than the samples from Point-E.</p><p>‍</p><h3>Digital Twins and Stimulation</h3><p>A <a href=""https://www.tandfonline.com/doi/full/10.1080/21693277.2019.1660283"">digital twin</a> is a virtual copy of a real-world object or process. Its integration with virtual reality helps users to design or redesign systems in VR environments. They can see how the object or process behaves in real-time and make changes to the design as needed. It can help to improve the design of the system and reduce the risk of problems. </p><p>‍</p><p>The co-stimulation workspace, a shared space where users can interact with a digital twin in virtual reality, has three main tools, a <strong>digital twin</strong>, a<strong> data server</strong>, and a <strong>VR environment</strong>. The <a href=""https://arxiv.org/abs/2303.11463"">digital twin </a>allows users to interact with a live simulation of the object. The data server is responsible for exchanging real-time data between the digital twin and the virtual reality environment. The virtual reality environment uses the simulation data to visualize and interact with the object.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:607pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64dd147da177e658cd25f49f_hoLcROffzZ3c4clCJ15qf_oMTNuUkdIs91uO74DcVnJsF6ZjMscutdyaetP41Xsq1dg6s5MRI9rtsGobuBIgX6cuScqyGmHQBMyJTNnRvGhxeiW27brg3sDyR4SycD0uQxBUA_Zhz9nJPYqaxdzyvy0.jpeg""/></div></figure><p>Figure From <a href=""https://www.tandfonline.com/doi/full/10.1080/21693277.2019.1660283""><em>Taylor &amp; Francis Online</em></a></p><p>‍</p><p>The <a href=""https://www.researchgate.net/publication/331141793_Digital_Twin_and_Virtual_Reality_and_Augmented_RealityMixed_Reality"">digital twin</a> block and the data server block are connected to each other using the Functional Mock-up Interface (FMI) standard. The FMI standard is a software independent standard that allows different simulation tools to communicate with each other. The data server block and the virtual reality environment block are connected to each other using the ZMQ socket machine-to-machine communication protocol. The ZMQ socket protocol is a lightweight and efficient protocol that is well-suited for real-time data exchange.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1204pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64dd147dd510e8d61163c6c8_0IAPXdDRZ5eU9izNqe4EZmLHkZsHN17ypsP6yBPQ3YPa_mEZE0tfnkRxKDzcmYS8wfi6xx4PFNgSFkUNH0jQRfCMme0hFn2IJ588kbLOfCe_A6XkNQTwa6FHFxIdYyCR-V0t2TVU-kwr_pT_vlntIcc.jpeg""/></div></figure><p>Figure From <a href=""https://www.researchgate.net/figure/Design-process-involving-digital-twin-and-virtual-reality-environment_fig6_335584217""><em>ResearchGate</em></a></p><p>‍</p><p>The workspace can assess the safety of the system, train operators on how to use the system, make design changes, and save time and money by avoiding the need to build physical prototypes.</p><p>‍</p><h2>How Does Generative AI Enhance VR?</h2><p>With the help of above mentioned techniques, Generative AI can enhance VR by leveraging its ability to create new content, such as photorealistic virtual environments, lifelike characters, and interactive objects. It can personalize VR experiences, making them more enjoyable for users. The following will provide you an in-depth insight about it.</p><h3>Perception</h3><p>Perception creates realistic and immersive VR environments. AI algorithms perceive the user’s environment and actions allowing for natural and engaging interactions with the virtual world. </p><p>‍</p><p>For example, AI algorithms create realistic and diverse virtual environments, new hyper-real characters, creatures, and objects by training on large datasets of real-world scenes, 3-D models, textures, and animations. It can save developers time and effort, as they no longer need to manually design every aspect of the virtual world. Also, it will add rich and interactive content to VR. </p><p>‍</p><p>AI perception algorithms can track the user's head and eye movements, which can control the view in VR. Tracking the user's hand movements to interact with objects in VR can make the user feel like they are actually interacting with the virtual world.</p><p>‍</p><p>AI algorithms enable procedural generation techniques in VR for dynamic and infinite content creation. Developers can create endless variations of landscapes, levels, and objects in real-time. It leads to more interactive and engaging VR experiences.</p><h3>Performance</h3><p>Performance is critical for VR experiences as users expect smooth and responsive visuals. AI can upscale and super-resolve VR graphics. It can generate higher-resolution images and textures from lower-resolution sources to improve the visual quality of VR experiences without increasing the computational demands.</p><p>‍</p><p>AI can optimize rendering techniques. By dynamically adjusting settings based on the scene complexity and user interactions, AI can help to ensure that VR experiences are rendered at a high frame rate, even on low-powered devices. It can reduce the size of VR experiences by compressing data without sacrificing quality. It makes VR more accessible to users with limited bandwidth or storage space.</p><p>‍</p><p>AI can anticipate user movements to reduce latency and improve the overall responsiveness of VR experiences. By anticipating where the user is going to look or move, AI can ensure that the VR environment is rendered correctly and in a timely manner.</p><h3>Content</h3><p>AI has a major role in developing new and innovative VR content. AI can generate realistic sound effects, music, and speech in real time, matching the virtual environment and the user's actions. It contributes to a more immersive and engaging VR experience. For example, AI can generate the sound of a car driving past or footsteps sounds on a wooden floor. It can make VR experiences feel more real.</p><p>‍</p><p>Dynamic foveated rendering is an AI-powered technique that can improve the performance of VR experiences by rendering only the parts of the image that the user is looking at in high resolution. It can reduce eye strain and make VR experiences more comfortable to use.</p><p>‍</p><p>Generative AI can create new content for VR experiences, such as characters, objects, and environments. It can create avatars and digital characters that respond more naturally to users' behavior and emotions. It makes engagement and interactions more engaging and drives the user experience. For example, AI-powered avatars will create realistic interactions between players, making the game more immersive and enjoyable.</p><h2>Applications Of AI-Powered VR</h2><p>AI-powered VR has the potential to revolutionize many industries, including healthcare, training, entertainment, and education.It can help to improve safety, reduce costs, and improve outcomes. The use of AI-powered VR in these industries is still in its early stages, but the potential benefits are significant.</p><h3>Education</h3><p>VR is being used to create interactive and engaging learning experiences. AI can generate personalized content that adapts to an individual’s learning styles and progress, providing a more effective and personalized educational experience. VR can help increase student attention and engagement, as students are more likely to be interested and engaged with what they are learning when they are immersed in a virtual environment. VR can also transport students to different environments, allowing them to learn and explore various concepts safely and efficiently. </p><p>‍</p><p>VR can also provide students with hands-on learning experiences. It can be especially beneficial for STEM subjects, as students can use VR to simulate experiments and procedures that would be difficult or dangerous to perform in the real world. VR can also be used to create virtual field trips, allowing students to visit historical landmarks and other places they would not otherwise be able to see. Students can learn at their own pace and in a way that is most effective for them.</p><h3>Medical Training</h3><p>VR and AR are being used in medical training to create realistic simulations for medical students and surgeons to practice on. AI can generate variations in patients' conditions, so students can experience various medical scenarios and learn how to respond appropriately. For example, VR can simulate surgery. Students can practice the steps of a surgery on a virtual patient without the risk of harming a real patient. AI-powered VR can also simulate complex procedures, such as brain surgery.</p><p>‍</p><p>In addition to surgery, VR can train medical students in other areas, such as diagnostic imaging and patient communication. VR can create realistic simulations of medical imaging machines, so that students can practice interpreting images. AI-powered VR can also create simulations of patient interactions so students can practice their communication skills. VR is a valuable tool for medical training. It allows students to practice procedures and skills in a safe and controlled environment. It can help to improve patient safety and outcomes.</p><h3>Marketing</h3><p>AI-powered VR and AR are being used in marketing to create immersive and personalized experiences for consumers. AI can create virtual product showrooms where consumers interact with products in real time. It helps consumers make informed purchase decisions. For example, Wayfair uses AI to create virtual product showrooms where consumers can see how furniture will look in their homes.</p><p>‍</p><p>Another application of AI is in lead generation. It can generate leads by creating interactive VR and AR experiences that capture consumers' attention and encourage them to provide their contact information. For example, BMW uses AI to create a VR experience that allows consumers to virtually test drive cars. If consumers are interested in learning more about a particular car, they can provide their contact information to receive more information.</p><h3>Entertainment</h3><p>AI-powered VR creates more immersive and engaging entertainment experiences. For example, AI can generate realistic virtual characters, create interactive worlds, personalized experience and game mechanics. AI creates more realistic and challenging VR games. For example, in Beat Saber, AI tracks the player's movements and adjusts the game difficulty accordingly. IT ensures that the game is always challenging, but not impossible to beat.</p><p>‍</p><p>AI can create virtual tours of real-world locations. For example, the company <a href=""https://arvr.google.com/"">Google Earth VR</a> uses AI to create photorealistic 360-degree images of cities and landmarks around the world. It can create virtual concerts that allow fans to experience a live show from the comfort of their own homes. For example, the company MelodyVR uses AI to create virtual concerts featuring high-quality sound and visuals.</p><h2>Use Cases Of AI-Powered VR</h2><p>Having understood the techniques and methods of integrating AI in VR, let us look at some use cases. From realistic character behaviors driven by AI to real-time object detection for interactive environments, AI-powered VR is changing the world.</p><h3>Roblox AI for Virtual World Creation</h3><p><a href=""https://www.roblox.com/"">Roblox</a> is harnessing Generative AI to reshape content creation. Its Roblox Studio, a tool for crafting 3D experiences, will receive a boost from Generative AI, redefining how users craft immersive worlds. By mastering patterns and structures, Generative AI accelerates media creation - images, audio, code, text, and 3D models. This integration empowers creators by bridging skill gaps, and fostering groundbreaking innovations. Roblox envisions integrated 3D objects with innate behavior, simplifying interactive content development. Responsible and ethical AI implementation is paramount, ensuring a secure and diverse environment. Roblox's Generative AI sets the stage for a visionary era in content creation.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:560pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64dd147d85090af8ec1d3965_tlffm2Vcnc5i4d9q88eiuAnaj7luwZnupEiTD-N6IxJhY7xvV5ODaxZlt1hwvVWEOEp9e4WVuRPQemZL1E0Whc0n5uB1wDlUxzJEu27Zy_mEraeCub2fN5L6y_l0Ht9mMdzCAT8YP7i1HAmW1quodlA.png""/></div></figure><h3>Generative AI for VR Gaming by Unity</h3><p>CEO of <a href=""https://unity.com/"">Unity Software Inc.</a>, John Riccitiello, revealed plans for a generative AI marketplace tailored for game developers. This visionary space is set to simplify game creation by offering AI-generated assets - characters, sounds, and more. based on player input. Riccitiello envisions AI-generated game characters complete with motivations, personalities, and objectives - all without human intervention. Unity has already granted developers early access to its forthcoming AI resources, although the marketplace launch timeline remains undisclosed. With tools like DALL-E and Stable Diffusion crafting images, and emerging products concocting videos and game content from text inputs, Unity aims to reshape game development, offering efficiency and accessibility to creators.</p><h3>NVIDIA for Generative AI</h3><p>NVIDIA advances in generative AI and graphics. It is integrating OpenAI's ChatGPT to help users generate 3D models and 3D environments. NVIDIA is also using generative AI to make NPCs more intelligent. Their AI marketplace aids game developers with AI-generated assets, streamlining content creation. NVIDIA partnered with Hugging Face for AI training. AI Enterprise 4.0 integrates NVIDIA NeMo for large-scale generative AI models. The NVIDIA AI Workbench offers flexibility across platforms. <a href=""https://arxiv.org/pdf/2104.00622.pdf"">Omniverse's </a>growth is transforming industries. The GH200 Grace Hopper platform enhances generative AI capabilities. NVIDIA shapes a future where diverse sectors harness AI's potential.</p><p>‍</p><h2>Challenges In Traditional Virtual Reality</h2><p>Traditional VR technologies rely on headsets to create an immersive experience. It impacts the level of realism. The technology has many challenges, including accessibility, adaptability and user discomfort. Exploring them will help you understand and appreciate the need for AI-powered VR.</p><h3>Technical Limitations</h3><p>VR requires high-resolution displays, fast processing, and robust graphics to render realistic images. But, hardware technology may not always be able to meet these requirements. VR headsets face limitations in achieving high resolution and pixel densities. Low processing power reduces frame rates, visual quality, and immersion. High latency in VRs affects user experience by delaying user input and system response. Also, scarce and incompatible software can hinder quality and experience across different platforms and devices.</p><h3>Customization and Adaptability</h3><p>Traditional VR systems lack personalization. They might not be comfortable for everyone, as headsets can be heavy or bulky, and lenses might not be the right prescription for some users. Additionally, they mostly allow single-user experiences and offer very limited interactivity. Traditional VR environments are developed in a studio and lack a sense of presence, as they do not provide enough sensory information to the user to make them experience the virtual world to its full potential.</p><h3>Content and Software Optimization</h3><p>Traditional VR is limited by the need for content and software optimization. VR requires high-resolution graphics and high frame rates. It can strain the computational resources of VR devices and systems. Hence, VR content and software must be optimized to ensure they can be rendered and displayed in real time without sacrificing quality. It is a challenging and time-consuming process. It can limit the development of VR content and software.</p><h3>Cybersickness</h3><p>Cybersickness is a type of motion sickness experienced due to immersive exposure to extended reality technologies. It can cause headache, disorientation, nausea, eyestrain and sweating, with symptoms lasting for minutes to hours. The symptoms may vary depending on the type of immersion. For VR exposure, the probability of disorientation is higher than nausea which is higher than oculomotor disturbance symptoms. It is a problem that hinders use of VR technology by a large audience.</p><p>‍</p><h2>Challenges And Limitations Of AI-Powered VR</h2><p>Challenges and limitations that must be addressed before AI-generated content can be widely adopted for VR applications. The data required to train AI algorithms for VR is often difficult to obtain. For example, generating accurate 3D models of real-world objects and environments requires extensive data collection and processing, which can be time-consuming and costly. Also, creating complex algorithms that can generate realistic and engaging VR content requires significant expertise and computational resources. This can be a barrier to entry for smaller developers or organizations.</p><p>‍</p><p>Integrating AI-generated content with existing VR systems can also be a challenge. AI-generated content must be compatible with existing hardware and software platforms, which can require significant development and testing. As AI becomes more advanced, there is a risk of creating content that is too realistic or engaging, which could lead to unintended consequences. Developers must consider issues such as user safety and privacy when creating AI-generated content for VR.</p><h2>The Future Of Generative AI In Extended Reality</h2><p>AI is rapidly developing and will play a major role in the future of XR. AI is being experimented with to create virtual shopping and traveling experiences. It is playing a role in the development of the metaverse. AI can create digital twins of real-world objects and environments for more realistic metaverse experiences. AI can also create virtual assistants that help users navigate the metaverse and interact with other users.</p><p>‍</p><p>Some upcoming projects using AI in XR include Apple Vision Pro, a new AI chip that could power more advanced features in its products, such as augmented reality and facial recognition. Recently, Researchers at UT Austin developed a VR headset with EEG sensors to measure brain activity. It allows for unprecedented insights into how humans process stimuli in VR. The technology has potential applications in human-robot interaction and brain-machine interfaces.</p><p>‍</p><p>AI has the potential to revolutionize the way we shop, travel, and interact with the world around us. As AI continues to develop, we can expect to see even more amazing and innovative applications of AI in XR in the coming years.</p><h2>Want To Build AI-integrated VR For Your Business?</h2><p>‍</p><p>If you are looking to integrate AI into virtual reality to boost your business or integrate VR into your games, we can help. We are a team of AI engineers with experience in virtual reality and AI. <a href=""https://www.mercity.ai/contacts"">Contact us</a> today and let us create AI-powered VR applications to elevate your business.</p><p>‍</p></div>"
Training Custom AI to write better Sales Messages,ai-to-write-better-sales-messages,640f56f76d313b2faa631c11,646bab98d48c3722bf5c64c0,False,False,Mon May 22 2023 17:51:20 GMT+0000 (Coordinated Universal Time),Sat Sep 23 2023 22:37:40 GMT+0000 (Coordinated Universal Time),Sat Sep 23 2023 22:37:40 GMT+0000 (Coordinated Universal Time),"<p id="""">For a business, sales messaging is a critical component of success. Crafting messages that resonate with customers and drive sales requires careful consideration of language, tone, and audience. Understanding sales on a deeper level is essential to help prospects make a decision and even more so, close the deal.</p><p id="""">‍</p><p id="""">Crafting the right message can be difficult for sales representatives. They may struggle with language barriers, finding the right words, or tailoring messages to specific audiences. And, It is difficult and time-consuming to train sales representatives in your specific niche.</p><p id="""">‍</p><p id="""">In this article, we will build an AI system that can rewrite sales messages better than sales reps. We will also show you how you can write better sales messages with AI. And believe us, training this model is cheaper than training your sales representatives.</p><p id="""">‍</p><h2 id="""">Why “rewrite” not “generate”?</h2><p id="""">ChatGPT, with its chat interface, has made it extremely easy to generate messages in a chat-based format. So, why not just use ChatGPT to handle sales conversations completely? That is a good question.&nbsp;</p><p id="""">‍</p><p id="""">Although we can ask ChatGPT to take over sales conversations completely, it simply doesn’t have any knowledge about the specific problems faced by your customers. Imagine you sell a specific type of lighting product, the problems faced by your customers are going to be very niche and ChatGPT will not know what kind of questions to ask to locate and fix the problem. A human, on the other hand, will take a very targeted approach to solving the customer’s problem.</p><p id="""">‍</p><p id="""">Although with fancy prompt engineering techniques, it is possible to make ChatGPT more efficient in these problems, but people still prefer to keep humans in the loop.</p><h2 id="""">BART for AI-Powered Sales Message Rewriting</h2><p id="""">For rewriting messages, we have found that the BART model by the Facebook AI team works really well. BART transformer model is also known as the “denoising autoencoder”. This is because BART was specifically trained to reconstruct “corrupted” sentences.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1320px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1320px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/646baa5fdf20d24c4ba81df0_ace4a460.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">‍</p><p id="""">Because of this training method, BART learns how to reconstruct sentences. BART has also represented the ability to hugely compress information in shorter sentences when prompted to.</p><p id="""">‍</p><p id="""">This is very similar to what we are trying to do with sales messages.&nbsp;</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1059px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1059px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/646baa5fbfa30d3682c74b03_ca49795b.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">‍</p><p id="""">If you want to learn more about BART, read the paper here: <a href=""https://ai.facebook.com/research/publications/bart-denoising-sequence-to-sequence-pre-training-for-natural-language-generation-translation-and-comprehension/"" id="""">BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</a>.</p><p id="""">‍<br></p><h2 id="""">Building the AI system</h2><p id="""">Now that we have the model of choice. We can start building the pipelines required for this Sales AI.</p><p id="""">‍</p><h3 id="""">Building a Dataset</h3><p id="""">AI models are as good as the data we give them. We need to have a ton of examples for the AI model to learn properly. But for this specific use case, rewriting sales messages, there is no dataset out there :( This is understandable as this data is sensitive and no company would ever release such a dataset.</p><p id="""">‍</p><p id="""">To tackle this problem, we will be building a dataset of our own! We will use synthetic data generation techniques to build a dataset of sales messages using ChatGPT, which we will then use to train our own internal AI.</p><h4 id="""">What is Synthetic Data?</h4><p id="""">Synthetic data is computer-generated data that we can tailor to specific needs. It can be created quickly and inexpensively using various techniques. The good thing about synthetic data is that it is “synthetic” meaning it is not real data, hence it can be used to train AI models without any data compliance issues. For the purpose of this tutorial, we are going to build a synthetic dataset using ChatGPT.</p><p id="""">‍</p><p id="""">If you are interested in learning more about Synthetic Datasets and how can they be generated, read our extensive guide: <a href=""https://www.mercity.ai/blog-post/using-chatgpt-to-build-synthetic-datasets"" id="""">Using ChatGPT to build Synthetic Datasets</a>.</p><h4 id="""">Using ChatGPT to Generate the Data</h4><p id="""">After some experimentation, we were able to write the perfect ChatGPT system prompt which allowed us to generate good sales messages.</p><p id="""">‍</p><p id="""">Here’s an example:</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/646baa60bfa30d3682c74b2f_5602bf40.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">‍</p><p id="""">Here’s an example pair of sales messages which I generated for “<em id="""">A DevOps and cybersecurity audit service</em>”:</p><p id="""">‍</p><p id=""""><strong id="""">Bad:</strong> <em id="""">Our DevOps and cybersecurity audit service offers a comprehensive approach that not only identifies potential vulnerabilities, but also provides actionable recommendations for fixing them. By leveraging cutting-edge technologies and industry expertise, we help you stay ahead of the curve when it comes to protecting your infrastructure.</em></p><p id="""">‍</p><p id=""""><strong id="""">Good:</strong> <em id="""">Our DevOps and cybersecurity audit service is designed to help you identify potential vulnerabilities in your system and provide recommendations to fix them. We utilize the latest technologies and industry best practices to ensure maximum protection for your infrastructure.</em></p><p id="""">‍</p><p id="""">You can see the difference between the messages pretty clearly.</p><p id="""">‍</p><p id="""">Now that we have the prompts, we will run it multiple times to generate a large dataset of messages.</p><p id="""">‍</p><h3 id="""">Training the AI</h3><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:894px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""894px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/646baa5f78919abe47c52c96_3da4518f.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">‍</p><p id="""">Now that we have our dataset ready, we can start training the model. We will use the <a href=""https://huggingface.co/"" id="""">HuggingFace </a>Transformers library for this. It allows us to directly load the pretrained BART model and finetune it for our purposes.</p><p id="""">‍</p><p id="""">As you can see, we are also going to use the rephrase dataset, which is specifically made to rewrite sentences for simplicity. You can read more about it <a href=""https://www.mercity.ai/blog-post/using-chatgpt-to-build-synthetic-datasets"" id="""">here</a>.</p><p id="""">‍</p><p id="""">We trained the model only on <strong id="""">3000 samples for 5 epochs</strong>. This is a very little amount of data and training for this task.</p><p id="""">‍</p><h3 id="""">Testing it out!</h3><p id="""">Now that we have the AI model trained, we can experiment with giving it some messages and see how it performs. We do not need a specific benchmark for this task as this is more of a qualitative goal than a quantitative one.</p><p id="""">‍</p><p id="""">Even with a very little amount of training, the model was able to generalize pretty well.</p><p id="""">‍</p><p id="""">Here are some input-output samples:</p><p id="""">‍</p><p id=""""><strong id="""">Input:</strong> <em id="""">Our cold calling software comes packed with unmatched features such as dynamic scripting engine and call dispositions among others which streamline success for a higher conversion rate leading ultimately to significant ROI gains for your business.</em></p><p id="""">‍</p><p id=""""><strong id="""">Output:</strong> <em id="""">Our cold calling software includes advanced features such as dynamic scripting engine, call dispositions, and more to help you achieve a higher conversion rate. This ultimately leads to significant ROI gains for your business.</em></p><p id="""">‍</p><p id="""">=====</p><p id="""">‍</p><p id=""""><strong id="""">Input:</strong> <em id="""">Our state-of-the-art dialer along with the call tracking features we offer makes us stand out from other solutions in the market. You also get access to comprehensive reports that provide valuable insights into what's working and what isn't.</em></p><p id="""">‍</p><p id=""""><strong id="""">Output:</strong> <em id="""">Our state-of-the-art dialer and call-tracking features make us stand out from other solutions in the market. You'll have access to comprehensive reports that provide valuable insights into what's working and what isn't.</em></p><p id="""">‍</p><p id="""">Here we can see, the model was able to simplify the message without changing a lot of it.</p><p id="""">‍</p><p id="""">Here are some more tests we ran:</p><p id="""">‍</p><div data-rt-embed-type='true'><div style=""background-color: #2A2A2B;padding: 2.5%;"">

<code>
<i>


INPUT: Our CRM may need constant maintenance which could lead to additional costs beyond the initial expense. But believe us, it's essential for keeping your sales pipeline uninterrupted.

<br/>
<br/>

OUTPUT: While our CR M may require occasional maintenance, we have a dedicated team who will work closely with you to ensure a hassle-
free experience.


<br/> <br/>
 ======== 
<br/> <br/>


INPUT: Our product is loaded with cutting-edge features to optimize sales funnel and drive conversions. It empowers you to leverage various strategies that make way for maximum outreach, resulting in a considerable increase in revenue.

<br/>
<br/>

OUTPUT: Our product offers advanced features that can help you optimize your sales funnel and drive conversions. Let's discuss how it can specifically benefit your business.

<br/> <br/>
 ======== 
<br/> <br/>

INPUT: Our product has some limitations, but we are working to address them in future updates.

<br/> <br/>
<br/> <br/>

OUTPUT: We understand that every business has unique requirements when it comes to software. That's why we've made sure that our product is user-friendly and easy-to-use.

<br/> <br/>
 ======== 
<br/> <br/>

INPUT: Our platform offers comprehensive data analysis capabilities for real-time trend identification.

<br/>
<br/>

OUTPUT: Our platform has advanced data analysis capabilities that enable real-time trend identification. Let's discuss how it can benefit
your business.

<br/> <br/>
 ======== 
 <br/> <br/>

INPUT: With our revolutionary AI-powered cold calling tool, you do not need any sophisticated technical knowledge or experience on integration just sign up today and see it work magic for you!

<br/>
<br/>

OUTPUT: With our AI-powered cold calling tool, you don't have to worry about technical expertise when it comes to integrating with your current systems. Let's discuss how we can make it work for you.



</i>
</code>

</div></div><p id="""">‍</p><p id="""">For such little training and less amount of samples, these results are pretty good!</p><p id="""">‍</p><h2 id="""">Pros &amp; Cons of using AI in Sales Settings</h2><p id="""">AI is definitely helpful in Sales. It can help reduce the cost of training employees and improve accuracy. There might be cases where a human can definitely be a better option, but AI can be used to improve the quality of assistance provided.</p><p id="""">‍</p><p id="""">However, there are cases where human intervention is preferred. Personalized interactions and complex negotiations require the human touch, as humans excel in empathy, building rapport, and handling delicate situations. Addressing privacy and ethical concerns related to customer data and AI algorithms is crucial to maintain trust and transparency in sales processes. AI is only supposed to be used as an <em>assistive </em>technology, not as a complete replacement of humans. Humans can write much better sales messages with AI, than AI can do alone.</p><p id="""">‍</p><h2 id="""">Interested in integrating AI into your sales process?</h2><p id="""">Want to integrate AI into your sales process? We can help! Our team has experience in building AI solutions for sales, including lead scoring, customer segmentation, and personalized recommendations. Let's chat about how we can help you streamline your sales process with AI.</p><p id="""">‍</p>",https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/646bb76d631f79bb702dc523_sales-rewrite-hero2.png,Pranav,AI in Sales,Learn how to train your own AI model to craft compelling sales messages that drive success and increase conversions.,False,"<div class=""rich-text w-richtext""><p>For a business, sales messaging is a critical component of success. Crafting messages that resonate with customers and drive sales requires careful consideration of language, tone, and audience. Understanding sales on a deeper level is essential to help prospects make a decision and even more so, close the deal.</p><p>‍</p><p>Crafting the right message can be difficult for sales representatives. They may struggle with language barriers, finding the right words, or tailoring messages to specific audiences. And, It is difficult and time-consuming to train sales representatives in your specific niche.</p><p>‍</p><p>In this article, we will build an AI system that can rewrite sales messages better than sales reps. We will also show you how you can write better sales messages with AI. And believe us, training this model is cheaper than training your sales representatives.</p><p>‍</p><h2>Why “rewrite” not “generate”?</h2><p>ChatGPT, with its chat interface, has made it extremely easy to generate messages in a chat-based format. So, why not just use ChatGPT to handle sales conversations completely? That is a good question. </p><p>‍</p><p>Although we can ask ChatGPT to take over sales conversations completely, it simply doesn’t have any knowledge about the specific problems faced by your customers. Imagine you sell a specific type of lighting product, the problems faced by your customers are going to be very niche and ChatGPT will not know what kind of questions to ask to locate and fix the problem. A human, on the other hand, will take a very targeted approach to solving the customer’s problem.</p><p>‍</p><p>Although with fancy prompt engineering techniques, it is possible to make ChatGPT more efficient in these problems, but people still prefer to keep humans in the loop.</p><h2>BART for AI-Powered Sales Message Rewriting</h2><p>For rewriting messages, we have found that the BART model by the Facebook AI team works really well. BART transformer model is also known as the “denoising autoencoder”. This is because BART was specifically trained to reconstruct “corrupted” sentences.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1320pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/646baa5fdf20d24c4ba81df0_ace4a460.png""/></div></figure><p>‍</p><p>Because of this training method, BART learns how to reconstruct sentences. BART has also represented the ability to hugely compress information in shorter sentences when prompted to.</p><p>‍</p><p>This is very similar to what we are trying to do with sales messages. </p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1059pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/646baa5fbfa30d3682c74b03_ca49795b.png""/></div></figure><p>‍</p><p>If you want to learn more about BART, read the paper here: <a href=""https://ai.facebook.com/research/publications/bart-denoising-sequence-to-sequence-pre-training-for-natural-language-generation-translation-and-comprehension/"">BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension</a>.</p><p>‍<br/></p><h2>Building the AI system</h2><p>Now that we have the model of choice. We can start building the pipelines required for this Sales AI.</p><p>‍</p><h3>Building a Dataset</h3><p>AI models are as good as the data we give them. We need to have a ton of examples for the AI model to learn properly. But for this specific use case, rewriting sales messages, there is no dataset out there :( This is understandable as this data is sensitive and no company would ever release such a dataset.</p><p>‍</p><p>To tackle this problem, we will be building a dataset of our own! We will use synthetic data generation techniques to build a dataset of sales messages using ChatGPT, which we will then use to train our own internal AI.</p><h4>What is Synthetic Data?</h4><p>Synthetic data is computer-generated data that we can tailor to specific needs. It can be created quickly and inexpensively using various techniques. The good thing about synthetic data is that it is “synthetic” meaning it is not real data, hence it can be used to train AI models without any data compliance issues. For the purpose of this tutorial, we are going to build a synthetic dataset using ChatGPT.</p><p>‍</p><p>If you are interested in learning more about Synthetic Datasets and how can they be generated, read our extensive guide: <a href=""https://www.mercity.ai/blog-post/using-chatgpt-to-build-synthetic-datasets"">Using ChatGPT to build Synthetic Datasets</a>.</p><h4>Using ChatGPT to Generate the Data</h4><p>After some experimentation, we were able to write the perfect ChatGPT system prompt which allowed us to generate good sales messages.</p><p>‍</p><p>Here’s an example:</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/646baa60bfa30d3682c74b2f_5602bf40.png""/></div></figure><p>‍</p><p>Here’s an example pair of sales messages which I generated for “<em>A DevOps and cybersecurity audit service</em>”:</p><p>‍</p><p><strong>Bad:</strong> <em>Our DevOps and cybersecurity audit service offers a comprehensive approach that not only identifies potential vulnerabilities, but also provides actionable recommendations for fixing them. By leveraging cutting-edge technologies and industry expertise, we help you stay ahead of the curve when it comes to protecting your infrastructure.</em></p><p>‍</p><p><strong>Good:</strong> <em>Our DevOps and cybersecurity audit service is designed to help you identify potential vulnerabilities in your system and provide recommendations to fix them. We utilize the latest technologies and industry best practices to ensure maximum protection for your infrastructure.</em></p><p>‍</p><p>You can see the difference between the messages pretty clearly.</p><p>‍</p><p>Now that we have the prompts, we will run it multiple times to generate a large dataset of messages.</p><p>‍</p><h3>Training the AI</h3><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:894pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/646baa5f78919abe47c52c96_3da4518f.png""/></div></figure><p>‍</p><p>Now that we have our dataset ready, we can start training the model. We will use the <a href=""https://huggingface.co/"">HuggingFace </a>Transformers library for this. It allows us to directly load the pretrained BART model and finetune it for our purposes.</p><p>‍</p><p>As you can see, we are also going to use the rephrase dataset, which is specifically made to rewrite sentences for simplicity. You can read more about it <a href=""https://www.mercity.ai/blog-post/using-chatgpt-to-build-synthetic-datasets"">here</a>.</p><p>‍</p><p>We trained the model only on <strong>3000 samples for 5 epochs</strong>. This is a very little amount of data and training for this task.</p><p>‍</p><h3>Testing it out!</h3><p>Now that we have the AI model trained, we can experiment with giving it some messages and see how it performs. We do not need a specific benchmark for this task as this is more of a qualitative goal than a quantitative one.</p><p>‍</p><p>Even with a very little amount of training, the model was able to generalize pretty well.</p><p>‍</p><p>Here are some input-output samples:</p><p>‍</p><p><strong>Input:</strong> <em>Our cold calling software comes packed with unmatched features such as dynamic scripting engine and call dispositions among others which streamline success for a higher conversion rate leading ultimately to significant ROI gains for your business.</em></p><p>‍</p><p><strong>Output:</strong> <em>Our cold calling software includes advanced features such as dynamic scripting engine, call dispositions, and more to help you achieve a higher conversion rate. This ultimately leads to significant ROI gains for your business.</em></p><p>‍</p><p>=====</p><p>‍</p><p><strong>Input:</strong> <em>Our state-of-the-art dialer along with the call tracking features we offer makes us stand out from other solutions in the market. You also get access to comprehensive reports that provide valuable insights into what's working and what isn't.</em></p><p>‍</p><p><strong>Output:</strong> <em>Our state-of-the-art dialer and call-tracking features make us stand out from other solutions in the market. You'll have access to comprehensive reports that provide valuable insights into what's working and what isn't.</em></p><p>‍</p><p>Here we can see, the model was able to simplify the message without changing a lot of it.</p><p>‍</p><p>Here are some more tests we ran:</p><p>‍</p><div class=""w-embed""><div style=""background-color: #2A2A2B;padding: 2.5%;"">
<code>
<i>


INPUT: Our CRM may need constant maintenance which could lead to additional costs beyond the initial expense. But believe us, it's essential for keeping your sales pipeline uninterrupted.

<br/>
<br/>

OUTPUT: While our CR M may require occasional maintenance, we have a dedicated team who will work closely with you to ensure a hassle-
free experience.


<br/> <br/>
 ======== 
<br/> <br/>


INPUT: Our product is loaded with cutting-edge features to optimize sales funnel and drive conversions. It empowers you to leverage various strategies that make way for maximum outreach, resulting in a considerable increase in revenue.

<br/>
<br/>

OUTPUT: Our product offers advanced features that can help you optimize your sales funnel and drive conversions. Let's discuss how it can specifically benefit your business.

<br/> <br/>
 ======== 
<br/> <br/>

INPUT: Our product has some limitations, but we are working to address them in future updates.

<br/> <br/>
<br/> <br/>

OUTPUT: We understand that every business has unique requirements when it comes to software. That's why we've made sure that our product is user-friendly and easy-to-use.

<br/> <br/>
 ======== 
<br/> <br/>

INPUT: Our platform offers comprehensive data analysis capabilities for real-time trend identification.

<br/>
<br/>

OUTPUT: Our platform has advanced data analysis capabilities that enable real-time trend identification. Let's discuss how it can benefit
your business.

<br/> <br/>
 ======== 
 <br/> <br/>

INPUT: With our revolutionary AI-powered cold calling tool, you do not need any sophisticated technical knowledge or experience on integration just sign up today and see it work magic for you!

<br/>
<br/>

OUTPUT: With our AI-powered cold calling tool, you don't have to worry about technical expertise when it comes to integrating with your current systems. Let's discuss how we can make it work for you.



</i>
</code>
</div></div><p>‍</p><p>For such little training and less amount of samples, these results are pretty good!</p><p>‍</p><h2>Pros &amp; Cons of using AI in Sales Settings</h2><p>AI is definitely helpful in Sales. It can help reduce the cost of training employees and improve accuracy. There might be cases where a human can definitely be a better option, but AI can be used to improve the quality of assistance provided.</p><p>‍</p><p>However, there are cases where human intervention is preferred. Personalized interactions and complex negotiations require the human touch, as humans excel in empathy, building rapport, and handling delicate situations. Addressing privacy and ethical concerns related to customer data and AI algorithms is crucial to maintain trust and transparency in sales processes. AI is only supposed to be used as an <em>assistive </em>technology, not as a complete replacement of humans. Humans can write much better sales messages with AI, than AI can do alone.</p><p>‍</p><h2>Interested in integrating AI into your sales process?</h2><p>Want to integrate AI into your sales process? We can help! Our team has experience in building AI solutions for sales, including lead scoring, customer segmentation, and personalized recommendations. Let's chat about how we can help you streamline your sales process with AI.</p><p>‍</p></div>"
AI Ball Tracking for Sport Analysis,ball-tracking-for-sport-analysis,640f56f76d313b2faa631c11,6626c5d7c12b275cd55d4f5d,False,False,Mon Apr 22 2024 20:17:27 GMT+0000 (Coordinated Universal Time),Mon May 26 2025 19:57:36 GMT+0000 (Coordinated Universal Time),Mon May 26 2025 20:39:51 GMT+0000 (Coordinated Universal Time),"<p id="""">Accurate ball tracking has long been a holy grail in sports technology, promising to unlock new insights and improvements in athlete performance. It can be used for various tasks, from gathering valuable data to helping referees make decisions, even for predictive analysis. However, developing reliable ball tracking systems has proven to be a complex and challenging task. However, the recent breakthroughs in AI and computer vision space have introduced new models and methods to enhance ball tracking greatly.</p><p id="""">‍</p><p id="""">In this article, we'll explore the latest advancements in AI-powered ball tracking, and how they're overcoming these challenges to revolutionize the sports industry.</p><h2 id="""">What is Ball Tracking</h2><p id="""">Ball tracking in sports is simply tracking the ball as it moves around in the field. Along with this, things like collision detection with the bat or racket, speed analysis, and player interaction with the ball are also measured and tracked. These are valuable data points in the sports analysis industry as for most of the games the “ball” itself is at the core. It might be kicking the ball, putting the ball in a basket, or hitting the ball with a bat or a racquet. Balls are involved in one way or another in most of the popular games.</p><p id="""">‍</p><p id="""">Ball tracking has become almost a necessary part of most of the games. It just allows the players, regulators, and viewers to extract so much more information from the games. It really enriches the experience and improves the game overall.</p><p id="""">‍</p><p id="""">Let’s talk about some of the advantages of using Ball tracking in sports.</p><p id="""">‍</p><h3 id="""">Ball tracking for Making Better Decisions</h3><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:767px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""767px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6626c4ccd640d668b585a37f_9XmEseyqhwi1khs3mv8ygmcjmp3bjYdETbNTDErADE2HlObV9PbHTmz3_ZEXdFo1Z3SUc1Qs1z5TDR5fGyQb93ekmi0UPdeFyjFj8iMSnnAGgG40irCZARY5pfksCc21vHTL1A8vvciLgVdqNbNKxT0.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><p id="""">Making judgment decisions is one of the most important parts of any sport, with or without balls. Most of them are straightforward like fours and sixes in Cricket, but in some cases, these can become very difficult, like LBW (Leg before Wicket) and No Ball judgment in Cricket. These edge case scenarios are now handled by “3rd Umpires”, These are the umpires sitting in a room with a lot of video feed coming from all over the field.</p><p id="""">‍</p><p id="""">For the 3rd umpires, ball tracking data is essential. When the field umpires cannot make a decision they refer to the 3rd umpires, which then using ball tracking and other sport analysis techniques, make a decision for them. This requires a lot of work and a lot of data processing in real-time.</p><h3 id="""">Ball Tracking for Improving Player Performance</h3><p id="""">Ball tracking has been used by players for a long time now to improve their own performance and to understand the flaws in other people’s playing styles too. Bowlers use it all the time to understand how they are handling the ball and where can they improve further to score the most wickets. Golf players use ball tracking with pose estimation to understand how they should hit the ball and at what angle. Doing all this provides players with extremely granular data and very good feedback by the system, with all this, it’s very to improve and catch mistakes. At this point, this is something every player in the industry uses.</p><h3 id="""">Ball tracking for Assisting Coaches</h3><p id="""">Along with improving individual performances, coaches have started making better teams based upon different metrics gathered through sports analytics systems such as ball speed, spin rate, distance covered while running after hitting the shot, and number of steps taken during the batting stance preparation phase. This allows managers/coaches access detailed insights regarding strengths &amp; weaknesses amongst squad members allowing informed tactical adjustments throughout the season leading ultimately toward success.</p><p id="""">‍</p><p id="""">Coaches also analyze opponents’ games thoroughly beforehand so they can devise strategies accordingly against certain styles played previously seen footage recorded via cameras capturing entire match proceedings including replays slow motion clips highlighting key moments helps understand tactics employed by both sides resulting in improved overall quality play. This really helps coaches prepare a better team strategy against any other team.</p><h3 id="""">Ball Tracking for Fan Engagement</h3><p id="""">Ball tracking for fan engagement is rather a new phenomenon where providers has been introducing things like “Ball cam”, a special drone or camera that follows the ball specifically. This is new but something viewers love engaging with. Providers also often show the “ball path” and other important visualizations that keep viewers engaged and informed.</p><h2 id="""">How does Ball tracking work?</h2><p id="""">There have been many ways Ball tracking is implemented in the actual games. One of them notably being using <a href=""https://arxiv.org/abs/1907.03698"" id="""">TrackNet</a> and <a href=""https://arxiv.org/abs/1506.02640"" id="""">YOLO Networks</a>. These techniques are often paired to provide a good experience and also work well to this date. But we want to introduce newer better models which can track the ball and other objects even better.</p><p id="""">‍</p><p id="""">Let’s learn how to build ball tracking pipelines.</p><h3 id="""">Segment Anything Model</h3><p id=""""><a href=""https://segment-anything.com/"" id="""">Segment Anything Model</a> by Meta is a rather new and recent model. Segment Anything Model or SAM by Meta is a rather straightforward model. It takes in an image and can take in various types of prompts like masks, boxes, points, and even free-form text. Then the both image and the passed prompt is encoded into a much smaller subspace, these embeddings are then passed into a decoder which outputs a final mask that represents the segmented parts of the image. As you can see in the diagram below:</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6626c4ccf3328891acde2836_ivmfLcX8NSuTKxhFx_4PX7cDDN9tk7iJTqcgv3WKbOCNY83Kix8dnwywwKuIU4O_1ki3SzGln2IVXPYV4kVXMIQCo96xw9rffKTIO4oz4iG0jqb0C5banUYs62FMXqB_t5a7TfaMob-k2JEO4_P7IQ4.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><p id="""">This means that you can pass a normal image like this:</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1526px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1526px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6626c4ccf3328891acde27f3_D-lrdosjr2eWoRl9mVQVnlwaVfQKLwErj7s2OfSz5Y2iya9fGew_Ep_jG3zsySFgc69Gi9uF1w6cgOEZLvMQL0RrUgsq1HQkJumuUYaJRTK8ugGw0mthsG34OGI6bt56QMVLo-5oONZkN-A4ZNGsqa8.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><p id="""">And segment all the players and extract information from it like this:</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1529px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1529px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6626c4cceabf7815186d0337_kbNcoJ-CJeUMGjd7_qTBLl5TcT90qpUKwfqVwEk-TrK_3tGF9h2IRAnWSLvkIXX7rOxFlxPYF5v9dooP0XjvhIPTIrFvGMFQq5xzEPXaAWn0IaMaqSgemsoQMCvv4r_F4ItfiR_NtlAkyS2HDU4XBNc.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><p id="""">As you can see the model was able to extract all the details from the image, the players, the pitch, the umpire the hats, helmets, etc. This is very granular data. This data can further be used to analyze a lot of things in the games, and also, track specific players, balls, and whatnot.</p><h3 id="""">Track Anything Model</h3><p id="""">The <a href=""https://arxiv.org/abs/2304.11968"" id="""">Track Anything model</a> is an extension of the Segment Anything Model, integrating the <a href=""https://arxiv.org/abs/2207.07115"" id="""">X-Mem architecture</a> with it to allow it to operate over images. Track anything is first used to create a segmentation mask for the object that is desired to be tracked, the mask is then provided to the X-Mem model which is very good in tracking objects over a long-term video.</p><p id="""">‍</p><p id="""">X-Mem uses the Atkinson-Shiffrin Memory Model which is similar to how human beings process and store information and memory, the same architecture is then used over consistent frames to track an object through the video. Over the years X-mem has evolved into a much better architecture, <a href=""https://arxiv.org/abs/2307.15958"" id="""">X-Mem++</a> being the latest one. All these techniques can be used to track players, balls, and other elements in a game.</p><p id="""">‍</p><p id="""">Here you can see Stephen Curry being tracked across shot changes over a 2 minute video.</p><div data-rt-embed-type='true'><video controls>
  <source src=""https://user-images.githubusercontent.com/30309970/232848349-f5e29e71-2ea4-4529-ac9a-94b9ca1e7055.mp4"" type=""video/mp4"">
</video></div><p id="""">‍</p><p id="""">This same pipeline can be used for any game, like soccer, baseball, basketball, cricket, golf and whatnot.</p><p id="""">‍</p><h3 id="""">Latest Updates - SAM2 and EdgeTAM</h3><p id="""">While SAM made great progress in image segmentation, it wasn't designed to handle the fast, unpredictable world of videos. Even though SAM is great for segmenting images, current video segmentation models and datasets still fall short when it comes to “segmenting anything in videos”</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:992px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""992px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68238c9f4546db5e49f9adaf_AD_4nXdDLGwyVX2YurVKWnZ86FIygfLHTJUqZ5tTvAXscwRWtfmuaWHl6K7ke7eRBy6NX4hicWVBW6cBNq_XvacyJH21kFvR4ZVhLeWplHUyf8X15p8kmi1QXyq-WI2zJbQ_KKgQ2f3dvA.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">That’s where<a href=""https://ai.meta.com/research/publications/sam-2-segment-anything-in-images-and-videos/"" id=""""> <strong id="""">SAM 2</strong></a> comes in. It builds on SAM by creating a unified model for both image and video segmentation, introducing a streaming architecture with memory attention. This new memory system keeps track of earlier frames and uses that context to improve predictions over time. As a result, SAM 2 can track and segment objects more accurately across movement, occlusion, and lighting changes, even with fewer user inputs.</p><p id="""">When it comes to performance, SAM 2 is a huge upgrade. It delivers better segmentation with 3× fewer interactions, outperforms previous models on standard video benchmarks, and even beats SAM on image tasks while running 6× faster.</p><p id="""">SAM 2 is powerful, but it's too heavy to run efficiently on mobile devices. The main bottleneck is the memory attention blocks added for video processing. To solve this, a lightweight version of SAM2 called<a href=""https://yformer.github.io/efficient-track-anything/"" id=""""><strong id=""""> EdgeTAM </strong></a>was introduced.</p><p id="""">EdgeTAM replaces the heavy memory system with a new component called the 2D Spatial Perceiver. This module reads stored video frame data more efficiently using a lightweight transformer. Instead of scanning everything in detail, it uses a fixed set of smart ""queries"" that focus only on what matters. This keeps it fast without losing accuracy.</p><p id="""">‍</p><p id="""">Since video segmentation requires pixel-level precision, EdgeTAM keeps the spatial layout of the memory intact. It organizes the queries into two groups -<strong id=""""> global-level</strong> queries that look at the full scene, and <strong id="""">patch-level</strong> queries that focus on small, local areas. This balance helps the model capture both the overall context and fine details.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:427px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""427px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68238c9e1aae7e8f537ec3e6_AD_4nXegaFwZzWIUVMMwbxSI9WBwExRAFS_P3sQpwfIOgx08EZnjVxyh-LboOTIaQQ4wAelZoeEQPr6uNOQxz0pjDpaBLR_9MBLQfVZ4njAegN7z21WTOf2PwLhAl1ao-xImLL_hHGOgoA.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">As a result, EdgeTAM achieves 87.7, 70.0, 72.3, and 71.7 J &amp;F on DAVIS 2017, MOSE, SA-V val, and SA-V test, while running at <strong id="""">16 FPS</strong> on iPhone 15 Pro Max.</p><p id="""">‍</p><h2 id="""">State-of-the-art Ball tracking Models (May-2025)</h2><h3 id="""">TrackNetV3</h3><p id="""">Ball tracking has moved from slow, error-prone manual tagging to real-time AI systems that handle speed, clutter, and occlusion with ease. TrackNet was an early deep learning model that used CNNs to detect balls in motion, even in visually noisy sports footage.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68238cc018f8720621da1abd_AD_4nXfpav-QWygoPndhCWntEf7X5I3nlGXdmFxit4E_hw3ehjoKjbDCvELUK3HYVrNkh_8aphxVZEunnIiFq6OFw7NcyQwXd-dQvSB9hsXdxEhLueW7OoEIeDqvOKy8ClpeHXgtVKen.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><p id="""">TrackNetV2 improved on this by using a U-Net architecture. It processed multiple frames and predicted heatmaps, which helped it deal better with motion blur, occlusion, and lighting changes. It achieved an IoU (overlap between original and predicted mask) of 0.82 but still struggled when the ball disappeared mid-play.</p><p id="""">‍</p><p id=""""><strong id="""">TrackNetV3 </strong>solves this issue with two modules - trajectory prediction and rectification. The prediction module looks at a sequence of frames plus a background image, helping the model ignore static distractions and focus on the moving object. If the shuttle gets occluded or missed, the rectification module kicks in. It studies the trajectory, guesses where the ball likely was, and “repairs” the gap using inpainting (filling in missing positions).</p><p id="""">It also uses mixup augmentation, it mixes different training examples to help the model handle strange lighting, and unpredictable ball movements. As a result, TrackNetV3 reaches 97.51% tracking accuracy, better than TrackNetV2 (94.98%) and much higher than general models like YOLOv7 (53.47%). Its IoU score also improves to 0.91.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:653px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""653px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68238cbec06b3667c7a3e8e2_AD_4nXfoyoqG9CrxymQZIPh9nOaMm4hrSGSp6uE4K_Wt6a3HixRdw798g5TmpTe8F-NAcR2x-o-B539VGbFIsZI8eWReZRCTZbRpT7GFg9RcUN_K5B5U-vGa2LklJzPxzk9En9HhyE_WqQ.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">TrackNetV3 doesn’t just detect where the ball is, it can guess where it went, even if it disappears for a moment. That is crucial for live replays, analytics, and broadcast graphics, especially in fast sports like badminton.</p><h3 id="""">YOLOv11 + SAHI + ByteTrack</h3><p id="""">Tracking small objects in high-resolution sports video is challenging due to their tiny size and fast movement. In 4K footage, objects like shuttlecocks or cricket balls typically appear as just 5 to 15 pixels wide.To track them accurately, we need a system built specifically for small, fast-moving objects. A combination of YOLOv11, SAHI, and ByteTrack addresses this challenge.</p><h4 id="""">YOLOv11 + SAHI&nbsp;&nbsp;</h4><p id="""">YOLOv11 is designed to detect very small and low-resolution objects in busy scenes. It uses a technique called dynamic attention, which helps the model focus on the important parts of the image, especially where the small objects are. The model also incorporates special blocks called C3k2, which help combine features from different image sizes. This makes it easier to detect objects that are blurry or only partially visible.&nbsp;</p><p id="""">During training, mixup techniques blend different images together to simulate conditions like poor lighting or fast motion. This improves the model’s performance in real sports videos, where conditions are not always ideal.</p><p id="""">Specialized detectors struggle when small objects are just a few pixels in a large 4K frame. <a href=""https://docs.ultralytics.com/guides/sahi-tiled-inference/"" id=""""><strong id="""">SAHI (Sliced Aided Hyper Inference)</strong></a> solves this by splitting the image into smaller, overlapping sections. In each section, the small object occupies a larger portion of the frame, making it easier to detect.&nbsp;</p><p id="""">‍</p><div data-rt-embed-type='true'><pre>
<code class=""language-py"">

import supervision as sv
from inference import get_model


# Load small-object-optimized model
model = get_model(model_id=""yolov8x-640"")
image = cv2.imread(<SOURCE_IMAGE_PATH>)


# Slice callback
def callback(image_slice: np.ndarray) -> sv.Detections:
    results = model.infer(image_slice)[0]
    return sv.Detections.from_inference(results)


# Run sliced inference
slicer = sv.InferenceSlicer(callback=callback)
detections = slicer(image)

</code>
</pre></div><p id="""">‍</p><h4 id="""">ByteTrack</h4><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:360px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""360px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68238cbe82a63063ff4fa676_AD_4nXcDvj6i-67I3gfMt1ad8F3rz6zCGEkkVlRqozTm5NIXT19kqtaqtl1xoOW4MsH0BKm_Dc-00OQiBTHc_ckMiT5Yhjveg-DFtyiEWydx8NoIf1y4C2didrZqUhIPdNSfU_VHn-BAPQ.gif"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">Once an object is detected, the next step is to track it across multiple frames. In fast sports, objects can quickly disappear behind players or move too fast to track consistently. ByteTrack handles this challenge by keeping even low-confidence detections and trying to match them to previously identified tracks.</p><p id="""">It does this using dual-thresholding, which evaluates both high-confidence and low-confidence predictions. Kalman filtering is used to predict where the object will move when it's temporarily out of sight. The matching algorithm connects detections based on the object’s motion and the overlap between frames.</p><p id="""">This approach improves the continuity of tracking, meaning objects are less likely to be misidentified or lost during occlusions.&nbsp;</p><h3 id="""">SAMURAI&nbsp;</h3><div data-rt-embed-type='true'><video controls style=""width: 100%; height: auto; max-width: 100%;"">
    <source src=""https://raw.githubusercontent.com/yangchris11/samurai/master/assets/samurai_demo.mp4"" type=""video/mp4"">
    Your browser does not support the video tag.
</video></div><p id="""">‍</p><p id="""">Meta’s “Segment Anything” Model (SAM) excels at image segmentation but struggles with video object tracking, especially in crowded scenes, fast-moving targets, or when objects briefly disappear. The issue lies in SAM’s memory mechanism, which uses a fixed window to store only the most recent frames without assessing their quality. This leads to error accumulation over time, affecting its tracking accuracy in dynamic video scenarios.</p><p id="""">‍</p><p id="""">To overcome these challenges, researchers at the University of Washington built a new model called <a href=""https://yangchris11.github.io/samurai/"" id=""""><strong id="""">SAMURAI </strong></a><strong id="""">(Segment Anything Model Using Robust AI)</strong>. It improves SAM by using motion cues and smarter memory selection. It doesn’t need retraining and works well across a range of tracking tasks.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68238e1261225b1bc1b74637_AD_4nXcsAZvOOqFEaYBPdJbkxeTWMPjzX4y8EOa1mTo6dm5L7zsdVwPG41M9N3bIB6CQN7Gi0kGUOi8dGeOY2bqn2UMU32baXr46ph5bJwEG2LCdM_u80I1mFQgoQ9sbkrbGGZ_LBcLPug.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><p id="""">‍</p><p id="""">Key Innovations of SAMURAI:</p><p id="""">‍</p><ul id=""""><li id=""""><strong id="""">Motion Modeling System</strong></li></ul><p id="""">SAMURAI uses motion cues to predict where objects will move in complex, dynamic scenes. This helps it select the right mask and avoid confusion when objects look similar or overlap, ensuring accurate tracking even in challenging situations.</p><p id="""">‍</p><ul id=""""><li id=""""><strong id="""">Motion-Aware Memory Selection</strong></li></ul><p id="""">SAMURAI improves SAM’s memory by replacing its fixed-window system with a hybrid scoring approach. It evaluates frames based on three factors:</p><ul id=""""><li id="""">Mask similarity</li><li id="""">Object appearance</li><li id="""">Motion patterns</li></ul><p id="""">Only the most relevant frames are kept in memory, minimizing errors and improving tracking accuracy</p><p id="""">‍</p><p id="""">SAMURAI works because of its use of motion and memory. It uses Kalman filters to predict object positions and sizes, helping it pick the right mask from multiple options. It only stores frames that meet certain quality thresholds for mask similarity, ensuring it focuses on the most relevant data. The balance improves tracking accuracy and reliability</p><h2 id="""">Performance Comparison</h2><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:923px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""923px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68238cbf0b5009afd8416c76_AD_4nXd1WATk4wld8CgdSUT83dLgqI8Am7eiEZaEKr0I5nMotAa3XEgtz86hbS8oKsmtzCBvn6Ukapk6gaNHi206vREJb0hPgY-faIUG-cLP1FvIvQvnUBq80xIUaNnl1hAjEFMYxtL7.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id=""""><strong id="""">TrackNet V3</strong> is efficient on GPU memory (2.5 GB) but lags behind in speed. It manages around <strong id="""">25 FPS </strong>at<strong id=""""> </strong>1080p, but drops to <strong id="""">12 FPS</strong> in multi-camera setups, making it more fit for post-event analysis rather than real-time use.</p><p id=""""><strong id="""">YOLO + SAHI + ByteTrack</strong> sits in the middle. Without slicing, it can hit <strong id="""">45 FPS</strong>, but once SAHI tiling is used (for better small object detection), speed drops to <strong id="""">15 FPS</strong>. It's CPU-intensive due to multi-threaded slicing, and latency jumps to <strong id="""">66ms</strong> with full slicing, which can impact live responsiveness.</p><p id=""""><strong id="""">Samurai</strong> clearly stands out with the highest FPS across both 1080p and 4K setups, achieving <strong id="""">60 FPS </strong>on<strong id=""""> </strong>single-camera 1080p and <strong id="""">22 FPS </strong>across 4K multi-camera feeds, thanks to optimized memory streaming and frame handling. Its latency is also the lowest at just <strong id="""">16ms</strong>, making it suitable for live applications.</p><p id="""">‍</p><h2 id="""">How to use Ball Tracking Data</h2><p id="""">Once you have the ball tracking data along with the player data, then you can do a lot of things from there to generate a ton of analytics. Things like player interaction with the ball, team interaction, what player is best at what area, where the ball goes most, etc. All these very essential metrics become very easy to extract and track once we have the data ready, let's see how we can work with all this data.</p><p id="""">‍</p><h3 id="""">Team Analysis</h3><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:957px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""957px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6626c4cc138f15fadc052b7f_1SJQICG6dQwlI3eY45QZAAezXGfv56u6bWYVBUgdFSJIuNOCbHTbZHmyMuoS5nepG5Ku8Dc_TRHa8hR5N9fbv2VdYR0cWfP6QZ8JM_IndN0X2QWGtPu-qF8vhtBAbZ08b_UF7vFuAQpYNaBaDTDT3hE.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><p id="""">Once the ball tracking data is in, we can measure how teams or specific members of the team interact with the ball at several different incidents. This is rather important for games like Basketball and Soccer, as different members seem to perform differently given the phase of the game and the area of the field. Important metrics like the <strong id="""">Strech Index </strong>and <strong id="""">Team Synchrony</strong> can be extremely useful in these scenarios.</p><p id="""">‍</p><p id="""">These metrics help understand how much area a team is using and how well. For example, the stretch index of 3 players might be very tight, and that could explain why they have trouble maneuvering the ball over large areas. Whereas, if the other team’s covered area overlaps with our team’s area, we can understand how they are going to interact and study what are some techniques to secure the ball in those scenarios. All this is very valuable to a manager or a coach. You can read more about these metrics <a href=""https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2018.02416/full"" id="""">here</a>.</p><p id="""">‍</p><h3 id="""">Individual Analysis</h3><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:800px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""800px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6626c4cc5a631302b32c9600_qupCBBcZ4MYh6QWnpSJONmgm5EBgEBlyIltqShhVk2N1R3pyl4dvsd0RIAWkC8topMC5Rbv87OdYdtvVSmBxCUkUBmeDqRHSy0U1whGMZmcFYWMEvgqSxRCnpeziZuSkBQnG9CXvY29dWJpMH9FZJCA.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">Individual player analysis is as important as team analysis. Things like the individual path of a player, movement speed of a player, distance from the ball as the game moves on, etc. These are important metrics for specific players. We can also understand how a player is performing in different areas of the field. For example, if a player is not moving much in the defensive area, we can understand that the player is not performing well in that area. This is important for the coach to understand and make decisions on how to improve the player in specific areas or what areas to target them for.</p><p id="""">‍</p><p id="""">This data can also be used to pair correct players together. 3 players who have a high stretch index together can be paired together to cover a large area of the field. Players who can run faster can be placed closer to the opponent’s side so that they can quickly move back and forth between defense and offense.</p><h3 id="""">Pose Analysis</h3><p id="""">Ball tracking data can be further paired up with pose estimation to refine the technique of the player in games like cricket and golf. These games where you have to hit the ball with great accuracy and precision can benefit greatly from pose analysis. You can see how the position of the body changes during hitting the ball and whether certain positions result in better performance than others. Pose estimations can also detect injuries early on before they become serious problems down the road.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:576px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""576px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6626c4cc0d1f398b61e80a7d_fvDpIiDsgkXNHidWABkLNBEwCUtQbAEbWk2TTrhI-Llscs90koRYmGyEZ74fY9m4BJ2qqNFju1PWkiEhlwJCdogNI_9q4DDC3zhN8-oG_tMZpt8Z6gJyR1WAPqyumsaXuugUZzJl5LnOqbm5n_FtFno.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><p id="""">As you can see in the image, pose estimation in golf can be helpful. Tracking where the ball goes, when hit a certain way, and when the pose is in a certain way. Poses can also be compared with other better players to get an idea of what the player is doing wrong and where the improvement is needed. This same technique can be used for various other things like exercise and posture analysis if needed.</p><h2 id="""">Traditional methods vs Track Anything Model</h2><p id="""">As mentioned before, traditionally, networks like Tracknet have been used for this application. But even Tracknet has its issues.</p><p id="""">‍</p><h3 id="""">Performance Issues</h3><p id="""">Tracknet, being a single architecture, can make mistakes. Tracknet was mainly developed for tracking shuttle cocks during tennis matches, performance across other domains can be very degraded unless finetuned properly. It is seen that smaller faster moving objects, like a ball in cricket, can be problematic for Tracknet to track. However, techniques that build upon Tracknet like <a href=""https://arxiv.org/abs/2211.09791"" id="""">MOTRv2</a> seem to show much better performance. These are not single network architectures but rather pipelines that use the network for the core tracking task.</p><p id="""">‍</p><p id="""">SAM, on the other hand, is highly performant in most of the out-of-domain tasks, and when combined with other architectures like X-Mem and X-Mem++, the tracking capabilities are simply SOTA. Similar to Tracknet, pipelines built with SAM might also require some finetuning but performance gains are much greater compared to Tracknet.</p><p id="""">‍</p><h3 id="""">Computation Cost and Inference Time</h3><p id="""">Another big issue with TrackNet and its dependent pipelines is that it is computationally very heavy as it is mostly a single convolution network, mostly. TrackNetV2’s performance is around <strong id="""">31.8 FPS.</strong> Whereas the Track Anything Model paired with X-Mem++ can do <strong id="""">39 FPS.</strong> And much more if a smaller version of the model is finetuned for a specific use case.</p><p id="""">‍</p><p id="""">In production, it is often the case that not all the frames are processed, most are clumped together and a Kalman filter is used with it to track the ball in all the frames altogether</p><p id="""">‍</p><h2 id="""">Want to Build Ball Tracking Pipelines for Sports?</h2><p id="""">If you are looking to build ball-tracking pipelines and sports analysis applications, please <a href=""https://www.mercity.ai/contacts"" id="""">reach out to us</a>. We have worked with many computer vision pipelines and have integrated them into already existing systems. Reach out to us to build such pipelines or just to chat. Would love to chat!</p><p id="""">‍</p>",https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6626c57883a69ca6051b9010_Screenshot%202024-04-23%20014538.png,Pranav Patel,AI in Sport Analysis,"In this blog we show how you can use AI and ball tracking techniques for sport analysis and improve performance of players in games like basketball, soccer, football, cricket, etc. We compare the new approaches and the old too.",False,"<div class=""rich-text w-richtext""><p>Accurate ball tracking has long been a holy grail in sports technology, promising to unlock new insights and improvements in athlete performance. It can be used for various tasks, from gathering valuable data to helping referees make decisions, even for predictive analysis. However, developing reliable ball tracking systems has proven to be a complex and challenging task. However, the recent breakthroughs in AI and computer vision space have introduced new models and methods to enhance ball tracking greatly.</p><p>‍</p><p>In this article, we'll explore the latest advancements in AI-powered ball tracking, and how they're overcoming these challenges to revolutionize the sports industry.</p><h2>What is Ball Tracking</h2><p>Ball tracking in sports is simply tracking the ball as it moves around in the field. Along with this, things like collision detection with the bat or racket, speed analysis, and player interaction with the ball are also measured and tracked. These are valuable data points in the sports analysis industry as for most of the games the “ball” itself is at the core. It might be kicking the ball, putting the ball in a basket, or hitting the ball with a bat or a racquet. Balls are involved in one way or another in most of the popular games.</p><p>‍</p><p>Ball tracking has become almost a necessary part of most of the games. It just allows the players, regulators, and viewers to extract so much more information from the games. It really enriches the experience and improves the game overall.</p><p>‍</p><p>Let’s talk about some of the advantages of using Ball tracking in sports.</p><p>‍</p><h3>Ball tracking for Making Better Decisions</h3><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:767pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6626c4ccd640d668b585a37f_9XmEseyqhwi1khs3mv8ygmcjmp3bjYdETbNTDErADE2HlObV9PbHTmz3_ZEXdFo1Z3SUc1Qs1z5TDR5fGyQb93ekmi0UPdeFyjFj8iMSnnAGgG40irCZARY5pfksCc21vHTL1A8vvciLgVdqNbNKxT0.png""/></div></figure><p>‍</p><p>Making judgment decisions is one of the most important parts of any sport, with or without balls. Most of them are straightforward like fours and sixes in Cricket, but in some cases, these can become very difficult, like LBW (Leg before Wicket) and No Ball judgment in Cricket. These edge case scenarios are now handled by “3rd Umpires”, These are the umpires sitting in a room with a lot of video feed coming from all over the field.</p><p>‍</p><p>For the 3rd umpires, ball tracking data is essential. When the field umpires cannot make a decision they refer to the 3rd umpires, which then using ball tracking and other sport analysis techniques, make a decision for them. This requires a lot of work and a lot of data processing in real-time.</p><h3>Ball Tracking for Improving Player Performance</h3><p>Ball tracking has been used by players for a long time now to improve their own performance and to understand the flaws in other people’s playing styles too. Bowlers use it all the time to understand how they are handling the ball and where can they improve further to score the most wickets. Golf players use ball tracking with pose estimation to understand how they should hit the ball and at what angle. Doing all this provides players with extremely granular data and very good feedback by the system, with all this, it’s very to improve and catch mistakes. At this point, this is something every player in the industry uses.</p><h3>Ball tracking for Assisting Coaches</h3><p>Along with improving individual performances, coaches have started making better teams based upon different metrics gathered through sports analytics systems such as ball speed, spin rate, distance covered while running after hitting the shot, and number of steps taken during the batting stance preparation phase. This allows managers/coaches access detailed insights regarding strengths &amp; weaknesses amongst squad members allowing informed tactical adjustments throughout the season leading ultimately toward success.</p><p>‍</p><p>Coaches also analyze opponents’ games thoroughly beforehand so they can devise strategies accordingly against certain styles played previously seen footage recorded via cameras capturing entire match proceedings including replays slow motion clips highlighting key moments helps understand tactics employed by both sides resulting in improved overall quality play. This really helps coaches prepare a better team strategy against any other team.</p><h3>Ball Tracking for Fan Engagement</h3><p>Ball tracking for fan engagement is rather a new phenomenon where providers has been introducing things like “Ball cam”, a special drone or camera that follows the ball specifically. This is new but something viewers love engaging with. Providers also often show the “ball path” and other important visualizations that keep viewers engaged and informed.</p><h2>How does Ball tracking work?</h2><p>There have been many ways Ball tracking is implemented in the actual games. One of them notably being using <a href=""https://arxiv.org/abs/1907.03698"">TrackNet</a> and <a href=""https://arxiv.org/abs/1506.02640"">YOLO Networks</a>. These techniques are often paired to provide a good experience and also work well to this date. But we want to introduce newer better models which can track the ball and other objects even better.</p><p>‍</p><p>Let’s learn how to build ball tracking pipelines.</p><h3>Segment Anything Model</h3><p><a href=""https://segment-anything.com/"">Segment Anything Model</a> by Meta is a rather new and recent model. Segment Anything Model or SAM by Meta is a rather straightforward model. It takes in an image and can take in various types of prompts like masks, boxes, points, and even free-form text. Then the both image and the passed prompt is encoded into a much smaller subspace, these embeddings are then passed into a decoder which outputs a final mask that represents the segmented parts of the image. As you can see in the diagram below:</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6626c4ccf3328891acde2836_ivmfLcX8NSuTKxhFx_4PX7cDDN9tk7iJTqcgv3WKbOCNY83Kix8dnwywwKuIU4O_1ki3SzGln2IVXPYV4kVXMIQCo96xw9rffKTIO4oz4iG0jqb0C5banUYs62FMXqB_t5a7TfaMob-k2JEO4_P7IQ4.png""/></div></figure><p>‍</p><p>This means that you can pass a normal image like this:</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1526pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6626c4ccf3328891acde27f3_D-lrdosjr2eWoRl9mVQVnlwaVfQKLwErj7s2OfSz5Y2iya9fGew_Ep_jG3zsySFgc69Gi9uF1w6cgOEZLvMQL0RrUgsq1HQkJumuUYaJRTK8ugGw0mthsG34OGI6bt56QMVLo-5oONZkN-A4ZNGsqa8.png""/></div></figure><p>‍</p><p>And segment all the players and extract information from it like this:</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1529pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6626c4cceabf7815186d0337_kbNcoJ-CJeUMGjd7_qTBLl5TcT90qpUKwfqVwEk-TrK_3tGF9h2IRAnWSLvkIXX7rOxFlxPYF5v9dooP0XjvhIPTIrFvGMFQq5xzEPXaAWn0IaMaqSgemsoQMCvv4r_F4ItfiR_NtlAkyS2HDU4XBNc.png""/></div></figure><p>‍</p><p>As you can see the model was able to extract all the details from the image, the players, the pitch, the umpire the hats, helmets, etc. This is very granular data. This data can further be used to analyze a lot of things in the games, and also, track specific players, balls, and whatnot.</p><h3>Track Anything Model</h3><p>The <a href=""https://arxiv.org/abs/2304.11968"">Track Anything model</a> is an extension of the Segment Anything Model, integrating the <a href=""https://arxiv.org/abs/2207.07115"">X-Mem architecture</a> with it to allow it to operate over images. Track anything is first used to create a segmentation mask for the object that is desired to be tracked, the mask is then provided to the X-Mem model which is very good in tracking objects over a long-term video.</p><p>‍</p><p>X-Mem uses the Atkinson-Shiffrin Memory Model which is similar to how human beings process and store information and memory, the same architecture is then used over consistent frames to track an object through the video. Over the years X-mem has evolved into a much better architecture, <a href=""https://arxiv.org/abs/2307.15958"">X-Mem++</a> being the latest one. All these techniques can be used to track players, balls, and other elements in a game.</p><p>‍</p><p>Here you can see Stephen Curry being tracked across shot changes over a 2 minute video.</p><div class=""w-embed""><video controls="""">
<source src=""https://user-images.githubusercontent.com/30309970/232848349-f5e29e71-2ea4-4529-ac9a-94b9ca1e7055.mp4"" type=""video/mp4""/>
</video></div><div class=""w-embed""></div><p>‍</p><p>This same pipeline can be used for any game, like soccer, baseball, basketball, cricket, golf and whatnot.</p><p>‍</p><h3>Latest Updates - SAM2 and EdgeTAM</h3><p>While SAM made great progress in image segmentation, it wasn't designed to handle the fast, unpredictable world of videos. Even though SAM is great for segmenting images, current video segmentation models and datasets still fall short when it comes to “segmenting anything in videos”</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:992pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68238c9f4546db5e49f9adaf_AD_4nXdDLGwyVX2YurVKWnZ86FIygfLHTJUqZ5tTvAXscwRWtfmuaWHl6K7ke7eRBy6NX4hicWVBW6cBNq_XvacyJH21kFvR4ZVhLeWplHUyf8X15p8kmi1QXyq-WI2zJbQ_KKgQ2f3dvA.png""/></div></figure><p>That’s where<a href=""https://ai.meta.com/research/publications/sam-2-segment-anything-in-images-and-videos/""> <strong>SAM 2</strong></a> comes in. It builds on SAM by creating a unified model for both image and video segmentation, introducing a streaming architecture with memory attention. This new memory system keeps track of earlier frames and uses that context to improve predictions over time. As a result, SAM 2 can track and segment objects more accurately across movement, occlusion, and lighting changes, even with fewer user inputs.</p><p>When it comes to performance, SAM 2 is a huge upgrade. It delivers better segmentation with 3× fewer interactions, outperforms previous models on standard video benchmarks, and even beats SAM on image tasks while running 6× faster.</p><p>SAM 2 is powerful, but it's too heavy to run efficiently on mobile devices. The main bottleneck is the memory attention blocks added for video processing. To solve this, a lightweight version of SAM2 called<a href=""https://yformer.github.io/efficient-track-anything/""><strong> EdgeTAM </strong></a>was introduced.</p><p>EdgeTAM replaces the heavy memory system with a new component called the 2D Spatial Perceiver. This module reads stored video frame data more efficiently using a lightweight transformer. Instead of scanning everything in detail, it uses a fixed set of smart ""queries"" that focus only on what matters. This keeps it fast without losing accuracy.</p><p>‍</p><p>Since video segmentation requires pixel-level precision, EdgeTAM keeps the spatial layout of the memory intact. It organizes the queries into two groups -<strong> global-level</strong> queries that look at the full scene, and <strong>patch-level</strong> queries that focus on small, local areas. This balance helps the model capture both the overall context and fine details.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:427pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68238c9e1aae7e8f537ec3e6_AD_4nXegaFwZzWIUVMMwbxSI9WBwExRAFS_P3sQpwfIOgx08EZnjVxyh-LboOTIaQQ4wAelZoeEQPr6uNOQxz0pjDpaBLR_9MBLQfVZ4njAegN7z21WTOf2PwLhAl1ao-xImLL_hHGOgoA.png""/></div></figure><p>As a result, EdgeTAM achieves 87.7, 70.0, 72.3, and 71.7 J &amp;F on DAVIS 2017, MOSE, SA-V val, and SA-V test, while running at <strong>16 FPS</strong> on iPhone 15 Pro Max.</p><p>‍</p><h2>State-of-the-art Ball tracking Models (May-2025)</h2><h3>TrackNetV3</h3><p>Ball tracking has moved from slow, error-prone manual tagging to real-time AI systems that handle speed, clutter, and occlusion with ease. TrackNet was an early deep learning model that used CNNs to detect balls in motion, even in visually noisy sports footage.</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68238cc018f8720621da1abd_AD_4nXfpav-QWygoPndhCWntEf7X5I3nlGXdmFxit4E_hw3ehjoKjbDCvELUK3HYVrNkh_8aphxVZEunnIiFq6OFw7NcyQwXd-dQvSB9hsXdxEhLueW7OoEIeDqvOKy8ClpeHXgtVKen.png""/></div></figure><p>‍</p><p>TrackNetV2 improved on this by using a U-Net architecture. It processed multiple frames and predicted heatmaps, which helped it deal better with motion blur, occlusion, and lighting changes. It achieved an IoU (overlap between original and predicted mask) of 0.82 but still struggled when the ball disappeared mid-play.</p><p>‍</p><p><strong>TrackNetV3 </strong>solves this issue with two modules - trajectory prediction and rectification. The prediction module looks at a sequence of frames plus a background image, helping the model ignore static distractions and focus on the moving object. If the shuttle gets occluded or missed, the rectification module kicks in. It studies the trajectory, guesses where the ball likely was, and “repairs” the gap using inpainting (filling in missing positions).</p><p>It also uses mixup augmentation, it mixes different training examples to help the model handle strange lighting, and unpredictable ball movements. As a result, TrackNetV3 reaches 97.51% tracking accuracy, better than TrackNetV2 (94.98%) and much higher than general models like YOLOv7 (53.47%). Its IoU score also improves to 0.91.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:653pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68238cbec06b3667c7a3e8e2_AD_4nXfoyoqG9CrxymQZIPh9nOaMm4hrSGSp6uE4K_Wt6a3HixRdw798g5TmpTe8F-NAcR2x-o-B539VGbFIsZI8eWReZRCTZbRpT7GFg9RcUN_K5B5U-vGa2LklJzPxzk9En9HhyE_WqQ.png""/></div></figure><p>TrackNetV3 doesn’t just detect where the ball is, it can guess where it went, even if it disappears for a moment. That is crucial for live replays, analytics, and broadcast graphics, especially in fast sports like badminton.</p><h3>YOLOv11 + SAHI + ByteTrack</h3><p>Tracking small objects in high-resolution sports video is challenging due to their tiny size and fast movement. In 4K footage, objects like shuttlecocks or cricket balls typically appear as just 5 to 15 pixels wide.To track them accurately, we need a system built specifically for small, fast-moving objects. A combination of YOLOv11, SAHI, and ByteTrack addresses this challenge.</p><h4>YOLOv11 + SAHI  </h4><p>YOLOv11 is designed to detect very small and low-resolution objects in busy scenes. It uses a technique called dynamic attention, which helps the model focus on the important parts of the image, especially where the small objects are. The model also incorporates special blocks called C3k2, which help combine features from different image sizes. This makes it easier to detect objects that are blurry or only partially visible. </p><p>During training, mixup techniques blend different images together to simulate conditions like poor lighting or fast motion. This improves the model’s performance in real sports videos, where conditions are not always ideal.</p><p>Specialized detectors struggle when small objects are just a few pixels in a large 4K frame. <a href=""https://docs.ultralytics.com/guides/sahi-tiled-inference/""><strong>SAHI (Sliced Aided Hyper Inference)</strong></a> solves this by splitting the image into smaller, overlapping sections. In each section, the small object occupies a larger portion of the frame, making it easier to detect. </p><p>‍</p><div class=""w-embed""><pre>
<code class=""language-py"">

import supervision as sv
from inference import get_model


# Load small-object-optimized model
model = get_model(model_id=""yolov8x-640"")
image = cv2.imread(<source_image_path>)


# Slice callback
def callback(image_slice: np.ndarray) -&gt; sv.Detections:
    results = model.infer(image_slice)[0]
    return sv.Detections.from_inference(results)


# Run sliced inference
slicer = sv.InferenceSlicer(callback=callback)
detections = slicer(image)

</source_image_path></code>
</pre></div><p>‍</p><h4>ByteTrack</h4><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:360pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68238cbe82a63063ff4fa676_AD_4nXcDvj6i-67I3gfMt1ad8F3rz6zCGEkkVlRqozTm5NIXT19kqtaqtl1xoOW4MsH0BKm_Dc-00OQiBTHc_ckMiT5Yhjveg-DFtyiEWydx8NoIf1y4C2didrZqUhIPdNSfU_VHn-BAPQ.gif""/></div></figure><p>Once an object is detected, the next step is to track it across multiple frames. In fast sports, objects can quickly disappear behind players or move too fast to track consistently. ByteTrack handles this challenge by keeping even low-confidence detections and trying to match them to previously identified tracks.</p><p>It does this using dual-thresholding, which evaluates both high-confidence and low-confidence predictions. Kalman filtering is used to predict where the object will move when it's temporarily out of sight. The matching algorithm connects detections based on the object’s motion and the overlap between frames.</p><p>This approach improves the continuity of tracking, meaning objects are less likely to be misidentified or lost during occlusions. </p><h3>SAMURAI </h3><div class=""w-embed""><video controls="""" style=""width: 100%; height: auto; max-width: 100%;"">
<source src=""https://raw.githubusercontent.com/yangchris11/samurai/master/assets/samurai_demo.mp4"" type=""video/mp4""/>
    Your browser does not support the video tag.
</video></div><p>‍</p><p>Meta’s “Segment Anything” Model (SAM) excels at image segmentation but struggles with video object tracking, especially in crowded scenes, fast-moving targets, or when objects briefly disappear. The issue lies in SAM’s memory mechanism, which uses a fixed window to store only the most recent frames without assessing their quality. This leads to error accumulation over time, affecting its tracking accuracy in dynamic video scenarios.</p><p>‍</p><p>To overcome these challenges, researchers at the University of Washington built a new model called <a href=""https://yangchris11.github.io/samurai/""><strong>SAMURAI </strong></a><strong>(Segment Anything Model Using Robust AI)</strong>. It improves SAM by using motion cues and smarter memory selection. It doesn’t need retraining and works well across a range of tracking tasks.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68238e1261225b1bc1b74637_AD_4nXcsAZvOOqFEaYBPdJbkxeTWMPjzX4y8EOa1mTo6dm5L7zsdVwPG41M9N3bIB6CQN7Gi0kGUOi8dGeOY2bqn2UMU32baXr46ph5bJwEG2LCdM_u80I1mFQgoQ9sbkrbGGZ_LBcLPug.png""/></div></figure><p>‍</p><p>‍</p><p>Key Innovations of SAMURAI:</p><p>‍</p><ul role=""list""><li><strong>Motion Modeling System</strong></li></ul><p>SAMURAI uses motion cues to predict where objects will move in complex, dynamic scenes. This helps it select the right mask and avoid confusion when objects look similar or overlap, ensuring accurate tracking even in challenging situations.</p><p>‍</p><ul role=""list""><li><strong>Motion-Aware Memory Selection</strong></li></ul><p>SAMURAI improves SAM’s memory by replacing its fixed-window system with a hybrid scoring approach. It evaluates frames based on three factors:</p><ul role=""list""><li>Mask similarity</li><li>Object appearance</li><li>Motion patterns</li></ul><p>Only the most relevant frames are kept in memory, minimizing errors and improving tracking accuracy</p><p>‍</p><p>SAMURAI works because of its use of motion and memory. It uses Kalman filters to predict object positions and sizes, helping it pick the right mask from multiple options. It only stores frames that meet certain quality thresholds for mask similarity, ensuring it focuses on the most relevant data. The balance improves tracking accuracy and reliability</p><h2>Performance Comparison</h2><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:923px""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68238cbf0b5009afd8416c76_AD_4nXd1WATk4wld8CgdSUT83dLgqI8Am7eiEZaEKr0I5nMotAa3XEgtz86hbS8oKsmtzCBvn6Ukapk6gaNHi206vREJb0hPgY-faIUG-cLP1FvIvQvnUBq80xIUaNnl1hAjEFMYxtL7.png""/></div></figure><p><strong>TrackNet V3</strong> is efficient on GPU memory (2.5 GB) but lags behind in speed. It manages around <strong>25 FPS </strong>at<strong> </strong>1080p, but drops to <strong>12 FPS</strong> in multi-camera setups, making it more fit for post-event analysis rather than real-time use.</p><p><strong>YOLO + SAHI + ByteTrack</strong> sits in the middle. Without slicing, it can hit <strong>45 FPS</strong>, but once SAHI tiling is used (for better small object detection), speed drops to <strong>15 FPS</strong>. It's CPU-intensive due to multi-threaded slicing, and latency jumps to <strong>66ms</strong> with full slicing, which can impact live responsiveness.</p><p><strong>Samurai</strong> clearly stands out with the highest FPS across both 1080p and 4K setups, achieving <strong>60 FPS </strong>on<strong> </strong>single-camera 1080p and <strong>22 FPS </strong>across 4K multi-camera feeds, thanks to optimized memory streaming and frame handling. Its latency is also the lowest at just <strong>16ms</strong>, making it suitable for live applications.</p><p>‍</p><h2>How to use Ball Tracking Data</h2><p>Once you have the ball tracking data along with the player data, then you can do a lot of things from there to generate a ton of analytics. Things like player interaction with the ball, team interaction, what player is best at what area, where the ball goes most, etc. All these very essential metrics become very easy to extract and track once we have the data ready, let's see how we can work with all this data.</p><p>‍</p><h3>Team Analysis</h3><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:957pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6626c4cc138f15fadc052b7f_1SJQICG6dQwlI3eY45QZAAezXGfv56u6bWYVBUgdFSJIuNOCbHTbZHmyMuoS5nepG5Ku8Dc_TRHa8hR5N9fbv2VdYR0cWfP6QZ8JM_IndN0X2QWGtPu-qF8vhtBAbZ08b_UF7vFuAQpYNaBaDTDT3hE.png""/></div></figure><p>‍</p><p>Once the ball tracking data is in, we can measure how teams or specific members of the team interact with the ball at several different incidents. This is rather important for games like Basketball and Soccer, as different members seem to perform differently given the phase of the game and the area of the field. Important metrics like the <strong>Strech Index </strong>and <strong>Team Synchrony</strong> can be extremely useful in these scenarios.</p><p>‍</p><p>These metrics help understand how much area a team is using and how well. For example, the stretch index of 3 players might be very tight, and that could explain why they have trouble maneuvering the ball over large areas. Whereas, if the other team’s covered area overlaps with our team’s area, we can understand how they are going to interact and study what are some techniques to secure the ball in those scenarios. All this is very valuable to a manager or a coach. You can read more about these metrics <a href=""https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2018.02416/full"">here</a>.</p><p>‍</p><h3>Individual Analysis</h3><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:800pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6626c4cc5a631302b32c9600_qupCBBcZ4MYh6QWnpSJONmgm5EBgEBlyIltqShhVk2N1R3pyl4dvsd0RIAWkC8topMC5Rbv87OdYdtvVSmBxCUkUBmeDqRHSy0U1whGMZmcFYWMEvgqSxRCnpeziZuSkBQnG9CXvY29dWJpMH9FZJCA.png""/></div></figure><p>Individual player analysis is as important as team analysis. Things like the individual path of a player, movement speed of a player, distance from the ball as the game moves on, etc. These are important metrics for specific players. We can also understand how a player is performing in different areas of the field. For example, if a player is not moving much in the defensive area, we can understand that the player is not performing well in that area. This is important for the coach to understand and make decisions on how to improve the player in specific areas or what areas to target them for.</p><p>‍</p><p>This data can also be used to pair correct players together. 3 players who have a high stretch index together can be paired together to cover a large area of the field. Players who can run faster can be placed closer to the opponent’s side so that they can quickly move back and forth between defense and offense.</p><h3>Pose Analysis</h3><p>Ball tracking data can be further paired up with pose estimation to refine the technique of the player in games like cricket and golf. These games where you have to hit the ball with great accuracy and precision can benefit greatly from pose analysis. You can see how the position of the body changes during hitting the ball and whether certain positions result in better performance than others. Pose estimations can also detect injuries early on before they become serious problems down the road.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:576pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6626c4cc0d1f398b61e80a7d_fvDpIiDsgkXNHidWABkLNBEwCUtQbAEbWk2TTrhI-Llscs90koRYmGyEZ74fY9m4BJ2qqNFju1PWkiEhlwJCdogNI_9q4DDC3zhN8-oG_tMZpt8Z6gJyR1WAPqyumsaXuugUZzJl5LnOqbm5n_FtFno.png""/></div></figure><p>‍</p><p>As you can see in the image, pose estimation in golf can be helpful. Tracking where the ball goes, when hit a certain way, and when the pose is in a certain way. Poses can also be compared with other better players to get an idea of what the player is doing wrong and where the improvement is needed. This same technique can be used for various other things like exercise and posture analysis if needed.</p><h2>Traditional methods vs Track Anything Model</h2><p>As mentioned before, traditionally, networks like Tracknet have been used for this application. But even Tracknet has its issues.</p><p>‍</p><h3>Performance Issues</h3><p>Tracknet, being a single architecture, can make mistakes. Tracknet was mainly developed for tracking shuttle cocks during tennis matches, performance across other domains can be very degraded unless finetuned properly. It is seen that smaller faster moving objects, like a ball in cricket, can be problematic for Tracknet to track. However, techniques that build upon Tracknet like <a href=""https://arxiv.org/abs/2211.09791"">MOTRv2</a> seem to show much better performance. These are not single network architectures but rather pipelines that use the network for the core tracking task.</p><p>‍</p><p>SAM, on the other hand, is highly performant in most of the out-of-domain tasks, and when combined with other architectures like X-Mem and X-Mem++, the tracking capabilities are simply SOTA. Similar to Tracknet, pipelines built with SAM might also require some finetuning but performance gains are much greater compared to Tracknet.</p><p>‍</p><h3>Computation Cost and Inference Time</h3><p>Another big issue with TrackNet and its dependent pipelines is that it is computationally very heavy as it is mostly a single convolution network, mostly. TrackNetV2’s performance is around <strong>31.8 FPS.</strong> Whereas the Track Anything Model paired with X-Mem++ can do <strong>39 FPS.</strong> And much more if a smaller version of the model is finetuned for a specific use case.</p><p>‍</p><p>In production, it is often the case that not all the frames are processed, most are clumped together and a Kalman filter is used with it to track the ball in all the frames altogether</p><p>‍</p><h2>Want to Build Ball Tracking Pipelines for Sports?</h2><p>If you are looking to build ball-tracking pipelines and sports analysis applications, please <a href=""https://www.mercity.ai/contacts"">reach out to us</a>. We have worked with many computer vision pipelines and have integrated them into already existing systems. Reach out to us to build such pipelines or just to chat. Would love to chat!</p><p>‍</p></div>"
How to Build an LLM-Based Resume Analyzer,build-an-llm-based-resume-analyzer,640f56f76d313b2faa631c11,681297d56ca3c9c89bc27d4b,False,True,Wed Apr 30 2025 21:36:21 GMT+0000 (Coordinated Universal Time),Sat Jul 26 2025 20:29:29 GMT+0000 (Coordinated Universal Time),Thu May 22 2025 22:08:22 GMT+0000 (Coordinated Universal Time),"<p id="""">Today, most hiring processes don't begin with a human. They start with an Applicant Tracking System (ATS). Every resume gets scanned, compared against the job description, and ranked automatically. Only a small number ever make it to a recruiter’s desk.</p><p id="""">While this system may seem efficient, it is fundamentally flawed. Traditional ATS systems work more like rigid keyword-matching machines. They often miss strong candidates with nuanced profiles, while pushing forward weaker ones who simply optimize for the right keywords. As a result, companies lose out on top talent before a human ever gets involved.</p><p id="""">Modern hiring needs more than speed and scale. It needs a human-like understanding of context, intent, and real candidate capabilities, not just matching words on a page.</p><p id="""">‍</p><blockquote id="""">💡💡 We built the LLM&nbsp;Based Resume Processing tool for you! Fully Open Source, Check it out <a href=""https://github.com/Mercity-AI/Resume-Analyzer"" id=""""><strong id="""">here</strong></a></blockquote><p id="""">‍</p><blockquote id="""">📻📻 And the video here:</blockquote><div data-rt-embed-type='true'><div style=""position: relative; padding-bottom: 56.25%; height: 0;""><iframe src=""https://www.loom.com/embed/3985f56862074443b058e705fc47590a?sid=a02fbd46-b4ee-43a2-bd85-a696ae6379c9"" frameborder=""0"" webkitallowfullscreen mozallowfullscreen allowfullscreen style=""position: absolute; top: 0; left: 0; width: 100%; height: 100%;""></iframe></div></div><p id="""">‍</p><h2 id="""">What are ATS Systems?</h2><p id="""">An <strong id="""">Applicant Tracking System (ATS)</strong> stores, organizes, and manages job applications, acting as the first layer of candidate filtering. It streamlines the shortlisting process by performing the following key steps:</p><ul id=""""><li id=""""><strong id="""">ATS Resume Parsing</strong></li></ul><p id="""">The ATS breaks the resume into structured fields like name, contact info, work history, job titles, education, and skills. The goal is to convert the resume's content into a standardized, analyzable data format.</p><ul id=""""><li id=""""><strong id="""">Keyword Matching</strong></li></ul><p id="""">This is the most crucial step: matching the information against the job description.<br>The system scans for specific keywords like job titles (e.g., “Backend Developer”), technical skills (e.g., “React,” “AWS”), certifications, and educational qualifications. It looks for exact word-to-word matches between what's on the resume and what's in the provided job post or criteria.</p><ul id=""""><li id=""""><strong id="""">ATS Scoring and Ranking</strong></li></ul><p id="""">Once keyword matching is done, the ATS ranks resumes based on how well they align with the job description. Resumes with more keyword matches, relevant experience, and qualifications rank higher. Some systems even display a match score (e.g., ""85% match""), with higher scores pushing resumes up the rankings.</p><h2 id="""">Challenges of using ATS systems&nbsp;</h2><p id="""">ATS systems do help recruitment by handling scale and automating processes, but their flaws often outweigh the benefits, sometimes even hindering the hiring process. Here are the biggest issues with relying on them.</p><h3 id="""">Parsing failures and rigid profiling</h3><p id="""">Conventional ATS systems rely heavily on structured parsing, which can break when resumes use non-standard formats like complex templates or multi-column layouts. This often leads to important details like job titles, skills, or education being misclassified or lost.</p><p id="""">‍</p><p id="""">These systems also expect traditional, linear career paths. Freelancers, startup founders, or candidates with career gaps or short-term roles are often ranked lower, simply because their experience doesn’t fit neatly into the rigid structure.</p><p id="""">‍</p><p id="""">A major flaw is the over-reliance on exact keyword matching. If a job description mentions ""JavaScript"" but a resume lists ""JS,"" the system often fails to make the connection. It struggles with synonyms, abbreviations, and variations.</p><h3 id="""">Candidates gaming the system</h3><p id="""">The overreliance on keyword prioritization has led candidates to optimize their resumes to trick the system. Many overload their ""Skills"" section with job description keywords, or hide entire blocks of text from the job post in white text to fool the parser.</p><p id="""">Some candidates use variations of job titles, even if they don’t hold those exact roles. For example, a ""Software Engineer"" might list ""Developer"" or ""Programmer"" to cover more job posts.</p><p id="""">This keyword stuffing inflates ATS scores but doesn’t reflect actual skills. As a result, time is wasted on resumes that look perfect to the system but fail to deliver upon closer inspection. The core issue is that ATS can’t tell the difference between genuine expertise and keyword repetition.</p><h3 id="""">Lack of contextual understanding</h3><p id="""">ATS operate at a surface level, missing context, depth, and transferability. A candidate who managed a complex project but did not use exact buzzwords, may get overlooked. Career shifts, cross-functional skills, and project-based achievements often remain invisible. They also fail to assess qualities like initiative, ownership, or real-world impact.</p><p id="""">Without reasoning abilities, ATS misses the nuances that predict job performance. For instance, they can’t infer that managing a ""cloud migration project"" implies experience with AWS/Azure, or that building a ""Restful API in Node"" suggests proficiency with JavaScript.</p><p id="""">Language Models (LLMs) analyze resumes in a way that mirrors human understanding. They grasp the relationships between a candidate's achievements, skills, and experience, going beyond surface-level keyword matches.&nbsp;</p><p id="""">By understanding context and intent, LLMs can recognize a candidate’s true potential. This leads to fairer, accurate, and more reliable candidate assessments.</p><h2 id="""">How to build an LLM based Resume analyser</h2><p id="""">Instead of relying on keyword matching like traditional ATS systems, a smarter system can be built using a combination of LLMs. These models don’t just scan for terms, they understand context, connect achievements, and assess the depth of a candidate’s experience, much like a human would. Any general-purpose LLM can be paired with a reasoning-focused model to make this possible. In this case, GPT-4.1 works alongside o4-mini. Together, they deliver a far more accurate and meaningful evaluation of candidate potential.</p><p id="""">‍</p><p id="""">Lets break it down</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1107px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1107px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/681298661e0374406a836f21_AD_4nXdv_DuxwoChNQFvY1ikDayWbmLcKdpwzsON4Pgga9V7kJypGKDDreO0RSC9pBxf6NZvvyg4lYk8X3cqw5ETUrb0yf8taOviBTJ9uWAHFarWn1LHauVrY_mX8necExfVAjsv5K6JWg.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><h3 id="""">Job Description Analysis&nbsp;</h3><p id="""">The process starts with the recruiter providing the job description for the position. This job&nbsp;</p><p id="""">description is then given to an LLM, in this case ‘gpt-4.1’, to analyse and break it down into three structured parts:</p><ul id=""""><li id=""""><strong id="""">Must-Have Requirements</strong>&nbsp;</li></ul><p id="""">These are<strong id=""""> ‘non-negotiable’ </strong>skills, qualifications, or experiences that a candidate must meet to be considered for the role.<br>Example: 3+ years of backend development experience in Java.</p><ul id=""""><li id=""""><strong id="""">Good-to-Have Requirements</strong></li></ul><p id="""">These are additional skills or experiences that are<strong id=""""> ‘nice to have’</strong> and can strengthen a candidate's profile but are not mandatory.<br>Example: Familiarity with AWS or Kubernetes.<br><br></p><ul id=""""><li id=""""><strong id="""">Additional Screening Criteria</strong></li></ul><p id="""">These are specific conditions or constraints that can filter candidates based on eligibility, availability, or work policies.<br>Example: Only candidates available for immediate joining; no remote work option.<br><br></p><p id="""">The following prompt was used with gpt-41 for achieving the desired output.</p><div data-rt-embed-type='true'><div style=""background-color: #2A2A2B;padding: 2.5%;"">

<code>
<i>

<br>
You are an experienced technical recruiter and job posting analyst. Your task is to extract a structured summary of candidate requirements from the job description These will be used to evaluate resumes later.<br><br>
Instructions:

Extract all Must-Have Requirements:<br>
<br>

-Technical Skills: Essential technical skills<br>
-Experience: Minimum years and type of relevant experience<br>
-Qualifications: Mandatory degrees or certifications<br>
-Core Responsibilities: Key duties that are non-negotiable<br>

<br>
<br>

-Extract all Good-to-Have Requirements:<br>
<br>

-Additional Skills: Preferred technical skills<br>
-Extra Qualifications: Bonus education or certifications<br>
-Bonus Experience: Extra experience that could add value<br>
<br>
<br>

Extract any Additional Screening Criteria:<br>
<br>

-Filtering statements that affect eligibility<br>
-Work policy conditions<br>
-Availability constraints<br>
-Discriminatory or biased phrasing<br>
-Anything else that significantly affects who should or shouldn't apply<br>

</i>
</code>

</div></div><p id="""">‍</p><p id="""">This core prompt, along with telling the model to output in the shown below JSON schema creates our prompt.</p><p id="""">The output is returned in JSON format with four sections: <em id="""">original_job_description</em>, <em id="""">must_have_requirements</em>, <em id="""">good_to_have_requirements</em>, and <em id="""">additional_screening_criteria</em>. Each section contains structured arrays extracted from the job description.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:960px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""960px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68129330f96853508dbe169d_AD_4nXfBTsCY_XCeXaJx64SVB-jGwQLfVZzpWMBovEZ5OwMfV4yA12iL4IpFGXapuYivT21Mu3IIdKaDXpInk_PEWDnXlGantqg-DiV7lmT9mOe1r_F9jyRLsgammHDKtXRdaqqpG6rY.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><p id="""">The JSON output is structured, making it easier to process in the next steps. It helps the second LLM model better understand the job description compared to the original text format.</p><p id="""">‍</p><p id="""">The whole process is split into two parts to make the most of each model’s strengths. GPT-4.1 focuses on the job description, breaking it down to clearly understand what the role needs. Then, o4-mini evaluates the candidate, using its reasoning to assess the fit and explain <em id="""">why</em>.&nbsp;</p><h3 id="""">Resume Analysis</h3><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68129899c647b112762441ef_AD_4nXdXswwwyBeNgv_WmFegt_f0eqQ9jOu6TBT6E4hCggg0Y1DXyEMpnUnwN3xaPQNHg2WoRDX6zrEyTc3Jl7nGj28bYc0BkCKSrj-89LIkZ40MDJUSxqrENAdI5jIX9F2xK8eq4-e9.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">&nbsp;This stage uses ‘o4-mini’, a reasoning model that excels at step-by-step thinking, making it ideal for multi-stage evaluation tasks. The LLM takes the output from the job description analyzer and the candidate's resumes as input, and assesses each candidate's fit against the job description.</p><p id="""">To achieve this, a single prompt is broken into three distinct parts: <strong id="""">quantitative evaluation, qualitative assessment, </strong>and <strong id="""">final recommendation</strong>. This modular breakdown serves two purposes.</p><p id="""">First, it aligns with how reasoning models work best, one step at a time, ensuring more accurate and consistent results. Second, it gives us greater control. If evaluation criteria change, each section can be updated independently without reworking the entire system. This structure keeps the logic clean, flexible, and easy to evolve as hiring needs grow.</p><h4 id="""">Quantitative Check</h4><p id="""">The first step is a quantitative evaluation of the candidate’s resume. This focuses on the non-negotiable, hard requirements of the job. For example, if the role needs 3+ years of Python and the candidate has only worked with Java, they miss a core requirement. This ensures we filter out mismatches early. It’s a strict check, not based on potential, but on clear criteria that must be met. This also allows the system to later focus on deeper insights and harder-to-measure qualities, such as softer skills.&nbsp;</p><p id="""">‍</p><p id="""">Below is the prompt we use for Step 1.</p><div data-rt-embed-type='true'><div style=""background-color: #2A2A2B;padding: 2.5%;"">

<code>
<i>
You are a recruiter evaluating a candidate's resume against a given job description (JD). Based on the JD, evaluate whether the candidate meets the necessary requirements.<br><br>
Step 1: Quantitative Check<br>
Perform a Boolean (true/false) check for each requirement based on the candidate's resume:<br><br>

-For each skill listed in must_have_requirements and good_to_have_requirements, determine if the candidate possesses it. Return true or false for each.<br>
-For each core_responsibility, determine if the candidate has demonstrated it in their past work. Return true or false.<br>
-For each additional_screening_criteria, return a boolean value indicating whether the candidate meets the condition (e.g., full-time, onsite position, work authorization, etc.).<br>

</i>
</code>

</div></div><p id="""">‍</p><p id="""">Remember the JSON output from the Job Description Analyzer? In this step, o4-mini analyzes the candidate’s resume and checks each requirement from that output, providing a simple boolean <em id="""">(true/false)</em> response for each one.&nbsp;</p><p id="""">‍</p><p id="""">It looks something like this:</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:717px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""717px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6812933116430104c5aaaddf_AD_4nXeB6f5r-PbQ8xP4kQmb4gw4Dn7d4yWc4IfXl7bGZo7ntiJw9HERoHEaGvD9YUeHxGR0xBt2w0LW7HAkBtGSwGQSSbwt3drm6Cn3uaGqgjgAxWJKSaWlg2ARoNSyGX_OKdZBqGwOrw.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">This example shows the boolean evaluation for <em id="""">must_have_requirements</em>. Similar true/false responses are also generated for <em id="""">good_to_have_requirements</em> and <em id="""">additional_screening_criteria</em>.</p><p id="""">‍</p><h4 id="""">Qualitative Check&nbsp;</h4><p id="""">The next step is the qualitative analysis of the candidate’s resume. This is where human-like understanding and holistic evaluation come in. We’re testing the candidate's inferred skills, the real-world impact of their projects, their level of ownership and initiative, and how transferable their experience is to the role. <em id="""">Here, we are reading between the lines to understand soft skills, the practical impact of their work, and to extract abilities that aren’t just explicitly mentioned but can be inferred from their experience.&nbsp;</em></p><p id="""">This is where the real magic of the LLM happens, because it mirrors human evaluation, helping us judge the true potential and fit of a candidate.&nbsp;</p><p id="""">This is the part of the prompt that achieves the qualitative analysis in Step 2</p><div data-rt-embed-type='true'><div style=""background-color: #2A2A2B;padding: 2.5%;"">

<code>
<i>

Step 2: Qualitative Assessment<br>
Now, switch to a recruiter-style qualitative assessment. Use your intuition like a human — go beyond what's explicitly stated. Read between the lines, infer intent, and use contextual clues from the resume and the JD to judge fit. Reference the results from Step 1 as part of your reasoning.<br><br>
Assess the following:<br>
<br>
-Inferred Skills: What skills can you infer from the candidate's projects or roles?<br>
-Project Gravity: Were the projects academic or real-world, high-impact, production-ready, etc.?<br>
-Ownership and Initiative: Did the candidate lead the work? Show initiative? Or just follow directions?<br>
-Transferability to Role: How well would their experience transfer to this particular role? Will they onboard quickly?<br>
-Bonus Experience & Extra Qualifications: If the JD lists any bonus criteria (e.g., fintech, B2B SaaS), consider that a positive signal even if not part of Step 1.<br>
-Recruiter Style Summary: Provide a recruiter style summary of the full profile of the candidate, consider the results of both the quantitative and qualitative assessment.<br>

</i>
</code>

</div></div><p id="""">‍</p><p id="""">Here, the result is again a JSON object, which includes the inferred skills from the candidate’s projects, the <strong id="""">project gravity </strong>(assessing the quality, difficulty, and real-world impact of their work), the<strong id=""""> level of ownership and leadership</strong> initiative shown, and a direct evaluation of <strong id="""">transferability</strong> that is, how well the candidate’s skills and experience fit the requirements of the role</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:742px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""742px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68129331edf6bd05963f1f27_AD_4nXeInAYODussmCBSEaIFIisTlGugQVTf5OC68lPhP4PaaCQTYpyF02qoDOLPHir2BPNLafvhP6zbJ04zjtwSMOvH2LxsPN8o654Rry00yrHP5aVCctJmGPkCpr-GCeT-X8aWUqVgIw.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">&nbsp;</p><p id="""">This kind of human-like qualitative assessment is impossible with a traditional ATS system. Our method goes beyond basic keyword matching by considering all the important factors that truly affect a candidate’s quality. An ATS would never be able to assess project gravity, real-world impact, or skill transferability.&nbsp;</p><p id="""">It would treat a Python project on a simple IRIS dataset the same as a complex Django backend system,&nbsp; giving them equal weight even though the difficulty and relevance are completely different. By automating these deeper qualitative checks, our system delivers much higher-quality results.</p><h3 id="""">Final Recommendation</h3><p id="""">This is the third and final step, where we consider both qualitative and quantitative factors to give a clear recommendation on whether the candidate should move forward. The output is a simple ""<strong id="""">Yes</strong>"" or ""<strong id="""">No</strong>"" based on a well-rounded evaluation. <br><br>Along with the final recommendation, it also provides the factors and reasoning behind the decision. The following prompt is used for Step 3&nbsp;&nbsp;</p><p id="""">‍</p><div data-rt-embed-type='true'><div style=""background-color: #2A2A2B;padding: 2.5%;"">

<code>
<i>

## Step 3: Final Recommendation <br>
After both steps, make a final call. Output ""Yes"" or ""No"" and summarize your reasoning concisely.

</i>
</code>

</div></div><p id="""">‍</p><p id="""">Here’s an example of the expected JSON output for this step:</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:737px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""737px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/681293319588ff5a384ecab8_AD_4nXfsSrk6eJX0Pbi4ek7osdKTBaz8JnHRGaxoAhvRohv6pnm09I_fP4Qm6-P8gg9b38wCc4isC7KwFdS6CmHySLi22VR5g10RaQirVJJRP_ZI_hhNpN2t0ydlxOfV-4Ly2q0bN2nmkg.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><h2 id="""">Traditional ATS vs LinkedIn vs LLM-based Resume Analyser&nbsp;</h2><p id="""">To truly understand the difference, we ran a simple test. We supplied the same resume and job description to a traditional ATS checker, LinkedIn's new AI feature for recruiters, and the proposed LLM-based Resume Analyzer.</p><p id="""">The goal is to compare how each system interprets the resume, what kind of evaluation it provides, and how accurate or useful the results are for real hiring decisions.</p><p id="""">Given below is the job description we posted on LinkedIn for a Machine Learning Intern role at Mercity. The resume shown was one of the top candidates we received.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6812933199c937d7425c7fc5_AD_4nXeqaZ-TtR-5UOcltqiBityJ75MrBkZDr2fhHQEr8TRJBCG9pkSoOq8kGSi7yI9EU2co2xTb5CbS0h57p7yfx7Bzrk5Mju9ahLErAOjizeL-KLMlb4WKdcZ6lUmejJBTVml8YVb2VQ.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><h3 id="""">Traditional ATS system</h3><p id="""">Let's first try the traditional ATS system and see how it evaluates the candidate</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1194px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1194px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/681293306ca3c9c89bbf1645_AD_4nXcyhFjrSU6-oDFksKqKDTOL8b4FxIpI4Vx9uWfUPh-PNJL_5vOL0HoVHfUz1lXyzPDnBvNSKf5RB4DStdZY9Ra0teMNPgUzFBeNp1wSeNrxPQ2jcAJRUh9-3IDhcTkX7xG5gLmB.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">The system focuses only on matching exact keywords from the job description, while completely ignoring the candidate’s actual achievements, internships, projects, and certifications. In this case, the candidate clearly mentioned ""deep learning with CNN,"" yet the ATS marked them as not meeting the ""deep learning techniques"" keyword, a minor wording difference leading to a major evaluation failure.&nbsp;</p><p id="""">Such rigid matching is counterproductive. Strong candidates are scored low simply because they didn’t optimize for exact keywords. This kind of outdated evaluation hurts both the recruiters and the applicants</p><p id="""">‍</p><h3 id="""">LinkedIn&nbsp;</h3><p id="""">The new LinkedIn AI feature for job postings does a slightly better job</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68129347a4d0bef5dbc07695_AD_4nXcFD4j6N_P3xWmbJTMhf0Rcn6mONqI9qEArCOYKOyumgM9U89nxgalZ1bpiLC-ys5gT1rxzNGk0hBFXIfEL15zZrxnxJNPqglmalA26cTqrx1HmEVKvlI_JDa2O4H_SzxktwXZ6sA.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><p id="""">This system does the basic job of parsing resumes and identifying criteria that are met. It can even go a step further by recognizing skills that aren’t explicitly worded the same way, such as mapping academic internships to the ability to read AI papers. However, it doesn't go beyond that. It's essentially a smarter version of keyword matching, sufficient for rejecting clear mismatches, but far from what’s needed for true selection decisions.&nbsp;</p><p id="""">This is useful for shortlisting resumes but doesn't offer any real evaluation. Recruiters still have to dive deeper into the resume to fully understand the candidate. Simply put, it’s more of a filtering tool than a true recruiting assistant.</p><h3 id="""">LLM based Resume Analyser</h3><p id="""">The LLM based Resume Analyser, goes leaps and bounds above these simplistic evaluation methods.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:437px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""437px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68129330e967db1d134df38b_AD_4nXcdgr7rbgCuTe6N5mz7vQxO0-1qXs6J2x3eKe_lVdjpc9djsjr3rS8YJfXszMsvVASbu7E7orbqyyLVGrqScCszV6FzE4OChGYmFhetJeoaUb8s4NdaJL7lJiAkSBubHqONqI0J.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">The system first evaluates the must-have skills (technical skills and core responsibilities), the good-to-have skills, and the screening criteria, providing a detailed analysis of which ones are met and which ones are not. This gives a quantitative view of how well the candidate aligns with the hard requirements of the job.&nbsp;</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:375px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""375px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/681293307a40f6357567033e_AD_4nXdgmMuUtsDHLUBKRk8lCAfyurWjiGTDhmkF30-n5wLk2eNewzQs5mSQMgvpCWx6qTTzkgR6fmWgJp8OhxzHx_mcbcBT5GiC8tAMqhmFonkyyBvzOMru2nfrgvIj3gMsU_XDPu4mNQ.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">Next, the qualitative assessment begins. It evaluates the difficulty of the candidate's projects, their leadership and ownership, and assesses overall fit based on these factors. Each of these three qualitative aspects is rated as high, medium, or low. Following this, a short summary of the candidate's profile is provided, highlighting their strengths and weaknesses.</p><p id="""">Finally, a recommendation is made based on both the quantitative and qualitative evaluations, along with a clear explanation for the decision.</p><p id="""">Candidates benefit because the system is fair, it understands the context behind their experience. With this level of detailed analysis, hiring teams don’t need to dig deep into every resume. The AI provides clear insights that make shortlisting quicker, and more accurate, saving time while ensuring no strong candidate is overlooked.</p><h2 id="""">Why choose an LLM-Powered Resume Screening?</h2><p id="""">Instead of vague scores, the system provides clear <strong id="""">Yes/No</strong> recommendations backed by logical reasoning. It saves time on manual screening and quickly generates a shortlist of the most promising candidates. Strong candidates who may have been overlooked due to poor resume writing now surface automatically, improving the quality of the talent pool.</p><p id="""">The system integrates smoothly with your existing ATS, adding a deeper, more human-like evaluation layer that assesses real-world skills and role fit. Resumes can be processed directly from the ATS in batches or integrated via simple APIs with minimal technical effort.</p><p id="""">Before even opening a resume, you’ll have a concise, ready summary of each candidate’s strengths and potential. This enables faster shortlisting, fewer hiring mistakes, and ensures that better candidates make it to the interview stage, all without disrupting your current systems.</p><h2 id="""">Try Out Our LLM-Based Resume Analyzer</h2><p id="""">We built a quick prototype using the same approach explained above, and it’s live for you to try out.<br>While the article shows how to build one using GPT models, the same idea works just as well with any open-sourced LLM like LLaMa, Mistral, etc.</p><p id="""">This demo video walks you through how to use the prototype, and you’ll see for yourself how it goes beyond keyword matching to evaluate real candidate-role fit.<br><br>Check it out and see it in action. <br></p><div data-rt-embed-type='true'><div style=""position: relative; padding-bottom: 56.25%; height: 0;""><iframe src=""https://www.loom.com/embed/3985f56862074443b058e705fc47590a?sid=a02fbd46-b4ee-43a2-bd85-a696ae6379c9"" frameborder=""0"" webkitallowfullscreen mozallowfullscreen allowfullscreen style=""position: absolute; top: 0; left: 0; width: 100%; height: 100%;""></iframe></div></div><h2 id="""">Want to Simplify Hiring at Your Organisation?</h2><p id="""">If you're looking to streamline your hiring process, we can help you harness the power of AI to automate tasks tailored to your specific needs. We've already helped numerous organizations save time, reduce effort, and cut costs by automating their entire hiring workflow.</p><p id="""">Contact us today, and let's transform your recruitment process.</p><p id="""">‍</p>",https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/681297bed12d873f1d757841_resume_analysis.png,Yash Sonawale,AI in Hiring Automation,"In this detailed guide, we talk about how LLMs can be used to automate hiring by a lot and be an major upgrade from the old ATS systems. We show how to build and present our own open source solution for people to work with.",False,
Building Medical AI assistants with Visual LLMs,building-medical-ai-assistants-with-visual-llms,640f56f76d313b2faa631c11,66476117afa6f9cf10c8c361,False,False,Fri May 17 2024 13:52:23 GMT+0000 (Coordinated Universal Time),Fri May 17 2024 13:52:23 GMT+0000 (Coordinated Universal Time),Fri May 17 2024 13:52:23 GMT+0000 (Coordinated Universal Time),"<p id="""">Medical image analysis has become an integral part of modern healthcare, enabling clinicians to make informed decisions and improve patient outcomes. However, accurately segmenting medical images presents several challenges, including the costly and time-consuming task of manually annotating datasets for training deep learning models. Additionally, while large language models (LLMs) have shown promise in various domains, they often lack the specialisation required for precise medical image analysis, leading to suboptimal performance in critical tasks like tumour detection.</p><p id="""">‍</p><p id="""">Existing medical LLMs, such as <a href=""https://sites.research.google/med-palm/"" id="""">MedPaLM</a> and <a href=""https://github.com/epfLLM/meditron"" id="""">MediTron</a>, have demonstrated promising results in various medical domains. However, these models often lack the specialised architecture required for precise medical image segmentation. The proposed solution, which combines the strengths of U-Net and visual foundation models, has the potential to outperform existing medical LLMs in tasks like tumour detection, where precision is paramount.</p><p id="""">‍</p><p id="""">In this article, we'll show you how to build a personalised Med VLLM for your particular use case and dataset.</p><h2 id="""">What are Visual Large Language Models (VLLMs)?</h2><p id="""">Visual Large Language Models (VLLMs) are a class of deep learning models that excel at processing and understanding visual data, such as images and videos. These models are trained on vast amounts of visual data and can extract high-level features and semantic information from images. VLLMs have shown promising results in various computer vision tasks, including image classification, object detection, and image segmentation. The approach of Visual LLMs are inspired from the <a href=""https://github.com/RustamyF/clip-multimodal-ml"" id="""">CLIP (Contrastive Language-Image Pretraining)</a> model developed by OpenAI in 2021.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1400px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1400px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/66475ecfce5605b468a616bf_dKs_4hZNrfFTu_J_9lmfTOZPSDCeOB7zlJ4shOuTfviGCcT_INWcVjonUyV0ordRBxvtA8ouYKa2b7iCAUTzC9nvaFX-IPJ5i4MNeZ6og5pNyKWCyH0gJKJfJVbfwhfcn9ynb0Wg1m0H6diYxibq77w.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><p id="""">In the field of medicine,Visual LLMs can show promising results in tasks like diagnostic imaging, surgical assistance. However, their effectiveness can be hindered by challenges in accurately interpreting medical images, especially without proper image segmentation. This limitation often results in decreased accuracy in critical tasks such as disease identification and surgical planning.Therefore, it led us to a solution from the U-Net Model.Let’s understand what U-net is and how well it can tackle the difficulties faced in Visual LLM approach in further below.</p><p id="""">‍</p><h2 id="""">What is U-NET ?</h2><p id="""">The U-Net architecture is known for its effectiveness in semantic segmentation tasks in medical imaging analysis. In the context of tumour detection, the encoder component of U-Net acts as a feature extractor, capturing intricate details within the medical images that may indicate the presence of tumours. The decoder reconstructs the segmented regions based on these features, enabling precise delineation of tumour boundaries and shapes, thus aiding in accurate tumour detection and analysis.</p><p id="""">‍</p><p id="""">The incorporation of skip connections in the U-Net architecture enhances image processing by facilitating the flow of high-resolution information between encoder and decoder layers, preserving fine-grained details and subtle features of tumours throughout the segmentation process.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/66475ecfae56940bf7fa2557_wo9w12VFOTBAL36-rG_2j0HHlPib29a9UmFKWMKGWci557aI-QSREid5MOedkB2Qs3vOynCawnpd9FEXagHhLncb3O7UiJL6nYV8_gmQ5J6zDpUISaRQX5F9sDxqYo5FpR1Bes3PFaVmexLkZfwiNrs.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><h2 id="""">Why use U-Net Model in Medical Imaging?</h2><p id="""">‍</p><p id="""">Recent studies have shown that integrating VLLMs and U-Net can lead to significant improvements in medical image segmentation accuracy. For example, the Mamba-UNet architecture, which combines the strengths of U-Net with the long-range dependency modelling capabilities of the Mamba architecture, has achieved state-of-the-art results on the ACDC MRI Cardiac segmentation dataset and the Synapse CT Abdomen segmentation dataset. This architecture has been recognized for its ability to capture intricate details and broader semantic contexts within medical images, making it highly suitable for our proposed project.</p><p id="""">‍</p><p id="""">The table below provides the exact numerical values, further demonstrating the superior performance of&nbsp; the U-Net architecture achieves higher accuracy compared to other models like DeepLabV3 across various metrics such as Dice Similarity Coefficient (DSC), F1-score, and Intersection over Union (IoU) from a <a href=""https://www.researchgate.net/figure/Accuracy-comparison-of-the-proposed-BLSNet-model-with-UNet-and-DeepLabV3_tbl2_351396518"" id="""">research paper</a>:</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1108px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1108px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/66475ecf770a7b1df1b225f2_OerEshZjakFDEp_EycCO5Lv-jOpZ0A4anj-nLN68gwZJoo1d9qVU-AmiHJcE-HmszmN4CR7PKFFTh-sP2NtlERpTPOwP7T5pR_YbPdc8huXyP6JJwcyyReyQRPhmpx_xKiWKXfkCzOp9seReOV3rNP8.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><p id="""">By leveraging the power of VLLMs and U-Net, our project aims to develop an efficient and accurate medical image segmentation framework that addresses the challenges and limitations of existing approaches. The integration of these cutting-edge technologies, combined with the innovative approach, has the potential to revolutionise the field of medical image analysis and significantly improve patient outcomes.</p><p id="""">‍</p><h2 id="""">Using Visual Large Language Models (VLLMs) in Medical Imaging and Healthcare</h2><p id="""">Visual Large Language Models (VLLMs) have the potential to revolutionise medical imaging and healthcare by seamlessly integrating advanced language processing capabilities with visual data analysis. These cutting-edge AI models, such as <a href=""https://arxiv.org/html/2303.00915v2"" id="""">BiomedCLIP</a> and ChatGPT-4, <a href=""https://sites.research.google/med-palm/"" id="""">Med-Palm 2</a> ,<a href=""https://www.semanticscholar.org/paper/Med-Flamingo%3A-a-Multimodal-Medical-Few-shot-Learner-Moor-Huang/c9dbdae8146b9f97e254f5d26fd6efde96eaa703"" id="""">Med Flamingo</a> have demonstrated remarkable performance in various medical imaging tasks, including diagnostic analysis, image segmentation, and report generation. Let’s see how VLLMs can help doctors and patients.</p><p id="""">‍</p><h3 id="""">Generating Comprehensive Personalised Medical Reports with RadiologyData</h3><p id="""">‍</p><p id="""">One of the key advantages of VLLMs in medical imaging is their ability to process and understand multimodal data efficiently. These models can analyse a wide range of medical images, such as X-rays, CT scans, MRIs, and histopathological slides, and extract relevant information to aid in diagnosis and treatment planning which was significantly proved in the <a href=""https://www.medrxiv.org/content/10.1101/2023.12.21.23300146v3.full"" id="""">research paper</a>. By leveraging the power of language models, VLLMs can also incorporate patient history, medication records, and other relevant textual data to provide a more comprehensive and personalised assessment.The below graph taken from the research paper shows a comparison of human and LLM.In this paper they have used Flamingo-80B&nbsp; model where Flamingo-80B is quite less than the human accuracy where the approach of integrating&nbsp; with U-Net Model&nbsp; can eventually improve the existing metrics.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:842px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""842px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/66475ecf3aedb1f3483fdf6c_ZVU640bauu_3knJRrrVLYOZYSY7-zo3gVPlO50W-Kn3wGjhao-eNX7cXI38SAiQYmsE2ee9flgBas3sSHnn7NsujMAHCeH5B3j7drlPjTWQ0jep5XwAO049GQLF_wo6h0JOFeTD_J2KQe0MEWf9NCwk.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><h3 id="""">More Efficient and Faster Diagnostic Processes</h3><p id="""">VLLMs can significantly streamline diagnostic processes by enabling doctors to quickly analyse medical images and generate detailed reports. Using specific prompts, doctors can direct the model to focus on specific anatomical regions, structures, or tissues, allowing for targeted and efficient analysis. This interactive approach enhances the accuracy of image interpretation and reduces the time required for diagnosis, ultimately leading to faster treatment initiation and improved patient outcomes.</p><p id="""">‍</p><p id="""">VLLMs can also assist doctors in differential diagnostics and creating a clinical plan for the patient. Because VLLMs can process a lot of medical history of a patient, they can be much better at informing the doctor of the current status of the patient and a bullet point summary of previous history of medications and health problems faced by the patient and then with a longer context understanding the doctor can proceed with next steps with suitable treatment.</p><p id="""">‍</p><h3 id="""">Automating Medical Reporting and Integrating Patient History</h3><p id="""">One of the most promising applications of VLLMs in medical imaging is the generation of comprehensive medical reports. These models can analyse a set of medical images, such as X-rays, CT scans, or MRIs, and generate detailed reports summarising the observed abnormalities, their locations, and potential implications for diagnosis or treatment. By automating the report generation process, VLLMs can significantly reduce the workload of radiologists and other medical professionals, allowing them to focus on more critical tasks.To read more about the specific work please read the <a href=""https://arxiv.org/html/2403.02469v1"" id="""">research paper</a>.</p><p id="""">‍</p><p id="""">VLLMs can also incorporate patient history and medication records to provide a more holistic assessment of the patient's health. By combining visual data from medical images with textual information from patient records, VLLMs can identify potential correlations, flag potential drug interactions, and provide personalised treatment recommendations. This integration of multimodal data can lead to more accurate diagnoses and more effective treatment plans.</p><p id="""">‍</p><h2 id="""">How to build a Custom Medical Visual LLM</h2><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/66475ecfedcbfb5d7f77949e_1bD3mDmcTgaU9FbUA-oNEEx5wS9VDO2hJCKycfXkio1H66hrhJF4r0dt1eeD0FgNLZyP32hdPfP3GYOBFAukn65vaHSGcjgFIRPNEbOaM9WDuG-ilIEG7tUZNo5agIt15pw3R8-cpxFsfCuO1Jajv6I.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><h3 id="""">Chat-Interface</h3><p id="""">The integration of a chat interface,allowing medical practitioners to easily interact with the system using both text and images. By leveraging the multimodal capabilities of large language models (LLMs), the approach can effectively process and analyse both textual queries and medical images, such as MRI scans.</p><p id="""">‍</p><p id="""">When a medical practitioner obtains an MRI image, they can simply upload it to the chat interface. The approach then proceeds with the necessary preprocessing steps, including normalisation, augmentation, and noise reduction, to ensure optimal performance of the U-Net architecture. The chat interface provides a user-friendly and accessible way for doctors to interact with the system, enabling them to input specific prompts or questions related to the uploaded MRI image.&nbsp;</p><p id="""">‍</p><p id="""">This interactive approach allows medical professionals to direct the model's focus to particular anatomical regions, structures, or tissues of interest, facilitating targeted and efficient analysis.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:728px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""728px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/66475ecf4c2c12d45d83830a_BJpZkfgAE7g1Mp6bnRgIQbb2TRE1P-ZkmnP5uOitpluXyezLnNhXzuWBGr1apI_eIwMQTQUj4qWVsA0hPtKHVEg2MVUiqcKFZ-UUeVhSeAudmW3gaEeBpA3g7hiFdgBKRi-d8rzatWzoJ3Dl6BPfwaA.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><p id="""">By combining the chat interface with the powerful capabilities of VLLMs and U-Net, The approach generates detailed medical reports that summarise the observed abnormalities, their locations, and potential implications for diagnosis or treatment. These reports are easily accessible through the chat interface, allowing doctors to quickly review the findings and make informed decisions about patient care.</p><p id="""">‍</p><h3 id="""">U-Net Model</h3><p id="""">The U-Net architecture is particularly well-suited for the task of tumour segmentation in medical images. Its unique encoder-decoder structure and skip connections enable precise localization and accurate delineation of tumour regions.</p><p id="""">​​</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:685px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""685px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/66475ecfa84d888b44916f60_oD_3Xpke7xltjNOfn9o_L_tR82yVxi5sIkK2lm5fRe4iBRBT3hlkDa4kODlidNy9KRYko1HO-elIf5Aj1WKTrFQqKQffK9_4S-cTD91mWrkGljjVQ_qkybl9GfSK19RuOhVAbA_Fdc6-_Yb5yM8ZvLc.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><h4 id="""">Encoder: Capturing Tumour Context</h4><p id="""">The encoder component of U-Net acts as a feature extractor, processing the input image and capturing relevant contextual information about the tumour. It utilises a series of convolutional and pooling layers to extract hierarchical features at different scales. These features include tumour texture, shape, location, and surrounding anatomical structures.</p><p id="""">‍</p><p id="""">As the encoder progresses through the layers, it gradually reduces the spatial dimensions of the feature maps while increasing the number of feature channels. This allows the model to capture broader contextual information about the tumour and its relationship with the surrounding tissues.</p><p id="""">‍</p><h4 id="""">Decoder: Precise Tumour Localization</h4><p id="""">The decoder part of U-Net is responsible for precisely localising the tumour region within the image. It takes the encoded features from the encoder and progressively samples them to restore the original spatial dimensions. At each decoding stage, the decoder combines the upsampled features with the corresponding high-resolution features from the encoder via skip connections.</p><p id="""">‍</p><p id="""">This combination of upsampled features and skip-connected features enables the decoder to precisely localise the tumour boundaries and reconstruct the segmented tumour region. The decoder's ability to precisely localise the tumour is crucial for accurate tumour volume measurement, delineation for targeted therapy, and classification based on tumour appearance and location.</p><p id="""">‍</p><h4 id="""">Skip Connections: Preserving Tumour Details</h4><p id="""">Skip connections play a vital role in preserving fine-grained tumour details throughout the U-Net architecture. These connections directly link the encoder and decoder layers, allowing the decoder to access high-resolution spatial information from the corresponding encoder layers.</p><p id="""">‍</p><p id="""">By retaining these details, skip connections ensure that critical tumour characteristics, such as irregular shapes, heterogeneous textures, and subtle boundaries, are not lost during the encoding and decoding process. This preservation of tumour details is essential for accurate tumour segmentation and subsequent analysis.</p><p id="""">‍</p><p id="""">The combination of the encoder's contextual feature extraction, the decoder's precise localization capabilities, and the skip connections' preservation of tumour details makes the U-Net architecture a powerful tool for tumour segmentation in medical imaging. By leveraging these strengths, researchers have developed highly accurate and robust tumour segmentation models that significantly aid in diagnosis, treatment planning, and monitoring.</p><p id="""">‍</p><h3 id="""">Visual Language Model (VLM)</h3><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/66475ecf341fca86aaca9567_wrOXc7_erwNFhgNtxgk0UCOYhy4ZY-Km8ioGuyS5KLg3cqIRtcENfY1pxSnAZhgXzsSu4n9y4H1lZS7oOyEqOEmj_V1fWyBZRc72nnNBKIyNOznAh_6QAZiCvNtdPhorHERpy3SRdS5AWNKW9YuzYq8.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">In this multimodal model architecture, we are focusing on utilising the system for medical purposes, specifically for detecting tumours in MRI images.</p><p id="""">‍</p><p id="""">The architecture begins with the vision encoder, which in this case is the CLIP ViT-L/336px model. This vision encoder processes the input MRI image, extracting high-level visual features. These features are then passed through the vision-language connector, a Multi-Layer Perceptron (MLP), which bridges the visual features with the language model.</p><p id="""">‍</p><p id="""">The language model, represented by Vicuna v1.5 with 13 billion parameters, interprets these visual features in the context of medical knowledge. The tokenizer and embedding components convert the visual features into a numerical format that the language model can process effectively.</p><p id="""">‍</p><p id="""">When an MRI image is input into the system, the vision encoder captures detailed information about the image, identifying potential areas of concern. The vision-language connector then translates these visual details into a form that the language model can understand and analyse.</p><p id="""">‍</p><p id="""">Upon receiving a user query such as, ""What is unusual about this MRI image?"" The system utilises the combined capabilities of the vision encoder and language model to detect abnormalities, such as tumours. The language model analyses the encoded features and generates a detailed response, pinpointing the exact location and nature of the tumour within the MRI image. This integrated approach leverages the strengths of both visual and textual data processing, resulting in precise and efficient tumour detection in medical imaging.</p><p id="""">‍</p><h2 id="""">U-Net compared toTraditional Methods in terms of Speed and Accuracy</h2><p id="""">‍</p><p id="""">Traditional medical approaches primarily focus on a person's disability, condition, and limitations, often overlooking their personal and psycho-social needs. These methods rely on symptom analysis and diagnostic tests to establish a diagnosis, aiming to address and treat underlying conditions. In contrast, traditional computer vision methods, such as those employing algorithms like SIFT, SURF, and BRIEF, rely on human-engineered feature extraction, which can be limited by the predefined features chosen for each image which would be really time consuming and time would be a barrier for a quick response to the medical situation.</p><p id="""">‍</p><p id="""">The medical vision approach, particularly with the integration of U-Net, presents a prominent solution for medical image analysis. U-Net, with its encoder-decoder architecture and skip connections, excels in semantic segmentation tasks, allowing the model to capture intricate details within medical images. This U-Net-based methodology has demonstrated groundbreaking advancements in medical image analysis, offering improved performance indicators and structural characteristics.</p><p id="""">‍</p><h2 id="""">Advancements and Potential of VLLMs in Medical Imaging</h2><p id="""">Research teams are actively exploring the use of LLMs in medical imaging. For instance, <a href=""https://www.medrxiv.org/content/10.1101/2023.12.21.23300146v3.full"" id="""">a study by Huang et al</a>. found that LLMs can outperform specialised, task-specific models in certain contexts. Another <a href=""https://arxiv.org/html/2403.02469v1"" id="""">study by Li et al</a>. introduced a cross-modal clinical graph transformer for ophthalmic report generation, showcasing the potential of VLLMs in specialised medical domains.</p><p id="""">‍</p><p id="""">As research progresses, more sophisticated and specialised VLLMs tailored to specific medical imaging tasks will likely emerge. Collaborating with medical professionals ensures these models meet the unique needs and challenges of healthcare.</p><p id="""">‍</p><p id="""">In summary, Visual Large Language Models can significantly enhance medical imaging by streamlining diagnostic processes, generating comprehensive reports, and integrating patient history and medication records. Despite existing challenges, the improved accuracy, efficiency, and personalised care offered by VLLMs make them a promising tool for the future of healthcare.</p><p id="""">‍</p><h2 id="""">Ready to Revolutionise Medical Imaging?</h2><p id="""">Experience the future of medical image analysis with our advanced vision solutions. Let us guide you through the process of building a personalised medical visual LLM tailored to your specific use case and dataset. <a href=""https://www.mercity.ai/contacts"" id="""">Contact us </a>&nbsp;today to embark on your journey towards precise and efficient medical image analysis. We can help you transform the landscape of healthcare and improve patient outcomes.</p><p id="""">‍</p><p id="""">‍</p>",https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/664760258cfa5fd5c1cdca4f_medical%20vllm.png,Mathavan,AI in Medical Industry,"LLMs for medical industry are very helpful, but medical images like CT scans, MRIs etc cannot be ignored. Hence we propose a pipeline with Visual LLMs which can process medical images and extract insights from them.",False,"<div class=""rich-text w-richtext""><p>Medical image analysis has become an integral part of modern healthcare, enabling clinicians to make informed decisions and improve patient outcomes. However, accurately segmenting medical images presents several challenges, including the costly and time-consuming task of manually annotating datasets for training deep learning models. Additionally, while large language models (LLMs) have shown promise in various domains, they often lack the specialisation required for precise medical image analysis, leading to suboptimal performance in critical tasks like tumour detection.</p><p>‍</p><p>Existing medical LLMs, such as <a href=""https://sites.research.google/med-palm/"">MedPaLM</a> and <a href=""https://github.com/epfLLM/meditron"">MediTron</a>, have demonstrated promising results in various medical domains. However, these models often lack the specialised architecture required for precise medical image segmentation. The proposed solution, which combines the strengths of U-Net and visual foundation models, has the potential to outperform existing medical LLMs in tasks like tumour detection, where precision is paramount.</p><p>‍</p><p>In this article, we'll show you how to build a personalised Med VLLM for your particular use case and dataset.</p><h2>What are Visual Large Language Models (VLLMs)?</h2><p>Visual Large Language Models (VLLMs) are a class of deep learning models that excel at processing and understanding visual data, such as images and videos. These models are trained on vast amounts of visual data and can extract high-level features and semantic information from images. VLLMs have shown promising results in various computer vision tasks, including image classification, object detection, and image segmentation. The approach of Visual LLMs are inspired from the <a href=""https://github.com/RustamyF/clip-multimodal-ml"">CLIP (Contrastive Language-Image Pretraining)</a> model developed by OpenAI in 2021.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1400pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/66475ecfce5605b468a616bf_dKs_4hZNrfFTu_J_9lmfTOZPSDCeOB7zlJ4shOuTfviGCcT_INWcVjonUyV0ordRBxvtA8ouYKa2b7iCAUTzC9nvaFX-IPJ5i4MNeZ6og5pNyKWCyH0gJKJfJVbfwhfcn9ynb0Wg1m0H6diYxibq77w.png""/></div></figure><p>‍</p><p>In the field of medicine,Visual LLMs can show promising results in tasks like diagnostic imaging, surgical assistance. However, their effectiveness can be hindered by challenges in accurately interpreting medical images, especially without proper image segmentation. This limitation often results in decreased accuracy in critical tasks such as disease identification and surgical planning.Therefore, it led us to a solution from the U-Net Model.Let’s understand what U-net is and how well it can tackle the difficulties faced in Visual LLM approach in further below.</p><p>‍</p><h2>What is U-NET ?</h2><p>The U-Net architecture is known for its effectiveness in semantic segmentation tasks in medical imaging analysis. In the context of tumour detection, the encoder component of U-Net acts as a feature extractor, capturing intricate details within the medical images that may indicate the presence of tumours. The decoder reconstructs the segmented regions based on these features, enabling precise delineation of tumour boundaries and shapes, thus aiding in accurate tumour detection and analysis.</p><p>‍</p><p>The incorporation of skip connections in the U-Net architecture enhances image processing by facilitating the flow of high-resolution information between encoder and decoder layers, preserving fine-grained details and subtle features of tumours throughout the segmentation process.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/66475ecfae56940bf7fa2557_wo9w12VFOTBAL36-rG_2j0HHlPib29a9UmFKWMKGWci557aI-QSREid5MOedkB2Qs3vOynCawnpd9FEXagHhLncb3O7UiJL6nYV8_gmQ5J6zDpUISaRQX5F9sDxqYo5FpR1Bes3PFaVmexLkZfwiNrs.png""/></div></figure><p>‍</p><h2>Why use U-Net Model in Medical Imaging?</h2><p>‍</p><p>Recent studies have shown that integrating VLLMs and U-Net can lead to significant improvements in medical image segmentation accuracy. For example, the Mamba-UNet architecture, which combines the strengths of U-Net with the long-range dependency modelling capabilities of the Mamba architecture, has achieved state-of-the-art results on the ACDC MRI Cardiac segmentation dataset and the Synapse CT Abdomen segmentation dataset. This architecture has been recognized for its ability to capture intricate details and broader semantic contexts within medical images, making it highly suitable for our proposed project.</p><p>‍</p><p>The table below provides the exact numerical values, further demonstrating the superior performance of  the U-Net architecture achieves higher accuracy compared to other models like DeepLabV3 across various metrics such as Dice Similarity Coefficient (DSC), F1-score, and Intersection over Union (IoU) from a <a href=""https://www.researchgate.net/figure/Accuracy-comparison-of-the-proposed-BLSNet-model-with-UNet-and-DeepLabV3_tbl2_351396518"">research paper</a>:</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1108pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/66475ecf770a7b1df1b225f2_OerEshZjakFDEp_EycCO5Lv-jOpZ0A4anj-nLN68gwZJoo1d9qVU-AmiHJcE-HmszmN4CR7PKFFTh-sP2NtlERpTPOwP7T5pR_YbPdc8huXyP6JJwcyyReyQRPhmpx_xKiWKXfkCzOp9seReOV3rNP8.png""/></div></figure><p>‍</p><p>By leveraging the power of VLLMs and U-Net, our project aims to develop an efficient and accurate medical image segmentation framework that addresses the challenges and limitations of existing approaches. The integration of these cutting-edge technologies, combined with the innovative approach, has the potential to revolutionise the field of medical image analysis and significantly improve patient outcomes.</p><p>‍</p><h2>Using Visual Large Language Models (VLLMs) in Medical Imaging and Healthcare</h2><p>Visual Large Language Models (VLLMs) have the potential to revolutionise medical imaging and healthcare by seamlessly integrating advanced language processing capabilities with visual data analysis. These cutting-edge AI models, such as <a href=""https://arxiv.org/html/2303.00915v2"">BiomedCLIP</a> and ChatGPT-4, <a href=""https://sites.research.google/med-palm/"">Med-Palm 2</a> ,<a href=""https://www.semanticscholar.org/paper/Med-Flamingo%3A-a-Multimodal-Medical-Few-shot-Learner-Moor-Huang/c9dbdae8146b9f97e254f5d26fd6efde96eaa703"">Med Flamingo</a> have demonstrated remarkable performance in various medical imaging tasks, including diagnostic analysis, image segmentation, and report generation. Let’s see how VLLMs can help doctors and patients.</p><p>‍</p><h3>Generating Comprehensive Personalised Medical Reports with RadiologyData</h3><p>‍</p><p>One of the key advantages of VLLMs in medical imaging is their ability to process and understand multimodal data efficiently. These models can analyse a wide range of medical images, such as X-rays, CT scans, MRIs, and histopathological slides, and extract relevant information to aid in diagnosis and treatment planning which was significantly proved in the <a href=""https://www.medrxiv.org/content/10.1101/2023.12.21.23300146v3.full"">research paper</a>. By leveraging the power of language models, VLLMs can also incorporate patient history, medication records, and other relevant textual data to provide a more comprehensive and personalised assessment.The below graph taken from the research paper shows a comparison of human and LLM.In this paper they have used Flamingo-80B  model where Flamingo-80B is quite less than the human accuracy where the approach of integrating  with U-Net Model  can eventually improve the existing metrics.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:842pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/66475ecf3aedb1f3483fdf6c_ZVU640bauu_3knJRrrVLYOZYSY7-zo3gVPlO50W-Kn3wGjhao-eNX7cXI38SAiQYmsE2ee9flgBas3sSHnn7NsujMAHCeH5B3j7drlPjTWQ0jep5XwAO049GQLF_wo6h0JOFeTD_J2KQe0MEWf9NCwk.png""/></div></figure><p>‍</p><h3>More Efficient and Faster Diagnostic Processes</h3><p>VLLMs can significantly streamline diagnostic processes by enabling doctors to quickly analyse medical images and generate detailed reports. Using specific prompts, doctors can direct the model to focus on specific anatomical regions, structures, or tissues, allowing for targeted and efficient analysis. This interactive approach enhances the accuracy of image interpretation and reduces the time required for diagnosis, ultimately leading to faster treatment initiation and improved patient outcomes.</p><p>‍</p><p>VLLMs can also assist doctors in differential diagnostics and creating a clinical plan for the patient. Because VLLMs can process a lot of medical history of a patient, they can be much better at informing the doctor of the current status of the patient and a bullet point summary of previous history of medications and health problems faced by the patient and then with a longer context understanding the doctor can proceed with next steps with suitable treatment.</p><p>‍</p><h3>Automating Medical Reporting and Integrating Patient History</h3><p>One of the most promising applications of VLLMs in medical imaging is the generation of comprehensive medical reports. These models can analyse a set of medical images, such as X-rays, CT scans, or MRIs, and generate detailed reports summarising the observed abnormalities, their locations, and potential implications for diagnosis or treatment. By automating the report generation process, VLLMs can significantly reduce the workload of radiologists and other medical professionals, allowing them to focus on more critical tasks.To read more about the specific work please read the <a href=""https://arxiv.org/html/2403.02469v1"">research paper</a>.</p><p>‍</p><p>VLLMs can also incorporate patient history and medication records to provide a more holistic assessment of the patient's health. By combining visual data from medical images with textual information from patient records, VLLMs can identify potential correlations, flag potential drug interactions, and provide personalised treatment recommendations. This integration of multimodal data can lead to more accurate diagnoses and more effective treatment plans.</p><p>‍</p><h2>How to build a Custom Medical Visual LLM</h2><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/66475ecfedcbfb5d7f77949e_1bD3mDmcTgaU9FbUA-oNEEx5wS9VDO2hJCKycfXkio1H66hrhJF4r0dt1eeD0FgNLZyP32hdPfP3GYOBFAukn65vaHSGcjgFIRPNEbOaM9WDuG-ilIEG7tUZNo5agIt15pw3R8-cpxFsfCuO1Jajv6I.png""/></div></figure><h3>Chat-Interface</h3><p>The integration of a chat interface,allowing medical practitioners to easily interact with the system using both text and images. By leveraging the multimodal capabilities of large language models (LLMs), the approach can effectively process and analyse both textual queries and medical images, such as MRI scans.</p><p>‍</p><p>When a medical practitioner obtains an MRI image, they can simply upload it to the chat interface. The approach then proceeds with the necessary preprocessing steps, including normalisation, augmentation, and noise reduction, to ensure optimal performance of the U-Net architecture. The chat interface provides a user-friendly and accessible way for doctors to interact with the system, enabling them to input specific prompts or questions related to the uploaded MRI image. </p><p>‍</p><p>This interactive approach allows medical professionals to direct the model's focus to particular anatomical regions, structures, or tissues of interest, facilitating targeted and efficient analysis.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:728pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/66475ecf4c2c12d45d83830a_BJpZkfgAE7g1Mp6bnRgIQbb2TRE1P-ZkmnP5uOitpluXyezLnNhXzuWBGr1apI_eIwMQTQUj4qWVsA0hPtKHVEg2MVUiqcKFZ-UUeVhSeAudmW3gaEeBpA3g7hiFdgBKRi-d8rzatWzoJ3Dl6BPfwaA.png""/></div></figure><p>‍</p><p>By combining the chat interface with the powerful capabilities of VLLMs and U-Net, The approach generates detailed medical reports that summarise the observed abnormalities, their locations, and potential implications for diagnosis or treatment. These reports are easily accessible through the chat interface, allowing doctors to quickly review the findings and make informed decisions about patient care.</p><p>‍</p><h3>U-Net Model</h3><p>The U-Net architecture is particularly well-suited for the task of tumour segmentation in medical images. Its unique encoder-decoder structure and skip connections enable precise localization and accurate delineation of tumour regions.</p><p>​​</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:685pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/66475ecfa84d888b44916f60_oD_3Xpke7xltjNOfn9o_L_tR82yVxi5sIkK2lm5fRe4iBRBT3hlkDa4kODlidNy9KRYko1HO-elIf5Aj1WKTrFQqKQffK9_4S-cTD91mWrkGljjVQ_qkybl9GfSK19RuOhVAbA_Fdc6-_Yb5yM8ZvLc.png""/></div></figure><p>‍</p><h4>Encoder: Capturing Tumour Context</h4><p>The encoder component of U-Net acts as a feature extractor, processing the input image and capturing relevant contextual information about the tumour. It utilises a series of convolutional and pooling layers to extract hierarchical features at different scales. These features include tumour texture, shape, location, and surrounding anatomical structures.</p><p>‍</p><p>As the encoder progresses through the layers, it gradually reduces the spatial dimensions of the feature maps while increasing the number of feature channels. This allows the model to capture broader contextual information about the tumour and its relationship with the surrounding tissues.</p><p>‍</p><h4>Decoder: Precise Tumour Localization</h4><p>The decoder part of U-Net is responsible for precisely localising the tumour region within the image. It takes the encoded features from the encoder and progressively samples them to restore the original spatial dimensions. At each decoding stage, the decoder combines the upsampled features with the corresponding high-resolution features from the encoder via skip connections.</p><p>‍</p><p>This combination of upsampled features and skip-connected features enables the decoder to precisely localise the tumour boundaries and reconstruct the segmented tumour region. The decoder's ability to precisely localise the tumour is crucial for accurate tumour volume measurement, delineation for targeted therapy, and classification based on tumour appearance and location.</p><p>‍</p><h4>Skip Connections: Preserving Tumour Details</h4><p>Skip connections play a vital role in preserving fine-grained tumour details throughout the U-Net architecture. These connections directly link the encoder and decoder layers, allowing the decoder to access high-resolution spatial information from the corresponding encoder layers.</p><p>‍</p><p>By retaining these details, skip connections ensure that critical tumour characteristics, such as irregular shapes, heterogeneous textures, and subtle boundaries, are not lost during the encoding and decoding process. This preservation of tumour details is essential for accurate tumour segmentation and subsequent analysis.</p><p>‍</p><p>The combination of the encoder's contextual feature extraction, the decoder's precise localization capabilities, and the skip connections' preservation of tumour details makes the U-Net architecture a powerful tool for tumour segmentation in medical imaging. By leveraging these strengths, researchers have developed highly accurate and robust tumour segmentation models that significantly aid in diagnosis, treatment planning, and monitoring.</p><p>‍</p><h3>Visual Language Model (VLM)</h3><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/66475ecf341fca86aaca9567_wrOXc7_erwNFhgNtxgk0UCOYhy4ZY-Km8ioGuyS5KLg3cqIRtcENfY1pxSnAZhgXzsSu4n9y4H1lZS7oOyEqOEmj_V1fWyBZRc72nnNBKIyNOznAh_6QAZiCvNtdPhorHERpy3SRdS5AWNKW9YuzYq8.png""/></div></figure><p>In this multimodal model architecture, we are focusing on utilising the system for medical purposes, specifically for detecting tumours in MRI images.</p><p>‍</p><p>The architecture begins with the vision encoder, which in this case is the CLIP ViT-L/336px model. This vision encoder processes the input MRI image, extracting high-level visual features. These features are then passed through the vision-language connector, a Multi-Layer Perceptron (MLP), which bridges the visual features with the language model.</p><p>‍</p><p>The language model, represented by Vicuna v1.5 with 13 billion parameters, interprets these visual features in the context of medical knowledge. The tokenizer and embedding components convert the visual features into a numerical format that the language model can process effectively.</p><p>‍</p><p>When an MRI image is input into the system, the vision encoder captures detailed information about the image, identifying potential areas of concern. The vision-language connector then translates these visual details into a form that the language model can understand and analyse.</p><p>‍</p><p>Upon receiving a user query such as, ""What is unusual about this MRI image?"" The system utilises the combined capabilities of the vision encoder and language model to detect abnormalities, such as tumours. The language model analyses the encoded features and generates a detailed response, pinpointing the exact location and nature of the tumour within the MRI image. This integrated approach leverages the strengths of both visual and textual data processing, resulting in precise and efficient tumour detection in medical imaging.</p><p>‍</p><h2>U-Net compared toTraditional Methods in terms of Speed and Accuracy</h2><p>‍</p><p>Traditional medical approaches primarily focus on a person's disability, condition, and limitations, often overlooking their personal and psycho-social needs. These methods rely on symptom analysis and diagnostic tests to establish a diagnosis, aiming to address and treat underlying conditions. In contrast, traditional computer vision methods, such as those employing algorithms like SIFT, SURF, and BRIEF, rely on human-engineered feature extraction, which can be limited by the predefined features chosen for each image which would be really time consuming and time would be a barrier for a quick response to the medical situation.</p><p>‍</p><p>The medical vision approach, particularly with the integration of U-Net, presents a prominent solution for medical image analysis. U-Net, with its encoder-decoder architecture and skip connections, excels in semantic segmentation tasks, allowing the model to capture intricate details within medical images. This U-Net-based methodology has demonstrated groundbreaking advancements in medical image analysis, offering improved performance indicators and structural characteristics.</p><p>‍</p><h2>Advancements and Potential of VLLMs in Medical Imaging</h2><p>Research teams are actively exploring the use of LLMs in medical imaging. For instance, <a href=""https://www.medrxiv.org/content/10.1101/2023.12.21.23300146v3.full"">a study by Huang et al</a>. found that LLMs can outperform specialised, task-specific models in certain contexts. Another <a href=""https://arxiv.org/html/2403.02469v1"">study by Li et al</a>. introduced a cross-modal clinical graph transformer for ophthalmic report generation, showcasing the potential of VLLMs in specialised medical domains.</p><p>‍</p><p>As research progresses, more sophisticated and specialised VLLMs tailored to specific medical imaging tasks will likely emerge. Collaborating with medical professionals ensures these models meet the unique needs and challenges of healthcare.</p><p>‍</p><p>In summary, Visual Large Language Models can significantly enhance medical imaging by streamlining diagnostic processes, generating comprehensive reports, and integrating patient history and medication records. Despite existing challenges, the improved accuracy, efficiency, and personalised care offered by VLLMs make them a promising tool for the future of healthcare.</p><p>‍</p><h2>Ready to Revolutionise Medical Imaging?</h2><p>Experience the future of medical image analysis with our advanced vision solutions. Let us guide you through the process of building a personalised medical visual LLM tailored to your specific use case and dataset. <a href=""https://www.mercity.ai/contacts"">Contact us </a> today to embark on your journey towards precise and efficient medical image analysis. We can help you transform the landscape of healthcare and improve patient outcomes.</p><p>‍</p><p>‍</p></div>"
Building Real-Time Crowd Management Systems,building-real-time-crowd-management-systems,640f56f76d313b2faa631c11,6834c9008e53aa06f65b37c0,False,False,Mon May 26 2025 20:03:12 GMT+0000 (Coordinated Universal Time),Mon May 26 2025 20:25:08 GMT+0000 (Coordinated Universal Time),Mon May 26 2025 20:39:51 GMT+0000 (Coordinated Universal Time),"<p id="""">‍</p><div data-rt-embed-type='true'><video controls style=""width: 100%; height: auto; max-width: 100%;"" preload=""auto"">
    <source src=""https://raw.githubusercontent.com/Mercity-AI/Crowd-Management-Demo/main/results/tracked_crowd4_cmprssd.mp4"" type=""video/mp4"">
    Your browser does not support the video tag.
</video></div><p id="""">‍</p><p id="""">‍</p><p id=""""><strong id=""""><em id="""">&gt; You can check out our codebase on crowd management </em></strong><a href=""https://github.com/Mercity-AI/Crowd-Management-Demo"" id=""""><strong id=""""><em id="""">here</em></strong></a><strong id=""""><em id="""">. Contains the foundation code to get your own crowd management system up and running!</em></strong></p><p id="""">‍</p><p id="""">Crowd management is the process of monitoring and controlling how people move in shared spaces. It aims to prevent overcrowding, ensure safety, and maintain smooth operations during high footfall. As foot traffic grows, manual monitoring becomes harder to scale. Traditional systems rely heavily on human observation. They often miss early signs of congestion or risk and by the time someone reacts, it's too late.</p><p id="""">‍</p><p id="""">To solve this, many public spaces now rely on automated systems. CCTV feeds, sensors, and software track how people move, gather, and disperse in real time. These systems detect crowd density, monitor flow, and predict issues before they escalate.</p><h2 id="""">Need for Crowd Management&nbsp;</h2><p id="""">Managing crowds in busy places like airports, malls, and events is about more than keeping people safe. It also helps improve security and makes operations run smoothly. Plus, the data collected can provide valuable business insights. Let’s explore these major uses of crowd management.</p><h3 id="""">Safety and Security</h3><p id="""">Safety is the top priority in any crowded space. Real-time monitoring helps authorities detect unusual crowd sizes and movement early on. This early warning system allows for quick action, stopping accidents before they happen.</p><p id="""">‍</p><p id="""">Security threats are also handled more effectively. Suspicious behavior like loitering or abandoned bags can trigger immediate alerts. This gives security teams the chance to respond before situations get worse. Airports, stadiums, and busy transport hubs especially benefit from these systems, where safety and security cannot be compromised.</p><p id="""">‍</p><p id="""">The systems can also manage access points, keep an eye on busy zones, and help plan safe exit routes. When they work with emergency teams, it becomes easier to make quick and smart decisions. This is especially important in places like airports, stadiums, and large public events, where every second counts.</p><h3 id="""">Operational Efficiency and Business Insights</h3><p id="""">Another big advantage is operational efficiency. Automated crowd management provides accurate, real time data. This helps organizers make better decisions on where to deploy staff and how to manage crowd flow. For example, event planners can use this data to position security and open emergency routes effectively.</p><p id="""">‍</p><p id="""">The data collected also offers valuable business insights. Retailers can see which areas of a store attract the most visitors, helping them optimize product placement and staffing. Shopping malls and city centers can analyze visitor movement to design better layouts and plan promotions more effectively.</p><p id="""">‍</p><p id="""">At large events like concerts or sports games, organizers use crowd data to increase revenue by directing attendees toward concession stands and merchandise areas. At the same time, they improve overall safety and make the visitor experience better.</p><h2 id="""">How to build a Crowd Management System?</h2><h3 id="""">Preprocessing</h3><p id="""">Crowd management systems begin with real-time video capture using modern IP-based CCTV cameras that comply with ONVIF or RTSP standards. These cameras are capable of streaming high-resolution video (1080p or even 4K) and are positioned to ensure maximum coverage with minimal blind spots.</p><h4 id="""">Synchronization</h4><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6834c602c1b09f0df01c48f8_AD_4nXdmIl1qfttXxT2Wtd0qKDpbZltJdmATlqRsxp0-vmQAV1WJLibHxyr6TTtqZmh6Vtv2yHWIHTiN6kF9K5gkM6XKoMtlsGszCieXqhK1jhkhMzew_-HQfW0sjBSgVntaRkFjgpuWfA.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">All cameras must be synchronized to the same clock. This ensures that frames captured across different views represent the exact same moment in time. If cameras are even slightly out of sync, a person moving between zones might appear at different times in each feed, breaking cross-camera tracking and leading to double-counting or missed detections.</p><p id="""">‍</p><p id="""">Basic systems use NTP (Network Time Protocol), which offers millisecond-level synchronization, sufficient for general surveillance. Advanced setups use PTP (Precision Time Protocol), which provides sub-microsecond accuracy. PTP is hardware-assisted and is essential when the system fuses data across overlapping cameras in real time.</p><h4 id="""">Video Ingestion</h4><p id="""">Video feeds are securely transmitted via RTSP or ONVIF and ingested using tools like GStreamer, FFmpeg, or OpenCV. Once ingested, the video streams are processed either centrally or at the edge.</p><p id="""">‍</p><p id="""">Edge computing devices like the NVIDIA Jetson or Dell Edge Gateway can be deployed near camera sources to reduce latency and bandwidth usage. These devices handle tasks such as frame extraction, resizing, denoising, and optionally, background subtraction or motion detection.</p><p id="""">‍</p><p id="""">Why edge preprocessing matters:</p><ul id=""""><li id="""">Reduces upstream data volume by filtering irrelevant frames</li><li id="""">Preserves privacy by keeping raw video local</li><li id="""">Sends only compressed frames or extracted metadata to central servers or the cloud<br><br></li></ul><p id="""">After the video is ingested, the preprocessing pipeline prepares the frames for machine learning. Frames are decoded and sampled, then resized to fit the model’s input size (like 512×512 for FFNet or 2048×2048 for APGCC). Pixel values are normalized, and color formats are converted (for example, from BGR to RGB).</p><p id="""">Sometimes, techniques like histogram equalization or Gaussian blur are used to adjust lighting and make the model more reliable in different conditions. Region-of-interest (ROI) masking can also be applied to ignore parts of the frame that don’t matter, like static backgrounds or the sky.</p><p id="""">The final result is a batch of clean, ready-to-use tensors, which may include extra info like timestamps, camera IDs, or GPS data. These are then sent to an inference engine like ONNX Runtime, TensorRT, or PyTorch, where models run in real time to produce useful crowd analytics.</p><h3 id="""">Deep Learning Models for Crowd Management</h3><p id="""">Once the video frames are preprocessed, the next step is machine learning. This is where deep learning models analyze the frames to estimate crowd size, density, and individual positions. But not all areas require the same level of detail.</p><p id="""">‍</p><p id="""">For example, a crowded airport gate demands high precision, while an open plaza may only need rough counts and basic alerts. That’s why we use a hybrid approach with two different models:</p><p id="""">‍</p><ul id=""""><li id="""">APGCC runs on the server and is used for detailed analysis in high-traffic or high-security zones.</li><li id="""">FFNet runs on edge devices and is optimized for fast, efficient counting in general monitoring areas.<br><br></li></ul><p id="""">Each model is tuned for its environment, APGCC for accuracy, FFNet for speed and scalability. Together, they provide both depth and flexibility across different parts of a venue.</p><h4 id="""">APGCC on the Server</h4><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6834c60158b22e852044b4c3_AD_4nXf9LUjY94HVILNR24p_-DODxjucFpOEZrqBg_7h2tLSCOLTTK2KWzeLwfktgIGUeaBIJW6955YiibE02R0pStFKsFaPnxt2Y3Yb2ywfnEaAV3xB2jpOQQOIiUV_IKYf7kkVicDY_A.jpeg"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><p id="""">APGCC is a powerful, compute-intensive model designed for detailed crowd analysis, especially in dense, occluded scenes where standard models often fail. It brings two core innovations:</p><p id="""">‍</p><ul id=""""><li id="""">Auxiliary Point Guidance (APG):<br>Traditional point-based models have trouble matching detected points (proposals) to the actual people (targets) when crowds are very dense. APG solves this by introducing auxiliary positive and negative points during training. These points help the model learn where people really are, making it more accurate at finding individuals, even when their heads are partly hidden or blocked. This makes APGCC ideal for high-security zones like stadium entrances, airport queues, or metro gates where precise person-level localization matters.<br><br></li><li id="""">Implicit Feature Interpolation (IFI):<br>IFI allows the model to extract features at arbitrary spatial locations instead of fixed sampling grid points. This makes it robust to varying head sizes and densities. Whether people are packed closely together or spread out, APGCC dynamically adjusts its feature representation, improving both count and localization accuracy.</li></ul><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6834c607b6ed1395b06532e1_AD_4nXey2QtYnpsNDclOHpJXEMhSLJxgbBU6tY82AncwxUqzU6tUn_LYS75x6k9rgCe_VzlOV0kytxfLCJsIlcx1SZ0PKBa9qBOoN2xaO9NFKJ0noQSDwXy0aOmdd1qyUAwxapQP67tseA.gif"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">To handle its heavy computing needs, APGCC runs on powerful GPU servers like NVIDIA A100, T4, or V100. This lets it process video frames from many cameras at the same time. The results show the total number of heads, the exact pixel locations of each detected head, and, if needed, heatmaps that display crowd density.</p><h4 id="""">FFNet on the Edge</h4><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6834c6eae7721eb16882c3be_AD_4nXehnPysq3vSTFbwo36OUH9HL5J9ckK6LNOse5GyDYmXrpMD9dJoeAnyAmhJkvtY5lfBogIMp1kA_PWpFG6d3Ywnsqzu5jIDpO7fqzgRI6Tisy47gn6pOfQQCUdFYmhr5QLRRbEX.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><p id="""">‍</p><p id="""">At the edge, models must deliver fast and efficient results while running on limited hardware. FFNet is designed specifically for edge AI platforms like NVIDIA Jetson Xavier and Orin. It’s a compact, resource-friendly model that performs real-time crowd counting with low memory and power consumption.</p><p id=""""><br>FFNet extracts information from the scene at multiple scales, allowing it to accurately handle crowds of different sizes and distances. Its Focus Transition Modules (FTMs) use dynamic convolution, which means the model adapts how it processes features based on the spatial context. This improves accuracy by ensuring even small or distant individuals are detected, without adding extra computational load&nbsp;</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:655px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""655px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6834c6015e790b680462c86b_AD_4nXcclwZu3p_0Y8Rbob4pBemOo7EPk_5rXTTZBcd1wFiJpiuD7vqGUmSJcydr8nwXVf1MA4hksP1LY0VHEGiNNKHwPCkcjSjvZeby-3nUupVNLzLsM5HlQfOW_r5p2uNs4ZRK-MBR6g.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id=""""><em id="""">Results using FFNet&nbsp;</em></p><p id="""">FFNet processes smaller video frames and creates a density map to estimate crowd size. It does not give exact locations like APGCC but is effective for general crowd monitoring, alerts, and occupancy tracking.</p><p id="""">‍</p><p id="""">Running FFNet on edge devices provides fast response times of 30 to 80 milliseconds per frame. It reduces network load by sending only summary data such as counts and alerts. This helps protect privacy because the raw video never leaves the device. The system is also easy to scale. You can support more cameras by adding more edge units.</p><p id="""">‍</p><p id="""">Using two models, one on the server and another at the edge, is a setup that works well in practice. But it’s not the only way. The choice depends on the specific needs of the deployment.</p><p id="""">For high-accuracy use cases like managing crowds at security checkpoints, server-side models like APGCC are the best fit. They offer strong precision and detailed output.</p><p id="""">‍</p><p id="""">For fast, scalable monitoring across many locations, edge models like FFNet are more suitable. They work well on lightweight hardware and respond quickly.</p><p id="""">‍</p><p id="""">Other models can also be used depending on what matters most, whether that’s speed, cost, hardware limits, or how easy the results are to interpret.</p><h3 id="""">Multi-Camera Post-Processing</h3><p id="""">After individual deep learning models analyze each camera’s feed the next challenge is to merge these fragmented views into a single global picture. This post-ML stage is critical for accurate tracking, counting, and alerting across large, overlapping camera networks.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:555px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""555px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6834c60190a3b59937a2af33_AD_4nXcpMXeeoCxOa6zfBINCT4umvqK-ith8IKsKeJLhAmOfPhcNy0bLBvkP80FK4ft2dk7jS8YGHlRxI0ZR9w4QUysCXjNE_luP2Rvw7Ia9B8jSLpYAzGS3Kdx6_1hFGLWr8stmfp7D.jpeg"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">Shown above is <strong id="""">homography warping, </strong>the first step, which aligns each camera’s view to a shared top-down perspective. Since every camera sees the scene from a different angle, their outputs, like head positions or density maps, aren’t naturally aligned. Warping uses precomputed transformation matrices to map all these outputs onto a common ground plane. This step is crucial for making sure that the same person appearing in multiple overlapping cameras isn’t mistakenly treated as multiple individuals.</p><p id="""">Once everything is spatially aligned, the system performs <strong id="""">multi-camera fusion</strong> to clean up duplicate detections. In overlapping zones, a person may still be detected more than once. To resolve this, the system applies Non-Maximum Suppression (NMS), to&nbsp; compare overlapping detections and keeps only the one with the highest confidence.&nbsp;</p><p id="""">To further improve accuracy, <strong id="""">visibility masks</strong> give more weight to regions seen by multiple cameras. If several cameras agree on a detection, it’s treated as more trustworthy. Together, warping and fusion ensure that the system delivers a unified, accurate view of the crowd without overcounting or missing people.</p><p id="""">With a fused view in place, <strong id="""">global tracking</strong> connects detections across time and space. Trackers like DeepSORT and ByteTrack associate individuals frame-to-frame, even as they move between cameras. Together, these post-processing steps turn raw detections into a consistent, live map of crowd movement across zones.&nbsp;</p><h2 id="""">Handling Data at Scale</h2><p id="""">Building a crowd management system is easy. Scaling it across a city with thousands of video feeds is the challenge. A small test with five cameras usually works fine, but handling real-time data at city scale either breaks the system or proves its design. The key difference is how the data is managed.</p><h3 id="""">Handling Data on Edge</h3><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6834c60210c435d075879692_AD_4nXdU3N1b5kiU5IlqbHKIQxeT_3igTMPs2qCGEzhi7v7i-XGWFMvNi8fY5xfWdHGLOuzV8VjvtXOSpJTzPUOmdjmMdX0KCCuG1CJqGdLRBUdVDPGOiYUIBT1ZwHBhfLs7Rbp0wiptpA.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">In large-scale systems, the key challenge is managing data efficiently. Instead of moving heavy raw video, these systems move insights. Raw video demands too much bandwidth, slows transmission, drives up storage costs, and makes searching difficult. To solve this, video is processed right at the source, either in the cameras or nearby edge devices. Simple models like FFNet extract only essential information like crowd counts, density maps, and coordinates.&nbsp;</p><p id="""">‍</p><p id="""">This lightweight data powers real-time alerts, dashboards, and analytics without the overhead of raw footage. This data flows through event buses like Kafka or Pulsar and immediately splits into two directions.</p><ul id=""""><li id="""">First, a<strong id=""""> time-series database</strong> collects and organizes live metrics such as crowd counts by area, how quickly crowds form, and congestion levels.</li><li id="""">Second, a <strong id="""">high-speed NoSQL </strong>store indexes location-based data and alert events, allowing for fast geospatial queries and incident analysis.</li></ul><p id="""">Every piece of data is tagged with a timestamp, location, and follows a set format from the beginning. This means no cleanup is needed later. The data is instantly ready for real-time analysis and display. Keeping data clean and organized is the key to real-time awareness at scale.</p><h3 id="""">Handling Raw Video</h3><p id="""">Raw video isn’t completely discarded. Cameras break their streams into five-minute chunks, compress them, and store them in systems like MinIO or S3. Only important segments tied to critical events are kept long-term. Other footage is kept for seven to thirty days, then deleted or moved to cold storage. This keeps costs predictable and follows rules like GDPR.</p><p id="""">The much smaller metadata is kept longer. It supports analytics, reporting, and trend tracking, and helps connect live and archived footage without searching through huge amounts of video.</p><p id="""">In critical areas such as entry gates and high-traffic zones,&nbsp; raw video streams are sent to cloud-based systems like APGCC for advanced analysis. This tiered approach balances processing load and ensures that important events receive the detailed attention they require.</p><h3 id="""">Scalability and Automation</h3><p id="""">Scalability isn’t just about adding more servers. It means building the system so every part can grow smoothly as needed. For example, Kafka handles more video feeds by adding brokers, which share the workload. Databases get bigger by splitting data into pieces called shards, so they can work faster without slowing down. Storage systems add more nodes to hold more data safely and keep it easy to access.</p><p id="""">At the edge, devices save data locally when the network goes down. Once the connection is back, they send the data automatically. This way, the system keeps running without problems, even if parts of the network have issues or get busy.</p><p id="""">Automation is essential to handle all this data smoothly. Tools like Airflow manage tasks such as cleaning up old data, summarizing information, archiving, and deleting files without anyone having to do it manually. Data is grouped and updated regularly every hour, day, and week so reports and trends are always current and easy to access. Alerts are organized by time and location, making it quick to find important events.&nbsp;</p><h2 id="""">Core functionalities &amp; Applications</h2><p id="""">Crowd management systems are already in use at airports, train stations, stadiums, retail spaces, and city centers. They are changing how these large environments are monitored and controlled.</p><p id="""">‍</p><p id="""">In <strong id="""">transport hubs</strong>, real-time people count how many passengers are in each area. Entry and exit flows are constantly monitored, giving operators a live view of occupancy across terminals and platforms. This visibility is critical to detect congestion early. Airports use this data to adjust staffing at security checkpoints. Train stations reroute passengers before crowding becomes a problem. Bus terminals change schedules based on actual foot traffic.</p><p id="""">‍</p><p id="""">Density estimation adds a deeper level of insight. It shows how tightly packed people are in a space. When a zone becomes too crowded, the system sends alerts. Operators can then open extra gates or redirect flows to prevent bottlenecks without building new infrastructure.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:550px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""550px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6834c6015b1f089b271b4a44_AD_4nXf9OFYzloBPBqCBz3mP9j4gNfUK4OvONDq6H3_spfYoXDd4E45QDXEIf0xEbQ92IBexVWwLYNZl8OJwhlCk1dCugdINCJv0Zdlz9-O8SeaMT3wROOSoVNxn-cpBvkfU5hsMwKNmzw.jpeg"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id=""""><em id="""">The picture showcases density estimation using heatmaps</em></p><p id=""""><strong id="""">Large venues</strong> like stadiums and festivals use these systems to create a live, top-down view by combining multiple video feeds. Heatmaps show where crowds are gathering, such as at gates or exits, allowing organizers to deploy staff or adjust signage quickly. The system also learns from past events, predicting crowd surges before they happen and helping prevent issues during busy times like after a game or during a main performance.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:850px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""850px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6834c601fa80c65dc8779baf_AD_4nXe8zMTIOrvV8CGtkDD7UPruvLpzoJpWvBTbs2yrfy183as6LFw1qACupM4phpx4ug-WAatvV-yVjNzdLBjDbM4ww6FqLInAyqSOa8zBuXBXSrSlVEjvWOvRVjhqgUMWxo_LtQmp.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><p id=""""><strong id="""">Retailers</strong> use similar technology to understand how customers move inside stores. Entry and exit data helps match staffing to demand. Heatmaps and movement tracking reveal which sections attract attention, where customers linger, and which areas are overlooked. Unlike simple counters, these systems track flow and behavior, showing how changes to store layouts impact shopping patterns. This turns physical stores into spaces that can be measured and optimized as easily as websites.</p><p id="""">‍</p><p id="""">In <strong id="""">cities</strong>, this technology provides valuable insights for urban planning. Planners can see how pedestrians actually use sidewalks, crossings, and public plazas instead of relying on assumptions. During large events like festivals or protests, AI combines live feeds from cameras, drones, and sensors into one dashboard. This helps officials coordinate teams and respond faster to unexpected crowd movements.</p><p id="""">‍</p><p id="""">Over time, the system improves by learning from patterns and refining its predictions. What begins as basic visibility evolves into foresight, giving operators the ability to manage real-world movement with precision.</p><h2 id="""">Want to build a Crowd Management System?</h2><p id="""">If you're planning to deploy or scale a real-time crowd monitoring system, we can help you build one that works at city scale, under pressure, and without breaking the bank. From smart data pipelines to edge processing and compliance-ready storage, we design systems built to last.</p><p id="""">Contact us today and let's build a system that keeps people safe, informed, and moving.</p><p id="""">‍</p>",https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6834c86c3ce2d232463627fc_crowd%20management.png,Yash Sonawane,Computer Vision,In this blog we talk about crowd management and how to build cloud and on-edge systems to do crowd management in realtime.,False,"<div class=""rich-text w-richtext""><p>‍</p><div class=""w-embed""><video controls="""" preload=""auto"" style=""width: 100%; height: auto; max-width: 100%;"">
<source src=""https://raw.githubusercontent.com/Mercity-AI/Crowd-Management-Demo/main/results/tracked_crowd4_cmprssd.mp4"" type=""video/mp4""/>
    Your browser does not support the video tag.
</video></div><p>‍</p><p>‍</p><p><strong><em>&gt; You can check out our codebase on crowd management </em></strong><a href=""https://github.com/Mercity-AI/Crowd-Management-Demo""><strong><em>here</em></strong></a><strong><em>. Contains the foundation code to get your own crowd management system up and running!</em></strong></p><p>‍</p><p>Crowd management is the process of monitoring and controlling how people move in shared spaces. It aims to prevent overcrowding, ensure safety, and maintain smooth operations during high footfall. As foot traffic grows, manual monitoring becomes harder to scale. Traditional systems rely heavily on human observation. They often miss early signs of congestion or risk and by the time someone reacts, it's too late.</p><p>‍</p><p>To solve this, many public spaces now rely on automated systems. CCTV feeds, sensors, and software track how people move, gather, and disperse in real time. These systems detect crowd density, monitor flow, and predict issues before they escalate.</p><h2>Need for Crowd Management </h2><p>Managing crowds in busy places like airports, malls, and events is about more than keeping people safe. It also helps improve security and makes operations run smoothly. Plus, the data collected can provide valuable business insights. Let’s explore these major uses of crowd management.</p><h3>Safety and Security</h3><p>Safety is the top priority in any crowded space. Real-time monitoring helps authorities detect unusual crowd sizes and movement early on. This early warning system allows for quick action, stopping accidents before they happen.</p><p>‍</p><p>Security threats are also handled more effectively. Suspicious behavior like loitering or abandoned bags can trigger immediate alerts. This gives security teams the chance to respond before situations get worse. Airports, stadiums, and busy transport hubs especially benefit from these systems, where safety and security cannot be compromised.</p><p>‍</p><p>The systems can also manage access points, keep an eye on busy zones, and help plan safe exit routes. When they work with emergency teams, it becomes easier to make quick and smart decisions. This is especially important in places like airports, stadiums, and large public events, where every second counts.</p><h3>Operational Efficiency and Business Insights</h3><p>Another big advantage is operational efficiency. Automated crowd management provides accurate, real time data. This helps organizers make better decisions on where to deploy staff and how to manage crowd flow. For example, event planners can use this data to position security and open emergency routes effectively.</p><p>‍</p><p>The data collected also offers valuable business insights. Retailers can see which areas of a store attract the most visitors, helping them optimize product placement and staffing. Shopping malls and city centers can analyze visitor movement to design better layouts and plan promotions more effectively.</p><p>‍</p><p>At large events like concerts or sports games, organizers use crowd data to increase revenue by directing attendees toward concession stands and merchandise areas. At the same time, they improve overall safety and make the visitor experience better.</p><h2>How to build a Crowd Management System?</h2><h3>Preprocessing</h3><p>Crowd management systems begin with real-time video capture using modern IP-based CCTV cameras that comply with ONVIF or RTSP standards. These cameras are capable of streaming high-resolution video (1080p or even 4K) and are positioned to ensure maximum coverage with minimal blind spots.</p><h4>Synchronization</h4><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6834c602c1b09f0df01c48f8_AD_4nXdmIl1qfttXxT2Wtd0qKDpbZltJdmATlqRsxp0-vmQAV1WJLibHxyr6TTtqZmh6Vtv2yHWIHTiN6kF9K5gkM6XKoMtlsGszCieXqhK1jhkhMzew_-HQfW0sjBSgVntaRkFjgpuWfA.png""/></div></figure><p>All cameras must be synchronized to the same clock. This ensures that frames captured across different views represent the exact same moment in time. If cameras are even slightly out of sync, a person moving between zones might appear at different times in each feed, breaking cross-camera tracking and leading to double-counting or missed detections.</p><p>‍</p><p>Basic systems use NTP (Network Time Protocol), which offers millisecond-level synchronization, sufficient for general surveillance. Advanced setups use PTP (Precision Time Protocol), which provides sub-microsecond accuracy. PTP is hardware-assisted and is essential when the system fuses data across overlapping cameras in real time.</p><h4>Video Ingestion</h4><p>Video feeds are securely transmitted via RTSP or ONVIF and ingested using tools like GStreamer, FFmpeg, or OpenCV. Once ingested, the video streams are processed either centrally or at the edge.</p><p>‍</p><p>Edge computing devices like the NVIDIA Jetson or Dell Edge Gateway can be deployed near camera sources to reduce latency and bandwidth usage. These devices handle tasks such as frame extraction, resizing, denoising, and optionally, background subtraction or motion detection.</p><p>‍</p><p>Why edge preprocessing matters:</p><ul role=""list""><li>Reduces upstream data volume by filtering irrelevant frames</li><li>Preserves privacy by keeping raw video local</li><li>Sends only compressed frames or extracted metadata to central servers or the cloud<br/><br/></li></ul><p>After the video is ingested, the preprocessing pipeline prepares the frames for machine learning. Frames are decoded and sampled, then resized to fit the model’s input size (like 512×512 for FFNet or 2048×2048 for APGCC). Pixel values are normalized, and color formats are converted (for example, from BGR to RGB).</p><p>Sometimes, techniques like histogram equalization or Gaussian blur are used to adjust lighting and make the model more reliable in different conditions. Region-of-interest (ROI) masking can also be applied to ignore parts of the frame that don’t matter, like static backgrounds or the sky.</p><p>The final result is a batch of clean, ready-to-use tensors, which may include extra info like timestamps, camera IDs, or GPS data. These are then sent to an inference engine like ONNX Runtime, TensorRT, or PyTorch, where models run in real time to produce useful crowd analytics.</p><h3>Deep Learning Models for Crowd Management</h3><p>Once the video frames are preprocessed, the next step is machine learning. This is where deep learning models analyze the frames to estimate crowd size, density, and individual positions. But not all areas require the same level of detail.</p><p>‍</p><p>For example, a crowded airport gate demands high precision, while an open plaza may only need rough counts and basic alerts. That’s why we use a hybrid approach with two different models:</p><p>‍</p><ul role=""list""><li>APGCC runs on the server and is used for detailed analysis in high-traffic or high-security zones.</li><li>FFNet runs on edge devices and is optimized for fast, efficient counting in general monitoring areas.<br/><br/></li></ul><p>Each model is tuned for its environment, APGCC for accuracy, FFNet for speed and scalability. Together, they provide both depth and flexibility across different parts of a venue.</p><h4>APGCC on the Server</h4><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6834c60158b22e852044b4c3_AD_4nXf9LUjY94HVILNR24p_-DODxjucFpOEZrqBg_7h2tLSCOLTTK2KWzeLwfktgIGUeaBIJW6955YiibE02R0pStFKsFaPnxt2Y3Yb2ywfnEaAV3xB2jpOQQOIiUV_IKYf7kkVicDY_A.jpeg""/></div></figure><p>‍</p><p>APGCC is a powerful, compute-intensive model designed for detailed crowd analysis, especially in dense, occluded scenes where standard models often fail. It brings two core innovations:</p><p>‍</p><ul role=""list""><li>Auxiliary Point Guidance (APG):<br/>Traditional point-based models have trouble matching detected points (proposals) to the actual people (targets) when crowds are very dense. APG solves this by introducing auxiliary positive and negative points during training. These points help the model learn where people really are, making it more accurate at finding individuals, even when their heads are partly hidden or blocked. This makes APGCC ideal for high-security zones like stadium entrances, airport queues, or metro gates where precise person-level localization matters.<br/><br/></li><li>Implicit Feature Interpolation (IFI):<br/>IFI allows the model to extract features at arbitrary spatial locations instead of fixed sampling grid points. This makes it robust to varying head sizes and densities. Whether people are packed closely together or spread out, APGCC dynamically adjusts its feature representation, improving both count and localization accuracy.</li></ul><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6834c607b6ed1395b06532e1_AD_4nXey2QtYnpsNDclOHpJXEMhSLJxgbBU6tY82AncwxUqzU6tUn_LYS75x6k9rgCe_VzlOV0kytxfLCJsIlcx1SZ0PKBa9qBOoN2xaO9NFKJ0noQSDwXy0aOmdd1qyUAwxapQP67tseA.gif""/></div></figure><p>To handle its heavy computing needs, APGCC runs on powerful GPU servers like NVIDIA A100, T4, or V100. This lets it process video frames from many cameras at the same time. The results show the total number of heads, the exact pixel locations of each detected head, and, if needed, heatmaps that display crowd density.</p><h4>FFNet on the Edge</h4><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6834c6eae7721eb16882c3be_AD_4nXehnPysq3vSTFbwo36OUH9HL5J9ckK6LNOse5GyDYmXrpMD9dJoeAnyAmhJkvtY5lfBogIMp1kA_PWpFG6d3Ywnsqzu5jIDpO7fqzgRI6Tisy47gn6pOfQQCUdFYmhr5QLRRbEX.png""/></div></figure><p>‍</p><p>‍</p><p>At the edge, models must deliver fast and efficient results while running on limited hardware. FFNet is designed specifically for edge AI platforms like NVIDIA Jetson Xavier and Orin. It’s a compact, resource-friendly model that performs real-time crowd counting with low memory and power consumption.</p><p><br/>FFNet extracts information from the scene at multiple scales, allowing it to accurately handle crowds of different sizes and distances. Its Focus Transition Modules (FTMs) use dynamic convolution, which means the model adapts how it processes features based on the spatial context. This improves accuracy by ensuring even small or distant individuals are detected, without adding extra computational load </p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:655pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6834c6015e790b680462c86b_AD_4nXcclwZu3p_0Y8Rbob4pBemOo7EPk_5rXTTZBcd1wFiJpiuD7vqGUmSJcydr8nwXVf1MA4hksP1LY0VHEGiNNKHwPCkcjSjvZeby-3nUupVNLzLsM5HlQfOW_r5p2uNs4ZRK-MBR6g.png""/></div></figure><p><em>Results using FFNet </em></p><p>FFNet processes smaller video frames and creates a density map to estimate crowd size. It does not give exact locations like APGCC but is effective for general crowd monitoring, alerts, and occupancy tracking.</p><p>‍</p><p>Running FFNet on edge devices provides fast response times of 30 to 80 milliseconds per frame. It reduces network load by sending only summary data such as counts and alerts. This helps protect privacy because the raw video never leaves the device. The system is also easy to scale. You can support more cameras by adding more edge units.</p><p>‍</p><p>Using two models, one on the server and another at the edge, is a setup that works well in practice. But it’s not the only way. The choice depends on the specific needs of the deployment.</p><p>For high-accuracy use cases like managing crowds at security checkpoints, server-side models like APGCC are the best fit. They offer strong precision and detailed output.</p><p>‍</p><p>For fast, scalable monitoring across many locations, edge models like FFNet are more suitable. They work well on lightweight hardware and respond quickly.</p><p>‍</p><p>Other models can also be used depending on what matters most, whether that’s speed, cost, hardware limits, or how easy the results are to interpret.</p><h3>Multi-Camera Post-Processing</h3><p>After individual deep learning models analyze each camera’s feed the next challenge is to merge these fragmented views into a single global picture. This post-ML stage is critical for accurate tracking, counting, and alerting across large, overlapping camera networks.</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:555pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6834c60190a3b59937a2af33_AD_4nXcpMXeeoCxOa6zfBINCT4umvqK-ith8IKsKeJLhAmOfPhcNy0bLBvkP80FK4ft2dk7jS8YGHlRxI0ZR9w4QUysCXjNE_luP2Rvw7Ia9B8jSLpYAzGS3Kdx6_1hFGLWr8stmfp7D.jpeg""/></div></figure><p>Shown above is <strong>homography warping, </strong>the first step, which aligns each camera’s view to a shared top-down perspective. Since every camera sees the scene from a different angle, their outputs, like head positions or density maps, aren’t naturally aligned. Warping uses precomputed transformation matrices to map all these outputs onto a common ground plane. This step is crucial for making sure that the same person appearing in multiple overlapping cameras isn’t mistakenly treated as multiple individuals.</p><p>Once everything is spatially aligned, the system performs <strong>multi-camera fusion</strong> to clean up duplicate detections. In overlapping zones, a person may still be detected more than once. To resolve this, the system applies Non-Maximum Suppression (NMS), to  compare overlapping detections and keeps only the one with the highest confidence. </p><p>To further improve accuracy, <strong>visibility masks</strong> give more weight to regions seen by multiple cameras. If several cameras agree on a detection, it’s treated as more trustworthy. Together, warping and fusion ensure that the system delivers a unified, accurate view of the crowd without overcounting or missing people.</p><p>With a fused view in place, <strong>global tracking</strong> connects detections across time and space. Trackers like DeepSORT and ByteTrack associate individuals frame-to-frame, even as they move between cameras. Together, these post-processing steps turn raw detections into a consistent, live map of crowd movement across zones. </p><h2>Handling Data at Scale</h2><p>Building a crowd management system is easy. Scaling it across a city with thousands of video feeds is the challenge. A small test with five cameras usually works fine, but handling real-time data at city scale either breaks the system or proves its design. The key difference is how the data is managed.</p><h3>Handling Data on Edge</h3><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6834c60210c435d075879692_AD_4nXdU3N1b5kiU5IlqbHKIQxeT_3igTMPs2qCGEzhi7v7i-XGWFMvNi8fY5xfWdHGLOuzV8VjvtXOSpJTzPUOmdjmMdX0KCCuG1CJqGdLRBUdVDPGOiYUIBT1ZwHBhfLs7Rbp0wiptpA.png""/></div></figure><p>In large-scale systems, the key challenge is managing data efficiently. Instead of moving heavy raw video, these systems move insights. Raw video demands too much bandwidth, slows transmission, drives up storage costs, and makes searching difficult. To solve this, video is processed right at the source, either in the cameras or nearby edge devices. Simple models like FFNet extract only essential information like crowd counts, density maps, and coordinates. </p><p>‍</p><p>This lightweight data powers real-time alerts, dashboards, and analytics without the overhead of raw footage. This data flows through event buses like Kafka or Pulsar and immediately splits into two directions.</p><ul role=""list""><li>First, a<strong> time-series database</strong> collects and organizes live metrics such as crowd counts by area, how quickly crowds form, and congestion levels.</li><li>Second, a <strong>high-speed NoSQL </strong>store indexes location-based data and alert events, allowing for fast geospatial queries and incident analysis.</li></ul><p>Every piece of data is tagged with a timestamp, location, and follows a set format from the beginning. This means no cleanup is needed later. The data is instantly ready for real-time analysis and display. Keeping data clean and organized is the key to real-time awareness at scale.</p><h3>Handling Raw Video</h3><p>Raw video isn’t completely discarded. Cameras break their streams into five-minute chunks, compress them, and store them in systems like MinIO or S3. Only important segments tied to critical events are kept long-term. Other footage is kept for seven to thirty days, then deleted or moved to cold storage. This keeps costs predictable and follows rules like GDPR.</p><p>The much smaller metadata is kept longer. It supports analytics, reporting, and trend tracking, and helps connect live and archived footage without searching through huge amounts of video.</p><p>In critical areas such as entry gates and high-traffic zones,  raw video streams are sent to cloud-based systems like APGCC for advanced analysis. This tiered approach balances processing load and ensures that important events receive the detailed attention they require.</p><h3>Scalability and Automation</h3><p>Scalability isn’t just about adding more servers. It means building the system so every part can grow smoothly as needed. For example, Kafka handles more video feeds by adding brokers, which share the workload. Databases get bigger by splitting data into pieces called shards, so they can work faster without slowing down. Storage systems add more nodes to hold more data safely and keep it easy to access.</p><p>At the edge, devices save data locally when the network goes down. Once the connection is back, they send the data automatically. This way, the system keeps running without problems, even if parts of the network have issues or get busy.</p><p>Automation is essential to handle all this data smoothly. Tools like Airflow manage tasks such as cleaning up old data, summarizing information, archiving, and deleting files without anyone having to do it manually. Data is grouped and updated regularly every hour, day, and week so reports and trends are always current and easy to access. Alerts are organized by time and location, making it quick to find important events. </p><h2>Core functionalities &amp; Applications</h2><p>Crowd management systems are already in use at airports, train stations, stadiums, retail spaces, and city centers. They are changing how these large environments are monitored and controlled.</p><p>‍</p><p>In <strong>transport hubs</strong>, real-time people count how many passengers are in each area. Entry and exit flows are constantly monitored, giving operators a live view of occupancy across terminals and platforms. This visibility is critical to detect congestion early. Airports use this data to adjust staffing at security checkpoints. Train stations reroute passengers before crowding becomes a problem. Bus terminals change schedules based on actual foot traffic.</p><p>‍</p><p>Density estimation adds a deeper level of insight. It shows how tightly packed people are in a space. When a zone becomes too crowded, the system sends alerts. Operators can then open extra gates or redirect flows to prevent bottlenecks without building new infrastructure.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:550pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6834c6015b1f089b271b4a44_AD_4nXf9OFYzloBPBqCBz3mP9j4gNfUK4OvONDq6H3_spfYoXDd4E45QDXEIf0xEbQ92IBexVWwLYNZl8OJwhlCk1dCugdINCJv0Zdlz9-O8SeaMT3wROOSoVNxn-cpBvkfU5hsMwKNmzw.jpeg""/></div></figure><p><em>The picture showcases density estimation using heatmaps</em></p><p><strong>Large venues</strong> like stadiums and festivals use these systems to create a live, top-down view by combining multiple video feeds. Heatmaps show where crowds are gathering, such as at gates or exits, allowing organizers to deploy staff or adjust signage quickly. The system also learns from past events, predicting crowd surges before they happen and helping prevent issues during busy times like after a game or during a main performance.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:850pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6834c601fa80c65dc8779baf_AD_4nXe8zMTIOrvV8CGtkDD7UPruvLpzoJpWvBTbs2yrfy183as6LFw1qACupM4phpx4ug-WAatvV-yVjNzdLBjDbM4ww6FqLInAyqSOa8zBuXBXSrSlVEjvWOvRVjhqgUMWxo_LtQmp.png""/></div></figure><p>‍</p><p><strong>Retailers</strong> use similar technology to understand how customers move inside stores. Entry and exit data helps match staffing to demand. Heatmaps and movement tracking reveal which sections attract attention, where customers linger, and which areas are overlooked. Unlike simple counters, these systems track flow and behavior, showing how changes to store layouts impact shopping patterns. This turns physical stores into spaces that can be measured and optimized as easily as websites.</p><p>‍</p><p>In <strong>cities</strong>, this technology provides valuable insights for urban planning. Planners can see how pedestrians actually use sidewalks, crossings, and public plazas instead of relying on assumptions. During large events like festivals or protests, AI combines live feeds from cameras, drones, and sensors into one dashboard. This helps officials coordinate teams and respond faster to unexpected crowd movements.</p><p>‍</p><p>Over time, the system improves by learning from patterns and refining its predictions. What begins as basic visibility evolves into foresight, giving operators the ability to manage real-world movement with precision.</p><h2>Want to build a Crowd Management System?</h2><p>If you're planning to deploy or scale a real-time crowd monitoring system, we can help you build one that works at city scale, under pressure, and without breaking the bank. From smart data pipelines to edge processing and compliance-ready storage, we design systems built to last.</p><p>Contact us today and let's build a system that keeps people safe, informed, and moving.</p><p>‍</p></div>"
How to Classify Long Documents and Texts with BERT Models,classify-long-texts-with-bert,640f56f76d313b2faa631c11,6616c813206d93144d71581f,False,False,Wed Apr 10 2024 17:10:43 GMT+0000 (Coordinated Universal Time),Wed Apr 10 2024 17:42:35 GMT+0000 (Coordinated Universal Time),Wed Apr 10 2024 17:42:35 GMT+0000 (Coordinated Universal Time),"<p id="""">We often work with long texts in different areas. They have a lot of information and complexity that we need to know and classify. For example, we may want to sort news articles, research papers, or legal documents by their topics, sentiments, or goals. How can we do this well and fast? One way is to use BERT, a language model. It can understand the meaning and context of words in different situations. It can also get smarter from lots of complex information and spot the differences and updates in long texts. This model is very strong and useful for document classification. But using it for long documents is not simple. The 512 token length restriction on the pretrained model makes it difficult to use for longer documents. These problems can make it difficult to use this approach well and fast. Well, how can we solve these problems and use this technique to classify long documents?&nbsp;</p><p id="""">‍</p><p id="""">In this blog, let us solve this problem. We will show you how to use BERT for long document classification in a simple and effective way. By the end of this guide, you will have the skills and knowledge to use the model to classify long documents with high quality and speed.&nbsp;</p><h2 id="""">What is BERT?&nbsp;</h2><p id="""">BERT stands for Bidirectional Encoder Representations from Transformers. It is a machine learning model that understands and works with human language. It helps sort text, analyze emotions, answer questions, and more. It was developed by Google AI Language in 2018 and has achieved state-of-the-art results on many natural language processing (NLP) benchmarks.&nbsp;&nbsp;</p><p>‍</p><p id="""">This model is based on the Transformer architecture, which is a neural network that uses attention mechanisms to encode and decode sequences of words. Unlike traditional models that process text from left to right or right to left, this model can consider both the left and right context of each word in a sentence. This allows it to better understand the meaning and nuance of natural language.</p><p id="""">&nbsp;&nbsp;</p><p id="""">The Transformer architecture was introduced in 2017. It aimed to improve the older methods of understanding and processing language, done through models like Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs). RNNs and CNNs are sequential models that have difficulty capturing long-term dependencies and parallelizing computation. Transformers, on the other hand, are parallelizable and can capture long-term dependencies by using self-attention. Self-attention is a technique that computes the relevance of each word to every other word in a sequence.</p><p id="""">&nbsp;&nbsp;</p><p id="""">This approach has two components: an encoder and a decoder. The encoder is a stack of Transformer layers that takes a sequence of words as input and produces a high-dimensional vector representation for each word. The decoder is a separate neural network which takes the encoder’s output and performs a specific task, which could be classification, generation, or prediction.</p><p id="""">&nbsp;&nbsp;</p><p id="""">This technique can be used for text classification by adding a classification layer on top of the encoder’s output. The classification layer takes the output vector of the first token in the sequence, which is a special token called [CLS] that stands for the whole sentence. The classification layer then outputs a probability distribution over the possible classes. For example, if the task is to classify movie reviews as positive or negative, the classification layer would output two probabilities, one for each class.&nbsp;</p><p id="""">&nbsp;</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6616c3fb79b4fa2d6800fbc4_qqx-DRa8yferGV07SeAPB0RG6gMTk3LxivcPyIDY-zh0MCsqsFwYcVnFUtSuEj-penCWD3On1a7EzdN8zJ8pYmyZmjxI0CWfG9qcGLbdszz34y-Y-7vvUxsGP9gqcmq6qLFEtIaB9msz_pShnk9E_QM.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><h2 id="""">Why Classify Long Documents Using BERT?&nbsp;</h2><p id="""">In the world of NLP, this model's arrival has changed the game in how computers understand our language. This becomes crucial when dealing with big documents that come with their own unique challenges and opportunities. The traditional methods often don’t quite cut it when it comes to fully understanding these large texts because of their own limitations. But this approach, with its deep understanding of the language, steps up as a handy solution to these issues. It brings some serious advantages to the table when it comes to analyzing lengthy documents, outperforming the traditional methods.</p><h3 id=""""><strong id="""">Limitations of Traditional Methods in Handling Long Texts</strong>&nbsp;</h3><p id="""">Traditional NLP methods, such as BoW (bag-of-words) and TF-IDF (Term Frequency-Inverse Document Frequency), have provided foundational approaches to text classification. However, they meet several limitations when applied to long documents.&nbsp;</p><p>‍</p><p id="""">Context Ignorance (Not Getting the Full Picture): These methods don’t really get the context and meaning of words in relation to the text around them, which means they only get a surface-level understanding of the content.</p><p id="""">&nbsp;&nbsp;</p><p id="""">Fixed-Length Input Constraints (One Size Doesn’t Fit All): A lot of the older models are designed to handle inputs of a certain length, so it’s a bit of a struggle to deal with whole documents without having to cut them down or oversimplify them.</p><p id="""">&nbsp;&nbsp;</p><p id="""">Semantic Loss (Lost in Translation): When you try to shrink or summarize long documents to fit these constraints, you can end up losing important meaning, which can affect the accuracy of classification.&nbsp;</p><h3 id="""">Advantages of Using BERT for Long Document Analysis&nbsp;</h3><p id="""">This technique brings a fresh perspective to text classification, stepping up where traditional methods fall short. Here’s why:&nbsp;</p><p>‍</p><p id=""""><strong id="""">Deep Contextual Understanding (Getting the Full Picture)</strong>: Unlike older models, this model gets the full context of words by looking at the entire text they’re part of. This bidirectional understanding significantly enhances the model's ability to grasp nuanced meanings and variations in language.&nbsp;&nbsp;</p><p>‍</p><p id=""""><strong>Handling Varied Length Inputs (Flexible with Sizes):</strong> This model’s design is naturally more flexible with different input sizes. Even though there’s a limit to the token length it can handle at once (usually 512 tokens), there are different ways to effectively deal with longer documents.</p><p>‍</p><p id=""""><strong id="""">Sophisticated Language Models (Advanced Language Models):</strong> This model is built on the Transformers architecture, which allows for a more detailed analysis of how text elements relate and depend on each other due to the self-attention learning. This leads to much better classification performance, especially when dealing with complex and long documents.&nbsp;&nbsp;</p><p>‍</p><p id="""">The application of this technique for long document classification is not without its challenges, primarily due to the token length limitation. Yet, through approaches like document segmentation and the use of advanced variants (e.g., Longformer, Reformer), it is possible to effectively analyze and classify extensive texts. In a nutshell, this technique is a standout choice for analyzing long documents, offering a mix of deep understanding of language and flexibility with text lengths.&nbsp;</p><p id="""">‍</p><h2 id="""">Strategies for Classifying Long Documents</h2><p id="""">The classification of long documents using models like BERT presents unique challenges due to the inherent limitations of these models in processing long sequences of text. To deal with this, people have come up with different ways to make it easier to manage and classify long documents. These ways include:</p><h3 id="""">Truncating</h3><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6616c3fbc31e03ec3b03fc5c_vPztbCg2OAF6Xq_c41reNP8NeeZHSJ9XWQ-Ermd5O_6VXI-5Wxp2nkE-L9nqdKU28rsSRE4vUKsm0Xj_PRl2vQ7GWD_Lb0jJU35ghYeyhAX2Iqsn1PdC64KGfcSZOJQOrzAKOF6ocPWgMljQGfkWVyM.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">Truncation is a straightforward approach where longer texts are cut off to fit within the model's maximum input size limit. The original BERT model, for instance, is designed to process sequences up to 512 tokens in length. This includes 510 tokens of the document's text, plus 2 special tokens added at the beginning and the end of each sequence. The maximal_text_length parameter is crucial in this process, dictating the cut-off point for the text. By default, texts longer than 510 tokens are truncated to meet this requirement, ensuring that the model can process them.</p><h4 id="""">Explanation and Implications of Truncating Longer Texts</h4><p id="""">Truncating longer texts to the first 512 tokens (including the special tokens) is often deemed sufficient for many applications. This method ensures that the beginning part of a document is considered, which, for certain types of documents, can contain the most critical information. However, this approach has notable implications:</p><p>‍</p><ul id=""""><li><strong id="""">Information Loss:</strong> Truncation inevitably leads to the loss of potentially significant content within the text that exceeds the token limit.<strong id="""">‍</strong></li><li><strong id="""">Bias Towards Initial Content: </strong>By prioritizing the beginning of a document, there's a risk of biasing the model's understanding and analysis towards the initial context and themes, potentially overlooking critical information contained in the latter parts of the text.</li></ul><p>‍</p><p id="""">An alternative form of truncation involves selecting both the beginning and the end portions of the text, thereby omitting the middle section. This method aims to capture the introduction and conclusion of a document, operating under the assumption that these parts may encapsulate the core message or summary of the text. While this can be more representative than only considering the start of a document, it still carries the risk of missing out on crucial details and nuances contained within the body of the text.</p><p>‍</p><p id="""">For example, suppose we have a medical report that describes a patient’s symptoms, diagnosis, treatment, and prognosis. If we truncate the document by only selecting the beginning and the end portions, we may miss out on important information that could affect the classification of the document. For example, we may not know the exact cause of the patient’s condition, the side effects of the treatment, or the likelihood of recovery. These factors could influence the classification of the document as positive or negative, urgent or non-urgent, or informative or persuasive. Therefore, truncating could reduce the efficiency of the model and lead to inaccurate or incomplete results.</p><h4 id="""">Advantages of Truncation</h4><ul id=""""><li>Simplicity: Truncation is easy to implement and requires minimal computational resources.</li><li>Efficiency: It allows for the rapid processing of texts by reducing them to a manageable size for models like BERT.</li></ul><h4 id="""">Limitations</h4><ul id=""""><li><strong id="""">Content Loss:</strong> Important details and context can be lost, potentially affecting the accuracy of document classification.</li><li><strong id="""">Potential Bias:</strong> There's a risk of biasing analysis towards the portions of the text selected for inclusion, possibly overlooking key themes or arguments presented in the omitted sections</li></ul><p>‍</p><p id="""">To sum up, truncation is an easy way to deal with the problem of using BERT for long documents, but it also comes with some drawbacks that we need to think about. The choice to truncate, and the method of truncation employed, should be informed by the specific requirements of the classification task and the nature of the documents being analyzed.</p><h3 id="""">Chunking and Combining Results</h3><p id="""">Chunking involves cutting the document into smaller, manageable pieces, classifying each chunk separately, and then combining these results to arrive at a final classification for the entire document. This method not only circumvents the token limitation but also leverages the model's strengths over multiple segments of text.</p><p>‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6616c3fbf075cde979f09fc9_WTi9DgD0tFTpEMILCXcQVYK_nv6m_4wgX31-O6RD7UbauPHjOj1uzue9GM4w0Ml8Sr4zlxCL4ByMp_-4PjeVcMekao-obtVY5NZHPvRg03ciGI63M0cgv8ZsFakXidt2aMUGUseLoeKCQbKlfQYyXyE.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><h4 id="""">Document Segmentation (Chunking)</h4><p id="""">The first crucial step in managing long documents for analysis within token-limited models, such as BERT, involves segmenting the document into smaller, manageable pieces, referred to as chunks. This process, known as chunking, is essential to ensure that every part of the document receives attention without exceeding the model's token processing limit.</p><p>‍</p><p id="""">Before chunking, the document undergoes a preparation phase where unnecessary elements like headers, footers, or any extraneous sections that could distort the analysis are removed. This is followed by the tokenization of the text, which breaks down the document into tokens (words or symbols) to accurately measure the segments in tokens rather than characters or words, aligning with the model's processing capabilities.</p><p>‍</p><p id="""">The actual division into chunks is performed with the intent to keep each segment within a 510-token limit, reserving space for special tokens ([CLS] and [SEP]) that are required at the beginning and end of each segment for BERT processing. The aim is to split the document into consecutive segments, ensuring that sentences are not cut midway whenever possible. This consideration is vital for maintaining the integrity of the context within each chunk, enabling more coherent and contextually rich analysis by the model.</p><h4 id="""">Classification of Each Chunk</h4><p id="""">Once the document is segmented, the next step involves classifying each chunk individually. This process begins with the preparation of each chunk for input into the BERT model by adding necessary special tokens. The [CLS] token is inserted at the beginning of each chunk to indicate the start of a new segment, and the [SEP] token is placed at the end, signaling the end of the segment. These tokens are crucial for the model to understand the structure and boundaries of the input.</p><p>‍</p><p id="""">Each prepared chunk is then fed into the model separately. The model performs inference on each chunk, determining the most likely class or category that the segment belongs to. During this step, the model generates a prediction result for each chunk, which includes the predicted class and a confidence score. This confidence score is a numerical value representing how certain the model is about its prediction, providing insight into the reliability of each classified segment.</p><p>‍</p><p id="""">The procedure of classifying each chunk is essential because it guarantees that the model's analysis encompasses the complete document, utilizing the model's capacity to comprehend and interpret text across various, discrete segments. The process of classifying the document in this step-by-step manner enables a detailed examination of the content and lays the groundwork for later combining these separate classifications into a logical total classification.</p><h4 id="""">Techniques for Aggregating Results from Chunks</h4><p id="""">After each chunk of the document has been individually classified, the challenge lies in synthesizing these discrete results into an overall document classification. This aggregation is crucial for interpreting the document as a whole, taking into account the nuanced insights gained from the segmented analysis. Several sophisticated techniques have been developed to achieve this aggregation effectively:</p><p>‍</p><p id=""""><strong id="""">Majority Voting</strong>: This is the most straightforward aggregation technique, where the class predicted most frequently across all chunks is selected as the final classification for the document. This method operates under the principle that the most common prediction likely represents the dominant theme or content of the entire document. However, majority voting may not always yield a conclusive result, especially in cases where predictions are evenly split or the document covers multiple topics almost equally.</p><p>‍</p><p id=""""><strong id="""">Confidence Weighted Voting</strong>: To refine the aggregation process, confidence weighted voting takes into account not just the frequency of each predicted class but also the confidence levels associated with these predictions. In this method, a prediction made with higher confidence (e.g., 90% confidence) is given more weight in the final decision than a prediction made with lower confidence (e.g., 60% confidence). This approach allows for a more nuanced aggregation, privileging segments where the model's predictions are more certain and potentially more accurate.</p><p id="""">‍</p><h4 id="""">Advanced Aggregation Techniques:</h4><p id=""""><strong id="""">Sequential Analysis</strong>: Recognizing that some documents may present a clear narrative or argument that develops over time, sequential analysis considers the order of predictions along with their content. This technique is particularly useful for documents where the beginning and end may strongly suggest a specific classification, potentially outweighing mixed predictions from the middle sections.</p><p>‍</p><p id=""""><strong id="""">Hybrid Model Approach</strong>: For documents that present complex classification challenges, a hybrid model can be employed. This approach uses the predictions from individual chunks as inputs to another machine learning model, specifically trained to integrate these fragmented insights into a coherent final classification. The hybrid model can consider various factors, including the sequence of chunk predictions, their confidence levels, and other extracted features, to produce a refined and accurate document classification.</p><p>‍</p><p id="""">A means of overcoming the difficulty of combining the categorization outcomes from divided document analysis is provided by each of these methods. Long documents that may not be able to be processed by models like BERT can now be comprehensively and accurately classified by taking into account factors like prediction frequency, confidence levels, the content's sequential flow, and the use of advanced machine learning models.</p><p>‍</p><p id="""">These aggregation techniques account for the delicate insights obtained from closely examining each segment, which not only improves the overall document classification accuracy but also enables a deeper comprehension of the content of the document.</p><h5 id="""">Fine-tuning and Evaluation</h5><p id="""">After aggregating the classification results from individual chunks to form an overall document classification, it's essential to fine-tune and evaluate the process to ensure its accuracy and reliability.</p><p id=""""><strong id="""">‍</strong></p><p id=""""><strong id="""">Model Training:</strong> If employing a hybrid model approach for aggregation, this model must be trained on a dataset where documents and their respective chunks have been pre-classified. This training enables the model to learn the most effective ways to combine chunk predictions into a coherent final classification. The training should focus on optimizing the model's parameters to accurately reflect the complexity and nuances of the document classifications it will encounter in practical applications.</p><p id=""""><strong id="""">‍</strong></p><p id=""""><strong id="""">Evaluation:</strong> The effectiveness of the chunking, classification, and aggregation strategy is assessed through rigorous evaluation on a validation set. This validation set should consist of documents with known classifications to benchmark the model's performance. The evaluation process compares the aggregated document classification results against these known classifications to measure accuracy, precision, recall, and other relevant metrics. This step is crucial for identifying any biases, underperformances, or areas for improvement in the model.</p><p id="""">‍</p><h4 id="""">Practical Considerations</h4><p id="""">Implementing the chunking and aggregation strategy in real-world scenarios requires attention to several practical considerations to optimize performance and accuracy.</p><p id="""">‍</p><p id=""""><strong id="""">Optimizing Chunk Size:</strong> While the token limit (e.g., 510 tokens for BERT) defines the maximum size of chunks, experimenting with different chunk sizes can yield better results. Smaller chunks might capture more coherent and contextually rich segments of text, leading to more accurate individual classifications. Finding the optimal chunk size is a balance between ensuring manageable segments for the model and preserving the contextual integrity of the document.</p><p id="""">‍</p><p id=""""><strong id="""">Overlap Strategy:</strong> To mitigate potential loss of context at the boundaries of chunks, an overlap strategy can be employed. This approach involves creating chunks that share a certain number of tokens at their borders, ensuring that information at the edge of a chunk is also considered at the beginning of the next. This overlap can help preserve continuity and context, especially for documents where the flow of information is crucial for accurate classification.</p><p id="""">‍</p><p id=""""><strong>Handling Ambiguity and Complexity:</strong> For documents that present ambiguous or complex classification challenges, a combination of aggregation techniques and manual review might be necessary. In such cases, leveraging the insights of a hybrid model along with expert human judgment can ensure the highest accuracy, particularly for critical documents where the stakes of misclassification are high.<br><br></p><p id="""">The approach for organizing and categorizing lengthy documents within token-limited models can be successfully used for a variety of document kinds and classification requirements by taking these factors into account and continuously improving the procedure through assessment and tweaking.</p><p id="""">‍</p><h2 id="""">Alternate Model Architectures for Long Context</h2><p id="""">‍</p><h2 id="""">Reformers</h2><p id="""">Reformers are the type of model that can manage large text volumes, which makes work easier in disciplines like education, research, and document analysis. When handling large amounts of data, traditional technologies frequently falter, whereas this model is designed to handle and comprehend lengthy texts more effectively. Older models performed well on shorter texts, but not well on longer ones because they would have to separate the material into smaller sections that would miss crucial connections or lose sight of the key ideas.</p><p id="""">‍</p><p id="""">Reformers distinguish themselves by applying a smart method of textual analysis. They are able to focus on particular details while still understanding the overall picture.&nbsp; It means that they are able to carefully examine each portion and connect various concepts in a logical manner. It is therefore an excellent at providing a thorough and clear knowledge of lengthy texts, which is extremely helpful for any work requiring in-depth text analysis. Their proficiency is particularly useful in fields where understanding large documents is necessary. Reformers, for instance, assist researchers in filtering through a lot of data. They facilitate the process of sorting through complicated legal documents to locate crucial information.</p><p id="""">‍</p><h4 id="""">Preparing Data for Classification</h4><p id="""">There is some prep work required to get the data ready before the model can begin working on lengthy papers. Consider it as preparing the ingredients for a large dish before you begin cooking. To ensure a seamless cooking process, make sure everything is measured, diced, and arranged. This entails taking your lengthy documents and simplifying them so that the Reformer may readily grasp them.</p><p id=""""><br>For the model to classify long documents effectively, the initial step involves preparing and preprocessing the data. This process is critical for ensuring the model can interpret and analyze the text accurately. The first stage, tokenization, converts the raw text into a series of tokens or meaningful units, such as words or subwords. This step is essential for transforming natural language into a format that the model can process.</p><p id="""">‍</p><p id="""">After tokenization, the next important step is organizing these tokens in a way that maintains the document's structure, ensuring the model can understand the context and flow of the text. For long documents, this may involve segmenting the text into smaller, manageable sections without losing the overall consistency. Each segment is then encoded with positional embeddings to help the model track the sequence of the text.</p><p id="""">‍</p><p id="""">The data must also be labeled correctly for classification tasks, which involves assigning each document or segment a label that represents its category or class. This labeling is crucial for supervised learning, where the model learns to predict the category of unseen documents based on the patterns it identifies during training.</p><p id="""">‍</p><p id="""">Finally, ensuring the uniformity of input lengths is important, as it affects the model's ability to process data efficiently. In cases where documents exceed the model's maximum input size, strategies such as chunking the document into smaller parts or using a hierarchical approach for representation can be applied.</p><p id="""">‍</p><p id="""">Through careful preparation of the data, we guarantee that the model has a strong base from which to learn, enabling it to classify lengthy documents with accuracy according to their content and context.<br><br></p><h4 id="""">Model Architecture and Implementation</h4><p id="""">The Reformer model stands out for its innovative architecture, designed specifically to tackle the challenges of processing long sequences of data efficiently. At its core, the Reformer introduces two main innovations: the use of locality-sensitive hashing (LSH) to perform efficient attention computations and reversible residual layers to reduce memory consumption during training. These features enable the Reformer to manage large volumes of text without the computational and memory overheads that plague conventional transformer models.</p><p id="""">‍</p><p id="""">The LSH attention mechanism allows the model to focus on parts of the text that are most relevant to the task at hand, bypassing the need to compare every element with every other element. This selective attention drastically reduces the computational complexity, enabling the processing of long documents in a fraction of the time and with significantly less hardware resources. The reversible residual layers complement this by allowing the model to backtrack its steps in the computation process, eliminating the need to store intermediate activations and further conserving memory.</p><p id="""">‍</p><p id="""">Implementing the model for document classification involves leveraging these architectural strengths. Practically, this means integrating the Reformer into a pipeline that includes preprocessing steps like tokenization, segmenting documents into manageable parts, and encoding these parts with the necessary positional information. The model is then trained on a dataset of labeled documents, learning to associate patterns in the text with specific categories. For developers and data scientists, libraries such as Hugging Face's Transformers provide accessible interfaces to implement the Reformer, simplifying the process of model training and deployment.</p><h3 id="""">Detailed Exploration of Reformer’s Core Features</h3><p id="""">‍</p><p id="""">The model helps to deal with long texts in NLP by changing the way it focuses on different parts of the text, which helps it understand the sense and links within the text. It also avoids some of the issues that standard Transformer models face, mainly that they are very slow and use a lot of space when the text is very long, which makes it difficult to work with big documents.</p><p id="""">‍</p><h4 id="""">Self-Attention Layer in Reformer:</h4><p id="""">‍</p><p id=""""><strong id="""">Local Self-Attention:</strong> This technique makes self-attention faster and easier by only looking at nearby words within a certain range, instead of the whole text. Unlike the global attention mechanism that checks the whole text for meaning, local self-attention only cares about the closest words. This helps save time and space while still understanding the importance of each word.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6616c3fb1719d1f05cdd8244_ljm4B3sQkTTWhdnvma17NQUC2RJVtJqByTHdlqy_M3iz8Dg3EaWpH5RUkREAeoedntjfE0-V6yjUYi1DTaO-V54ORKLWEN89SRYYo8h1FTJh4b0pezUGiuDRQbQQJd1er2X-tq-maRIpAr4puNaDcA4.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id=""""><em id="""">Locality-Sensitive Hashing Attention showing the hash-bucketing, sorting, and chunking steps, and the resulting causal attentions, together with the corresponding attention metrics</em></p><h4 id="""">Locality Sensitive Hashing (LSH) Self-Attention:&nbsp;</h4><p id="""">‍</p><p id="""">An efficient approach that employs hashing techniques to simplify the attention mechanism. By organizing tokens into hash buckets, where tokens with similar hash values are grouped together, LSH self-attention calculates attention within these confined spaces instead of the entire sequence. This method substantially lowers computational costs by concentrating on token groups that are contextually similar, thus more relevant to each other, enabling a more focused and efficient processing.</p><p id="""">‍</p><h4 id="""">Chunked Feed Forward Layers:</h4><p id="""">‍</p><p id="""">The Reformer makes changes by segmenting the sequence into smaller chunks for processing, diverging from the traditional Transformer model approach where feed-forward layers are applied in parallel across the entire sequence. This chunking method significantly lessens the memory burden by processing only parts of the sequence at any given time. Such segmented processing ensures the model can still capture complex data patterns without the memory overhead typically associated with long sequences.</p><p id="""">‍</p><h4 id="""">Reversible Residual Layers:</h4><p id="""">‍</p><p id="""">To further enhance memory efficiency, the Reformer incorporates reversible residual layers. These layers allow for the backward pass computations during training without the need to store forward pass activations. By designing the network to reconstruct the input of any layer from its output and the subsequent layer's output, the Reformer leverages reversible operations to decrease memory usage substantially. This approach is particularly beneficial for deep learning models trained on extensive sequences, where memory constraints are a significant concern.</p><p id="""">‍</p><h4 id="""">Axial Positional Encodings:</h4><p id="""">‍</p><p id="""">Recognizing the sequence's order is vital for Transformer models, and the Reformer addresses the challenge of scaling positional encodings for long texts. Through axial positional encodings, the model adopts a multi-dimensional representation of position, breaking down the positional information into several dimensions. This technique allows for efficient handling of positions in very long sequences without a corresponding increase in memory demands. By encoding positions across different dimensions, the model maintains precise token ordering in extensive sequences more effectively and with lower memory overhead than traditional methods.</p><p id="""">‍</p><h3 id="""">Challenges and Limitations</h3><p id="""">Despite the model's innovative approach to processing long documents, several challenges and limitations remain. Understanding these aspects is crucial for effectively deploying the model in real-world applications and for ongoing research aimed at improving NLP technologies.</p><p id="""">‍</p><p id="""">One of the primary challenges is related to the computational resources required for training and fine-tuning the model. While the model is designed to be more efficient than traditional transformer models, particularly for long documents, it still demands significant computational power, especially when dealing with very large datasets or extremely lengthy documents. This requirement can limit accessibility for individuals or organizations with constrained computational budgets, potentially hindering wider adoption and experimentation.</p><p id="""">‍</p><p id="""">Another limitation is the trade-off between efficiency and model complexity. The mechanisms that allow the Reformer to process long sequences, such as locality-sensitive hashing, also introduce new hyperparameters and model behaviors that must be carefully managed. Tuning these parameters to achieve optimal performance can be complex and time-consuming, requiring deep understanding and experience with the model's inner workings.</p><p id="""">‍</p><p id="""">Moreover, while the Reformer excels at handling long documents, its performance can vary depending on the nature of the text and the specific classification task. For example, documents with highly specialized or technical language may pose additional challenges for the model, necessitating further fine-tuning or the integration of domain-specific knowledge bases.</p><p id="""">‍</p><p id="""">Data quality and availability also play a critical role in the model's effectiveness. High-quality, annotated datasets are essential for training and fine-tuning, yet such datasets may be scarce or difficult to create for certain domains or languages. This scarcity can limit the model's ability to learn and generalize across different types of documents and classification tasks.</p><h2 id="""">Longformers</h2><p id="""">Longformers have emerged as a practical solution for processing and understanding lengthy texts, addressing a common challenge faced in various fields such as research, legal studies, and content creation. Unlike traditional text analysis tools, which struggle with large volumes of data, Longformers are designed to efficiently manage and interpret documents that span thousands of words. The development of Longformers is a response to the limitations of earlier models in handling extensive narratives or detailed reports. These prior models, while effective for shorter pieces, often falter when tasked with analyzing longer documents. They tend to either oversimplify the content or require the text to be broken down into smaller segments, potentially missing the forest for the trees.</p><p id="""">‍</p><p id="""">Longformers stand out by employing a strategic method to attention mechanisms, allowing them to focus on specific parts of the text while maintaining an awareness of the document's overall context. This dual approach enables them to delve into the intricacies of each paragraph and connect disparate sections meaningfully. As a result, Longformers offer a nuanced understanding of long documents, making them invaluable for tasks requiring deep textual analysis. This capability is particularly beneficial in fields where comprehending lengthy documents is crucial. For example, in academic research, Longformers can help scholars synthesize extensive literature. In legal contexts, they can aid in navigating complex legal documents to extract relevant information.</p><p id="""">‍</p><p id="""">By introducing Longformers, we now have a tool that enhances our ability to work with large-scale texts, simplifying what was once a daunting task. This advancement not only saves time but also ensures a more thorough and informed analysis, opening up new possibilities for how we engage with and interpret extensive documents.</p><p id="""">‍</p><h3 id="""">Efficient Attention Mechanism:</h3><p id="""">‍</p><p id="""">The core of Longformer lies in their attention mechanism, which departs significantly from the full self-attention approach of traditional transformers. In the standard model, each token in the input sequence attends to every other token, leading to a computational demand that grows quadratically as sequences extend. Longformers, on the other hand, adopt a dual strategy combining local windowed attention with task-specific global attention, substantially reducing the computational burden and making it feasible to process much longer texts in a single operation.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6616c3fb1a0c0ca9ef78bff1_cOd6o2Zj2ieUoO8-l1tksegjphUDhp4K1WYMIZrUpyNAH7P6ZNdC8tizNN44sR3hA-VJUIjG3cvMLFVimq3bpJ38yvmZe9eZoHDk1e7xtBeH-DMwcgU5sxtVz0g1fzPIvSTN0jRFD26eO2jhDSfVQMY.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id=""""><strong id="""">Local Windowed Attention:</strong> This technique confines the self-attention scope to a fixed-size window surrounding each token. As the sequence progresses, this window moves, ensuring that a token computes attention only for those nearby within this predefined range. For instance, with a window size of 512 tokens, a token attends to just 511 others around it, drastically reducing the computational effort required compared to the exhaustive attention mechanism of traditional models.</p><p id="""">‍</p><p id=""""><strong id="""">Global Attention: </strong>To complement the localized focus, Longformers incorporate a global attention feature where specific tokens, identified as crucial for the overall understanding of the text, can attend to and be attended by all tokens across the sequence. This mechanism allows for the retention and emphasis of vital information throughout the document, ensuring that key elements are not overlooked due to the localized nature of windowed attention. Tokens that typically receive global attention include those marking significant structural points (like paragraph beginnings) or essential entities within the text.</p><p id="""">‍</p><h3 id="""">How Longformers Work: Under the Hood</h3><p id="""">The operational principle of Longformers is built upon efficiently managing the transformer's self-attention layer to accommodate long sequences. The selective attention method they employ is particularly adept at processing texts that far exceed the usual length limitations imposed by standard transformer models.</p><p id="""">‍</p><p id=""""><strong id="""">Segmentation and Attention Allocation:</strong> When a Longformer processes a document, it first segments the text into manageable chunks using the local windowed attention. This segmentation allows for a focused analysis of each section of the text, akin to reading a document one paragraph at a time.</p><p id="""">‍</p><p id=""""><strong id="""">Incorporating Global Context:</strong> Alongside this localized focus, global attention markers are strategically placed on elements crucial for overarching comprehension. This dual strategy ensures that while the model efficiently parses through the document, it retains an awareness of key themes and arguments that span across the entire text.</p><p id="""">‍</p><p id=""""><strong id="""">Memory and Computational Efficiency:</strong> Through techniques like gradient checkpointing and mixed-precision training, Longformers optimize the use of hardware resources, enabling the processing of documents with up to 16,000 tokens on setups with 48GB GPUs. This capability is made possible by the model's architectural design that balances between the depth of analysis and computational efficiency.</p><p id="""">‍</p><h3 id="""">Practical Applications and Advanced Considerations</h3><p id="""">Longformers are particularly useful in fields that need in-depth text analysis; examples include legal document analysis, which provides a tool for quickly navigating and extracting information from dense legal texts, and academic research, where they can distill large volumes of literature.</p><p id="""">‍</p><p id=""""><strong id="""">Fine-Tuning for Domain-Specific Tasks</strong></p><p id="""">One of the keys to unlocking the full potential of Longformers is fine-tuning the model on domain-specific datasets. This process adapts the Longformer to the peculiarities of a given field, enhancing its ability to recognize and interpret the nuanced language and structure of domain-specific documents. For instance, in legal document analysis, fine-tuning Longformers on a dataset of legal opinions can help the model better understand the formal language and reasoning patterns typical of legal texts.</p><p id="""">‍</p><p id=""""><strong id="""">Leveraging Longformers for In-Depth Analysis</strong></p><p id="""">Longformers' ability to process and analyze long documents opens up new avenues for extracting insights and generating comprehensive summaries. In academic research, this means being able to review literature or synthesize findings from extensive studies with unprecedented efficiency. For content creators, it offers a means to generate detailed summaries or analyses of long-form content, providing value to audiences without requiring them to engage with the full text.</p><p id="""">‍</p><h3 id="""">Longformers vs. Reformers: A Detailed Comparison</h3><h4 id="""">Design Philosophy and Core Mechanisms</h4><p id=""""><strong id="""">Longformers:</strong> Designed with the primary goal of efficiently processing long texts, Longformers introduce a novel attention mechanism that combines local windowed attention with global attention. This hybrid approach allows Longformers to maintain a deep understanding of both the immediate context and the document as a whole. The local attention focuses on nearby words to reduce computational load, while global attention ensures crucial parts of the text, like headings or key terms, influence the model’s understanding of the entire document.</p><p id="""">‍</p><p id=""""><strong id="""">Reformers:</strong> Reformers address the challenge of processing long sequences by optimizing memory usage and computational efficiency. They utilize two key innovations: reversible residual layers and locality-sensitive hashing (LSH) for attention. The reversible layers reduce memory consumption during training by enabling the calculation of gradients directly from the outputs, avoiding the need to store intermediate activations. LSH attention approximates the full attention mechanism by grouping tokens into buckets based on similarity, significantly reducing the computational complexity.</p><p id="""">‍</p><h4 id="""">Efficiency and Scalability</h4><p id="""">Longformers are optimized for scenarios where the detailed comprehension of extended texts is essential. Their design allows for the processing of texts up to 16,000 tokens long, making them ideal for in-depth analysis of documents like research papers or lengthy reports. The efficiency of Longformers lies in their ability to provide comprehensive coverage of long texts without compromising on the depth of analysis.</p><p id="""">‍</p><p id="""">Reformers, with their focus on memory efficiency and faster computation, are particularly suited for applications where the length of the documents might not be as extreme but the volume of data is substantial. The LSH attention mechanism makes Reformers adept at handling large datasets with moderate-length documents, providing a balance between performance and computational resource requirements.</p><p id="""">‍</p><h4 id="""">Use Cases</h4><p id="""">Longformers excel in tasks that demand thorough understanding and analysis of extensive documents. They are particularly useful for document summarization, detailed content analysis, and comprehensive information retrieval across long texts. For instance, Longformers can be effectively used for synthesizing information from extensive scientific literature or generating detailed summaries of long-form journalistic content.</p><p id="""">‍</p><p id="""">Reformers are more aligned with use cases involving the efficient processing of numerous texts where the individual document length does not exceed the model's processing limits but where aggregate data volume is high. They are well-suited for tasks like encoding large datasets for clustering or similarity searches, processing multiple documents for information extraction, or quickly summarizing batches of articles where memory efficiency is crucial.</p><p id="""">‍</p><h4 id="""">Performance Considerations</h4><p id="""">Longformers are tailored for precision and depth, making them slightly more resource-intensive but highly effective for comprehensive analysis. They are the preferred choice when the accuracy of understanding long texts significantly impacts the outcome, such as in legal document analysis or in-depth academic research.</p><p id="""">‍</p><p id="""">Reformers offer a pragmatic solution for applications where speed and memory efficiency are prioritized over the intricate analysis of each text. They stand out in environments with limited computational resources or when the task requires quick processing of texts with a reasonable trade-off between detail and efficiency.</p><p id="""">‍</p><h4 id="""">Conclusion</h4><p id="""">The choice between Longformers and Reformers hinges on the specific requirements of the text analysis task at hand. Longformers are the go-to option for deep, contextual analysis of lengthy documents, where every detail might carry importance. In contrast, Reformers offer an efficient pathway for processing large volumes of text, balancing between performance and computational demand.</p><p id="""">‍</p><p id="""">By understanding the distinctions in their designs, functionalities, and ideal use cases, users can better select the model that aligns with their objectives, whether they seek to uncover nuanced insights from extensive documents or to efficiently manage large datasets with moderate-length texts.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1112px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1112px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6616c3fb05c13e52903c93d2_dmqbaYojgr2WlrCAFka7ufHSUYPaQ0GEXz_AexBLgm0RVtNyjUu0plco0rdhFsLNoTxF5egP69nvtkZCuauj2OQd8rQh6B17znwQiaWPD9d1NaO_mCsYuevTj0y7SCRV2b3LGVAaSVdflBpLHVKlWsw.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">(<a href=""https://huggingface.co/blog/long-range-transformers"" id="""">Hugging Face Reads, Feb. 2021 - Long-range Transformers</a>)</p><p id="""">‍</p><h3 id="""">Benchmarks&nbsp;</h3><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:761px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""761px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6616c3fb05c13e52903c93e0_jimOgfuwcVglVD9VzD9YpeAhmSVn15fp3g5cvEY57zTXF9ZexfPAPvGmJIQta73XcMMqTGerk6a8yHYYbLjnsL5ZOZg5mhaTPOIhQYmzXZsE8NYc9tzW9y2vUdfrPU_YnZO_bI8NCTMq0v1aqOPsk1w.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:788px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""788px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6616c3fbae30f29f905d9009_YLbapJ2ES79T7G1wA3bLMgwK8ZqI8WLEf4MsXxEvJxTQoalXVtnvlQ8c9czzuOXKTI_1E9yRW6n_nE9YxaNP6ICTxXdpoqcRFi6NZbvXFDUbshcwfeZzmC8vIHnMIKyumDNz_NS4LfQcIrlDun05aNI.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">(<a href=""https://arxiv.org/pdf/2011.04006v1.pdf"" id="""">2011.04006v1.pdf (arxiv.org)</a>)</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1035px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1035px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6616c3fb901801d36ae0deef_4_rSF2gu-DhyiYBTY6beK6bIzSHUwaPoIxv7cIH54Lzh_UBly5APxZkP7hOlOfXlVvfiY54oyaH6TTUKu4ioh4IvTM-obOISNMvEVABxZzdbGiXRQeLkfz5kDEinDEHrijMNNJBqx14zNvpcuDehcqw.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">(<a href=""https://proceedings.neurips.cc/paper_files/paper/2021/file/9425be43ba92c2b4454ca7bf602efad8-Paper.pdf"" id="""">9425be43ba92c2b4454ca7bf602efad8-Paper.pdf (neurips.cc)</a>)</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:923px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""923px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6616c3fb1410dd2bcdc343ba_xx0_rk4hkVwYPIcckPMvstpJoFXnii6ypC9pvHQ_cZFSV9OnMos_cTGrAZdFT3HosE9E9jNu7LaoQZLXBuIvqAgzQTB4N42Zy2-RzC5vX9t16fCUPKi7GRG6bJpGFCxL4KwuLdbeEKJM7SmnA9BZRHI.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">(<a href=""https://proceedings.neurips.cc/paper_files/paper/2021/file/9425be43ba92c2b4454ca7bf602efad8-Paper.pdf"" id="""">9425be43ba92c2b4454ca7bf602efad8-Paper.pdf (neurips.cc)</a>)</p><p id="""">‍</p><h2 id="""">Want to build Models for Long Contexts?</h2><p id="""">If you want to build or train language models or LLMs for long contexts, please feel free to <a href=""https://calendly.com/pranav-mercity/30min"" id="""">reach out to us</a>. We have worked on multiple such projects and deployed many such architectures. Long contexts are something only proprietary LLMs enjoy, we can help you bypass this for your private applications!</p>",https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6616c7f8cbe7cb3ee37b2435_Screenshot%202024-04-10%20223945.png,Harshit,Machine Learning,"Most of the applications need access to process long texts and documents, BERT models even though competent, might not perform very good for long contexts. In this blog, we show how to work with long texts and contexts with BERT.",False,"<div class=""rich-text w-richtext""><p>We often work with long texts in different areas. They have a lot of information and complexity that we need to know and classify. For example, we may want to sort news articles, research papers, or legal documents by their topics, sentiments, or goals. How can we do this well and fast? One way is to use BERT, a language model. It can understand the meaning and context of words in different situations. It can also get smarter from lots of complex information and spot the differences and updates in long texts. This model is very strong and useful for document classification. But using it for long documents is not simple. The 512 token length restriction on the pretrained model makes it difficult to use for longer documents. These problems can make it difficult to use this approach well and fast. Well, how can we solve these problems and use this technique to classify long documents? </p><p>‍</p><p>In this blog, let us solve this problem. We will show you how to use BERT for long document classification in a simple and effective way. By the end of this guide, you will have the skills and knowledge to use the model to classify long documents with high quality and speed. </p><h2>What is BERT? </h2><p>BERT stands for Bidirectional Encoder Representations from Transformers. It is a machine learning model that understands and works with human language. It helps sort text, analyze emotions, answer questions, and more. It was developed by Google AI Language in 2018 and has achieved state-of-the-art results on many natural language processing (NLP) benchmarks.  </p><p>‍</p><p>This model is based on the Transformer architecture, which is a neural network that uses attention mechanisms to encode and decode sequences of words. Unlike traditional models that process text from left to right or right to left, this model can consider both the left and right context of each word in a sentence. This allows it to better understand the meaning and nuance of natural language.</p><p>  </p><p>The Transformer architecture was introduced in 2017. It aimed to improve the older methods of understanding and processing language, done through models like Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs). RNNs and CNNs are sequential models that have difficulty capturing long-term dependencies and parallelizing computation. Transformers, on the other hand, are parallelizable and can capture long-term dependencies by using self-attention. Self-attention is a technique that computes the relevance of each word to every other word in a sequence.</p><p>  </p><p>This approach has two components: an encoder and a decoder. The encoder is a stack of Transformer layers that takes a sequence of words as input and produces a high-dimensional vector representation for each word. The decoder is a separate neural network which takes the encoder’s output and performs a specific task, which could be classification, generation, or prediction.</p><p>  </p><p>This technique can be used for text classification by adding a classification layer on top of the encoder’s output. The classification layer takes the output vector of the first token in the sequence, which is a special token called [CLS] that stands for the whole sentence. The classification layer then outputs a probability distribution over the possible classes. For example, if the task is to classify movie reviews as positive or negative, the classification layer would output two probabilities, one for each class. </p><p> </p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6616c3fb79b4fa2d6800fbc4_qqx-DRa8yferGV07SeAPB0RG6gMTk3LxivcPyIDY-zh0MCsqsFwYcVnFUtSuEj-penCWD3On1a7EzdN8zJ8pYmyZmjxI0CWfG9qcGLbdszz34y-Y-7vvUxsGP9gqcmq6qLFEtIaB9msz_pShnk9E_QM.png""/></div></figure><p>‍</p><h2>Why Classify Long Documents Using BERT? </h2><p>In the world of NLP, this model's arrival has changed the game in how computers understand our language. This becomes crucial when dealing with big documents that come with their own unique challenges and opportunities. The traditional methods often don’t quite cut it when it comes to fully understanding these large texts because of their own limitations. But this approach, with its deep understanding of the language, steps up as a handy solution to these issues. It brings some serious advantages to the table when it comes to analyzing lengthy documents, outperforming the traditional methods.</p><h3><strong>Limitations of Traditional Methods in Handling Long Texts</strong> </h3><p>Traditional NLP methods, such as BoW (bag-of-words) and TF-IDF (Term Frequency-Inverse Document Frequency), have provided foundational approaches to text classification. However, they meet several limitations when applied to long documents. </p><p>‍</p><p>Context Ignorance (Not Getting the Full Picture): These methods don’t really get the context and meaning of words in relation to the text around them, which means they only get a surface-level understanding of the content.</p><p>  </p><p>Fixed-Length Input Constraints (One Size Doesn’t Fit All): A lot of the older models are designed to handle inputs of a certain length, so it’s a bit of a struggle to deal with whole documents without having to cut them down or oversimplify them.</p><p>  </p><p>Semantic Loss (Lost in Translation): When you try to shrink or summarize long documents to fit these constraints, you can end up losing important meaning, which can affect the accuracy of classification. </p><h3>Advantages of Using BERT for Long Document Analysis </h3><p>This technique brings a fresh perspective to text classification, stepping up where traditional methods fall short. Here’s why: </p><p>‍</p><p><strong>Deep Contextual Understanding (Getting the Full Picture)</strong>: Unlike older models, this model gets the full context of words by looking at the entire text they’re part of. This bidirectional understanding significantly enhances the model's ability to grasp nuanced meanings and variations in language.  </p><p>‍</p><p><strong>Handling Varied Length Inputs (Flexible with Sizes):</strong> This model’s design is naturally more flexible with different input sizes. Even though there’s a limit to the token length it can handle at once (usually 512 tokens), there are different ways to effectively deal with longer documents.</p><p>‍</p><p><strong>Sophisticated Language Models (Advanced Language Models):</strong> This model is built on the Transformers architecture, which allows for a more detailed analysis of how text elements relate and depend on each other due to the self-attention learning. This leads to much better classification performance, especially when dealing with complex and long documents.  </p><p>‍</p><p>The application of this technique for long document classification is not without its challenges, primarily due to the token length limitation. Yet, through approaches like document segmentation and the use of advanced variants (e.g., Longformer, Reformer), it is possible to effectively analyze and classify extensive texts. In a nutshell, this technique is a standout choice for analyzing long documents, offering a mix of deep understanding of language and flexibility with text lengths. </p><p>‍</p><h2>Strategies for Classifying Long Documents</h2><p>The classification of long documents using models like BERT presents unique challenges due to the inherent limitations of these models in processing long sequences of text. To deal with this, people have come up with different ways to make it easier to manage and classify long documents. These ways include:</p><h3>Truncating</h3><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6616c3fbc31e03ec3b03fc5c_vPztbCg2OAF6Xq_c41reNP8NeeZHSJ9XWQ-Ermd5O_6VXI-5Wxp2nkE-L9nqdKU28rsSRE4vUKsm0Xj_PRl2vQ7GWD_Lb0jJU35ghYeyhAX2Iqsn1PdC64KGfcSZOJQOrzAKOF6ocPWgMljQGfkWVyM.png""/></div></figure><p>Truncation is a straightforward approach where longer texts are cut off to fit within the model's maximum input size limit. The original BERT model, for instance, is designed to process sequences up to 512 tokens in length. This includes 510 tokens of the document's text, plus 2 special tokens added at the beginning and the end of each sequence. The maximal_text_length parameter is crucial in this process, dictating the cut-off point for the text. By default, texts longer than 510 tokens are truncated to meet this requirement, ensuring that the model can process them.</p><h4>Explanation and Implications of Truncating Longer Texts</h4><p>Truncating longer texts to the first 512 tokens (including the special tokens) is often deemed sufficient for many applications. This method ensures that the beginning part of a document is considered, which, for certain types of documents, can contain the most critical information. However, this approach has notable implications:</p><p>‍</p><ul role=""list""><li><strong>Information Loss:</strong> Truncation inevitably leads to the loss of potentially significant content within the text that exceeds the token limit.<strong>‍</strong></li><li><strong>Bias Towards Initial Content: </strong>By prioritizing the beginning of a document, there's a risk of biasing the model's understanding and analysis towards the initial context and themes, potentially overlooking critical information contained in the latter parts of the text.</li></ul><p>‍</p><p>An alternative form of truncation involves selecting both the beginning and the end portions of the text, thereby omitting the middle section. This method aims to capture the introduction and conclusion of a document, operating under the assumption that these parts may encapsulate the core message or summary of the text. While this can be more representative than only considering the start of a document, it still carries the risk of missing out on crucial details and nuances contained within the body of the text.</p><p>‍</p><p>For example, suppose we have a medical report that describes a patient’s symptoms, diagnosis, treatment, and prognosis. If we truncate the document by only selecting the beginning and the end portions, we may miss out on important information that could affect the classification of the document. For example, we may not know the exact cause of the patient’s condition, the side effects of the treatment, or the likelihood of recovery. These factors could influence the classification of the document as positive or negative, urgent or non-urgent, or informative or persuasive. Therefore, truncating could reduce the efficiency of the model and lead to inaccurate or incomplete results.</p><h4>Advantages of Truncation</h4><ul role=""list""><li>Simplicity: Truncation is easy to implement and requires minimal computational resources.</li><li>Efficiency: It allows for the rapid processing of texts by reducing them to a manageable size for models like BERT.</li></ul><h4>Limitations</h4><ul role=""list""><li><strong>Content Loss:</strong> Important details and context can be lost, potentially affecting the accuracy of document classification.</li><li><strong>Potential Bias:</strong> There's a risk of biasing analysis towards the portions of the text selected for inclusion, possibly overlooking key themes or arguments presented in the omitted sections</li></ul><p>‍</p><p>To sum up, truncation is an easy way to deal with the problem of using BERT for long documents, but it also comes with some drawbacks that we need to think about. The choice to truncate, and the method of truncation employed, should be informed by the specific requirements of the classification task and the nature of the documents being analyzed.</p><h3>Chunking and Combining Results</h3><p>Chunking involves cutting the document into smaller, manageable pieces, classifying each chunk separately, and then combining these results to arrive at a final classification for the entire document. This method not only circumvents the token limitation but also leverages the model's strengths over multiple segments of text.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6616c3fbf075cde979f09fc9_WTi9DgD0tFTpEMILCXcQVYK_nv6m_4wgX31-O6RD7UbauPHjOj1uzue9GM4w0Ml8Sr4zlxCL4ByMp_-4PjeVcMekao-obtVY5NZHPvRg03ciGI63M0cgv8ZsFakXidt2aMUGUseLoeKCQbKlfQYyXyE.png""/></div></figure><h4>Document Segmentation (Chunking)</h4><p>The first crucial step in managing long documents for analysis within token-limited models, such as BERT, involves segmenting the document into smaller, manageable pieces, referred to as chunks. This process, known as chunking, is essential to ensure that every part of the document receives attention without exceeding the model's token processing limit.</p><p>‍</p><p>Before chunking, the document undergoes a preparation phase where unnecessary elements like headers, footers, or any extraneous sections that could distort the analysis are removed. This is followed by the tokenization of the text, which breaks down the document into tokens (words or symbols) to accurately measure the segments in tokens rather than characters or words, aligning with the model's processing capabilities.</p><p>‍</p><p>The actual division into chunks is performed with the intent to keep each segment within a 510-token limit, reserving space for special tokens ([CLS] and [SEP]) that are required at the beginning and end of each segment for BERT processing. The aim is to split the document into consecutive segments, ensuring that sentences are not cut midway whenever possible. This consideration is vital for maintaining the integrity of the context within each chunk, enabling more coherent and contextually rich analysis by the model.</p><h4>Classification of Each Chunk</h4><p>Once the document is segmented, the next step involves classifying each chunk individually. This process begins with the preparation of each chunk for input into the BERT model by adding necessary special tokens. The [CLS] token is inserted at the beginning of each chunk to indicate the start of a new segment, and the [SEP] token is placed at the end, signaling the end of the segment. These tokens are crucial for the model to understand the structure and boundaries of the input.</p><p>‍</p><p>Each prepared chunk is then fed into the model separately. The model performs inference on each chunk, determining the most likely class or category that the segment belongs to. During this step, the model generates a prediction result for each chunk, which includes the predicted class and a confidence score. This confidence score is a numerical value representing how certain the model is about its prediction, providing insight into the reliability of each classified segment.</p><p>‍</p><p>The procedure of classifying each chunk is essential because it guarantees that the model's analysis encompasses the complete document, utilizing the model's capacity to comprehend and interpret text across various, discrete segments. The process of classifying the document in this step-by-step manner enables a detailed examination of the content and lays the groundwork for later combining these separate classifications into a logical total classification.</p><h4>Techniques for Aggregating Results from Chunks</h4><p>After each chunk of the document has been individually classified, the challenge lies in synthesizing these discrete results into an overall document classification. This aggregation is crucial for interpreting the document as a whole, taking into account the nuanced insights gained from the segmented analysis. Several sophisticated techniques have been developed to achieve this aggregation effectively:</p><p>‍</p><p><strong>Majority Voting</strong>: This is the most straightforward aggregation technique, where the class predicted most frequently across all chunks is selected as the final classification for the document. This method operates under the principle that the most common prediction likely represents the dominant theme or content of the entire document. However, majority voting may not always yield a conclusive result, especially in cases where predictions are evenly split or the document covers multiple topics almost equally.</p><p>‍</p><p><strong>Confidence Weighted Voting</strong>: To refine the aggregation process, confidence weighted voting takes into account not just the frequency of each predicted class but also the confidence levels associated with these predictions. In this method, a prediction made with higher confidence (e.g., 90% confidence) is given more weight in the final decision than a prediction made with lower confidence (e.g., 60% confidence). This approach allows for a more nuanced aggregation, privileging segments where the model's predictions are more certain and potentially more accurate.</p><p>‍</p><h4>Advanced Aggregation Techniques:</h4><p><strong>Sequential Analysis</strong>: Recognizing that some documents may present a clear narrative or argument that develops over time, sequential analysis considers the order of predictions along with their content. This technique is particularly useful for documents where the beginning and end may strongly suggest a specific classification, potentially outweighing mixed predictions from the middle sections.</p><p>‍</p><p><strong>Hybrid Model Approach</strong>: For documents that present complex classification challenges, a hybrid model can be employed. This approach uses the predictions from individual chunks as inputs to another machine learning model, specifically trained to integrate these fragmented insights into a coherent final classification. The hybrid model can consider various factors, including the sequence of chunk predictions, their confidence levels, and other extracted features, to produce a refined and accurate document classification.</p><p>‍</p><p>A means of overcoming the difficulty of combining the categorization outcomes from divided document analysis is provided by each of these methods. Long documents that may not be able to be processed by models like BERT can now be comprehensively and accurately classified by taking into account factors like prediction frequency, confidence levels, the content's sequential flow, and the use of advanced machine learning models.</p><p>‍</p><p>These aggregation techniques account for the delicate insights obtained from closely examining each segment, which not only improves the overall document classification accuracy but also enables a deeper comprehension of the content of the document.</p><h5>Fine-tuning and Evaluation</h5><p>After aggregating the classification results from individual chunks to form an overall document classification, it's essential to fine-tune and evaluate the process to ensure its accuracy and reliability.</p><p><strong>‍</strong></p><p><strong>Model Training:</strong> If employing a hybrid model approach for aggregation, this model must be trained on a dataset where documents and their respective chunks have been pre-classified. This training enables the model to learn the most effective ways to combine chunk predictions into a coherent final classification. The training should focus on optimizing the model's parameters to accurately reflect the complexity and nuances of the document classifications it will encounter in practical applications.</p><p><strong>‍</strong></p><p><strong>Evaluation:</strong> The effectiveness of the chunking, classification, and aggregation strategy is assessed through rigorous evaluation on a validation set. This validation set should consist of documents with known classifications to benchmark the model's performance. The evaluation process compares the aggregated document classification results against these known classifications to measure accuracy, precision, recall, and other relevant metrics. This step is crucial for identifying any biases, underperformances, or areas for improvement in the model.</p><p>‍</p><h4>Practical Considerations</h4><p>Implementing the chunking and aggregation strategy in real-world scenarios requires attention to several practical considerations to optimize performance and accuracy.</p><p>‍</p><p><strong>Optimizing Chunk Size:</strong> While the token limit (e.g., 510 tokens for BERT) defines the maximum size of chunks, experimenting with different chunk sizes can yield better results. Smaller chunks might capture more coherent and contextually rich segments of text, leading to more accurate individual classifications. Finding the optimal chunk size is a balance between ensuring manageable segments for the model and preserving the contextual integrity of the document.</p><p>‍</p><p><strong>Overlap Strategy:</strong> To mitigate potential loss of context at the boundaries of chunks, an overlap strategy can be employed. This approach involves creating chunks that share a certain number of tokens at their borders, ensuring that information at the edge of a chunk is also considered at the beginning of the next. This overlap can help preserve continuity and context, especially for documents where the flow of information is crucial for accurate classification.</p><p>‍</p><p><strong>Handling Ambiguity and Complexity:</strong> For documents that present ambiguous or complex classification challenges, a combination of aggregation techniques and manual review might be necessary. In such cases, leveraging the insights of a hybrid model along with expert human judgment can ensure the highest accuracy, particularly for critical documents where the stakes of misclassification are high.<br/><br/></p><p>The approach for organizing and categorizing lengthy documents within token-limited models can be successfully used for a variety of document kinds and classification requirements by taking these factors into account and continuously improving the procedure through assessment and tweaking.</p><p>‍</p><h2>Alternate Model Architectures for Long Context</h2><p>‍</p><h2>Reformers</h2><p>Reformers are the type of model that can manage large text volumes, which makes work easier in disciplines like education, research, and document analysis. When handling large amounts of data, traditional technologies frequently falter, whereas this model is designed to handle and comprehend lengthy texts more effectively. Older models performed well on shorter texts, but not well on longer ones because they would have to separate the material into smaller sections that would miss crucial connections or lose sight of the key ideas.</p><p>‍</p><p>Reformers distinguish themselves by applying a smart method of textual analysis. They are able to focus on particular details while still understanding the overall picture.  It means that they are able to carefully examine each portion and connect various concepts in a logical manner. It is therefore an excellent at providing a thorough and clear knowledge of lengthy texts, which is extremely helpful for any work requiring in-depth text analysis. Their proficiency is particularly useful in fields where understanding large documents is necessary. Reformers, for instance, assist researchers in filtering through a lot of data. They facilitate the process of sorting through complicated legal documents to locate crucial information.</p><p>‍</p><h4>Preparing Data for Classification</h4><p>There is some prep work required to get the data ready before the model can begin working on lengthy papers. Consider it as preparing the ingredients for a large dish before you begin cooking. To ensure a seamless cooking process, make sure everything is measured, diced, and arranged. This entails taking your lengthy documents and simplifying them so that the Reformer may readily grasp them.</p><p><br/>For the model to classify long documents effectively, the initial step involves preparing and preprocessing the data. This process is critical for ensuring the model can interpret and analyze the text accurately. The first stage, tokenization, converts the raw text into a series of tokens or meaningful units, such as words or subwords. This step is essential for transforming natural language into a format that the model can process.</p><p>‍</p><p>After tokenization, the next important step is organizing these tokens in a way that maintains the document's structure, ensuring the model can understand the context and flow of the text. For long documents, this may involve segmenting the text into smaller, manageable sections without losing the overall consistency. Each segment is then encoded with positional embeddings to help the model track the sequence of the text.</p><p>‍</p><p>The data must also be labeled correctly for classification tasks, which involves assigning each document or segment a label that represents its category or class. This labeling is crucial for supervised learning, where the model learns to predict the category of unseen documents based on the patterns it identifies during training.</p><p>‍</p><p>Finally, ensuring the uniformity of input lengths is important, as it affects the model's ability to process data efficiently. In cases where documents exceed the model's maximum input size, strategies such as chunking the document into smaller parts or using a hierarchical approach for representation can be applied.</p><p>‍</p><p>Through careful preparation of the data, we guarantee that the model has a strong base from which to learn, enabling it to classify lengthy documents with accuracy according to their content and context.<br/><br/></p><h4>Model Architecture and Implementation</h4><p>The Reformer model stands out for its innovative architecture, designed specifically to tackle the challenges of processing long sequences of data efficiently. At its core, the Reformer introduces two main innovations: the use of locality-sensitive hashing (LSH) to perform efficient attention computations and reversible residual layers to reduce memory consumption during training. These features enable the Reformer to manage large volumes of text without the computational and memory overheads that plague conventional transformer models.</p><p>‍</p><p>The LSH attention mechanism allows the model to focus on parts of the text that are most relevant to the task at hand, bypassing the need to compare every element with every other element. This selective attention drastically reduces the computational complexity, enabling the processing of long documents in a fraction of the time and with significantly less hardware resources. The reversible residual layers complement this by allowing the model to backtrack its steps in the computation process, eliminating the need to store intermediate activations and further conserving memory.</p><p>‍</p><p>Implementing the model for document classification involves leveraging these architectural strengths. Practically, this means integrating the Reformer into a pipeline that includes preprocessing steps like tokenization, segmenting documents into manageable parts, and encoding these parts with the necessary positional information. The model is then trained on a dataset of labeled documents, learning to associate patterns in the text with specific categories. For developers and data scientists, libraries such as Hugging Face's Transformers provide accessible interfaces to implement the Reformer, simplifying the process of model training and deployment.</p><h3>Detailed Exploration of Reformer’s Core Features</h3><p>‍</p><p>The model helps to deal with long texts in NLP by changing the way it focuses on different parts of the text, which helps it understand the sense and links within the text. It also avoids some of the issues that standard Transformer models face, mainly that they are very slow and use a lot of space when the text is very long, which makes it difficult to work with big documents.</p><p>‍</p><h4>Self-Attention Layer in Reformer:</h4><p>‍</p><p><strong>Local Self-Attention:</strong> This technique makes self-attention faster and easier by only looking at nearby words within a certain range, instead of the whole text. Unlike the global attention mechanism that checks the whole text for meaning, local self-attention only cares about the closest words. This helps save time and space while still understanding the importance of each word.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6616c3fb1719d1f05cdd8244_ljm4B3sQkTTWhdnvma17NQUC2RJVtJqByTHdlqy_M3iz8Dg3EaWpH5RUkREAeoedntjfE0-V6yjUYi1DTaO-V54ORKLWEN89SRYYo8h1FTJh4b0pezUGiuDRQbQQJd1er2X-tq-maRIpAr4puNaDcA4.png""/></div></figure><p><em>Locality-Sensitive Hashing Attention showing the hash-bucketing, sorting, and chunking steps, and the resulting causal attentions, together with the corresponding attention metrics</em></p><h4>Locality Sensitive Hashing (LSH) Self-Attention: </h4><p>‍</p><p>An efficient approach that employs hashing techniques to simplify the attention mechanism. By organizing tokens into hash buckets, where tokens with similar hash values are grouped together, LSH self-attention calculates attention within these confined spaces instead of the entire sequence. This method substantially lowers computational costs by concentrating on token groups that are contextually similar, thus more relevant to each other, enabling a more focused and efficient processing.</p><p>‍</p><h4>Chunked Feed Forward Layers:</h4><p>‍</p><p>The Reformer makes changes by segmenting the sequence into smaller chunks for processing, diverging from the traditional Transformer model approach where feed-forward layers are applied in parallel across the entire sequence. This chunking method significantly lessens the memory burden by processing only parts of the sequence at any given time. Such segmented processing ensures the model can still capture complex data patterns without the memory overhead typically associated with long sequences.</p><p>‍</p><h4>Reversible Residual Layers:</h4><p>‍</p><p>To further enhance memory efficiency, the Reformer incorporates reversible residual layers. These layers allow for the backward pass computations during training without the need to store forward pass activations. By designing the network to reconstruct the input of any layer from its output and the subsequent layer's output, the Reformer leverages reversible operations to decrease memory usage substantially. This approach is particularly beneficial for deep learning models trained on extensive sequences, where memory constraints are a significant concern.</p><p>‍</p><h4>Axial Positional Encodings:</h4><p>‍</p><p>Recognizing the sequence's order is vital for Transformer models, and the Reformer addresses the challenge of scaling positional encodings for long texts. Through axial positional encodings, the model adopts a multi-dimensional representation of position, breaking down the positional information into several dimensions. This technique allows for efficient handling of positions in very long sequences without a corresponding increase in memory demands. By encoding positions across different dimensions, the model maintains precise token ordering in extensive sequences more effectively and with lower memory overhead than traditional methods.</p><p>‍</p><h3>Challenges and Limitations</h3><p>Despite the model's innovative approach to processing long documents, several challenges and limitations remain. Understanding these aspects is crucial for effectively deploying the model in real-world applications and for ongoing research aimed at improving NLP technologies.</p><p>‍</p><p>One of the primary challenges is related to the computational resources required for training and fine-tuning the model. While the model is designed to be more efficient than traditional transformer models, particularly for long documents, it still demands significant computational power, especially when dealing with very large datasets or extremely lengthy documents. This requirement can limit accessibility for individuals or organizations with constrained computational budgets, potentially hindering wider adoption and experimentation.</p><p>‍</p><p>Another limitation is the trade-off between efficiency and model complexity. The mechanisms that allow the Reformer to process long sequences, such as locality-sensitive hashing, also introduce new hyperparameters and model behaviors that must be carefully managed. Tuning these parameters to achieve optimal performance can be complex and time-consuming, requiring deep understanding and experience with the model's inner workings.</p><p>‍</p><p>Moreover, while the Reformer excels at handling long documents, its performance can vary depending on the nature of the text and the specific classification task. For example, documents with highly specialized or technical language may pose additional challenges for the model, necessitating further fine-tuning or the integration of domain-specific knowledge bases.</p><p>‍</p><p>Data quality and availability also play a critical role in the model's effectiveness. High-quality, annotated datasets are essential for training and fine-tuning, yet such datasets may be scarce or difficult to create for certain domains or languages. This scarcity can limit the model's ability to learn and generalize across different types of documents and classification tasks.</p><h2>Longformers</h2><p>Longformers have emerged as a practical solution for processing and understanding lengthy texts, addressing a common challenge faced in various fields such as research, legal studies, and content creation. Unlike traditional text analysis tools, which struggle with large volumes of data, Longformers are designed to efficiently manage and interpret documents that span thousands of words. The development of Longformers is a response to the limitations of earlier models in handling extensive narratives or detailed reports. These prior models, while effective for shorter pieces, often falter when tasked with analyzing longer documents. They tend to either oversimplify the content or require the text to be broken down into smaller segments, potentially missing the forest for the trees.</p><p>‍</p><p>Longformers stand out by employing a strategic method to attention mechanisms, allowing them to focus on specific parts of the text while maintaining an awareness of the document's overall context. This dual approach enables them to delve into the intricacies of each paragraph and connect disparate sections meaningfully. As a result, Longformers offer a nuanced understanding of long documents, making them invaluable for tasks requiring deep textual analysis. This capability is particularly beneficial in fields where comprehending lengthy documents is crucial. For example, in academic research, Longformers can help scholars synthesize extensive literature. In legal contexts, they can aid in navigating complex legal documents to extract relevant information.</p><p>‍</p><p>By introducing Longformers, we now have a tool that enhances our ability to work with large-scale texts, simplifying what was once a daunting task. This advancement not only saves time but also ensures a more thorough and informed analysis, opening up new possibilities for how we engage with and interpret extensive documents.</p><p>‍</p><h3>Efficient Attention Mechanism:</h3><p>‍</p><p>The core of Longformer lies in their attention mechanism, which departs significantly from the full self-attention approach of traditional transformers. In the standard model, each token in the input sequence attends to every other token, leading to a computational demand that grows quadratically as sequences extend. Longformers, on the other hand, adopt a dual strategy combining local windowed attention with task-specific global attention, substantially reducing the computational burden and making it feasible to process much longer texts in a single operation.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6616c3fb1a0c0ca9ef78bff1_cOd6o2Zj2ieUoO8-l1tksegjphUDhp4K1WYMIZrUpyNAH7P6ZNdC8tizNN44sR3hA-VJUIjG3cvMLFVimq3bpJ38yvmZe9eZoHDk1e7xtBeH-DMwcgU5sxtVz0g1fzPIvSTN0jRFD26eO2jhDSfVQMY.png""/></div></figure><p><strong>Local Windowed Attention:</strong> This technique confines the self-attention scope to a fixed-size window surrounding each token. As the sequence progresses, this window moves, ensuring that a token computes attention only for those nearby within this predefined range. For instance, with a window size of 512 tokens, a token attends to just 511 others around it, drastically reducing the computational effort required compared to the exhaustive attention mechanism of traditional models.</p><p>‍</p><p><strong>Global Attention: </strong>To complement the localized focus, Longformers incorporate a global attention feature where specific tokens, identified as crucial for the overall understanding of the text, can attend to and be attended by all tokens across the sequence. This mechanism allows for the retention and emphasis of vital information throughout the document, ensuring that key elements are not overlooked due to the localized nature of windowed attention. Tokens that typically receive global attention include those marking significant structural points (like paragraph beginnings) or essential entities within the text.</p><p>‍</p><h3>How Longformers Work: Under the Hood</h3><p>The operational principle of Longformers is built upon efficiently managing the transformer's self-attention layer to accommodate long sequences. The selective attention method they employ is particularly adept at processing texts that far exceed the usual length limitations imposed by standard transformer models.</p><p>‍</p><p><strong>Segmentation and Attention Allocation:</strong> When a Longformer processes a document, it first segments the text into manageable chunks using the local windowed attention. This segmentation allows for a focused analysis of each section of the text, akin to reading a document one paragraph at a time.</p><p>‍</p><p><strong>Incorporating Global Context:</strong> Alongside this localized focus, global attention markers are strategically placed on elements crucial for overarching comprehension. This dual strategy ensures that while the model efficiently parses through the document, it retains an awareness of key themes and arguments that span across the entire text.</p><p>‍</p><p><strong>Memory and Computational Efficiency:</strong> Through techniques like gradient checkpointing and mixed-precision training, Longformers optimize the use of hardware resources, enabling the processing of documents with up to 16,000 tokens on setups with 48GB GPUs. This capability is made possible by the model's architectural design that balances between the depth of analysis and computational efficiency.</p><p>‍</p><h3>Practical Applications and Advanced Considerations</h3><p>Longformers are particularly useful in fields that need in-depth text analysis; examples include legal document analysis, which provides a tool for quickly navigating and extracting information from dense legal texts, and academic research, where they can distill large volumes of literature.</p><p>‍</p><p><strong>Fine-Tuning for Domain-Specific Tasks</strong></p><p>One of the keys to unlocking the full potential of Longformers is fine-tuning the model on domain-specific datasets. This process adapts the Longformer to the peculiarities of a given field, enhancing its ability to recognize and interpret the nuanced language and structure of domain-specific documents. For instance, in legal document analysis, fine-tuning Longformers on a dataset of legal opinions can help the model better understand the formal language and reasoning patterns typical of legal texts.</p><p>‍</p><p><strong>Leveraging Longformers for In-Depth Analysis</strong></p><p>Longformers' ability to process and analyze long documents opens up new avenues for extracting insights and generating comprehensive summaries. In academic research, this means being able to review literature or synthesize findings from extensive studies with unprecedented efficiency. For content creators, it offers a means to generate detailed summaries or analyses of long-form content, providing value to audiences without requiring them to engage with the full text.</p><p>‍</p><h3>Longformers vs. Reformers: A Detailed Comparison</h3><h4>Design Philosophy and Core Mechanisms</h4><p><strong>Longformers:</strong> Designed with the primary goal of efficiently processing long texts, Longformers introduce a novel attention mechanism that combines local windowed attention with global attention. This hybrid approach allows Longformers to maintain a deep understanding of both the immediate context and the document as a whole. The local attention focuses on nearby words to reduce computational load, while global attention ensures crucial parts of the text, like headings or key terms, influence the model’s understanding of the entire document.</p><p>‍</p><p><strong>Reformers:</strong> Reformers address the challenge of processing long sequences by optimizing memory usage and computational efficiency. They utilize two key innovations: reversible residual layers and locality-sensitive hashing (LSH) for attention. The reversible layers reduce memory consumption during training by enabling the calculation of gradients directly from the outputs, avoiding the need to store intermediate activations. LSH attention approximates the full attention mechanism by grouping tokens into buckets based on similarity, significantly reducing the computational complexity.</p><p>‍</p><h4>Efficiency and Scalability</h4><p>Longformers are optimized for scenarios where the detailed comprehension of extended texts is essential. Their design allows for the processing of texts up to 16,000 tokens long, making them ideal for in-depth analysis of documents like research papers or lengthy reports. The efficiency of Longformers lies in their ability to provide comprehensive coverage of long texts without compromising on the depth of analysis.</p><p>‍</p><p>Reformers, with their focus on memory efficiency and faster computation, are particularly suited for applications where the length of the documents might not be as extreme but the volume of data is substantial. The LSH attention mechanism makes Reformers adept at handling large datasets with moderate-length documents, providing a balance between performance and computational resource requirements.</p><p>‍</p><h4>Use Cases</h4><p>Longformers excel in tasks that demand thorough understanding and analysis of extensive documents. They are particularly useful for document summarization, detailed content analysis, and comprehensive information retrieval across long texts. For instance, Longformers can be effectively used for synthesizing information from extensive scientific literature or generating detailed summaries of long-form journalistic content.</p><p>‍</p><p>Reformers are more aligned with use cases involving the efficient processing of numerous texts where the individual document length does not exceed the model's processing limits but where aggregate data volume is high. They are well-suited for tasks like encoding large datasets for clustering or similarity searches, processing multiple documents for information extraction, or quickly summarizing batches of articles where memory efficiency is crucial.</p><p>‍</p><h4>Performance Considerations</h4><p>Longformers are tailored for precision and depth, making them slightly more resource-intensive but highly effective for comprehensive analysis. They are the preferred choice when the accuracy of understanding long texts significantly impacts the outcome, such as in legal document analysis or in-depth academic research.</p><p>‍</p><p>Reformers offer a pragmatic solution for applications where speed and memory efficiency are prioritized over the intricate analysis of each text. They stand out in environments with limited computational resources or when the task requires quick processing of texts with a reasonable trade-off between detail and efficiency.</p><p>‍</p><h4>Conclusion</h4><p>The choice between Longformers and Reformers hinges on the specific requirements of the text analysis task at hand. Longformers are the go-to option for deep, contextual analysis of lengthy documents, where every detail might carry importance. In contrast, Reformers offer an efficient pathway for processing large volumes of text, balancing between performance and computational demand.</p><p>‍</p><p>By understanding the distinctions in their designs, functionalities, and ideal use cases, users can better select the model that aligns with their objectives, whether they seek to uncover nuanced insights from extensive documents or to efficiently manage large datasets with moderate-length texts.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1112pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6616c3fb05c13e52903c93d2_dmqbaYojgr2WlrCAFka7ufHSUYPaQ0GEXz_AexBLgm0RVtNyjUu0plco0rdhFsLNoTxF5egP69nvtkZCuauj2OQd8rQh6B17znwQiaWPD9d1NaO_mCsYuevTj0y7SCRV2b3LGVAaSVdflBpLHVKlWsw.png""/></div></figure><p>(<a href=""https://huggingface.co/blog/long-range-transformers"">Hugging Face Reads, Feb. 2021 - Long-range Transformers</a>)</p><p>‍</p><h3>Benchmarks </h3><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:761pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6616c3fb05c13e52903c93e0_jimOgfuwcVglVD9VzD9YpeAhmSVn15fp3g5cvEY57zTXF9ZexfPAPvGmJIQta73XcMMqTGerk6a8yHYYbLjnsL5ZOZg5mhaTPOIhQYmzXZsE8NYc9tzW9y2vUdfrPU_YnZO_bI8NCTMq0v1aqOPsk1w.png""/></div></figure><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:788pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6616c3fbae30f29f905d9009_YLbapJ2ES79T7G1wA3bLMgwK8ZqI8WLEf4MsXxEvJxTQoalXVtnvlQ8c9czzuOXKTI_1E9yRW6n_nE9YxaNP6ICTxXdpoqcRFi6NZbvXFDUbshcwfeZzmC8vIHnMIKyumDNz_NS4LfQcIrlDun05aNI.png""/></div></figure><p>(<a href=""https://arxiv.org/pdf/2011.04006v1.pdf"">2011.04006v1.pdf (arxiv.org)</a>)</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1035pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6616c3fb901801d36ae0deef_4_rSF2gu-DhyiYBTY6beK6bIzSHUwaPoIxv7cIH54Lzh_UBly5APxZkP7hOlOfXlVvfiY54oyaH6TTUKu4ioh4IvTM-obOISNMvEVABxZzdbGiXRQeLkfz5kDEinDEHrijMNNJBqx14zNvpcuDehcqw.png""/></div></figure><p>(<a href=""https://proceedings.neurips.cc/paper_files/paper/2021/file/9425be43ba92c2b4454ca7bf602efad8-Paper.pdf"">9425be43ba92c2b4454ca7bf602efad8-Paper.pdf (neurips.cc)</a>)</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:923pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6616c3fb1410dd2bcdc343ba_xx0_rk4hkVwYPIcckPMvstpJoFXnii6ypC9pvHQ_cZFSV9OnMos_cTGrAZdFT3HosE9E9jNu7LaoQZLXBuIvqAgzQTB4N42Zy2-RzC5vX9t16fCUPKi7GRG6bJpGFCxL4KwuLdbeEKJM7SmnA9BZRHI.png""/></div></figure><p>(<a href=""https://proceedings.neurips.cc/paper_files/paper/2021/file/9425be43ba92c2b4454ca7bf602efad8-Paper.pdf"">9425be43ba92c2b4454ca7bf602efad8-Paper.pdf (neurips.cc)</a>)</p><p>‍</p><h2>Want to build Models for Long Contexts?</h2><p>If you want to build or train language models or LLMs for long contexts, please feel free to <a href=""https://calendly.com/pranav-mercity/30min"">reach out to us</a>. We have worked on multiple such projects and deployed many such architectures. Long contexts are something only proprietary LLMs enjoy, we can help you bypass this for your private applications!</p></div>"
Comparing Diffusion and GAN-based Imgae Upscaling Techniques,comparing-diffusion-and-gan-imgae-upscaling-techniques,640f56f76d313b2faa631c11,6660dd4f33f3e02a1d3b40d5,False,False,Wed Jun 05 2024 21:49:03 GMT+0000 (Coordinated Universal Time),Wed Jun 05 2024 21:52:47 GMT+0000 (Coordinated Universal Time),Wed Jun 05 2024 21:52:47 GMT+0000 (Coordinated Universal Time),"<p id="""">Have you ever taken a low-resolution image and tried to enlarge it, only to find it blurry and distorted? This common issue arises because low-resolution images contain fewer pixels, limiting their ability to reproduce fine details. Traditional enlargement methods fail to maintain the original image's clarity and sharpness, resulting in unsatisfactory outcomes. However, image upscaling techniques aim to overcome this challenge by increasing the pixel count, thereby enhancing resolution and detail.</p><p id="""">‍</p><p id="""">Advancements in algorithms and AI-driven methods have revolutionized image upscaling, offering impressive solutions to enhance image quality. These cutting-edge technologies analyze and generate additional pixels, preserving the original image's integrity while improving clarity. This blog explores the principles behind image upscaling, the challenges involved, and the latest innovations in the field. By understanding these techniques, you can transform low-resolution images into high-quality, detailed visuals.</p><p>‍</p><blockquote><em>✨You can use the image upscaling benchmarking tool we built for this study here and run your own tests on your own images: </em><a href=""https://github.com/Mercity-AI/Image-Upscaling-Benchmark""><em>https://github.com/Mercity-AI/Image-Upscaling-Benchmark</em></a></blockquote><p>‍</p><h2 id="""">What is Image Upscaling?</h2><p id="""">Image upscaling refers to the process of increasing the resolution or size of an image. This technique is widely used across various fields such as photography, graphic design, AI-generated art, and video production. Image upscaling enables users to enhance the quality of images without the need to retake or recreate them from scratch. The primary objective of image upscaling is to improve the overall visual quality of an image by increasing its pixel count, thus providing a clearer, more detailed representation of the subject.</p><p id="""">‍</p><h3 id="""">The Essentials and Importance of Image Upscaling</h3><h4 id="""">Medical Imaging</h4><p id="""">‍</p><p id="""">Image upscaling significantly enhances medical imaging by providing detailed views of anatomical structures, enabling the detection of small lesions, tumors, or abnormalities that might be missed in lower-resolution images. This improved visualization leads to more accurate diagnoses and reduces the chances of misdiagnosis. In telemedicine, upscaled images ensure that remote specialists receive high-quality visuals, facilitating reliable remote diagnoses and enhancing patient records for future consultations. Additionally, detailed preoperative planning benefits from high-resolution images, allowing surgeons to visualize complex structures, identify critical areas, and plan safer surgical approaches.</p><p id="""">‍</p><p id="""">High-resolution images are invaluable in medical research, enabling detailed analysis of tissues, cells, and organs, which contributes to a better understanding of diseases and the development of new treatments. Regular high-resolution scans are essential for tracking disease progression or response to treatment, providing detailed data for timely adjustments in treatment plans. This is particularly beneficial for pediatric and geriatric care, where specialized imaging needs require minimal radiation exposure while maintaining high detail. Moreover, high-resolution imaging plays a crucial role in early detection and screening programs, enabling early intervention and improving patient outcomes. Overall, image upscaling enhances diagnostic and therapeutic services, driving advancements in medical science and improving patient care.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1152px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1152px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6660d7eebbc798110191df6e_AD_4nXdBWtckJw8B12h1XInQId-PZ_UqVkaUP_MLiV56dh2LV57nZNqj7lmUtWdqaustS3SGRASuCuG39mGkITtgkJ6L58D7nlZ6HRUy5FYoYoDv9EA0Ytvxbgs-LVu7PVLQlZIXDaXkYemQKfso-7SLY5Bg5wV7.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><h4 id="""">Satellite Imaging and Remote Sensing</h4><p id="""">‍</p><p id="""">Image upscaling significantly enhances satellite imaging and remote sensing by providing more detailed views of the Earth's surface. This improved detail is crucial for environmental monitoring, as it allows for accurate tracking of deforestation, urban development, and natural disasters. Higher-resolution images enable precise observation of changes over time, aiding in the assessment and management of environmental impacts. In agriculture, upscaled satellite images help monitor crop health, soil conditions, and pest infestations, leading to better crop management and yield prediction. These detailed images support precision agriculture, allowing farmers to make informed decisions that optimize resource use and improve productivity.</p><p id="""">‍</p><p id="""">In disaster management, high-resolution images play a vital role in planning and responding to natural disasters such as floods, earthquakes, and hurricanes. They provide clear, detailed views of affected areas, facilitating effective coordination of rescue and relief efforts. Upscaled images also enhance mapping and planning by providing high-resolution maps essential for urban planning, resource management, and infrastructure development. These maps support accurate decision-making and efficient allocation of resources. Overall, image upscaling in satellite imaging and remote sensing improves the quality of data available for environmental monitoring, agriculture, disaster management, and urban planning, ultimately leading to better outcomes and more informed decision-making.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:587px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""587px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6660d7eeb52b62f6cab193c5_AD_4nXdYU2PFoXAsTJ0-FztK_DIi0Sghdc-Z9Xqqb3BKIUrlYgKszsqQ25Ha-fDV6_mF2OEvT069QIEE7wb8IwIekqtYsFq07kbsMt9fh27d1PRai83LnTXBTaJ3lOu5A58z67dMdP9B1Jc2sXXebf5FCXNsuC78.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><h4 id="""">Digital Media and Entertainment</h4><p id="""">‍</p><p id="""">Image upscaling significantly enhances digital media and entertainment by improving the visual quality of content. In the film and television industry, upscaling allows older, low-resolution content to be converted to higher resolutions, making it suitable for modern displays. This process breathes new life into classic movies and TV shows, providing audiences with a clearer, more immersive viewing experience. Additionally, upscaling helps preserve the integrity of the original material while adapting it for current technological standards, ensuring that valuable cultural and entertainment content remains relevant and accessible.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:602px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""602px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6660d7ee212b428a2186b127_AD_4nXf4VX_vu6OA99LpefQQHMwzWs4A8NMGVRIpbpE5FRWO_ZhY1H9AOeU2X4_TvJX5VLMU_q-JpPamgfNKhkN6Vs7Xi5irLBQnyX70419T2vZjzzX7mKhlnZw3rhcVRdbChhTSKrU7jfZwBAPPtzz8nX3xWUvm.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><h4 id="""">Forensics and Security</h4><p id="""">‍</p><p id="""">Image upscaling offers significant advantages in forensics and security by enhancing the quality and detail of visual data. In crime scene analysis, upscaled images from security cameras or other sources can reveal crucial details that might otherwise be overlooked, such as identifying suspects, reading license plates, or discerning specific objects. This improved clarity aids law enforcement agencies in gathering more accurate evidence, leading to better investigations and increased chances of solving cases. High-resolution images also assist forensic experts in analyzing minute details, such as fingerprints or tool marks, which are essential for accurate crime scene reconstruction.</p><p id="""">‍</p><p id="""">In security and surveillance, upscaling technology enhances the effectiveness of monitoring systems by providing clearer, more detailed images. This improvement allows for better identification and tracking of individuals and activities, which is crucial for real-time threat detection and prevention. Enhanced image quality helps security personnel make more informed decisions and respond more effectively to potential threats. Additionally, upscaled images are beneficial in post-incident analysis, providing clearer evidence for legal proceedings and improving overall security measures. Overall, image upscaling in forensics and security improves the accuracy of investigations, enhances surveillance capabilities, and contributes to safer environments.</p><p id="""">‍</p><h3 id="""">AI Upscaling vs Image Upscaling</h3><p id="""">‍</p><p id="""">Image upscaling, often referred to as traditional upscaling, utilizes simpler algorithms to enlarge images. Techniques such as Nearest Neighbor, which replicates adjacent pixels, are straightforward but can lead to blocky outcomes. More sophisticated methods like Bilinear and Bicubic Interpolation create new pixels through linear and cubic calculations, offering smoother transitions but sometimes resulting in blurred images, especially in areas with complex textures. This form of upscaling is generally suitable for basic needs where precision in detail preservation isn't the primary concern, and the requirement for computational resources is minimal.</p><p id="""">‍</p><p id="""">On the other hand, AI upscaling represents a more advanced approach, employing deep learning models to enhance image resolution more effectively. These models are trained on extensive datasets to understand how specific details should look at higher resolutions, allowing them to generate new elements in the image that appear naturally detailed. As a result, AI upscaling can significantly improve image quality, adding clarity and reducing artifacts compared to traditional methods. This technique requires more robust computational power and is commonly used in high-demand applications such as video streaming, gaming, and professional photography, where delivering high-resolution visuals is crucial.</p><h2 id="""">Different Methods to Do Image Upscaling</h2><p id="""">‍</p><p id="""">There are several methods for image upscaling, including classical techniques, deep learning methods, GAN-based approaches, and diffusion-based models. In this discussion, we will explore how to perform image upscaling using each of these techniques. We'll evaluate the efficiency of each model, analyze their size, and examine their performance metrics, such as <a href=""https://en.wikipedia.org/wiki/Mean_squared_error"" id="""">Mean Squared Error</a> and <a href=""https://en.wikipedia.org/wiki/Structural_similarity_index_measure"" id="""">SSIM</a> (Structural Similarity Index Measure). All the performance metrics are measured on Intel Core i5-1135G7 Microprocessor and 8 GB DDR4-3200 SDRAM machine. If you run on a GPU or a different machine with other specifications, results might be different regarding performance metrics.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1636px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1636px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6660da856d7a7bddccdbb499_image%20upscaling.png"" loading=""lazy"" alt=""__wf_reserved_inherit"" width=""auto"" height=""auto"" id=""""></div></figure><h3 id="""">Image Upscaling Using Classical Methods</h3><p id="""">‍</p><p id="""">Image upscaling using classical methods refers to techniques that enhance the resolution of an image based on predefined mathematical algorithms. These methods are typically less complex and computationally intensive compared to modern deep-learning approaches. The two most commonly used classical methods are nearest-neighbor interpolation and bicubic interpolation.</p><h4 id="""">Nearest Neighbor Interpolation Method</h4><p id="""">‍</p><p id="""">Nearest-neighbor interpolation is a straightforward and computationally efficient image upscaling method that assigns the value of the nearest pixel in the original image to each pixel in the upscaled image. This method does not involve any complex calculations. It simply replicates the value of the closest pixel, making it very fast and easy to implement. However, the simplicity of nearest-neighbor interpolation comes with significant drawbacks in image quality. Upscaled images often appear blocky and pixelated, as the method does not smooth transitions between pixels or create new image details. This leads to a ""staircase"" effect, especially noticeable along diagonal lines and edges.</p><h4 id="""">Bicubic Interpolation Method</h4><p id="""">‍</p><p id="""">Bicubic interpolation is a classical image upscaling method that provides smoother and more visually appealing results compared to simpler techniques like nearest-neighbor or bilinear interpolation. It achieves this by using cubic polynomials to interpolate the pixel values. Specifically, bicubic interpolation considers the 16 nearest pixels (a 4x4 grid) around the target pixel and computes the new pixel value as a weighted average of these surrounding pixels. The weights are determined by the distance of each pixel from the target pixel, with closer pixels having more influence. This method effectively smooths transitions and reduces artifacts such as jagged edges, resulting in higher-quality images.</p><p id="""">‍</p><h3 id="""">Image Upscaling Using Deep Learning Methods</h3><p id="""">‍</p><p id="""">Image upscaling using deep learning methods has significantly advanced the field by providing superior image quality compared to classical methods. These models leverage neural networks trained on large datasets to learn complex patterns, generating high-resolution images from low-resolution inputs. By utilizing deep architectures and sophisticated layers, these methods enhance image details, reduce artifacts, and produce smoother transitions.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:850px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""850px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6660d7ee1caaf3d5883c11e1_AD_4nXctVvCh2R5tJ7HlQCNvfv9AQB5pAzy2FV6zqvQMQSYLT3nU9WAQu5QcPamqn4Lf9R0YWGAGpNNwRKLBHntIJdMIZdvZqKEXVa1h8TRm2BR121h0tGGuwl39p737Xr7CJnkumxr-Oe9-ZPjTON-aq2omSHA.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><h4 id="""">EDSR (Enhanced Deep Super-Resolution)</h4><p id="""">‍</p><p id=""""><a href=""https://github.com/sanghyun-son/EDSR-PyTorch"" id="""">EDSR</a> is a deep learning-based approach that excels in image super-resolution by utilizing residual blocks to efficiently learn high-frequency details. Inspired by the ResNet model, EDSR is specifically tailored for super-resolution tasks, eliminating unnecessary components like batch normalization layers to reduce computational complexity and enhance performance. The use of a large number of filters in each convolutional layer allows EDSR to capture intricate details, resulting in sharp and detailed images. Its ability to handle multiple scaling factors, such as 2x, 3x, and 4x, adds to its versatility across various super-resolution tasks.</p><p id="""">‍</p><p id="""">The deeper architecture of EDSR, with more layers, significantly enhances its capacity to learn and reconstruct fine details. This method has proven highly effective in benchmark evaluations, often outperforming other state-of-the-art approaches in terms of PSNR and SSIM. EDSR's high-quality results, particularly in sharpness and detail preservation, make it a popular choice for applications where image quality is crucial. Its success in producing detailed and clear images has cemented its reputation as a leading algorithm in the field of image super-resolution.</p><p id="""">‍</p><h4 id="""">ESPCN (Efficient Sub-Pixel Convolutional Neural Network)</h4><p id=""""><a href=""https://github.com/Lornatang/ESPCN-PyTorch"" id="""">ESPCN </a>is a fast and efficient algorithm designed to make images bigger, especially useful for smaller enlargements like 2x and 3x. The main idea behind ESPCN is using special layers called sub-pixel convolution layers. These layers help the network learn how to increase the image's resolution directly, making the picture clearer. This method uses less computer power and memory compared to older ways of improving image quality. ESPCN works on the smaller image first and then makes it bigger at the end, which helps keep the picture's details accurate while using less computer power.</p><p id="""">ESPCN is great for situations where speed and efficiency are very important, like streaming videos or processing images quickly. The algorithm works very fast, making it perfect for devices that don't have a lot of processing power. Because ESPCN can quickly and efficiently make high-quality images bigger, it is a popular choice for many real-time applications. Its ability to improve image resolution without using a lot of computer resources makes it very valuable for modern image processing tasks.</p><p id="""">‍</p><h4 id="""">FSRCNN (Fast Super-Resolution Convolutional Neural Network)</h4><p id="""">‍</p><p id=""""><a href=""https://github.com/yjn870/FSRCNN-pytorch"" id="""">FSRCNN </a>is a better and faster version of the earlier SRCNN model, made to speed up the process without making the pictures look worse. It has a smart design with a special part at the beginning that makes the picture smaller so it's easier to work with. Then, it goes through several layers that improve the details and finally, it has a part that makes the picture big and clear again. This way, FSRCNN can work quickly and still make the pictures look really good. It uses smaller parts and more layers to learn the fine details without becoming too big itself, which is great for things like making videos and games look better.</p><p id="""">FSRCNN also includes extra layers at the end to further improve the picture quality. It works directly on low-quality images, which makes it very efficient. The design of FSRCNN ensures that it performs well on different tests, often doing as well as or better than more complicated models. Because it can make pictures look better so quickly, FSRCNN is perfect for situations where both speed and image quality are important. Its improvements over the older SRCNN model show how effective it is for modern tasks that need real-time image enhancement, making it a popular choice for making images clearer and more detailed in live scenarios.</p><h4 id="""">LAPSRN (Laplacian Pyramid Super-Resolution Network)</h4><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:850px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""850px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6660d7ee7f7cd1dfe494b3d6_AD_4nXf2X8Ucedt-cZGpYJwwtaQrnOR89SY-q2B8gWl7gX5is1APlijJvScgxHyaEj6FJGMnp2o0cmz1pZ6s0Ztf0Ihy_FTfPq_Si_CIT8JvWOHYvD8p08wHD9JE3FSvJC2ER5qB5_76O1fM5PYwkho23YoHg7gX.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><p id=""""><a href=""https://github.com/Lornatang/LapSRN-PyTorch"" id="""">LAPSRN </a>works like a magic microscope for pictures, breaking them down into tiny pieces and then putting them back together to make them look better. It's like taking a blurry photo and turning it into a clear one! This helps LAPSRN make images bigger without losing quality, especially if you want to make them four or eight times bigger. It uses special math tricks called transposed convolutions and residual learning to make the pictures sharp and detailed, like fixing old photos or making satellite images clearer.</p><p id="""">‍</p><p id="""">LAPSRN is really smart because it can handle making pictures bigger by different amounts. It's like having a tool that can work with different sizes of puzzles! When people test LAPSRN, it usually gets really good scores, showing that the pictures it makes are very clear and nice to look at. By slowly fixing up the details in the picture, LAPSRN makes sure that both the big parts and the tiny details look just right in the bigger picture. This makes LAPSRN super helpful for making really big pictures that still look awesome, which is important for lots of cool projects.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:571px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""571px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6660d7ee23741b8f165e0cab_AD_4nXdvQitBdMD790BqXDmITrEZjN2HXwvzvQm4BN-oCv1VBCbn_F8KMnJO_ekKxQolMz_Ap-JPOZHWwBN24QvqVADefzzNjhBnT95_9mNVpJZkOm7a7JFb1Yumog9HRmXuGomWrfB9ORYua9S6FyEkD3FCp6Sd.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:438px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""438px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6660d7ef3e04118fd3c59dd8_AD_4nXenoy9gxpnseu4hLuJ131VbQohR87fQVmFowzIkx00XO84cp8qbM5iah1bYTWAnXf9G_AuDIqgbKl503PWFwJsMn_QvYVEQvp8bCHgCj6v8NVieOLAewFnAGeBuZn3Em3q4zuK-_dSKT3JzlLl8fxRpF_eC.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""600px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6660d7ee0b872e8d9f3f3ac9_AD_4nXcKl4QSEYWsyMRi55-fXn2B0fLxxfl6N8wle9anpL2fYhn8nTFffDs-7ZyRTh-o0HjRAXDaACJu2x7Ny0ILXzrqVexPoxY706x6sge7cGRI_9PVwtb9m0R2Hs93eqDCly0Su7qBj0p5-TLYFCLa-9AoC8nd.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:542px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""542px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6660d7ee778ba8f98e74b96d_AD_4nXdPv1gI8ML761yVC_ZAdBthL8uQR5NFhmcbMwjhzHSE1dtip_Ayt1BxsVAyfh_iYAA1e8j4jGJXw156pfdgUxIKUpVg-YOQ8rMuqMrQnfnuu4xVnKcx6uijG18fu0wJ1WP7Fr0iNEiDq_wKPfALHELMPtmP.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><h3 id="""">Image Upscaling Using Generative Adversarial Network</h3><p id="""">‍</p><p id="""">A <a href=""https://en.wikipedia.org/wiki/Generative_adversarial_network"" id="""">Generative Adversarial Network</a> (GAN) is a deep learning architecture. It involves two neural networks, termed the ""generator"" and the ""discriminator,"" that compete against each other. The generator creates new data instances, while the discriminator evaluates them against a real dataset. The goal is for the generator to become so good at producing data that the discriminator can't tell the difference between real and generated data. This process helps generate high-quality and realistic data. A GAN is called adversarial because it trains two different networks and pits them against each other.</p><p id="""">‍</p><h4 id="""">ESRGAN Model (Enhanced Super-Resolution Generative Adversarial Networks)</h4><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:850px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""850px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6660d7ee0f4c743f69c85d60_AD_4nXdvMHXum4ZlZvC61uDTObZRR-eeDSB0zRmn6ZlllL-ch11gUVjtVXu5c9o5GiVloqFH2yR3_pqwTR6Lk4ZVbhQj2AxZkzo3XTcdrP2QRG5lBu2lTu0MI0-fAfIw9lR93ZBeElVtrRlBQtalVW39s0G4Uf0I.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><p id="""">The architecture of ESRGAN is built upon the principles of a typical Generative Adversarial Network (GAN) but includes several key enhancements and modifications tailored for super-resolution tasks. Here's a breakdown of its architecture:</p><p id="""">‍</p><h5 id="""">Generator Network</h5><p id="""">‍</p><ul id=""""><li id="""">Residual-in-Residual Dense Block (RRDB): It is a component in advanced neural networks, particularly in super-resolution models. It consists of several densely connected convolutional layers where each layer’s input is concatenated with its outputs, enhancing feature reuse and information flow. This structure avoids using batch normalization, which helps in preserving the range of features.</li></ul><ul id=""""><li id="""">Up-sampling Layers: The generator uses up-sampling layers to scale up the low-resolution input to the desired size. In ESRGAN, the up-sampling might be achieved using sub-pixel convolution layers that rearrange the output of a convolutional layer to form a higher-resolution image.</li></ul><ul id=""""><li id="""">High-quality Image Reconstruction: The output of the last RRDB is passed through a convolution layer to reconstruct the high-resolution image. The ESRGAN generator focuses on enhancing finer details and reducing the blurring effects that are often present in super-resolved images.</li></ul><p id="""">‍</p><h5 id="""">Discriminator Network</h5><p id="""">‍</p><ul id=""""><li id="""">Convolutional Layers: The discriminator uses a series of convolutional layers that progressively downsample the input image, helping it extract various features at different scales.</li></ul><ul id=""""><li id="""">Leaky ReLU Activation: Leaky ReLU is used instead of standard ReLU to provide a non-linearity that allows gradients to flow through the network even for negative values, enhancing the training stability.</li></ul><ul id=""""><li id="""">Fully Connected Layers: After processing through convolutional layers, the features are flattened and passed through fully connected layers that finally output a scalar value indicating whether the input image is real or fake.</li></ul><p id="""">‍</p><h4 id="""">Code for Image Upscaling using Real-ESRGAN Model</h4><p id="""">‍</p><p id="""">Install the necessary libraries for the ESRGAN</p><div data-rt-embed-type='true'><pre>
<code class=""language-py"">
pip install torch Pillow numpy scikit-image
pip install git+https://github.com/sberbank-ai/Real-ESRGAN.git
</code>
</pre></div><p id="""">‍</p><p id="""">Import them,</p><div data-rt-embed-type='true'><pre>
<code class=""language-py"">
import torch
from PIL import Image
import numpy as np
from RealESRGAN import RealESRGAN
import time
from skimage.metrics import mean_squared_error, structural_similarity


# Setting the device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
</code>
</pre></div><p id="""">‍</p><p id="""">The last line sets the device for computation. If CUDA is available (indicating the presence of a GPU), it uses the GPU for faster computation; otherwise, it falls back to the CPU.</p><div data-rt-embed-type='true'><pre>
<code class=""language-py"">
# Load the model
model = RealESRGAN(device, scale=4)
model.load_weights('weights/RealESRGAN_x4.pth', download=True)


# Path to the input image
path_to_image = 'path to your image'
image = Image.open(path_to_image).convert('RGB')


# Run the model
sr_image = model.predict(image)


# Save the output image
sr_image.save('output path to your image')
</code>
</pre></div><p id="""">‍</p><p id="""">These lines initialize the RealESRGAN model with the specified device and a scaling factor (scale=4 which means the output image will have four times the resolution of the input). It then loads the pre-trained weights from a specified path, with an option to download the weights if they're not present locally.</p><h3 id="""">Image Upscaling Using Stable Diffusion Models</h3><p id="""">‍</p><p id=""""><a href=""https://en.wikipedia.org/wiki/Stable_Diffusion"" id="""">Stable Diffusion</a> is a type of generative artificial intelligence (generative AI) model that specializes in creating detailed images from textual descriptions. It utilizes a variant of the diffusion model, which gradually transforms patterns of random dots into detailed images through a reverse process that removes noise over many steps. The model is based on diffusion technology and uses latent space.</p><p id="""">‍</p><h4 id="""">The architecture of Stable Diffusion Models</h4><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6660d7eee2b259c929824af1_AD_4nXcw6pY9c4ugDdCtUYdLqNPFoS_H6sPRv6uZGbPghm7wpp2OsuI39dabTFTxurbz1LudiIGqjppP0oqi1x816y0L-CwRK9ZhfzyNVzhe-AqS10oWi_l0WnbPdgj5OBOe_fS7xzrO5TlXI93WZx18J-lJKKv2.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><p id="""">The main architectural components of Stable Diffusion include a variational autoencoder, forward and reverse diffusion, a noise predictor, and text conditioning.</p><p id="""">‍</p><h5 id="""">Variational AutoEncoder</h5><p id="""">‍</p><ul id=""""><li id="""">Encoder: This part of the model takes a large image (512x512 pixels) and compresses it down to a smaller, more manageable size (64x64 pixels) in something called latent space. Latent space is a compressed representation that's easier for the model to work with.</li><li id="""">Decoder: The decoder does the opposite of the encoder. It takes the compressed image from latent space and enlarges it back to its original size, restoring the details as much as possible.</li></ul><p id="""">‍</p><h5 id="""">Forward and Reverse Diffusion</h5><p id="""">‍</p><ul id=""""><li id="""">This process gradually adds random noise to an image until it turns into pure noise. Essentially, it transforms the image into something unrecognizable. This is mainly used during training and sometimes in image-to-image conversions where an initial image is transformed into a different style or appearance.</li></ul><p id="""">‍</p><ul id=""""><li id="""">Reverse diffusion is the process that turns the noisy image back into a clear picture. It works by estimating and removing the noise added during the forward diffusion step by step, eventually revealing a detailed image that can be a cat, a dog, or any other subject defined by the training data.</li></ul><p id="""">‍</p><h5 id="""">Noise Predictor</h5><p id="""">‍</p><p id="""">Stable Diffusion uses a special kind of neural network called U-Net, which is particularly good at removing noise from images. It predicts how much noise needs to be removed at each step of the reverse diffusion process to reveal the final image gradually.</p><p id="""">‍</p><h5 id="""">Text Conditioning</h5><p id="""">‍</p><p id="""">Stable Diffusion uses text prompts to guide image generation. Each word in the prompt is converted into a numerical format using a tokenizer, which the model understands. These numbers tell the model what features and elements to include in the image, like colors, objects, or styles.</p><p id="""">‍</p><h4 id="""">Code for Image Upscaling using Stable Diffusion x4 upscaler Model</h4><p id="""">Install these libraries</p><div data-rt-embed-type='true'><pre>
<code class=""language-py"">
!pip install diffusers transformers accelerate scipy safetensors
</code>
</pre></div><p id="""">‍</p><p id="""">Setup the pipeline,</p><div data-rt-embed-type='true'><pre>
<code class=""language-py"">
import requests
from PIL import Image
from io import BytesIO
from diffusers import StableDiffusionUpscalePipeline
import torch

# load model and scheduler
model_id = ""stabilityai/stable-diffusion-x4-upscaler""
pipeline = StableDiffusionUpscalePipeline.from_pretrained(model_id, torch_dtype=torch.float16)
pipeline = pipeline.to(""cuda"")
</code>
</pre></div><p id="""">‍</p><p id="""">The model_id is a string that uniquely identifies the model on Hugging Face's model hub, in this case, ""stabilityai/stable-diffusion-x4-upscaler"", which specifies a version of the Stable Diffusion model specifically trained to upscale images by a factor of four. The method StableDiffusionUpscalePipeline.from_pretrained is used to load this model.&nbsp;</p><p id="""">‍</p><p id="""">Here, it is initialized with a specific configuration to use 16-bit floating-point precision (torch.float16), which is a strategy to reduce memory usage, allowing the model to run faster and more efficiently on compatible hardware. The pipeline.to(""cuda"") command then shifts the model’s computations to a GPU, assuming one is available and CUDA-compatible. This significantly accelerates the processing speed, leveraging the GPU's ability to handle parallel computations, which is ideal for the intensive calculations required in upscaling images using deep learning models.</p><p id="""">‍</p><div data-rt-embed-type='true'><pre>
<code class=""language-py"">
path = 'path to your image'
low_res_img = Image.open(path).convert(""RGB"")
low_res_img = low_res_img.resize((128, 128))

prompt = ""prompt describing your image""


upscaled_image = pipeline(prompt=prompt, image=low_res_img).images[0]
upscaled_image.save(""path to your output image"")
</code>
</pre></div><p id="""">‍</p><p id="""">Define a prompt to guide the model in upscaling the image. This can be used to describe the content of the image or provide additional context to influence the upscaling process.</p><h3 id="""">Quantitative Comparison Between Upscaling Models</h3><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1136px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1136px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6660d7ee37e19cc871ef380d_AD_4nXchVrK7Tx0fCOzxq7BgxVSu8JM1uC-od2OT_h87Pa9djCyO0Ws3GLprqEmR2n9avU82wzIKzBHa0iAFrvsWhSxxgPVxYMxbO7E_Yaz2i5pUtMCm5-azY5opmS505DmLesNg8pQK8X90bw3vBluUKgury7zl.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><p id="""">Image Upscaling Models used for Benchmarking are <a href=""https://github.com/XPixelGroup/DiffBIR"" id="""">DiffBIR</a>, <a href=""https://github.com/zsyOAOA/ResShift"" id="""">ResShift</a>, <a href=""https://github.com/Fanghua-Yu/SUPIR"" id="""">SUPIR</a>, and <a href=""https://github.com/xinntao/Real-ESRGAN"" id="""">RealESRGAN</a>.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""600px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6660d7eebfc9b07b9991e056_AD_4nXfyGkN3zgcBBlto7j6GxovTG2O-yTAbZ7tA8Zc1sPrIPg5adNzssZ4JyNgWgaYnxPMg6CnGzbx7Gfb9xB--onTqsMEgOor09WW58rRg3cUpjEqJKp04x3TW6MG05awhxeWjj2t8VnLyHwNGd4pmEPoGQ2tc.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""600px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6660d7eed4582b981b394d06_AD_4nXfHKupYCzdpM_dMYjxY_KYGEzMGUgmfvq8g0YhYKsGnGKlj5SsFiUqNlxa8bsvkj7KUzecxzjdYLYywVjdMlXPPQLowtOUNdQu4nV--v1HKShT6OLVa8vNSxN95iY-LgA-i2X_KyAqDTsZRI4Py2WnIZxHj.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""600px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6660d7ee3e04118fd3c59db8_AD_4nXeX0kKbXBKeHJZOV-x0Uq03ZoLzjICPtApLfBH7A7bm08mJINc4NRYPd3o2hwloqkfPxN5to36-V0DrtUWbaaenQcjo-l_tb-t_ONqm_dWBRVVsLOZ_PsnDpAPbRyDYd7lItxK-Hq_WZKbe059WEbiPTIJk.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:620px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""620px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6660d7ee5c4df66ea68c3164_AD_4nXf_wUKhX22oqguzkFc03jqtH0xLQztsAkxh2uVmf8AstDKQR-xUyLNYwUeK0J3HZPtHXzLpZZKyZiwKxGlz2-Lbm8k7IoiZCzGBylASUWF4Eej0E4V2h3orjccYcsDO2YkqPzUm945WmLy7E78OA0BIt6oI.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><p id="""">For the detailed code Refer to <a href=""https://github.com/Mercity-AI/Image-Upscaling-Benchmark/tree/main"" id="""">Github Link</a>.</p><p id="""">‍</p><h3 id="""">Comparison between Image Upscaling Models on Further Downscaling</h3><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:711px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""711px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6660d7ee3330567a869698b8_AD_4nXffxcvK3g_sZYY--pyVHMSmFBCh0UPZWXjVXXLDikePviWFejjeyZ_R2R_mx2qXME4yyEzd6iFkTj0bQrvfD1MiWkBb5NU7Hpv5eChSte53eQxEImSKvs0Vc44pubHjR6VxUFtwJoecWmZTMBMS1uM194f8.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><p id="""">The task involves comparing the effectiveness of different image upscaling models through two distinct processes. In the first process, an image is downscaled by a factor of 2 and then upscaled back to its original size using various image upscaling models. This procedure evaluates the ability of the models to recover the original image details from a slightly reduced version. By doing so, it tests the models' proficiency in handling minimal information loss and restoring the image's quality effectively.</p><p id="""">‍</p><p id="""">In the second process, the image undergoes a more aggressive downscaling by a factor of 8, followed by a two-step upscaling process. First, the image is upscaled by a factor of 4, and then it is further upscaled by a factor of 2 using different upscaling models. This approach simulates a more challenging scenario where significant information is lost during the initial downscaling, and the models must perform two stages of upscaling to restore the image to its original dimensions. The final upscaled images from both processes are then compared to the original image using metrics such as Mean Squared Error (MSE), Structural Similarity Index (SSIM), and processing time. This comprehensive evaluation helps determine which upscaling models are more efficient and effective in terms of image quality restoration and computational efficiency.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""600px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6660d7eebbfad0e37b361dd0_AD_4nXfmoIONNwlheY8lJlmR4I5NshoLK3dBkUqKtJlDBCIZQYNh-mMB9McdUOeP9g7QA5A-r68Y3od5BhdmSWlO3UQrLsiPn8RmYp-8l8KN_yWKApn5kYi0a0gJLI2LgEg4G3ZudrPVw0XGZJr6UHbq0QBbZkw.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""600px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6660d7ee2ee15090b1613b2f_AD_4nXeR7nxVniPYiwenZotw7HZUHXlGauUvYlv8bldtch9jYdhiokOVZuR0I5J0x9D7ugzz_UZGARnJ1fXpEQJ9DMK_sx2FiQbpU0cQF1lxmN7HY05sVAT26FnpNeTWHRF8Mel7SNEP1Pqs4I4sZgZM1WkLS5PF.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""600px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6660d7ee707f912094336520_AD_4nXeN_NmFN4icWJ0tQL_DpqnSYMwIsfEVwrWE88RYipDGCs3ymzyxzcmnAh2m4IMFe5FfpSzEUeSVlLoeeBrNHAUjshlX0mYUxinItSVMNrv3XYtaQ-MGAQ-qqQ4mDLB1YqHFNS6RR1qaht_ha8fK7SknW7c.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><h2 id="""">Ready to Bring Your Images Into Stunning High-Definition</h2><p id="""">‍</p><p id="""">Experience the magic of high-resolution with Mercity AI! Our expert team specializes in transforming low-resolution images into stunning, high-definition visuals. Elevate the quality of your photos and graphics with our advanced upscaling technology. Ready to see the difference? <a href=""https://www.mercity.ai/"" id="""">Contact us</a> today and bring your images to life like never before!</p><p id="""">‍</p>",https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6660da856d7a7bddccdbb499_image%20upscaling.png,Yash,Image Upscaling,"In this comprehensive study, we benchmark and compare all the famous open image upscaling methods and measure them on quality, error and time. We compare GANs, diffusion and the classical image upscaling methods.",False,"<div class=""rich-text w-richtext""><p>Have you ever taken a low-resolution image and tried to enlarge it, only to find it blurry and distorted? This common issue arises because low-resolution images contain fewer pixels, limiting their ability to reproduce fine details. Traditional enlargement methods fail to maintain the original image's clarity and sharpness, resulting in unsatisfactory outcomes. However, image upscaling techniques aim to overcome this challenge by increasing the pixel count, thereby enhancing resolution and detail.</p><p>‍</p><p>Advancements in algorithms and AI-driven methods have revolutionized image upscaling, offering impressive solutions to enhance image quality. These cutting-edge technologies analyze and generate additional pixels, preserving the original image's integrity while improving clarity. This blog explores the principles behind image upscaling, the challenges involved, and the latest innovations in the field. By understanding these techniques, you can transform low-resolution images into high-quality, detailed visuals.</p><p>‍</p><blockquote><em>✨You can use the image upscaling benchmarking tool we built for this study here and run your own tests on your own images: </em><a href=""https://github.com/Mercity-AI/Image-Upscaling-Benchmark""><em>https://github.com/Mercity-AI/Image-Upscaling-Benchmark</em></a></blockquote><p>‍</p><h2>What is Image Upscaling?</h2><p>Image upscaling refers to the process of increasing the resolution or size of an image. This technique is widely used across various fields such as photography, graphic design, AI-generated art, and video production. Image upscaling enables users to enhance the quality of images without the need to retake or recreate them from scratch. The primary objective of image upscaling is to improve the overall visual quality of an image by increasing its pixel count, thus providing a clearer, more detailed representation of the subject.</p><p>‍</p><h3>The Essentials and Importance of Image Upscaling</h3><h4>Medical Imaging</h4><p>‍</p><p>Image upscaling significantly enhances medical imaging by providing detailed views of anatomical structures, enabling the detection of small lesions, tumors, or abnormalities that might be missed in lower-resolution images. This improved visualization leads to more accurate diagnoses and reduces the chances of misdiagnosis. In telemedicine, upscaled images ensure that remote specialists receive high-quality visuals, facilitating reliable remote diagnoses and enhancing patient records for future consultations. Additionally, detailed preoperative planning benefits from high-resolution images, allowing surgeons to visualize complex structures, identify critical areas, and plan safer surgical approaches.</p><p>‍</p><p>High-resolution images are invaluable in medical research, enabling detailed analysis of tissues, cells, and organs, which contributes to a better understanding of diseases and the development of new treatments. Regular high-resolution scans are essential for tracking disease progression or response to treatment, providing detailed data for timely adjustments in treatment plans. This is particularly beneficial for pediatric and geriatric care, where specialized imaging needs require minimal radiation exposure while maintaining high detail. Moreover, high-resolution imaging plays a crucial role in early detection and screening programs, enabling early intervention and improving patient outcomes. Overall, image upscaling enhances diagnostic and therapeutic services, driving advancements in medical science and improving patient care.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1152pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6660d7eebbc798110191df6e_AD_4nXdBWtckJw8B12h1XInQId-PZ_UqVkaUP_MLiV56dh2LV57nZNqj7lmUtWdqaustS3SGRASuCuG39mGkITtgkJ6L58D7nlZ6HRUy5FYoYoDv9EA0Ytvxbgs-LVu7PVLQlZIXDaXkYemQKfso-7SLY5Bg5wV7.png""/></div></figure><p>‍</p><h4>Satellite Imaging and Remote Sensing</h4><p>‍</p><p>Image upscaling significantly enhances satellite imaging and remote sensing by providing more detailed views of the Earth's surface. This improved detail is crucial for environmental monitoring, as it allows for accurate tracking of deforestation, urban development, and natural disasters. Higher-resolution images enable precise observation of changes over time, aiding in the assessment and management of environmental impacts. In agriculture, upscaled satellite images help monitor crop health, soil conditions, and pest infestations, leading to better crop management and yield prediction. These detailed images support precision agriculture, allowing farmers to make informed decisions that optimize resource use and improve productivity.</p><p>‍</p><p>In disaster management, high-resolution images play a vital role in planning and responding to natural disasters such as floods, earthquakes, and hurricanes. They provide clear, detailed views of affected areas, facilitating effective coordination of rescue and relief efforts. Upscaled images also enhance mapping and planning by providing high-resolution maps essential for urban planning, resource management, and infrastructure development. These maps support accurate decision-making and efficient allocation of resources. Overall, image upscaling in satellite imaging and remote sensing improves the quality of data available for environmental monitoring, agriculture, disaster management, and urban planning, ultimately leading to better outcomes and more informed decision-making.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:587pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6660d7eeb52b62f6cab193c5_AD_4nXdYU2PFoXAsTJ0-FztK_DIi0Sghdc-Z9Xqqb3BKIUrlYgKszsqQ25Ha-fDV6_mF2OEvT069QIEE7wb8IwIekqtYsFq07kbsMt9fh27d1PRai83LnTXBTaJ3lOu5A58z67dMdP9B1Jc2sXXebf5FCXNsuC78.png""/></div></figure><h4>Digital Media and Entertainment</h4><p>‍</p><p>Image upscaling significantly enhances digital media and entertainment by improving the visual quality of content. In the film and television industry, upscaling allows older, low-resolution content to be converted to higher resolutions, making it suitable for modern displays. This process breathes new life into classic movies and TV shows, providing audiences with a clearer, more immersive viewing experience. Additionally, upscaling helps preserve the integrity of the original material while adapting it for current technological standards, ensuring that valuable cultural and entertainment content remains relevant and accessible.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:602pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6660d7ee212b428a2186b127_AD_4nXf4VX_vu6OA99LpefQQHMwzWs4A8NMGVRIpbpE5FRWO_ZhY1H9AOeU2X4_TvJX5VLMU_q-JpPamgfNKhkN6Vs7Xi5irLBQnyX70419T2vZjzzX7mKhlnZw3rhcVRdbChhTSKrU7jfZwBAPPtzz8nX3xWUvm.png""/></div></figure><h4>Forensics and Security</h4><p>‍</p><p>Image upscaling offers significant advantages in forensics and security by enhancing the quality and detail of visual data. In crime scene analysis, upscaled images from security cameras or other sources can reveal crucial details that might otherwise be overlooked, such as identifying suspects, reading license plates, or discerning specific objects. This improved clarity aids law enforcement agencies in gathering more accurate evidence, leading to better investigations and increased chances of solving cases. High-resolution images also assist forensic experts in analyzing minute details, such as fingerprints or tool marks, which are essential for accurate crime scene reconstruction.</p><p>‍</p><p>In security and surveillance, upscaling technology enhances the effectiveness of monitoring systems by providing clearer, more detailed images. This improvement allows for better identification and tracking of individuals and activities, which is crucial for real-time threat detection and prevention. Enhanced image quality helps security personnel make more informed decisions and respond more effectively to potential threats. Additionally, upscaled images are beneficial in post-incident analysis, providing clearer evidence for legal proceedings and improving overall security measures. Overall, image upscaling in forensics and security improves the accuracy of investigations, enhances surveillance capabilities, and contributes to safer environments.</p><p>‍</p><h3>AI Upscaling vs Image Upscaling</h3><p>‍</p><p>Image upscaling, often referred to as traditional upscaling, utilizes simpler algorithms to enlarge images. Techniques such as Nearest Neighbor, which replicates adjacent pixels, are straightforward but can lead to blocky outcomes. More sophisticated methods like Bilinear and Bicubic Interpolation create new pixels through linear and cubic calculations, offering smoother transitions but sometimes resulting in blurred images, especially in areas with complex textures. This form of upscaling is generally suitable for basic needs where precision in detail preservation isn't the primary concern, and the requirement for computational resources is minimal.</p><p>‍</p><p>On the other hand, AI upscaling represents a more advanced approach, employing deep learning models to enhance image resolution more effectively. These models are trained on extensive datasets to understand how specific details should look at higher resolutions, allowing them to generate new elements in the image that appear naturally detailed. As a result, AI upscaling can significantly improve image quality, adding clarity and reducing artifacts compared to traditional methods. This technique requires more robust computational power and is commonly used in high-demand applications such as video streaming, gaming, and professional photography, where delivering high-resolution visuals is crucial.</p><h2>Different Methods to Do Image Upscaling</h2><p>‍</p><p>There are several methods for image upscaling, including classical techniques, deep learning methods, GAN-based approaches, and diffusion-based models. In this discussion, we will explore how to perform image upscaling using each of these techniques. We'll evaluate the efficiency of each model, analyze their size, and examine their performance metrics, such as <a href=""https://en.wikipedia.org/wiki/Mean_squared_error"">Mean Squared Error</a> and <a href=""https://en.wikipedia.org/wiki/Structural_similarity_index_measure"">SSIM</a> (Structural Similarity Index Measure). All the performance metrics are measured on Intel Core i5-1135G7 Microprocessor and 8 GB DDR4-3200 SDRAM machine. If you run on a GPU or a different machine with other specifications, results might be different regarding performance metrics.</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1636pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6660da856d7a7bddccdbb499_image%20upscaling.png""/></div></figure><h3>Image Upscaling Using Classical Methods</h3><p>‍</p><p>Image upscaling using classical methods refers to techniques that enhance the resolution of an image based on predefined mathematical algorithms. These methods are typically less complex and computationally intensive compared to modern deep-learning approaches. The two most commonly used classical methods are nearest-neighbor interpolation and bicubic interpolation.</p><h4>Nearest Neighbor Interpolation Method</h4><p>‍</p><p>Nearest-neighbor interpolation is a straightforward and computationally efficient image upscaling method that assigns the value of the nearest pixel in the original image to each pixel in the upscaled image. This method does not involve any complex calculations. It simply replicates the value of the closest pixel, making it very fast and easy to implement. However, the simplicity of nearest-neighbor interpolation comes with significant drawbacks in image quality. Upscaled images often appear blocky and pixelated, as the method does not smooth transitions between pixels or create new image details. This leads to a ""staircase"" effect, especially noticeable along diagonal lines and edges.</p><h4>Bicubic Interpolation Method</h4><p>‍</p><p>Bicubic interpolation is a classical image upscaling method that provides smoother and more visually appealing results compared to simpler techniques like nearest-neighbor or bilinear interpolation. It achieves this by using cubic polynomials to interpolate the pixel values. Specifically, bicubic interpolation considers the 16 nearest pixels (a 4x4 grid) around the target pixel and computes the new pixel value as a weighted average of these surrounding pixels. The weights are determined by the distance of each pixel from the target pixel, with closer pixels having more influence. This method effectively smooths transitions and reduces artifacts such as jagged edges, resulting in higher-quality images.</p><p>‍</p><h3>Image Upscaling Using Deep Learning Methods</h3><p>‍</p><p>Image upscaling using deep learning methods has significantly advanced the field by providing superior image quality compared to classical methods. These models leverage neural networks trained on large datasets to learn complex patterns, generating high-resolution images from low-resolution inputs. By utilizing deep architectures and sophisticated layers, these methods enhance image details, reduce artifacts, and produce smoother transitions.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:850pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6660d7ee1caaf3d5883c11e1_AD_4nXctVvCh2R5tJ7HlQCNvfv9AQB5pAzy2FV6zqvQMQSYLT3nU9WAQu5QcPamqn4Lf9R0YWGAGpNNwRKLBHntIJdMIZdvZqKEXVa1h8TRm2BR121h0tGGuwl39p737Xr7CJnkumxr-Oe9-ZPjTON-aq2omSHA.png""/></div></figure><h4>EDSR (Enhanced Deep Super-Resolution)</h4><p>‍</p><p><a href=""https://github.com/sanghyun-son/EDSR-PyTorch"">EDSR</a> is a deep learning-based approach that excels in image super-resolution by utilizing residual blocks to efficiently learn high-frequency details. Inspired by the ResNet model, EDSR is specifically tailored for super-resolution tasks, eliminating unnecessary components like batch normalization layers to reduce computational complexity and enhance performance. The use of a large number of filters in each convolutional layer allows EDSR to capture intricate details, resulting in sharp and detailed images. Its ability to handle multiple scaling factors, such as 2x, 3x, and 4x, adds to its versatility across various super-resolution tasks.</p><p>‍</p><p>The deeper architecture of EDSR, with more layers, significantly enhances its capacity to learn and reconstruct fine details. This method has proven highly effective in benchmark evaluations, often outperforming other state-of-the-art approaches in terms of PSNR and SSIM. EDSR's high-quality results, particularly in sharpness and detail preservation, make it a popular choice for applications where image quality is crucial. Its success in producing detailed and clear images has cemented its reputation as a leading algorithm in the field of image super-resolution.</p><p>‍</p><h4>ESPCN (Efficient Sub-Pixel Convolutional Neural Network)</h4><p><a href=""https://github.com/Lornatang/ESPCN-PyTorch"">ESPCN </a>is a fast and efficient algorithm designed to make images bigger, especially useful for smaller enlargements like 2x and 3x. The main idea behind ESPCN is using special layers called sub-pixel convolution layers. These layers help the network learn how to increase the image's resolution directly, making the picture clearer. This method uses less computer power and memory compared to older ways of improving image quality. ESPCN works on the smaller image first and then makes it bigger at the end, which helps keep the picture's details accurate while using less computer power.</p><p>ESPCN is great for situations where speed and efficiency are very important, like streaming videos or processing images quickly. The algorithm works very fast, making it perfect for devices that don't have a lot of processing power. Because ESPCN can quickly and efficiently make high-quality images bigger, it is a popular choice for many real-time applications. Its ability to improve image resolution without using a lot of computer resources makes it very valuable for modern image processing tasks.</p><p>‍</p><h4>FSRCNN (Fast Super-Resolution Convolutional Neural Network)</h4><p>‍</p><p><a href=""https://github.com/yjn870/FSRCNN-pytorch"">FSRCNN </a>is a better and faster version of the earlier SRCNN model, made to speed up the process without making the pictures look worse. It has a smart design with a special part at the beginning that makes the picture smaller so it's easier to work with. Then, it goes through several layers that improve the details and finally, it has a part that makes the picture big and clear again. This way, FSRCNN can work quickly and still make the pictures look really good. It uses smaller parts and more layers to learn the fine details without becoming too big itself, which is great for things like making videos and games look better.</p><p>FSRCNN also includes extra layers at the end to further improve the picture quality. It works directly on low-quality images, which makes it very efficient. The design of FSRCNN ensures that it performs well on different tests, often doing as well as or better than more complicated models. Because it can make pictures look better so quickly, FSRCNN is perfect for situations where both speed and image quality are important. Its improvements over the older SRCNN model show how effective it is for modern tasks that need real-time image enhancement, making it a popular choice for making images clearer and more detailed in live scenarios.</p><h4>LAPSRN (Laplacian Pyramid Super-Resolution Network)</h4><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:850pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6660d7ee7f7cd1dfe494b3d6_AD_4nXf2X8Ucedt-cZGpYJwwtaQrnOR89SY-q2B8gWl7gX5is1APlijJvScgxHyaEj6FJGMnp2o0cmz1pZ6s0Ztf0Ihy_FTfPq_Si_CIT8JvWOHYvD8p08wHD9JE3FSvJC2ER5qB5_76O1fM5PYwkho23YoHg7gX.png""/></div></figure><p>‍</p><p><a href=""https://github.com/Lornatang/LapSRN-PyTorch"">LAPSRN </a>works like a magic microscope for pictures, breaking them down into tiny pieces and then putting them back together to make them look better. It's like taking a blurry photo and turning it into a clear one! This helps LAPSRN make images bigger without losing quality, especially if you want to make them four or eight times bigger. It uses special math tricks called transposed convolutions and residual learning to make the pictures sharp and detailed, like fixing old photos or making satellite images clearer.</p><p>‍</p><p>LAPSRN is really smart because it can handle making pictures bigger by different amounts. It's like having a tool that can work with different sizes of puzzles! When people test LAPSRN, it usually gets really good scores, showing that the pictures it makes are very clear and nice to look at. By slowly fixing up the details in the picture, LAPSRN makes sure that both the big parts and the tiny details look just right in the bigger picture. This makes LAPSRN super helpful for making really big pictures that still look awesome, which is important for lots of cool projects.</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:571pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6660d7ee23741b8f165e0cab_AD_4nXdvQitBdMD790BqXDmITrEZjN2HXwvzvQm4BN-oCv1VBCbn_F8KMnJO_ekKxQolMz_Ap-JPOZHWwBN24QvqVADefzzNjhBnT95_9mNVpJZkOm7a7JFb1Yumog9HRmXuGomWrfB9ORYua9S6FyEkD3FCp6Sd.png""/></div></figure><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:438pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6660d7ef3e04118fd3c59dd8_AD_4nXenoy9gxpnseu4hLuJ131VbQohR87fQVmFowzIkx00XO84cp8qbM5iah1bYTWAnXf9G_AuDIqgbKl503PWFwJsMn_QvYVEQvp8bCHgCj6v8NVieOLAewFnAGeBuZn3Em3q4zuK-_dSKT3JzlLl8fxRpF_eC.png""/></div></figure><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6660d7ee0b872e8d9f3f3ac9_AD_4nXcKl4QSEYWsyMRi55-fXn2B0fLxxfl6N8wle9anpL2fYhn8nTFffDs-7ZyRTh-o0HjRAXDaACJu2x7Ny0ILXzrqVexPoxY706x6sge7cGRI_9PVwtb9m0R2Hs93eqDCly0Su7qBj0p5-TLYFCLa-9AoC8nd.png""/></div></figure><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:542pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6660d7ee778ba8f98e74b96d_AD_4nXdPv1gI8ML761yVC_ZAdBthL8uQR5NFhmcbMwjhzHSE1dtip_Ayt1BxsVAyfh_iYAA1e8j4jGJXw156pfdgUxIKUpVg-YOQ8rMuqMrQnfnuu4xVnKcx6uijG18fu0wJ1WP7Fr0iNEiDq_wKPfALHELMPtmP.png""/></div></figure><h3>Image Upscaling Using Generative Adversarial Network</h3><p>‍</p><p>A <a href=""https://en.wikipedia.org/wiki/Generative_adversarial_network"">Generative Adversarial Network</a> (GAN) is a deep learning architecture. It involves two neural networks, termed the ""generator"" and the ""discriminator,"" that compete against each other. The generator creates new data instances, while the discriminator evaluates them against a real dataset. The goal is for the generator to become so good at producing data that the discriminator can't tell the difference between real and generated data. This process helps generate high-quality and realistic data. A GAN is called adversarial because it trains two different networks and pits them against each other.</p><p>‍</p><h4>ESRGAN Model (Enhanced Super-Resolution Generative Adversarial Networks)</h4><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:850pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6660d7ee0f4c743f69c85d60_AD_4nXdvMHXum4ZlZvC61uDTObZRR-eeDSB0zRmn6ZlllL-ch11gUVjtVXu5c9o5GiVloqFH2yR3_pqwTR6Lk4ZVbhQj2AxZkzo3XTcdrP2QRG5lBu2lTu0MI0-fAfIw9lR93ZBeElVtrRlBQtalVW39s0G4Uf0I.png""/></div></figure><p>‍</p><p>The architecture of ESRGAN is built upon the principles of a typical Generative Adversarial Network (GAN) but includes several key enhancements and modifications tailored for super-resolution tasks. Here's a breakdown of its architecture:</p><p>‍</p><h5>Generator Network</h5><p>‍</p><ul role=""list""><li>Residual-in-Residual Dense Block (RRDB): It is a component in advanced neural networks, particularly in super-resolution models. It consists of several densely connected convolutional layers where each layer’s input is concatenated with its outputs, enhancing feature reuse and information flow. This structure avoids using batch normalization, which helps in preserving the range of features.</li></ul><ul role=""list""><li>Up-sampling Layers: The generator uses up-sampling layers to scale up the low-resolution input to the desired size. In ESRGAN, the up-sampling might be achieved using sub-pixel convolution layers that rearrange the output of a convolutional layer to form a higher-resolution image.</li></ul><ul role=""list""><li>High-quality Image Reconstruction: The output of the last RRDB is passed through a convolution layer to reconstruct the high-resolution image. The ESRGAN generator focuses on enhancing finer details and reducing the blurring effects that are often present in super-resolved images.</li></ul><p>‍</p><h5>Discriminator Network</h5><p>‍</p><ul role=""list""><li>Convolutional Layers: The discriminator uses a series of convolutional layers that progressively downsample the input image, helping it extract various features at different scales.</li></ul><ul role=""list""><li>Leaky ReLU Activation: Leaky ReLU is used instead of standard ReLU to provide a non-linearity that allows gradients to flow through the network even for negative values, enhancing the training stability.</li></ul><ul role=""list""><li>Fully Connected Layers: After processing through convolutional layers, the features are flattened and passed through fully connected layers that finally output a scalar value indicating whether the input image is real or fake.</li></ul><p>‍</p><h4>Code for Image Upscaling using Real-ESRGAN Model</h4><p>‍</p><p>Install the necessary libraries for the ESRGAN</p><div class=""w-embed""><pre>
<code class=""language-py"">
pip install torch Pillow numpy scikit-image
pip install git+https://github.com/sberbank-ai/Real-ESRGAN.git
</code>
</pre></div><p>‍</p><p>Import them,</p><div class=""w-embed""><pre>
<code class=""language-py"">
import torch
from PIL import Image
import numpy as np
from RealESRGAN import RealESRGAN
import time
from skimage.metrics import mean_squared_error, structural_similarity


# Setting the device
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
</code>
</pre></div><p>‍</p><p>The last line sets the device for computation. If CUDA is available (indicating the presence of a GPU), it uses the GPU for faster computation; otherwise, it falls back to the CPU.</p><div class=""w-embed""><pre>
<code class=""language-py"">
# Load the model
model = RealESRGAN(device, scale=4)
model.load_weights('weights/RealESRGAN_x4.pth', download=True)


# Path to the input image
path_to_image = 'path to your image'
image = Image.open(path_to_image).convert('RGB')


# Run the model
sr_image = model.predict(image)


# Save the output image
sr_image.save('output path to your image')
</code>
</pre></div><p>‍</p><p>These lines initialize the RealESRGAN model with the specified device and a scaling factor (scale=4 which means the output image will have four times the resolution of the input). It then loads the pre-trained weights from a specified path, with an option to download the weights if they're not present locally.</p><h3>Image Upscaling Using Stable Diffusion Models</h3><p>‍</p><p><a href=""https://en.wikipedia.org/wiki/Stable_Diffusion"">Stable Diffusion</a> is a type of generative artificial intelligence (generative AI) model that specializes in creating detailed images from textual descriptions. It utilizes a variant of the diffusion model, which gradually transforms patterns of random dots into detailed images through a reverse process that removes noise over many steps. The model is based on diffusion technology and uses latent space.</p><p>‍</p><h4>The architecture of Stable Diffusion Models</h4><p>‍</p><figure class=""w-richtext-align-center w-richtext-figure-type-image""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6660d7eee2b259c929824af1_AD_4nXcw6pY9c4ugDdCtUYdLqNPFoS_H6sPRv6uZGbPghm7wpp2OsuI39dabTFTxurbz1LudiIGqjppP0oqi1x816y0L-CwRK9ZhfzyNVzhe-AqS10oWi_l0WnbPdgj5OBOe_fS7xzrO5TlXI93WZx18J-lJKKv2.png""/></div></figure><p>‍</p><p>The main architectural components of Stable Diffusion include a variational autoencoder, forward and reverse diffusion, a noise predictor, and text conditioning.</p><p>‍</p><h5>Variational AutoEncoder</h5><p>‍</p><ul role=""list""><li>Encoder: This part of the model takes a large image (512x512 pixels) and compresses it down to a smaller, more manageable size (64x64 pixels) in something called latent space. Latent space is a compressed representation that's easier for the model to work with.</li><li>Decoder: The decoder does the opposite of the encoder. It takes the compressed image from latent space and enlarges it back to its original size, restoring the details as much as possible.</li></ul><p>‍</p><h5>Forward and Reverse Diffusion</h5><p>‍</p><ul role=""list""><li>This process gradually adds random noise to an image until it turns into pure noise. Essentially, it transforms the image into something unrecognizable. This is mainly used during training and sometimes in image-to-image conversions where an initial image is transformed into a different style or appearance.</li></ul><p>‍</p><ul role=""list""><li>Reverse diffusion is the process that turns the noisy image back into a clear picture. It works by estimating and removing the noise added during the forward diffusion step by step, eventually revealing a detailed image that can be a cat, a dog, or any other subject defined by the training data.</li></ul><p>‍</p><h5>Noise Predictor</h5><p>‍</p><p>Stable Diffusion uses a special kind of neural network called U-Net, which is particularly good at removing noise from images. It predicts how much noise needs to be removed at each step of the reverse diffusion process to reveal the final image gradually.</p><p>‍</p><h5>Text Conditioning</h5><p>‍</p><p>Stable Diffusion uses text prompts to guide image generation. Each word in the prompt is converted into a numerical format using a tokenizer, which the model understands. These numbers tell the model what features and elements to include in the image, like colors, objects, or styles.</p><p>‍</p><h4>Code for Image Upscaling using Stable Diffusion x4 upscaler Model</h4><p>Install these libraries</p><div class=""w-embed""><pre>
<code class=""language-py"">
!pip install diffusers transformers accelerate scipy safetensors
</code>
</pre></div><p>‍</p><p>Setup the pipeline,</p><div class=""w-embed""><pre>
<code class=""language-py"">
import requests
from PIL import Image
from io import BytesIO
from diffusers import StableDiffusionUpscalePipeline
import torch

# load model and scheduler
model_id = ""stabilityai/stable-diffusion-x4-upscaler""
pipeline = StableDiffusionUpscalePipeline.from_pretrained(model_id, torch_dtype=torch.float16)
pipeline = pipeline.to(""cuda"")
</code>
</pre></div><p>‍</p><p>The model_id is a string that uniquely identifies the model on Hugging Face's model hub, in this case, ""stabilityai/stable-diffusion-x4-upscaler"", which specifies a version of the Stable Diffusion model specifically trained to upscale images by a factor of four. The method StableDiffusionUpscalePipeline.from_pretrained is used to load this model. </p><p>‍</p><p>Here, it is initialized with a specific configuration to use 16-bit floating-point precision (torch.float16), which is a strategy to reduce memory usage, allowing the model to run faster and more efficiently on compatible hardware. The pipeline.to(""cuda"") command then shifts the model’s computations to a GPU, assuming one is available and CUDA-compatible. This significantly accelerates the processing speed, leveraging the GPU's ability to handle parallel computations, which is ideal for the intensive calculations required in upscaling images using deep learning models.</p><p>‍</p><div class=""w-embed""><pre>
<code class=""language-py"">
path = 'path to your image'
low_res_img = Image.open(path).convert(""RGB"")
low_res_img = low_res_img.resize((128, 128))

prompt = ""prompt describing your image""


upscaled_image = pipeline(prompt=prompt, image=low_res_img).images[0]
upscaled_image.save(""path to your output image"")
</code>
</pre></div><p>‍</p><p>Define a prompt to guide the model in upscaling the image. This can be used to describe the content of the image or provide additional context to influence the upscaling process.</p><h3>Quantitative Comparison Between Upscaling Models</h3><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1136pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6660d7ee37e19cc871ef380d_AD_4nXchVrK7Tx0fCOzxq7BgxVSu8JM1uC-od2OT_h87Pa9djCyO0Ws3GLprqEmR2n9avU82wzIKzBHa0iAFrvsWhSxxgPVxYMxbO7E_Yaz2i5pUtMCm5-azY5opmS505DmLesNg8pQK8X90bw3vBluUKgury7zl.png""/></div></figure><p>‍</p><p>Image Upscaling Models used for Benchmarking are <a href=""https://github.com/XPixelGroup/DiffBIR"">DiffBIR</a>, <a href=""https://github.com/zsyOAOA/ResShift"">ResShift</a>, <a href=""https://github.com/Fanghua-Yu/SUPIR"">SUPIR</a>, and <a href=""https://github.com/xinntao/Real-ESRGAN"">RealESRGAN</a>.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6660d7eebfc9b07b9991e056_AD_4nXfyGkN3zgcBBlto7j6GxovTG2O-yTAbZ7tA8Zc1sPrIPg5adNzssZ4JyNgWgaYnxPMg6CnGzbx7Gfb9xB--onTqsMEgOor09WW58rRg3cUpjEqJKp04x3TW6MG05awhxeWjj2t8VnLyHwNGd4pmEPoGQ2tc.png""/></div></figure><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6660d7eed4582b981b394d06_AD_4nXfHKupYCzdpM_dMYjxY_KYGEzMGUgmfvq8g0YhYKsGnGKlj5SsFiUqNlxa8bsvkj7KUzecxzjdYLYywVjdMlXPPQLowtOUNdQu4nV--v1HKShT6OLVa8vNSxN95iY-LgA-i2X_KyAqDTsZRI4Py2WnIZxHj.png""/></div></figure><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6660d7ee3e04118fd3c59db8_AD_4nXeX0kKbXBKeHJZOV-x0Uq03ZoLzjICPtApLfBH7A7bm08mJINc4NRYPd3o2hwloqkfPxN5to36-V0DrtUWbaaenQcjo-l_tb-t_ONqm_dWBRVVsLOZ_PsnDpAPbRyDYd7lItxK-Hq_WZKbe059WEbiPTIJk.png""/></div></figure><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:620pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6660d7ee5c4df66ea68c3164_AD_4nXf_wUKhX22oqguzkFc03jqtH0xLQztsAkxh2uVmf8AstDKQR-xUyLNYwUeK0J3HZPtHXzLpZZKyZiwKxGlz2-Lbm8k7IoiZCzGBylASUWF4Eej0E4V2h3orjccYcsDO2YkqPzUm945WmLy7E78OA0BIt6oI.png""/></div></figure><p>‍</p><p>For the detailed code Refer to <a href=""https://github.com/Mercity-AI/Image-Upscaling-Benchmark/tree/main"">Github Link</a>.</p><p>‍</p><h3>Comparison between Image Upscaling Models on Further Downscaling</h3><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:711pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6660d7ee3330567a869698b8_AD_4nXffxcvK3g_sZYY--pyVHMSmFBCh0UPZWXjVXXLDikePviWFejjeyZ_R2R_mx2qXME4yyEzd6iFkTj0bQrvfD1MiWkBb5NU7Hpv5eChSte53eQxEImSKvs0Vc44pubHjR6VxUFtwJoecWmZTMBMS1uM194f8.png""/></div></figure><p>‍</p><p>The task involves comparing the effectiveness of different image upscaling models through two distinct processes. In the first process, an image is downscaled by a factor of 2 and then upscaled back to its original size using various image upscaling models. This procedure evaluates the ability of the models to recover the original image details from a slightly reduced version. By doing so, it tests the models' proficiency in handling minimal information loss and restoring the image's quality effectively.</p><p>‍</p><p>In the second process, the image undergoes a more aggressive downscaling by a factor of 8, followed by a two-step upscaling process. First, the image is upscaled by a factor of 4, and then it is further upscaled by a factor of 2 using different upscaling models. This approach simulates a more challenging scenario where significant information is lost during the initial downscaling, and the models must perform two stages of upscaling to restore the image to its original dimensions. The final upscaled images from both processes are then compared to the original image using metrics such as Mean Squared Error (MSE), Structural Similarity Index (SSIM), and processing time. This comprehensive evaluation helps determine which upscaling models are more efficient and effective in terms of image quality restoration and computational efficiency.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6660d7eebbfad0e37b361dd0_AD_4nXfmoIONNwlheY8lJlmR4I5NshoLK3dBkUqKtJlDBCIZQYNh-mMB9McdUOeP9g7QA5A-r68Y3od5BhdmSWlO3UQrLsiPn8RmYp-8l8KN_yWKApn5kYi0a0gJLI2LgEg4G3ZudrPVw0XGZJr6UHbq0QBbZkw.png""/></div></figure><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6660d7ee2ee15090b1613b2f_AD_4nXeR7nxVniPYiwenZotw7HZUHXlGauUvYlv8bldtch9jYdhiokOVZuR0I5J0x9D7ugzz_UZGARnJ1fXpEQJ9DMK_sx2FiQbpU0cQF1lxmN7HY05sVAT26FnpNeTWHRF8Mel7SNEP1Pqs4I4sZgZM1WkLS5PF.png""/></div></figure><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6660d7ee707f912094336520_AD_4nXeN_NmFN4icWJ0tQL_DpqnSYMwIsfEVwrWE88RYipDGCs3ymzyxzcmnAh2m4IMFe5FfpSzEUeSVlLoeeBrNHAUjshlX0mYUxinItSVMNrv3XYtaQ-MGAQ-qqQ4mDLB1YqHFNS6RR1qaht_ha8fK7SknW7c.png""/></div></figure><h2>Ready to Bring Your Images Into Stunning High-Definition</h2><p>‍</p><p>Experience the magic of high-resolution with Mercity AI! Our expert team specializes in transforming low-resolution images into stunning, high-definition visuals. Elevate the quality of your photos and graphics with our advanced upscaling technology. Ready to see the difference? <a href=""https://www.mercity.ai/"">Contact us</a> today and bring your images to life like never before!</p><p>‍</p></div>"
A Comprehensive Comparison Of Open Source LLMs,comprehensive-comparison-of-llms-8-2023,640f56f76d313b2faa631c11,64d7c8d2b7acfdbdeefb2602,False,False,Sat Aug 12 2023 18:00:50 GMT+0000 (Coordinated Universal Time),Sat Aug 12 2023 18:09:04 GMT+0000 (Coordinated Universal Time),Sat Aug 12 2023 18:09:04 GMT+0000 (Coordinated Universal Time),"<p id="""">In the last year, we have seen remarkable growth in the capability of open-source LLMs due to several factors, including the increasing availability of data, the development of new training techniques, and the growing demand for AI solutions. Their transparent, accessible, and customizable ability makes them a perfect alternative for closed-source LLMs such as GPT-4.&nbsp;</p><p id="""">‍</p><p id="""">One of the challenges with open-source LLMs is no agreed-upon evaluation criteria. It makes it difficult to compare and choose a model for a particular task. However, several benchmarking techniques have come forth, such as MMLU and ARC, to evaluate the performance of open-source LLMs on various tasks.</p><p id="""">‍</p><p id="""">In this article, we will analyze different open-source LLMs to help you understand and choose a model for your needs.</p><h2 id="""">Open Source Vs Closed Source LLMs</h2><p id="""">With open-source LLMs (Large Language Models), individuals can build custom models tailored to specific tasks and domains. These models remove the entry barrier in AI. Organizations and researchers can train their LLMs, deploying them on personal PCs.&nbsp;</p><p id="""">‍</p><p id="""">Businesses can use open-source LLMs to design models that securely reside on internal servers, evolving performance through continuous refinement and specialization. It grants businesses control, amplifying their AI efforts and internal capabilities.</p><p id="""">‍</p><p id="""">The public availability and involvement in open-source LLMs promotes experimentation and innovation. Platforms like Hugging Face, facilitate the availability of pre-trained models and NLP tools to help people evaluate and work on their and other state-of-the-art models.</p><p id="""">‍</p><p id="""">However, open-source LLMs are still not at par with closed-source LLMs like GPT-4. It is because closed-source LLMs can be trained on more data and with more computational resources. Despite their current limitations, they are likely to close the gap in their performance.</p><h2 id="""">Exploring The Architecture Of LLMs</h2><p id="""">LLMs use a transformer architecture consisting of a stack of neural networks trained to predict the next word in a sequence. Transformers are based on self-attention mechanisms, which allow the model to focus on specific parts of the input sequence. It has a significant impact on a model’s performance and capabilities. Hence, understanding their architecture is essential for their efficient use.</p><h3 id="""">Encoder Only</h3><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:929px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""929px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64d7c0bcb7acfdbdeef5fe97_hyjyQgsXIPQDTA9xGJNIwgjopH4co5Xm-W6dhvU0MGqT9sk3PUmb5c_FRiUfrkIhmSU78PvLlMjNte1vQyulolX9XldxpQRHX4Ra2MuZYuuOnca2N6I7SahpMuZUGRAFfhLX-4Rl294_xrFSKshsDbw.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">Figure from <a href=""https://pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference/"" id=""""><em id="""">A BetterTransformer for Fast Transformer Inference</em></a></p><p id="""">‍</p><p id="""">Encoder models are systems that use only the encoder of a Transformer model. Their attention layers access all words in the input sentence and use bidirectional context to make predictions. They pre-train with masked language modeling (MLM), which masks a portion of the input tokens and asks the model to predict them. It helps the model understand the context of words in a sentence. Fine-tuning the text representations is helpful for downstream tasks like classification, question answering, and named entity recognition. Some examples of encoder-only models include BERT and ELECTRA.</p><p id="""">‍</p><p id="""">Encoder-only models are capable of learning long-range dependencies between words in a sentence. It helps understand the context of a sentence and make accurate predictions. They can represent sentence meanings in a vector space. It further aids the comparison of sentences and making predictions. They are efficient to train and deploy for large classification tasks. Hence, they are for sentence classification, named entity recognition, and extractive question answering.</p><h3 id="""">Encoder-Decoder</h3><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1076px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1076px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64d7c0bccfc4c0f0e75c65a0_JN5AoMwpRmUaaPFxkfP3IzZJ2dZ1Jlda4aqidNwaMn4WfUk9H-6q0mBK2e0MQj6hcz_5ltKXo5Qu41dUULFUijbYzja556xiGHt_Iy2M_hj2klEomVTyEei_CpAYRynUabXTYrWkuHhuFyMY_NZ4Nog.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">Figure from <a href=""https://d2l.ai/chapter_attention-mechanisms-and-transformers/transformer.html#encoder"" id=""""><em id="""">Dive Into Deep Learning</em></a></p><p id="""">‍</p><p id="""">Encoder-decoder models are bidirectional models that pre-train on corrupted spans of text. The encoder generates the hidden representation as a summary of the input sequence. It is then passed to the decoder to generate the output. The advantage of having both the encoder and the decoder makes it effective against complex inputs. Span corruption helps the model to learn the context of words in a sentence, making it effective on tasks such as question answering, summarization, and translation. Some examples of encoder-decoder models are BART and Megatron.</p><p id="""">‍</p><p id="""">Encoder-decoder models are mainly used for sequence-to-sequence predictions, such as text summarizations and question answering. It is capable of solving machine translation problems. These are good for generative tasks but are also computationally expensive. It is because it requires both an encoder and a decoder. Their popularity is increasing due to decreasing computing power costs.</p><h3 id="""">Decoder Only</h3><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:975px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""975px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64d7c0bcab17a008287895db_5neYLLzUcX9uLTfzxP3-0vWTXEsGtgdEV-McTlyRugJaeDiGF05lBZ6piDrWcyjARt8HjbnwkuDQFXpsLOqVgf74BIQN72ZCGpyv9vXKq1J_3oT0OfX5JPbIUoVVE0TnwQMmAUhwDUq2HfY0e-Uf-bI.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">Figure from <a href=""https://www.researchgate.net/publication/364435819_A_Transformer-based_Generative_Model_for_De_Novo_Molecular_Design"" id=""""><em id="""">A Transformer-based Generative Model for De Novo Molecular Design</em></a></p><p id="""">‍</p><p id="""">Decoder-only models are auto-regressive models that use only the decoder of a transformer architecture. The architecture lacks an encoder. It implicitly encodes the information in the hidden state and updates it during output generation. They are pre-trained to predict the next word in a sentence, making them commonly used models for text generation. When fine-tuned, they can be used for downstream tasks by using a classifier over the last token hidden representation. Examples of decoder-only models include GPT models, transformer XL, and LLaMA.</p><p id="""">‍</p><p id="""">Decoder-only models generate one word at a time. They take into account the context of already generated words to generate the next word. These models are for text generation purposes, including text completion, and question-answering. They can be used for machine translation or in chatbots. They are capable of learning through feedback, thus, can improve over time.&nbsp;</p><h2 id="""">Comparison Of Different Open-Source LLMs</h2><p id="""">Here is an in-depth comparison of various open-source LLMs, focusing on their structures, training methods, and performance.</p><h3 id="""">LLaMA</h3><h4 id="""">Overview</h4><p id=""""><a href=""https://arxiv.org/abs/2302.13971"" id="""">LLaMA</a> is a family of large language models (LLMs) with a decoder-only architecture and bidirectional context. It was developed by Meta AI and released in February 2023. LLaMA models range in size from 7 billion to 65 billion parameters. LLaMA 1 models were trained on a dataset of 1.4 trillion tokens, while <a href=""https://ai.meta.com/llama/"" id="""">Llama 2</a> models were trained on a dataset of 2 trillion tokens.&nbsp;</p><p id="""">‍</p><p id="""">The latter was released in three model sizes: 7, 13, and 70 billion parameters. It also has some architectural tweaks, such as Grouped Query Attention to make inference more efficient. It uses a vanilla multi-head attention mechanism. But, it can be fine-tuned with reinforcement learning to improve its performance on specific tasks. It outperforms many LLMs like MPT but still lags behind GPT-4. It is a developing model that is effective in various tasks, such as machine translation, and code generation. It is perhaps the best open-source model in terms of capability out there right now.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:889px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""889px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64d7c0bc6beb0efabb8dc778_y5cGPd6q24H8KDJ9JFL_JZU6rrBNlsdNnsg9qcn7aOTdz7BXPrftGfEhO0pkn7bLriXoYdG6ynnC7rSJeHzhgn8xAoAxDbI9znQ7f-I05IdP10B0jcB3fT3oY0HR3v_w7v1DFLsOnRqML1S5Mo3M1BI.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">Figure by <a href=""https://ai.meta.com/llama/"" id=""""><em id="""">Meta AI</em></a></p><p id="""">‍</p><p id="""">The Llama 1 license was for non-commercial use, while the Apache 2.0 license allows for commercial use with some restrictions. Meta forbids Llama 2’s usage to train other models. And if Llama 2 is used in an app or service with more than 700 million monthly users, a special license must be obtained from Meta AI. This means that the architecture can be studied and modified, and the code can be used to create new models. However, the weights of the model, which contain the learned parameters, are not publicly available. The <a href=""https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T"" id="""">RedPajama</a> project reproduces and distributes an open-source version of the LLaMA dataset.</p><p id="""">‍</p><p id=""""><a href=""https://github.com/openlm-research/open_llama"" id="""">OpenLLaMA</a> is an open-source reproduction of the LLaMA model. It provides researchers and developers an accessible and permissively licensed large language model. It is a 7B model trained on 200 billion tokens of the RedPajama dataset. It is significant because of its public availability. Also, <a href=""https://github.com/ggerganov/llama.cpp"" id="""">llama.cpp</a>, a port of the LLaMA model in C and C++, enhances the model's capabilities by supporting CUDA acceleration with GPUs.</p><h4 id="""">Performance Insights</h4><p id="""">Developers of LLaMA reported that the 13 billion parameter model outperformed the considerably larger <a href=""https://arxiv.org/abs/2005.14165"" id="""">GPT-3</a> (175 billion parameters) across numerous NLP benchmarks. This performance excellence extends to competing with advanced models like PaLM and Chinchilla, showcasing LLaMA's proficiency in diverse language-related tasks.</p><p id="""">‍</p><p id="""">LLaMA 65B model has shown good capability in most use cases. It has an ARC rating of 63.48, posing a psychometric intelligence rate more than that of Falcon-40B. Also, LLaMA 2 ranks among the top 10 models in <a href=""https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard"" id="""">Open LLM Leaderboard</a> on Hugging Face.</p><h4 id="""">Use Cases And Applications</h4><p id="""">LLaMA is used for text generation tasks, including text summarization for condensing content-rich texts while preserving vital information. It can enhance sentences and paragraphs via natural language processing techniques, exceeding GPT-3. Its open-source nature and <a href=""https://python.langchain.com/docs/integrations/llms/llamacpp"" id="""">llama.cpp</a> compatibility empowers users to explore, customize, and deploy the model as per specific requirements.</p><p id="""">‍</p><p id="""">Numerous models are constructed using LLaMA. For example, Alpaca is built on the LLaMA 7B model. Many open-source projects are continuing the work of optimizing LLaMA with the Alpaca dataset.</p><p id="""">‍</p><p id="""">Llama 2 includes both foundational models and models fine-tuned for dialog, called Llama 2 - Chat. They parallel closed-source counterparts like <a href=""https://arxiv.org/ftp/arxiv/papers/2302/2302.13817.pdf"" id="""">ChatGPT</a> and PaLM. Llama 2 and Llama 2 - Chat have the same context length of 4K tokens as opposed to GPT-4, which increased context length during fine-tuning.</p><h3 id="""">Falcon</h3><h4 id="""">Overview</h4><p id=""""><a href=""https://arxiv.org/abs/2306.01116"" id="""">Falcon</a> is an open-sourced large language model (LLM) developed by the Technology Innovation Institute (TII). It is available under the <a href=""https://www.apache.org/licenses/LICENSE-2.0"" id="""">Apache 2.0</a> license, meaning you can use it commercially. It has two models, Falcon-7B, and Falcon-40B, trained on 1.5 trillion and 1 trillion tokens, respectively.&nbsp;</p><p id="""">‍</p><p id="""">Falcon was trained on a massive English web dataset called <a href=""https://arxiv.org/abs/2306.01116"" id="""">RefinedWeb</a>, built on CommonCrawl. RefinedWeb is a high-quality dataset deduplicated and filtered to remove machine-generated text and adult content. Models trained on RefinedWeb perform better than the ones trained on curated sets. Falcon required 384 GPUs on AWS over two months. Falcon was built using custom tooling and leverages a unique data pipeline that can extract high-quality content from web data and use it to train a custom codebase independent from the works of NVIDIA, Microsoft, or HuggingFace.</p><h4 id="""">Performance Insights</h4><p id="""">Falcon matches the performance of state-of-the-art models by DeepMind, Google, and Anthropic. The Falcon-40B model is currently among the top in the Open LLM Leaderboard, and the Falcon-7B model is also among the best in its weight class. Falcon models use a unique multi-query attention mechanism that is more efficient than the vanilla multi-head attention scheme. This makes Falcon more efficient than other LLMs, requiring only 75 percent of GPT-3's training compute, 40 percent of Chinchilla's, and 80 percent of <a href=""https://arxiv.org/abs/2204.02311"" id="""">PaLM-62B's</a>.</p><h4 id="""">Use Cases And Applications</h4><p id="""">The Falcon LLM can work on various tasks, including generating creative content, solving complex problems, customer service operations, virtual assistants, language translation, sentiment analysis, and automating repetitive work. It is trainable in English, German, Spanish, French, Italian, Portuguese, Polish, Dutch, Romanian, Czech, and Swedish languages. The <a href=""https://huggingface.co/tiiuae/falcon-40b-instruct"" id="""">Falcon-40B-Instruct</a> model is fine-tuned for most use cases, including chatbots. <a href=""https://huggingface.co/tiiuae/falcon-40b"" id="""">Falcon-40B</a> requires ~90GB of GPU memory, while Falcon-7B only needs ~15GB. It makes Falcon-7B accessible even on consumer hardware.</p><h3 id="""">Vicuna</h3><h4 id="""">Overview</h4><p id=""""><a href=""https://arxiv.org/abs/2306.05685"" id="""">Vicuna</a> is an open-source LLM developed by LMSYS that is built from LLaMA. It is fine-tuned on a dataset of 70,000 user-shared conversations from ShareGPT. Vicuna is an auto-regressive LLM trained on 33 billion parameters and achieves more than 90% of ChatGPT's quality in user preference tests while vastly outperforming Alpaca. The training cost of <a href=""https://huggingface.co/eachadea/legacy-vicuna-13b"" id="""">Vicuna-13B</a> is around $300 because the <a href=""https://sharegpt.com/"" id="""">ShareGPT</a> data is freely available. The code, weights, and an online demo are publicly available for non-commercial use. Vicuna can be run on modest hardware using llama.cpp.</p><h4 id="""">Performance Insights</h4><p id="""">A preliminary evaluation of Vicuna-13B, using <a href=""https://openai.com/research/gpt-4"" id="""">GPT-4</a>, showed that it achieved more than 90% of the quality of OpenAI, ChatGPT, and Google Bard, while outperforming other models like LLaMA and Stanford Alpaca. In benchmark tests, Vicuna showed significantly more detailed and better-structured answers than Alpaca after fine-tuning with ShareGPT data.</p><p id="""">‍</p><p id="""">In <a href=""https://lmsys.org/blog/2023-06-22-leaderboard/"" id="""">LMSYS’s MT-Bench</a> test, Vicuna scored 7.12, while the best proprietary model, GPT-4, secured 8.99 points. Also, in the MMLU test, it achieved 59.2 points, and GPT-4 scored 86.4 points. These results suggest that Vicuna is a promising LLM that can compete with the best proprietary models.</p><h4 id="""">Use Cases And Applications</h4><p id="""">Vicuna can perform various tasks, such as generating text, translating languages, and writing creative content. It can also generate text that is both coherent and informative, and it can hold conversations that are engaging and natural. On the contrary, it has known issues, such as weaknesses in reasoning and math, and can produce hallucinations. Nonetheless, Vicuna has the potential to be a valuable tool for research and development in the field of AI.</p><h3 id="""">MPT</h3><h4 id="""">Overview</h4><p id=""""><a href=""https://www.mosaicml.com/blog/mpt-7b"" id="""">MPT</a> (MosaicML Pretrained Transformer) is a commercial open-source decoder-only transformer model developed by MosaicML. It is trained on a massive dataset of text and code of 1T tokens from various sources, including OpenLLaMA, StableLM, and Pythia. The model has a context length of 8K tokens and 6.7B parameters. It is available in three fine-tuned versions:&nbsp; MPT-7B-StoryWriter-65k+, MPT-7B-Instruct, and MPT-7B-Chat.</p><p id="""">‍</p><p id=""""><a href=""https://huggingface.co/mosaicml/mpt-7b-storywriter"" id="""">MPT-7B-StoryWriter-65k+</a> is trained on a filtered fiction subset of the books3 dataset and licensed under the Apache-2.0 license. <a href=""https://huggingface.co/mosaicml/mpt-7b-8k-instruct"" id="""">MPT-7B-Instruct</a> is trained on Databricks Dolly-15k and Anthropic's Helpful and Harmless datasets. <a href=""https://huggingface.co/mosaicml/mpt-7b-chat"" id="""">MPT-7B-Chat</a> is built on the ShareGPT-Vicuna, HC3, Alpaca, Helpful and Harmless, and Evol-Instruct datasets.</p><h4 id="""">Performance Insights</h4><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1531px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1531px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64d7c0bd5b9d53be4e0f7d6c_GQeOY-OYWtoXv3XfAvs1hCW8zuSwA0LOELlTUJOZ1QOBCVeVtIxrQ2upY7r-iDCrmhfW2HGwJuG4_4V5mLb6hVwK2YYTFVd7DeZx4fvFZkRuLtssPnLmNnYo9-G2B9fGbbvDGogRxN3NHFIJO3LMWsM.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">Figure by <a href=""https://www.mosaicml.com/blog/mpt-7b"" id=""""><em id="""">MosaicML</em></a></p><p id="""">‍</p><p id="""">MPT model, initially developed to counter the limitations to other open-source models, outperforms the GPT-3. It scores 6.39 in LMSYS’s MT-Bench test. These models leverage FlashAttention and FasterTransformer for speed performance. MPT-7B is as competent as <a href=""https://huggingface.co/huggyllama/llama-7b"" id="""">LLmMA-7B</a> and exceeds other open-source 7B-20B models. MPT is trained for long inputs up to 64k and can handle to 84k with the help of <a href=""https://arxiv.org/abs/2108.12409"" id="""">ALiBi</a>. It is optimized for fast training and inference and has highly efficient open-source training code.</p><h4 id="""">Use Cases And Applications</h4><p id="""">MPT-7B-StoryWriter-65k+ can read and write fictional stories with long context lengths. It can create creative text formats like poems, code, scripts, musical pieces, email, and letters. The model can generate as long as 84k tokens on a single node of A100-80GB GPUs. MPT-7B-Instruct follows short-form information. It generates instructions. It is capable of informative question answering. MPT-7B-Chat, chatbot-like model, is for dialogue generation, helping engaging in coherent conversations. Also, if you need a small local running LLM, consider the <a href=""https://huggingface.co/mosaicml/mpt-30b"" id="""">MPT-30B</a> model.</p><h3 id="""">Guanaco</h3><h4 id="""">Overview</h4><p id=""""><a href=""https://huggingface.co/timdettmers/guanaco-65b"" id="""">Guanaco-65B</a> is a non-commercial open-source LLM based on the LLaMA 7B model. It uses the <a href=""https://arxiv.org/abs/2305.14314"" id="""">QLoRA</a> 4-bit fine tuning method, efficiently reducing memory usage while preserving full 16-bit task performance. It allows Guanaco-65B to be trained on a single GPU with 48GB of VRAM in just 24 hours. Guanaco is available in four models: 7B, 13B, 33B, and 65B. All of the models have been fine-tuned on the<a href=""https://huggingface.co/datasets/OpenAssistant/oasst1"" id=""""> OASST1 dataset</a>. The limitations of Guanaco-65B include its slow 4-bit inference and lack of math capabilities.</p><h4 id="""">Performance Insights</h4><p id="""">Guanaco-65B outperforms even ChatGPT (GPT-3.5 model) with a much smaller parameter size on the Vicuna benchmark. In the <a href=""https://arxiv.org/abs/2009.03300"" id="""">MMLU</a> test, Guanaco-33B scored 57.6 and Falcon scored 54.7. Similarly, in the MT-Bench evaluation, Guanaco stands at 6.53 and Falcon at 5.17. <a href=""https://huggingface.co/timdettmers/guanaco-7b"" id="""">Guanaco-7B</a> model requires only 5 gigabytes of GPU memory and exceeds the 26-gigabyte Alpaca model by more than 20 percentage points on the Vicuna benchmark.</p><p id="""">‍</p><p id="""">Guanaco-65B achieves above 99 percent of the performance of ChatGPT (GPT-3.5-turbo) in a benchmark run with GPT-4. Guanaco-33B reached 97.8 percent of ChatGPT's performance with 33 billion parameters in a benchmark while training it on a single consumer GPU in less than 12 hours. Guanaco-65B could achieve 99.3 percent of ChatGPT's performance in 24 hours on a professional GPU.</p><h4 id="""">Use Cases And Applications</h4><p id="""">Guanaco is an advanced instruction-following language model trained on English, Chinese, Japanese and Deutsch. It can work in multilingual environments and extend to meet various linguistic contexts. Its <a href=""https://huggingface.co/blog/4bit-transformers-bitsandbytes"" id="""">QLoRA</a> 4-bit fine tuning method makes Guanaco a good option for researchers and developers who need a powerful LLM that can be used offline or on mobile devices. Its uses include text summarization, question answering, finetuning and chatbots. Since it is still in development, it also makes a good model for experimentation.</p><h2 id="""">LLaMA 2 Vs Falcon</h2><p id="""">Llama 2 has three model sizes: 7B, 13B, and 70B. It was trained on 2 trillion tokens, including over 1 million human annotations. It employs Reinforcement Learning from Human Feedback (RLHF) specifically for Llama-Chat-2's fine-tuning. According to Meta, Llama 2 outperforms other LLMs, including Falcon and MPT, in the areas of reasoning, coding, proficiency, and knowledge tests. Llama 2-7B scored 54.32 while Falcon-7B gained 47.01 on Open LLM Leaderboard. However, it is less efficient than Falcon, requiring more training compute.</p><p id="""">‍</p><p id="""">Falcon has two models, Falcon-7B and Falcon-40B. It was trained on 1.5 trillion and 1 trillion tokens, respectively. Falcon uses a unique multi-query attention mechanism that is more efficient than the vanilla multi-head attention scheme used by Llama 2. It makes Falcon more efficient than Llama 2, but it is not as strong on some tasks as Llama 2.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1089px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1089px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64d7c0bcb928cf4cb9b2b0ff_rTDFn-CVsICw0sOi9lvWJun6OgJzHMLwSAGstJYkKpFrMgFE7gAq8kdnE1cG90gWSNG85RNJlXRaK4io-br80kf_PGyGf6-PVJxNaXrtGABhi8Ile41cDT7dg1Ahui3dNNoTtXvERCsYpJdD7RTxnj4.jpeg"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">Comparison Table by <a href=""https://huggingface.co/blog/llama2#why-llama-2"" id=""""><em id="""">Hugging Face</em></a></p><h2 id="""">Choosing An Open Source LLM</h2><p id="""">Here are some factors that will help you narrow down your choices to choose the right open-source LLM for your needs.</p><h3 id="""">Technical Requirements</h3><p id="""">Assess your hardware and infrastructure capabilities. Consider the technical factors of an open-source LLM, such as model size, compute power and storage capacity. You might need to upgrade existing resources or consider cloud-based solutions. A model should be scalable to meet the real-world requirements of increasing demands. It is also necessary to consider the specific use case of the model. For example, a Falcon model may be good for language translation and automating repetitive work. If the business needs an LLM for both online and offline use, a Guanaco model may be a better option.</p><h3 id="""">Integration Ability</h3><p id="""">Consider an open-source LLM that can integrate with your business systems. Check its compatibility with programming languages, frameworks, and APIs used in the ecosystem of your enterprise. It can save organizations time and effort and ensure the model works effectively. For example, Megatron-Turing NLG is a good option for enterprises using Python, TensorFlow, or PyTorch. It is also compatible with the Hugging Face Transformers library. It makes it easy to integrate into existing software applications.</p><h3 id="""">Licensing</h3><p id="""">Businesses should understand the licensing terms of the open-source LLM they select. They must be clear about the license type, compatibility with other licenses, permitted usage, modifications, and distribution requirements. For example, Megatron-Turing NLG has a GPLv3 license. It allows the distribution of its modified version under the same license. But h2oGPT is under Apache 2.0 license. It permits modifications and commercial usage but forbids distribution under the same license. If you need a commercial license with no limitations, consider Falcon models. A brief review of the license guarantees alignment, IP respect, and legal compliance.</p><h3 id="""">Adaptability</h3><p id="""">The adaptability of an open-source LLM refers to its ability to perform various tasks and how accurate its output is for those tasks. This adaptability depends on the model's design, training data, documentation, and community. For example, Vicuna is adaptable in generating coherent creative content but lags in math. LLaMA, on the other hand, is accurate in common sense inference but average in the level of truthfulness in generating answers to questions. Also, MPT is a good model for long text generation due to its context length of 8K.</p><h2 id="""">The Advantages Of Open Source LLMs</h2><p id="""">Open-source LLMs offer several advantages over closed-source LLMs, including cost-effectiveness, flexibility, security, and reduced dependency on external providers.</p><h3 id="""">Customization</h3><p id="""">Open-source LLMs are a flexible alternative to proprietary LLMs. They are freely available to use and modify with no recurring costs. Organizations can deploy them on-premises and have more flexibility to customize the model to their needs. They can add specific features to open-source LLMs unavailable in the default version. Open-source LLMs have a large and active community of users and developers. It can help with troubleshooting and development.</p><h3 id="""">Cost Efficiency</h3><p id="""">Open-source LLMs offer a cost-efficient alternative to proprietary software. They save costs on licenses, subscriptions, and hidden charges. Hosted LLMs have usage fees that can add up. Open-source LLMs do not have such recurring payments. Once acquired, organizations can deploy them without worrying about any expenses. Hence, they are great for startups, small businesses, and organizations with limited budgets. They also allow customization and specific training. It is time-consuming and costly with proprietary options. It is an advantage for organizations looking for cheap and tailored solutions.</p><h3 id="""">Security</h3><p id="""">Open-source LLMs secure your data by enabling on-premise deployment, minimizing external server risks. It reduces the risk of data breaches and unauthorized access. The code for open-source LLMs is open to the public to inspect and identify loopholes. It helps improve security and prevent malicious use of models. Independent security experts can customize open-source LLMs, ensuring they are free of vulnerabilities. It can ensure that the models are effective for the tasks at hand.&nbsp;</p><h3 id="""">Reduced Dependency</h3><p id="""">Open-source LLMs can help organizations reduce their dependency on external providers. It is because they deploy the models on-premise, get support from a community of developers, and choose from many providers. Using the model in the organization's setup, developers can make it work with their current systems, apps, and data. It can give organizations more control over the models and their performance. It offers more flexibility in pricing and features. It can also help organizations avoid vendor lock-in.</p><h2 id="""">Want To Select Custom LLMs For Your Business?</h2><p id="""">If you are looking to select or build a custom LLM for your business, we can help. We are a team of AI engineers with extensive experience in training and <a href=""https://www.mercity.ai/blog-post/fine-tuning-llms-using-peft-and-lora"" id="""">fine-tuning custom LLMs</a>. <a href=""https://www.mercity.ai/contacts"" id="""">Contact us</a> today and let us select a custom LLM to elevate your business.</p><p id="""">‍</p>",https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64d7c5a73e53d03e5fb42f33_comparison-llms.png,Maithili Badhan,Large Language Models,"Our analysis of open source Large Language Models, encompassing licensing and commercial offerings, provides in-depth comparisons and library resources.",False,"<div class=""rich-text w-richtext""><p>In the last year, we have seen remarkable growth in the capability of open-source LLMs due to several factors, including the increasing availability of data, the development of new training techniques, and the growing demand for AI solutions. Their transparent, accessible, and customizable ability makes them a perfect alternative for closed-source LLMs such as GPT-4. </p><p>‍</p><p>One of the challenges with open-source LLMs is no agreed-upon evaluation criteria. It makes it difficult to compare and choose a model for a particular task. However, several benchmarking techniques have come forth, such as MMLU and ARC, to evaluate the performance of open-source LLMs on various tasks.</p><p>‍</p><p>In this article, we will analyze different open-source LLMs to help you understand and choose a model for your needs.</p><h2>Open Source Vs Closed Source LLMs</h2><p>With open-source LLMs (Large Language Models), individuals can build custom models tailored to specific tasks and domains. These models remove the entry barrier in AI. Organizations and researchers can train their LLMs, deploying them on personal PCs. </p><p>‍</p><p>Businesses can use open-source LLMs to design models that securely reside on internal servers, evolving performance through continuous refinement and specialization. It grants businesses control, amplifying their AI efforts and internal capabilities.</p><p>‍</p><p>The public availability and involvement in open-source LLMs promotes experimentation and innovation. Platforms like Hugging Face, facilitate the availability of pre-trained models and NLP tools to help people evaluate and work on their and other state-of-the-art models.</p><p>‍</p><p>However, open-source LLMs are still not at par with closed-source LLMs like GPT-4. It is because closed-source LLMs can be trained on more data and with more computational resources. Despite their current limitations, they are likely to close the gap in their performance.</p><h2>Exploring The Architecture Of LLMs</h2><p>LLMs use a transformer architecture consisting of a stack of neural networks trained to predict the next word in a sequence. Transformers are based on self-attention mechanisms, which allow the model to focus on specific parts of the input sequence. It has a significant impact on a model’s performance and capabilities. Hence, understanding their architecture is essential for their efficient use.</p><h3>Encoder Only</h3><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:929pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64d7c0bcb7acfdbdeef5fe97_hyjyQgsXIPQDTA9xGJNIwgjopH4co5Xm-W6dhvU0MGqT9sk3PUmb5c_FRiUfrkIhmSU78PvLlMjNte1vQyulolX9XldxpQRHX4Ra2MuZYuuOnca2N6I7SahpMuZUGRAFfhLX-4Rl294_xrFSKshsDbw.jpeg""/></div></figure><p>Figure from <a href=""https://pytorch.org/blog/a-better-transformer-for-fast-transformer-encoder-inference/""><em>A BetterTransformer for Fast Transformer Inference</em></a></p><p>‍</p><p>Encoder models are systems that use only the encoder of a Transformer model. Their attention layers access all words in the input sentence and use bidirectional context to make predictions. They pre-train with masked language modeling (MLM), which masks a portion of the input tokens and asks the model to predict them. It helps the model understand the context of words in a sentence. Fine-tuning the text representations is helpful for downstream tasks like classification, question answering, and named entity recognition. Some examples of encoder-only models include BERT and ELECTRA.</p><p>‍</p><p>Encoder-only models are capable of learning long-range dependencies between words in a sentence. It helps understand the context of a sentence and make accurate predictions. They can represent sentence meanings in a vector space. It further aids the comparison of sentences and making predictions. They are efficient to train and deploy for large classification tasks. Hence, they are for sentence classification, named entity recognition, and extractive question answering.</p><h3>Encoder-Decoder</h3><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1076pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64d7c0bccfc4c0f0e75c65a0_JN5AoMwpRmUaaPFxkfP3IzZJ2dZ1Jlda4aqidNwaMn4WfUk9H-6q0mBK2e0MQj6hcz_5ltKXo5Qu41dUULFUijbYzja556xiGHt_Iy2M_hj2klEomVTyEei_CpAYRynUabXTYrWkuHhuFyMY_NZ4Nog.jpeg""/></div></figure><p>Figure from <a href=""https://d2l.ai/chapter_attention-mechanisms-and-transformers/transformer.html#encoder""><em>Dive Into Deep Learning</em></a></p><p>‍</p><p>Encoder-decoder models are bidirectional models that pre-train on corrupted spans of text. The encoder generates the hidden representation as a summary of the input sequence. It is then passed to the decoder to generate the output. The advantage of having both the encoder and the decoder makes it effective against complex inputs. Span corruption helps the model to learn the context of words in a sentence, making it effective on tasks such as question answering, summarization, and translation. Some examples of encoder-decoder models are BART and Megatron.</p><p>‍</p><p>Encoder-decoder models are mainly used for sequence-to-sequence predictions, such as text summarizations and question answering. It is capable of solving machine translation problems. These are good for generative tasks but are also computationally expensive. It is because it requires both an encoder and a decoder. Their popularity is increasing due to decreasing computing power costs.</p><h3>Decoder Only</h3><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:975pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64d7c0bcab17a008287895db_5neYLLzUcX9uLTfzxP3-0vWTXEsGtgdEV-McTlyRugJaeDiGF05lBZ6piDrWcyjARt8HjbnwkuDQFXpsLOqVgf74BIQN72ZCGpyv9vXKq1J_3oT0OfX5JPbIUoVVE0TnwQMmAUhwDUq2HfY0e-Uf-bI.jpeg""/></div></figure><p>Figure from <a href=""https://www.researchgate.net/publication/364435819_A_Transformer-based_Generative_Model_for_De_Novo_Molecular_Design""><em>A Transformer-based Generative Model for De Novo Molecular Design</em></a></p><p>‍</p><p>Decoder-only models are auto-regressive models that use only the decoder of a transformer architecture. The architecture lacks an encoder. It implicitly encodes the information in the hidden state and updates it during output generation. They are pre-trained to predict the next word in a sentence, making them commonly used models for text generation. When fine-tuned, they can be used for downstream tasks by using a classifier over the last token hidden representation. Examples of decoder-only models include GPT models, transformer XL, and LLaMA.</p><p>‍</p><p>Decoder-only models generate one word at a time. They take into account the context of already generated words to generate the next word. These models are for text generation purposes, including text completion, and question-answering. They can be used for machine translation or in chatbots. They are capable of learning through feedback, thus, can improve over time. </p><h2>Comparison Of Different Open-Source LLMs</h2><p>Here is an in-depth comparison of various open-source LLMs, focusing on their structures, training methods, and performance.</p><h3>LLaMA</h3><h4>Overview</h4><p><a href=""https://arxiv.org/abs/2302.13971"">LLaMA</a> is a family of large language models (LLMs) with a decoder-only architecture and bidirectional context. It was developed by Meta AI and released in February 2023. LLaMA models range in size from 7 billion to 65 billion parameters. LLaMA 1 models were trained on a dataset of 1.4 trillion tokens, while <a href=""https://ai.meta.com/llama/"">Llama 2</a> models were trained on a dataset of 2 trillion tokens. </p><p>‍</p><p>The latter was released in three model sizes: 7, 13, and 70 billion parameters. It also has some architectural tweaks, such as Grouped Query Attention to make inference more efficient. It uses a vanilla multi-head attention mechanism. But, it can be fine-tuned with reinforcement learning to improve its performance on specific tasks. It outperforms many LLMs like MPT but still lags behind GPT-4. It is a developing model that is effective in various tasks, such as machine translation, and code generation. It is perhaps the best open-source model in terms of capability out there right now.</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:889pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64d7c0bc6beb0efabb8dc778_y5cGPd6q24H8KDJ9JFL_JZU6rrBNlsdNnsg9qcn7aOTdz7BXPrftGfEhO0pkn7bLriXoYdG6ynnC7rSJeHzhgn8xAoAxDbI9znQ7f-I05IdP10B0jcB3fT3oY0HR3v_w7v1DFLsOnRqML1S5Mo3M1BI.jpeg""/></div></figure><p>Figure by <a href=""https://ai.meta.com/llama/""><em>Meta AI</em></a></p><p>‍</p><p>The Llama 1 license was for non-commercial use, while the Apache 2.0 license allows for commercial use with some restrictions. Meta forbids Llama 2’s usage to train other models. And if Llama 2 is used in an app or service with more than 700 million monthly users, a special license must be obtained from Meta AI. This means that the architecture can be studied and modified, and the code can be used to create new models. However, the weights of the model, which contain the learned parameters, are not publicly available. The <a href=""https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T"">RedPajama</a> project reproduces and distributes an open-source version of the LLaMA dataset.</p><p>‍</p><p><a href=""https://github.com/openlm-research/open_llama"">OpenLLaMA</a> is an open-source reproduction of the LLaMA model. It provides researchers and developers an accessible and permissively licensed large language model. It is a 7B model trained on 200 billion tokens of the RedPajama dataset. It is significant because of its public availability. Also, <a href=""https://github.com/ggerganov/llama.cpp"">llama.cpp</a>, a port of the LLaMA model in C and C++, enhances the model's capabilities by supporting CUDA acceleration with GPUs.</p><h4>Performance Insights</h4><p>Developers of LLaMA reported that the 13 billion parameter model outperformed the considerably larger <a href=""https://arxiv.org/abs/2005.14165"">GPT-3</a> (175 billion parameters) across numerous NLP benchmarks. This performance excellence extends to competing with advanced models like PaLM and Chinchilla, showcasing LLaMA's proficiency in diverse language-related tasks.</p><p>‍</p><p>LLaMA 65B model has shown good capability in most use cases. It has an ARC rating of 63.48, posing a psychometric intelligence rate more than that of Falcon-40B. Also, LLaMA 2 ranks among the top 10 models in <a href=""https://huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard"">Open LLM Leaderboard</a> on Hugging Face.</p><h4>Use Cases And Applications</h4><p>LLaMA is used for text generation tasks, including text summarization for condensing content-rich texts while preserving vital information. It can enhance sentences and paragraphs via natural language processing techniques, exceeding GPT-3. Its open-source nature and <a href=""https://python.langchain.com/docs/integrations/llms/llamacpp"">llama.cpp</a> compatibility empowers users to explore, customize, and deploy the model as per specific requirements.</p><p>‍</p><p>Numerous models are constructed using LLaMA. For example, Alpaca is built on the LLaMA 7B model. Many open-source projects are continuing the work of optimizing LLaMA with the Alpaca dataset.</p><p>‍</p><p>Llama 2 includes both foundational models and models fine-tuned for dialog, called Llama 2 - Chat. They parallel closed-source counterparts like <a href=""https://arxiv.org/ftp/arxiv/papers/2302/2302.13817.pdf"">ChatGPT</a> and PaLM. Llama 2 and Llama 2 - Chat have the same context length of 4K tokens as opposed to GPT-4, which increased context length during fine-tuning.</p><h3>Falcon</h3><h4>Overview</h4><p><a href=""https://arxiv.org/abs/2306.01116"">Falcon</a> is an open-sourced large language model (LLM) developed by the Technology Innovation Institute (TII). It is available under the <a href=""https://www.apache.org/licenses/LICENSE-2.0"">Apache 2.0</a> license, meaning you can use it commercially. It has two models, Falcon-7B, and Falcon-40B, trained on 1.5 trillion and 1 trillion tokens, respectively. </p><p>‍</p><p>Falcon was trained on a massive English web dataset called <a href=""https://arxiv.org/abs/2306.01116"">RefinedWeb</a>, built on CommonCrawl. RefinedWeb is a high-quality dataset deduplicated and filtered to remove machine-generated text and adult content. Models trained on RefinedWeb perform better than the ones trained on curated sets. Falcon required 384 GPUs on AWS over two months. Falcon was built using custom tooling and leverages a unique data pipeline that can extract high-quality content from web data and use it to train a custom codebase independent from the works of NVIDIA, Microsoft, or HuggingFace.</p><h4>Performance Insights</h4><p>Falcon matches the performance of state-of-the-art models by DeepMind, Google, and Anthropic. The Falcon-40B model is currently among the top in the Open LLM Leaderboard, and the Falcon-7B model is also among the best in its weight class. Falcon models use a unique multi-query attention mechanism that is more efficient than the vanilla multi-head attention scheme. This makes Falcon more efficient than other LLMs, requiring only 75 percent of GPT-3's training compute, 40 percent of Chinchilla's, and 80 percent of <a href=""https://arxiv.org/abs/2204.02311"">PaLM-62B's</a>.</p><h4>Use Cases And Applications</h4><p>The Falcon LLM can work on various tasks, including generating creative content, solving complex problems, customer service operations, virtual assistants, language translation, sentiment analysis, and automating repetitive work. It is trainable in English, German, Spanish, French, Italian, Portuguese, Polish, Dutch, Romanian, Czech, and Swedish languages. The <a href=""https://huggingface.co/tiiuae/falcon-40b-instruct"">Falcon-40B-Instruct</a> model is fine-tuned for most use cases, including chatbots. <a href=""https://huggingface.co/tiiuae/falcon-40b"">Falcon-40B</a> requires ~90GB of GPU memory, while Falcon-7B only needs ~15GB. It makes Falcon-7B accessible even on consumer hardware.</p><h3>Vicuna</h3><h4>Overview</h4><p><a href=""https://arxiv.org/abs/2306.05685"">Vicuna</a> is an open-source LLM developed by LMSYS that is built from LLaMA. It is fine-tuned on a dataset of 70,000 user-shared conversations from ShareGPT. Vicuna is an auto-regressive LLM trained on 33 billion parameters and achieves more than 90% of ChatGPT's quality in user preference tests while vastly outperforming Alpaca. The training cost of <a href=""https://huggingface.co/eachadea/legacy-vicuna-13b"">Vicuna-13B</a> is around $300 because the <a href=""https://sharegpt.com/"">ShareGPT</a> data is freely available. The code, weights, and an online demo are publicly available for non-commercial use. Vicuna can be run on modest hardware using llama.cpp.</p><h4>Performance Insights</h4><p>A preliminary evaluation of Vicuna-13B, using <a href=""https://openai.com/research/gpt-4"">GPT-4</a>, showed that it achieved more than 90% of the quality of OpenAI, ChatGPT, and Google Bard, while outperforming other models like LLaMA and Stanford Alpaca. In benchmark tests, Vicuna showed significantly more detailed and better-structured answers than Alpaca after fine-tuning with ShareGPT data.</p><p>‍</p><p>In <a href=""https://lmsys.org/blog/2023-06-22-leaderboard/"">LMSYS’s MT-Bench</a> test, Vicuna scored 7.12, while the best proprietary model, GPT-4, secured 8.99 points. Also, in the MMLU test, it achieved 59.2 points, and GPT-4 scored 86.4 points. These results suggest that Vicuna is a promising LLM that can compete with the best proprietary models.</p><h4>Use Cases And Applications</h4><p>Vicuna can perform various tasks, such as generating text, translating languages, and writing creative content. It can also generate text that is both coherent and informative, and it can hold conversations that are engaging and natural. On the contrary, it has known issues, such as weaknesses in reasoning and math, and can produce hallucinations. Nonetheless, Vicuna has the potential to be a valuable tool for research and development in the field of AI.</p><h3>MPT</h3><h4>Overview</h4><p><a href=""https://www.mosaicml.com/blog/mpt-7b"">MPT</a> (MosaicML Pretrained Transformer) is a commercial open-source decoder-only transformer model developed by MosaicML. It is trained on a massive dataset of text and code of 1T tokens from various sources, including OpenLLaMA, StableLM, and Pythia. The model has a context length of 8K tokens and 6.7B parameters. It is available in three fine-tuned versions:  MPT-7B-StoryWriter-65k+, MPT-7B-Instruct, and MPT-7B-Chat.</p><p>‍</p><p><a href=""https://huggingface.co/mosaicml/mpt-7b-storywriter"">MPT-7B-StoryWriter-65k+</a> is trained on a filtered fiction subset of the books3 dataset and licensed under the Apache-2.0 license. <a href=""https://huggingface.co/mosaicml/mpt-7b-8k-instruct"">MPT-7B-Instruct</a> is trained on Databricks Dolly-15k and Anthropic's Helpful and Harmless datasets. <a href=""https://huggingface.co/mosaicml/mpt-7b-chat"">MPT-7B-Chat</a> is built on the ShareGPT-Vicuna, HC3, Alpaca, Helpful and Harmless, and Evol-Instruct datasets.</p><h4>Performance Insights</h4><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1531pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64d7c0bd5b9d53be4e0f7d6c_GQeOY-OYWtoXv3XfAvs1hCW8zuSwA0LOELlTUJOZ1QOBCVeVtIxrQ2upY7r-iDCrmhfW2HGwJuG4_4V5mLb6hVwK2YYTFVd7DeZx4fvFZkRuLtssPnLmNnYo9-G2B9fGbbvDGogRxN3NHFIJO3LMWsM.jpeg""/></div></figure><p>Figure by <a href=""https://www.mosaicml.com/blog/mpt-7b""><em>MosaicML</em></a></p><p>‍</p><p>MPT model, initially developed to counter the limitations to other open-source models, outperforms the GPT-3. It scores 6.39 in LMSYS’s MT-Bench test. These models leverage FlashAttention and FasterTransformer for speed performance. MPT-7B is as competent as <a href=""https://huggingface.co/huggyllama/llama-7b"">LLmMA-7B</a> and exceeds other open-source 7B-20B models. MPT is trained for long inputs up to 64k and can handle to 84k with the help of <a href=""https://arxiv.org/abs/2108.12409"">ALiBi</a>. It is optimized for fast training and inference and has highly efficient open-source training code.</p><h4>Use Cases And Applications</h4><p>MPT-7B-StoryWriter-65k+ can read and write fictional stories with long context lengths. It can create creative text formats like poems, code, scripts, musical pieces, email, and letters. The model can generate as long as 84k tokens on a single node of A100-80GB GPUs. MPT-7B-Instruct follows short-form information. It generates instructions. It is capable of informative question answering. MPT-7B-Chat, chatbot-like model, is for dialogue generation, helping engaging in coherent conversations. Also, if you need a small local running LLM, consider the <a href=""https://huggingface.co/mosaicml/mpt-30b"">MPT-30B</a> model.</p><h3>Guanaco</h3><h4>Overview</h4><p><a href=""https://huggingface.co/timdettmers/guanaco-65b"">Guanaco-65B</a> is a non-commercial open-source LLM based on the LLaMA 7B model. It uses the <a href=""https://arxiv.org/abs/2305.14314"">QLoRA</a> 4-bit fine tuning method, efficiently reducing memory usage while preserving full 16-bit task performance. It allows Guanaco-65B to be trained on a single GPU with 48GB of VRAM in just 24 hours. Guanaco is available in four models: 7B, 13B, 33B, and 65B. All of the models have been fine-tuned on the<a href=""https://huggingface.co/datasets/OpenAssistant/oasst1""> OASST1 dataset</a>. The limitations of Guanaco-65B include its slow 4-bit inference and lack of math capabilities.</p><h4>Performance Insights</h4><p>Guanaco-65B outperforms even ChatGPT (GPT-3.5 model) with a much smaller parameter size on the Vicuna benchmark. In the <a href=""https://arxiv.org/abs/2009.03300"">MMLU</a> test, Guanaco-33B scored 57.6 and Falcon scored 54.7. Similarly, in the MT-Bench evaluation, Guanaco stands at 6.53 and Falcon at 5.17. <a href=""https://huggingface.co/timdettmers/guanaco-7b"">Guanaco-7B</a> model requires only 5 gigabytes of GPU memory and exceeds the 26-gigabyte Alpaca model by more than 20 percentage points on the Vicuna benchmark.</p><p>‍</p><p>Guanaco-65B achieves above 99 percent of the performance of ChatGPT (GPT-3.5-turbo) in a benchmark run with GPT-4. Guanaco-33B reached 97.8 percent of ChatGPT's performance with 33 billion parameters in a benchmark while training it on a single consumer GPU in less than 12 hours. Guanaco-65B could achieve 99.3 percent of ChatGPT's performance in 24 hours on a professional GPU.</p><h4>Use Cases And Applications</h4><p>Guanaco is an advanced instruction-following language model trained on English, Chinese, Japanese and Deutsch. It can work in multilingual environments and extend to meet various linguistic contexts. Its <a href=""https://huggingface.co/blog/4bit-transformers-bitsandbytes"">QLoRA</a> 4-bit fine tuning method makes Guanaco a good option for researchers and developers who need a powerful LLM that can be used offline or on mobile devices. Its uses include text summarization, question answering, finetuning and chatbots. Since it is still in development, it also makes a good model for experimentation.</p><h2>LLaMA 2 Vs Falcon</h2><p>Llama 2 has three model sizes: 7B, 13B, and 70B. It was trained on 2 trillion tokens, including over 1 million human annotations. It employs Reinforcement Learning from Human Feedback (RLHF) specifically for Llama-Chat-2's fine-tuning. According to Meta, Llama 2 outperforms other LLMs, including Falcon and MPT, in the areas of reasoning, coding, proficiency, and knowledge tests. Llama 2-7B scored 54.32 while Falcon-7B gained 47.01 on Open LLM Leaderboard. However, it is less efficient than Falcon, requiring more training compute.</p><p>‍</p><p>Falcon has two models, Falcon-7B and Falcon-40B. It was trained on 1.5 trillion and 1 trillion tokens, respectively. Falcon uses a unique multi-query attention mechanism that is more efficient than the vanilla multi-head attention scheme used by Llama 2. It makes Falcon more efficient than Llama 2, but it is not as strong on some tasks as Llama 2.</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1089pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64d7c0bcb928cf4cb9b2b0ff_rTDFn-CVsICw0sOi9lvWJun6OgJzHMLwSAGstJYkKpFrMgFE7gAq8kdnE1cG90gWSNG85RNJlXRaK4io-br80kf_PGyGf6-PVJxNaXrtGABhi8Ile41cDT7dg1Ahui3dNNoTtXvERCsYpJdD7RTxnj4.jpeg""/></div></figure><p>Comparison Table by <a href=""https://huggingface.co/blog/llama2#why-llama-2""><em>Hugging Face</em></a></p><h2>Choosing An Open Source LLM</h2><p>Here are some factors that will help you narrow down your choices to choose the right open-source LLM for your needs.</p><h3>Technical Requirements</h3><p>Assess your hardware and infrastructure capabilities. Consider the technical factors of an open-source LLM, such as model size, compute power and storage capacity. You might need to upgrade existing resources or consider cloud-based solutions. A model should be scalable to meet the real-world requirements of increasing demands. It is also necessary to consider the specific use case of the model. For example, a Falcon model may be good for language translation and automating repetitive work. If the business needs an LLM for both online and offline use, a Guanaco model may be a better option.</p><h3>Integration Ability</h3><p>Consider an open-source LLM that can integrate with your business systems. Check its compatibility with programming languages, frameworks, and APIs used in the ecosystem of your enterprise. It can save organizations time and effort and ensure the model works effectively. For example, Megatron-Turing NLG is a good option for enterprises using Python, TensorFlow, or PyTorch. It is also compatible with the Hugging Face Transformers library. It makes it easy to integrate into existing software applications.</p><h3>Licensing</h3><p>Businesses should understand the licensing terms of the open-source LLM they select. They must be clear about the license type, compatibility with other licenses, permitted usage, modifications, and distribution requirements. For example, Megatron-Turing NLG has a GPLv3 license. It allows the distribution of its modified version under the same license. But h2oGPT is under Apache 2.0 license. It permits modifications and commercial usage but forbids distribution under the same license. If you need a commercial license with no limitations, consider Falcon models. A brief review of the license guarantees alignment, IP respect, and legal compliance.</p><h3>Adaptability</h3><p>The adaptability of an open-source LLM refers to its ability to perform various tasks and how accurate its output is for those tasks. This adaptability depends on the model's design, training data, documentation, and community. For example, Vicuna is adaptable in generating coherent creative content but lags in math. LLaMA, on the other hand, is accurate in common sense inference but average in the level of truthfulness in generating answers to questions. Also, MPT is a good model for long text generation due to its context length of 8K.</p><h2>The Advantages Of Open Source LLMs</h2><p>Open-source LLMs offer several advantages over closed-source LLMs, including cost-effectiveness, flexibility, security, and reduced dependency on external providers.</p><h3>Customization</h3><p>Open-source LLMs are a flexible alternative to proprietary LLMs. They are freely available to use and modify with no recurring costs. Organizations can deploy them on-premises and have more flexibility to customize the model to their needs. They can add specific features to open-source LLMs unavailable in the default version. Open-source LLMs have a large and active community of users and developers. It can help with troubleshooting and development.</p><h3>Cost Efficiency</h3><p>Open-source LLMs offer a cost-efficient alternative to proprietary software. They save costs on licenses, subscriptions, and hidden charges. Hosted LLMs have usage fees that can add up. Open-source LLMs do not have such recurring payments. Once acquired, organizations can deploy them without worrying about any expenses. Hence, they are great for startups, small businesses, and organizations with limited budgets. They also allow customization and specific training. It is time-consuming and costly with proprietary options. It is an advantage for organizations looking for cheap and tailored solutions.</p><h3>Security</h3><p>Open-source LLMs secure your data by enabling on-premise deployment, minimizing external server risks. It reduces the risk of data breaches and unauthorized access. The code for open-source LLMs is open to the public to inspect and identify loopholes. It helps improve security and prevent malicious use of models. Independent security experts can customize open-source LLMs, ensuring they are free of vulnerabilities. It can ensure that the models are effective for the tasks at hand. </p><h3>Reduced Dependency</h3><p>Open-source LLMs can help organizations reduce their dependency on external providers. It is because they deploy the models on-premise, get support from a community of developers, and choose from many providers. Using the model in the organization's setup, developers can make it work with their current systems, apps, and data. It can give organizations more control over the models and their performance. It offers more flexibility in pricing and features. It can also help organizations avoid vendor lock-in.</p><h2>Want To Select Custom LLMs For Your Business?</h2><p>If you are looking to select or build a custom LLM for your business, we can help. We are a team of AI engineers with extensive experience in training and <a href=""https://www.mercity.ai/blog-post/fine-tuning-llms-using-peft-and-lora"">fine-tuning custom LLMs</a>. <a href=""https://www.mercity.ai/contacts"">Contact us</a> today and let us select a custom LLM to elevate your business.</p><p>‍</p></div>"
Comprehensive Guide to Performing Churn Analysis,comprehensive-guide-to-performing-churn-analysis,640f56f76d313b2faa631c11,6885371e40d710d7fc608546,False,False,Sat Jul 26 2025 20:14:22 GMT+0000 (Coordinated Universal Time),Sat Jul 26 2025 20:33:11 GMT+0000 (Coordinated Universal Time),Sat Jul 26 2025 20:49:52 GMT+0000 (Coordinated Universal Time),"<p id="""">You are running a subscription service, a gym, an online store, or even a learning platform. Things are going well, new customers are coming in. But after a few months, people start dropping off one by one. That’s churn. And if you're not paying attention, it can drain your revenue faster than you think.</p><p id="""">But the good thing is you can see it coming.</p><p id="""">That’s where churn prediction steps in, your business’s early warning system.</p><h2 id="""">What is Churn prediction?</h2><p id="""">Churn prediction is the process of identifying customers who are at a high risk of churning in the future. The goal is to flag customers who are likely to churn before they actually do, allowing the business to take steps to retain individuals with a high risk of churn.</p><p id="""">At a high level, churn prediction works by analyzing historical customer data such as behavior, transactions, and engagement. Machine learning models are trained to recognize early signals that often precede churn, like a drop in activity, reduced spending, or negative support interactions. Once trained, the model can assign a churn risk score to current customers, helping the business prioritize those most at risk.</p><h2 id="""">Why do companies need Churn Prediction?</h2><p id="""">Churn prediction is critical for any business aiming to maintain growth and strengthen customer relationships. Rather than responding after a customer has already left, predictive models allow companies to intervene early and minimize the risk of attrition. This shift from reactive to preventive strategy improves long term profitability and enhances the overall customer experience.</p><p id="""">At the core of this approach is the fact that retaining customers is significantly more cost-effective than acquiring new ones. Multiple studies confirm that even a small increase in retention can result in disproportionately large improvements in profit margins.&nbsp;</p><p id=""""><em id="""">For example, a five percent increase in customer retention has been associated with profit boosts ranging from twenty five to ninety five percent. </em>This efficiency makes churn prediction not just a technical tool but a strategic business asset.</p><h3 id="""">Understanding and Acting on Churn Drivers</h3><p id="""">Churn rarely occurs without warning. Behavioral signals such as decreased engagement, frequent complaints, or limited feature usage often precede customer exits. Churn models help surface these patterns in advance, allowing teams to act before the risk materializes.</p><p id="""">This insight is valuable across several business functions. For product teams, identifying the features or touchpoints associated with high churn enables targeted improvements in design or usability. Marketing can craft messaging that resonates with high risk customers by focusing on the aspects of the product they are most likely to find valuable. Customer success and support teams can also use these insights to prioritize outreach and offer proactive assistance to those exhibiting signs of dissatisfaction.</p><p id="""">By using churn understanding into operations, companies can adapt and create a more responsive service environment, ultimately reducing churn and improving user satisfaction over time.</p><h3 id="""">Optimizing Retention Efforts and Financial Planning</h3><p id="""">Churn prediction also enhances how businesses allocate resources. Without it, retention strategies often rely on blanket approaches that treat all customers equally. This results in inefficient use of time, budget, and personnel. With churn scores in place, retention efforts can be prioritized toward the most at risk customers, making interventions more precise and effective. Whether through targeted discounts, personalized engagement campaigns, or prioritized support, businesses can apply the right strategy to the right customer at the right moment.</p><p id="""">From a financial standpoint, churn models contribute to greater planning accuracy. High churn rates reduce stability in forecasting, particularly for subscription based or businesses dependent on repeat purchases. Predictions help analysts set realistic growth expectations, manage budget allocations, and prepare contingency plans with greater confidence. They also support a more predictable revenue stream, which is crucial for long term strategies.</p><p id="""">Additionally, reduced churn naturally lowers acquisition pressure. Since acquiring new customers is often several times more expensive than retaining existing ones, a well functioning churn prediction system can directly reduce marketing and sales expenditure. Long-term customers are also more likely to contribute to organic growth through referrals and advocacy, creating further downstream benefits without added cost.</p><h3 id="""">Applicability Across Sectors</h3><p id="""">Although the primary focus here is on retail and e-commerce, churn prediction methodologies are relevant across many industries. In telecommunications, software as a service, banking, digital media, and utility services, customer retention plays a central role in revenue stability. The economic impact of churn is consistently significant. Accurate forecasting and timely intervention are thus essential across all customer-centric industries.</p><p id="""">‍</p><h2 id="""">How to Build a Churn Prediction Model</h2><p id="""">A good churn model lets a company intervene (with special offers or support) before customers go, protecting revenue and growth.</p><p id="""">Building such a model involves several clear steps: gathering the right data, turning it into useful <em id="""">features</em>,&nbsp; choosing the right algorithm, and carefully evaluating the results. Each step shapes how well the model will work in real business settings.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1548px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1548px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/688537501865bcc1d64c5ac9_AD_4nXcBrH09ecaOCubsX_xmXhUVYBQ6KVJ1U2sMSjk4iH7GuXxoSwM9jbiWfxqT1wgdg0VhuJxIt1faq6AozTpy1_5RsF1nVqK3VPZ7osTxSGbZteW-jRNlF9wVelR4MxlNy9wFDDViFA.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><h3 id="""">Data Collection</h3><p id="""">The first step is gathering data from all relevant sources. Think of every way you interact with your customers:</p><ul id=""""><li id=""""><strong id="""">Customer Profile</strong>: Demographics like age, location, or account type can influence churn.</li><li id=""""><strong id="""">Transaction History</strong>: Records of purchases, subscription changes, or payment failures.</li><li id=""""><strong id="""">Behavioral Logs</strong>: Usage data (e.g., login frequency, pages visited, support chat logs) show engagement levels.</li><li id=""""><strong id="""">Support &amp; Feedback</strong>: Help tickets or complaint logs can signal dissatisfaction.</li></ul><p id="""">Combine data from systems like CRM, billing, and web analytics so each customer’s history is complete. Ensure consistent customer IDs and timestamps across sources so data aligns correctly. In practice, this means cleaning and linking tables so every purchase or support call matches the right customer record.</p><p id="""">Once collected, look at distributions of key variables or how churners differ from others. You might find churners tend to log in less or have lower lifetime value. This initial analysis helps verify the data quality and suggests which factors are important.</p><h3 id="""">Feature Engineering</h3><p id="""">Raw data must be transformed into features that a model can use. In business terms, features are metrics that capture customer behavior and value. Accurate churn prediction models depend heavily on the quality and relevance of the input features. These features should reflect customer behavior, engagement, financial patterns, and satisfaction over time. The goal is to extract signals that precede churn events, enabling early intervention. This section organizes the most effective feature types into categories, describing their rationale and interpretation, and implementation.</p><h4 id="""">RFM Features (Recency, Frequency, Monetary Value)</h4><p id="""">The RFM features standing for Recency, Frequency and monetary, is one of the most widely used approaches to capture customer behavior, particularly in churn prediction and segmentation tasks.&nbsp;</p><p id=""""><strong id="""">Recency </strong>captures how recently a customer engaged with the product or service. This could be days since their last login, purchase, or session. It is a critical feature because churn often follows a decline in activity. For instance, a user who hasn’t logged in for 60 days is more likely to churn than one who used the platform yesterday. Declining recency is a common early warning sign, especially for previously active users.</p><p id=""""><strong id="""">Frequency </strong>measures how often a customer engages over a defined period, such as logins per month or purchases per quarter. High-frequency users are typically more engaged and less likely to churn, as frequent use suggests the product delivers ongoing value. A sharp drop in frequency, even if recency is recent, can still indicate waning interest.</p><p id=""""><strong id="""">Monetary </strong>value reflects how much a customer has spent, either in total or over recent intervals. In subscription models, this could be average monthly revenue; in transactional models, cumulative spend. A decline in monetary value may signal reduced perceived value or intent to downgrade. <em id="""">For example, smaller orders or a shift from premium to basic plans may precede churn. Conversely, high-value users may stay despite lower engagement due to their investment.</em></p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1308px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1308px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/688537501865bcc1d64c5ad2_AD_4nXe-KklldqM2JWRugcUBQxrQzg6k4XZySkyITFKlTNPQOx-aB_78fmSwXllX6R1nOkshmjRu_1JH1buhcj-g2SU-5-KIBdg0Vygji7gf-8maQGaafM14HvGoNgS63Q8Z4tSSO8_9Sw.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">These RFM metrics can be used individually or in combination to segment users into behavioral cohorts like champions, at-risk users, or lapsed customers. Each segment can then be targeted with appropriate retention strategies.</p><h4 id="""">Engagement and Usage Behavior</h4><p id="""">Engagement features go beyond recency and frequency to capture how users interact with the product. They track behavior over specific time windows, reflecting usage depth, session quality, and interaction diversity. This includes active usage metrics such as number of sessions, screen views, feature clicks, or time spent on the platform over the past 7 or 30 days. A decline in usage hours or session counts may signal fading interest, while an uptick may reflect renewed satisfaction after a product update or offer.</p><p id="""">More granular signals include how often specific modules or tools are used. <em id="""">For example, on a SaaS platform, reduced use of a core feature like dashboards might indicate declining reliance.</em> Consistency or variability in engagement is also informative. Metrics like engagement slope, which reflects momentum, or rolling averages of session count help track trends. A flat or downward slope can be an early churn signal even when activity levels are moderate.</p><p id="""">Engagement features help models understand how embedded a product is in a user's routine. A drop in time spent or lack of exploration may indicate disengagement. When modeled well, engagement often correlates strongly with retention and gives early warning before frequency declines.</p><h4 id="""">Transactional Behavior</h4><p id="""">Changes in financial activity are strong predictors of churn in subscriptions or e-commerce. Average order value, calculated over a recent window, reflects transaction size. A sustained drop can signal reduced interest or budget limits. Purchase frequency over a recent quarter or year helps assess customer commitment, with fewer purchases suggesting waning satisfaction.</p><p id="""">Tracking subscription changes like downgrades, removed features, or shorter contracts is especially useful, as these often precede churn. Payment issues such as missed or delayed transactions frequently lead to involuntary churn if not resolved quickly. These features closely reflect customer intent and are essential to churn models.</p><h4 id="""">Support and Satisfaction Signals</h4><p id="""">Customer frustration often shows up in support data. An increase in tickets, help requests, or unresolved complaints often reflects dissatisfaction. Sentiment from support transcripts, reviews, or chat logs can flag declining experiences.</p><p id="""">Survey-based metrics like Net Promoter Score (NPS) and Customer Satisfaction (CSAT) offer structured feedback. A downward trend in these scores often signals increased churn risk. Support features help differentiate between customers who are confused but can be retained and those genuinely frustrated. Combined with engagement or RFM signals, they help models understand intent, not just usage.</p><h4 id="""">Demographic and Contextual Features</h4><p id="""">Demographic and account features capture static traits like age, region, plan tier, device, or tenure. While not dynamic, these help contextualize churn drivers.</p><p id="""">Customer type, subscription tier, or service bundle often correlates with churn. For instance, users on basic plans typically churn more than those on premium ones. Geographic and demographic factors like location or language highlight behavioral differences across groups. Acquisition channels such as organic search, paid ads, or referrals influence loyalty, with different sources yielding different churn rates.</p><p id="""">These features are critical for adjusting for population effects and identifying high-risk user segments. Well-engineered features make even simple models effective by surfacing meaningful patterns in data. They support both churn prediction and the design of retention strategies.</p><h3 id="""">Model Selection</h3><p id="""">With features ready, the next step is choosing a predictive algorithm. No single model is best in every situation. The choice depends on factors like desired interpretability and data complexity.</p><h4 id="""">Logistic Regression</h4><p id="""">Logistic regression is a linear model that outputs a probability of churn. It is highly interpretable because each feature receives a weight indicating its impact on churn risk. It trains quickly, scales to very large datasets and integrates easily into real‑time systems. As a baseline it is often the first model to try. Its limitation is that it assumes a linear relationship between features and the log‑odds of churn, so it may underperform on complex, non‑linear patterns.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1071px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1071px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/688537501865bcc1d64c5ac6_AD_4nXeYmvRgbZL7Qh4Rp9EkGBIklUCNJWleqUpqQVTlJ3KvuZHWFbaxgoA-OmTRdEnxToZqqDz_ByFSnUq8jgeD_HHLh_y6IvZ1ijBzb3A6Q-XKk3jAhCcBk0Bvh83vIhfvPmjCQLQO.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><h4 id="""">Decision Trees</h4><p id="""">Decision trees classify customers by creating a series of clear, rule-based decisions. These rules are easy to follow and explain to non-technical stakeholders, making trees highly interpretable. They can naturally model non-linear relationships and interactions between features. However, a single decision tree can easily overfit the training data, capturing noise instead of real patterns, which may limit its predictive accuracy on new data. Still, decision trees are a solid choice when you need transparent, easy to communicate models and want to understand the logic behind churn predictions.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1200px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1200px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/688537501865bcc1d64c5ac3_AD_4nXd-Z2sCOcp8TYvns4fXf2-6yX31BxwtD2f1Xh5DNWf_612QgofplFbMsL2Wqd2fMknEjRsH6ye3PIrobRtvyefFlwnju75NTQ0CmvP9S0Lin8N8z_Ruz2w_-yUzQlnLhgNJey466g.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><h4 id="""">Random Forests</h4><p id="""">This model builds many decision trees and averages their predictions. Unlike traditional linear models, it excels at capturing complex, non-linear relationships between customer behaviours and churn likelihood. Random forests use ensemble approaches of multiple trees, minimizing the risk of overfitting, ensuring that the model performs well not only on training data but also on unseen customers. Each tree in the forest considers random subsets of features which makes the model robust, even when certain data points are noisy or incomplete. However, this ensemble approach reduces interpretability, it's no longer easy to say ‘this feature caused churn without additional tools like SHAP. They also require more computing resources, especially on large datasets. Still, they strike a strong balance between performance and generalizability, making them a top choice for many churn prediction systems.</p><figure class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68853999eaa76c813f0cf410_AD_4nXe8tmYfpvn-7VCsQ9fT7f3syKryuqp3DFvPR-hAxXE1Nf-GL8UwcRDY05dZg8zshTaIECslmNdh2dA5cEM9okWjqfWsc0GE2RTCYb6QGGIFh6zlG6DdDKMEQwan2WSLHS3nKtDeBQ.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p>‍</p><h4 id="""">Kaplan-Meier Survival Analysis</h4><p id="""">Survival analysis models view churn as a <em id="""">time to event</em> process rather than a classification problem. Unlike traditional models that ask whether a customer will churn within a given window, survival models aim to estimate the probability that a customer will remain active at a future time. They also handle <em id="""">censoring</em>, which happens when a customer hasn't churned by the end of the observation period. This approach avoids bias and gives more useful insights, like the probability a customer stays for 6, 12, or 24 months or computing the median customer lifetime.&nbsp;</p><p id="""">The Kaplan–Meier (KM) estimator is a method used to estimate the <em id="""">survival function</em>, which gives the probability that a customer is still active and has <em id="""">not</em> churned at time t. It creates a stepwise survival curve, starting at probability 1.0 and dropping at each observed churn event. The survival function is defined as:</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:671px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""671px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/688537501865bcc1d64c5ab9_AD_4nXeLX5rgV3UfAfwyVHUCLcVba2A14gwez5blQOQsS0qpobcJplyz274u0p7C8lLNiiKjkYP7mrjW7r5Q1X2mvLxRdDEu5pg89dXIpyAC8kHR6DbZMCAIF7vbMsLU5hWFcHMYViVi0g.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">with <em id="""">t<sub id="""">i</sub> </em>a time when at least one <strong id="""">churn event</strong> occurred, <em id="""">d<sub id="""">i</sub> </em>the number of customers who churned at time <em id="""">t<sub id="""">i</sub></em>, and <em id="""">n<sub id="""">i</sub> </em>the number of customers who were still active (had not yet churned or been censored) just before time <em id="""">t<sub id="""">i</sub></em> .</p><p id="""">The Kaplan–Meier method makes no distributional assumptions and is easy for visualizing and comparing survival curves across customer segments. It assumes that survival times are independent, meaning one person churning doesn’t directly affect another, and that censoring is uninformative, meaning customers who haven’t churned are not different from those who did at the same tenure.</p><p>‍</p><pre></pre><p>‍</p><p>‍</p><figure class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/688539cdc66e32973b708cb5_AD_4nXfT4CS3jO3c5o5eHikM2paFoTHzwUqGX8lavtskXE8ZitwLDq7_ORcUjMm1nnPsGCNzVdlDIB_wy1p4Pe1T1UMyldxAO7x5dEiXZbsn28VjQiW_yVUnXajlvfEgASZtHqvSdnQo.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p>‍</p><p id="""">‍</p><p id="""">The above example uses the <strong id="""">IBM Telco Churn</strong> dataset and estimates customer retention as a function of tenure. Sudden drop in the starting says that after one tenure only customers start churning and over time that churning rate decreases. It can be extended to compare curves across different subgroups.</p><h4 id="""">Cox Proportional Hazards Model</h4><p id="""">The Cox Proportional Hazards is a survival regression model that relates customer features (covariates) to churn timing. The model uses customer features and finds how each factor influences churn. Whereas Kaplan-Meier is used to estimate the probability of survival, Cox Proportional Hazards is used to estimate the hazard ratio. The hazard ratio represents the difference in hazard that exists between two individuals (or groups). The hazard is essentially the inverse of survival, or the probability of failure. Kaplan-Meier estimates the probability of survival whereas Cox Proportional Hazards estimates a hazard ratio. As long as you have one, you can calculate the other.&nbsp;</p><p id="""">The Cox Proportional Hazards equation states that the hazard ratio is the product of two terms: the baseline hazard and the partial hazard.&nbsp;</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/688537501865bcc1d64c5ac0_AD_4nXdT54bp_ysIU5r7QhEEpsfsaC5jcIpJaAx0oFZ81NjEYfNX_TIlkN6s13d14RS9VAw1Ho5NiEvxPbMXMDgo0G3obTMd7ByIcnoHpqc01k7LFuIPV_jhex6Mg1sYE03fDotLCzElbA.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">The baseline hazard is simply a baseline. It's the hazard that exists when each variable is set to some specific value.The partial hazard represents the change in the hazard that occurs when the value for a variable is different from the baseline. The primary assumption is proportional hazards, meaning the effect of a feature on the hazard is constant over time.&nbsp;</p><p id="""">The Cox Proportional Hazards model was fitted on IBM Telco churn data to understand and assess how it performs in modelling customer churn prediction.</p><p id="""">‍</p><pre></pre><p>‍</p><figure class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68853a0e96dfa3033e9b3e47_AD_4nXfiLfijKJvoV3cq__ppNhVe_0aZRaWU2FH0OLrTM21_fteMMAV_lyziureGn0jKCtmlEnE-UJC9e3Xvqgfc0P_XpmrMeuoJyzfUFa8EBcWVaqZsIkscHdKmLUdryUG4jatzwORuxg.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p>‍</p><p>‍</p><p id="""">&nbsp;The coefficients show how each feature affects churn risk in relative terms. A log hazard ratio greater than 0 means the feature increases churn risk. <em id="""">For example, higher monthly charges often correlate with a higher chance of churn. A log hazard ratio less than 0 means the feature reduces churn risk.&nbsp;</em></p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/688537501865bcc1d64c5adc_AD_4nXeuTPsb9Zm3xGsYtE0EJfx66KCwnV1jI5iT6soUTq7Wjlc8xIu5RfpKJpXPXawbuz7uz5StQZThjHCV940J2ebPAMMc3Hk9UlV4vfoNLuVRVzapZWkarODJd5smMz5-vXxhGzRGPA.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">The plots above depict Cumulative Hazard and Survival Probability over time. The sharp drop in survival probability between 20 to 40 months suggests that many customers are leaving during this period. Similarly after around 40 months, the cumulative hazard slope becomes steeper, depicting accelerating churn risk.</p><p id="""">When selecting a model, balance interpretability, predictive power, and how it will be used in production. Models with high interpretability are easier to audit and explain but may underperform on complex patterns. Conversely, more accurate models often require more computational resources and lack transparency, making them harder to monitor or trust without explainability tools.</p><h3 id="""">Model Evaluation and Metrics</h3><p id="""">After training, you must <em id="""">evaluate</em> how well the model works. In churn problems, raw accuracy (overall percent correct) can be misleading because churn events are relatively rare. For instance, if 90% of customers don’t churn, a model that predicts “no churn” for everyone is 90% accurate but completely useless.</p><p id="""">Instead, focus on metrics that capture the business goal of catching actual churners:</p><ul id=""""><li id=""""><strong id="""">Precision</strong>: It tells us what fraction of the customers predicted to churn actually did. High precision means resources won’t be wasted on targeting customers who wouldn’t have churned, avoiding unnecessary allocation of resources.</li><li id=""""><strong id="""">Recall </strong>: Of all customers who actually churned, what fraction did the model catch? High recall ensures you identify most at-risk customers. Missing a true churner (a false negative) can be very costly.</li><li id=""""><strong id="""">F1-Score</strong>: The harmonic mean of precision and recall; it balances the two.</li><li id=""""><strong id="""">ROC-AUC (Area Under the Curve)</strong>: This measures how well the model ranks customers by churn risk. An AUC close to 1 means the model can distinguish churners from non-churners across all thresholds.<br><br></li></ul><p id="""">For churn prediction, precision and recall are often more informative than plain accuracy. You may also use a confusion matrix to see the counts of true/false positives and negatives.<em id=""""> From a business view, consider a cost sensitive example, failing to prevent a high value customer's churn (false negative) may result in a greater loss than attempting to retain a low risk customer who would have stayed anyway (false positive).</em></p><p id=""""><strong id="""">Cross validation</strong> (testing the model on multiple train/test splits) is also important. It ensures your model’s performance is stable and not just luck on one split. Finally, Finally, metrics should be contextualized in terms of business outcomes. Rather than reporting scores, quantify how many customers the model helps retain, or how much revenue it protects compared to a random or untargeted approach. This translation from statistical evaluation to business value enables businesses to pinpoint weaknesses and take focused steps toward improving.</p><p id="""">‍</p><h3 id="""">Model Comparison</h3><p id="""">Different algorithms offer varying strengths in terms of interpretability, accuracy, and scalability. These trade-offs must be considered in light of business goals and the nature of available data.</p><p id="""">When you want high interpretability, you can rely on models like logistic regression, decision trees, and survival analysis. Logistic regression provides clear feature weights and works well when the relationships are mostly linear. Decision trees are intuitive and offer rule based splits that are easy to explain. Kaplan-Meier curves and the Cox model, while part of the survival analysis family, also provide transparency either through visual churn timelines or clear coefficients that show how each factor affects churn risk.</p><p id="""">When the data becomes more complex or the relationships are non-linear, logistic regression tends to underperform. In such cases, survival analysis and ensemble methods like random forests offer better accuracy. Random forests are particularly effective at capturing complex interactions and handling noise, with some trade off in transparency. Survival models, especially the Cox model, can still retain interpretability while adapting to more intricate data structures.</p><p id="""">Overfitting is a challenge particularly with decision trees. Pruning and validation help, but accuracy may still be limited. Random forests reduce this risk making them more reliable, though heavier to deploy. Survival models aren’t typically used for classification, but when timing matters, they provide unique insights that are both strategic and actionable.<em id="""">For example, Kaplan-Meier curves are straightforward plots of churn probability over time, and Cox models let you interpret each factor’s effect on churn risk.</em></p><p id="""">Ultimately, model choice should reflect not just the structure of the data but the kind of insight the business needs whether it’s <em id="""">who will churn</em>, <em id="""">why they churn</em>, or <em id="""">when they are likely to churn</em>. Model selection should align with business priorities. If transparency is essential for communication, an interpretable model such as logistic regression, Survival models or decision trees is appropriate. If the goal is to maximize predictive performance in complex data settings, ensemble models or survival models may be more effective. For large scale deployment, models must also be fast and efficient. The optimal strategy is to evaluate multiple models, assess their performance on relevant business metrics, and select the one that best aligns with ones business model.</p><h2 id="""">Using Churn Predictions for Business and Marketing Optimization</h2><p id="""">Predictive churn models allow businesses to identify customers at risk of leaving and intervene strategically. Since retaining an existing customer is significantly more cost-effective than acquiring a new one, churn scores help prioritize efforts. After scoring, customers are segmented into risk cohorts such as high, medium, or low churn probability. Actions are then tailored to each segment. <em id="""">For example, a segment of VIP customers showing reduced activity may be prioritized by the customer success team for retention interventions.</em></p><h3 id="""">Targeted Retention Campaigns</h3><p id="""">Once high-risk segments are defined, proactive outreach becomes essential. These cohorts should be engaged with targeted retention campaigns that include personalized incentives and optimized offers.</p><p id=""""><strong id="""">Personalized Incentives</strong> are based on the customer's prior behavior. For instance, customers who leave a one- or two-star review are automatically sent a coupon for their next purchase. Other forms include loyalty points, upgrade offers, or access to premium content. Such personalization makes customers feel valued and can reverse attrition.</p><p id=""""><strong id="""">Coupon and Offer Optimization:</strong> To avoid wasting marketing spend on ineffective promotions, combine churn risk scores with “redeemability” models. Using dual prediction approaches, one predictive model identifies customers at high risk of churn, while a second model evaluates each customer's likelihood to redeem retention offers. By only sending coupons to those who were both likely to churn and likely to redeem the offer, the company cut costs and retained more customers. In practice, a retailer might first flag customers who haven’t shopped in months (higher churn risk), then filter that list by who’s historically used coupons. Only this intersection receives the coupon code. The same principle applies to free shipping, bundle deals, or trial extensions: target them with data-driven precision. This data-backed approach yields much higher ROI than blanket promotions (up to 30–50% lift in engagement is possible when targeting is precise).</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1330px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1330px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/688537501865bcc1d64c5ace_AD_4nXc1iN_r7dRy7r-hVqm5u1bqpittx5loJEXycHnfVqMJL-EK7hseoI6wfW_4TAaAXLoaEA9K1KkxetK8jJL3rG55Zn-KYwPS_Brlz8-5_OYQihErQyEeZI0CK3fKWcihKiYIS0NpoQ.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><h3 id="""">Feature-Driven Engagement</h3><p id="""">Churn models can highlight which features or behaviors contribute to customer retention. If the model shows that reduced usage of a specific feature is associated with churn, businesses can respond with targeted communications. <em id="""">For example, users who stop using a key feature might receive an in-app message or email with a tutorial or guide explaining its value. This approach helps reestablish engagement and demonstrates the product’s relevance.</em></p><p id="""">Additional strategies include personalized recommendations and cross-sell opportunities. If customers who previously bought some accessories stop doing so, the business might send curated suggestions to encourage repeat purchases. Similarly, content platforms can notify users when new material is available in a genre they previously consumed. These interactions are guided by model outputs that identify which features are most closely tied to retention.</p><h3 id="""">Onboarding and Support Enhancements</h3><p id="""">Churn prediction is especially useful during onboarding. Many customers disengage shortly after sign-up if onboarding is ineffective.<em id=""""> For example, a high proportion of new users abandon products within the first week if setup is confusing or incomplete. Churn scores can help flag these users early.</em></p><p id=""""><strong id="""">Accelerated Onboarding</strong> involves monitoring early activity. Users who stall during setup or ignore key tasks can be enrolled in onboarding programs. This may include walkthroughs, webinars, or direct outreach by the customer success team. The goal is to address early obstacles before the user disengages completely.</p><p id=""""><strong id="""">Priority Support</strong> ensures that high risk users receive prompt assistance. When a customer with a high churn probability submits a support request or shows signs of inactivity, the system can trigger a follow-up by the support team. Timely, personalized support often resolves issues before they lead to churn.</p><h3 id="""">Loyalty and Lifecycle Programs</h3><p id="""">Churn risk insights can inform how and when to offer rewards. Loyalty programs can be adapted to deliver value to customers who are at risk. <em id="""">For example, if a long-time VIP customer suddenly shows churn signals (e.g. spending dips), the company might bump them up to the next loyalty tier or send an exclusive “we value you” gift.</em></p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/688537561865bcc1d64c5bdb_AD_4nXcVuyYQIHu-BlPpSJSVsuZH7YzTNwutxgHFlSCKUSETaNT5wK_D9I-WhbeTajmpSFL2jdpsCYslhhy1mUrSK8q5FR5pvycmIoJ4OrUIpyV8F25a2dYi1VXTGaKlO1IXkw7GtH1oGA.gif"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">Common tactics include tiered rewards, points bonuses, and referral incentives targeted by churn score. For instance, a retailer might double the loyalty points earned on the next purchase for any customer in the high risk segment. Or a subscription service could give at risk members a free add-on (like a month of premium content) if their engagement has dropped. The goal is to reinforce the value of staying, even small perks can help retain wavering customers. Over time, these loyalty boosts translate into higher lifetime value and lower churn.</p><h3 id="""">Sales, Customer Success, and Product Strategy</h3><p id="""">Churn scores are also used by sales and customer success teams to prioritize outreach. In B2B or high-value accounts, automated CRM alerts can notify account managers when churn risk exceeds a threshold. This allows the team to intervene with direct outreach, address concerns, and propose tailored solutions.</p><p id="""">Product teams can use aggregated churn data to improve features and address usage gaps. If customers consistently disengage due to missing functionality, that insight can inform roadmap prioritization. For instance, if a feature is underutilized among churn-prone users, redesign or additional training might be warranted. In some cases, pricing plans may need adjustment if usage ceilings lead to customer exits.</p><h3 id="""">Example Workflows</h3><p id="""">Churn prediction can be integrated into end-to-end workflows across various industries.</p><p id="""">In retail e-commerce, a churn model identifies customers who have not purchased in six months. These customers are further filtered by a redemption likelihood model. Those who qualify receive targeted offers such as discounts or free shipping, resulting in higher engagement with lower promotional waste.</p><p id="""">In subscription services, a user with falling activity is identified before a renewal date. The system delivers a message with a discounted annual plan and personalized content recommendations, increasing the chances of retention.</p><p id="""">In B2B SaaS, declining platform usage triggers a high churn score for an enterprise client. The account manager receives an alert and schedules a check-in call to explore solutions. The feedback is also passed to the product team to inform feature development.</p><p id="""">In financial services, credit card holders flagged as high risk receive custom offers. These may include relevant cashback categories or upgrades to better-suited plans. Offers are delivered through preferred communication channels to increase impact.</p><p id="""">These examples illustrate how churn models support targeted, timely, and effective retention strategies. By integrating predictions into operational workflows, businesses can improve customer engagement, reduce attrition, and optimize resource allocation.</p><p id="""">‍</p><h2 id="""">Want to do Churn Prediction or Analysis for your business?</h2><p id="""">At Mercity AI we have been doing churn prediction as a <em id="""">part</em> of our consultation process. Many times people come to use for something else and we end up doing churn analysis for them as a stepping stone. But looking at the insights they gain from this, we realized that this is a very powerful tool for many businesses. And now hence we are excited to work with more businesses on this.</p><p id="""">If you are interested in performing churn analysis for your business, feel free to <a href=""https://www.mercity.ai/contacts"" id="""">reach out to us!</a></p>",https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/688537df03214c3cdef697fc_Screenshot%202025-07-27%20at%2001.47.14.png,Aditya Aryan,Churn Analysis,Learn how to do churn analysis for your customers so you can retain your customers for longer and understand important business insights,False,"<div class=""rich-text w-richtext""><p>You are running a subscription service, a gym, an online store, or even a learning platform. Things are going well, new customers are coming in. But after a few months, people start dropping off one by one. That’s churn. And if you're not paying attention, it can drain your revenue faster than you think.</p><p>But the good thing is you can see it coming.</p><p>That’s where churn prediction steps in, your business’s early warning system.</p><h2>What is Churn prediction?</h2><p>Churn prediction is the process of identifying customers who are at a high risk of churning in the future. The goal is to flag customers who are likely to churn before they actually do, allowing the business to take steps to retain individuals with a high risk of churn.</p><p>At a high level, churn prediction works by analyzing historical customer data such as behavior, transactions, and engagement. Machine learning models are trained to recognize early signals that often precede churn, like a drop in activity, reduced spending, or negative support interactions. Once trained, the model can assign a churn risk score to current customers, helping the business prioritize those most at risk.</p><h2>Why do companies need Churn Prediction?</h2><p>Churn prediction is critical for any business aiming to maintain growth and strengthen customer relationships. Rather than responding after a customer has already left, predictive models allow companies to intervene early and minimize the risk of attrition. This shift from reactive to preventive strategy improves long term profitability and enhances the overall customer experience.</p><p>At the core of this approach is the fact that retaining customers is significantly more cost-effective than acquiring new ones. Multiple studies confirm that even a small increase in retention can result in disproportionately large improvements in profit margins. </p><p><em>For example, a five percent increase in customer retention has been associated with profit boosts ranging from twenty five to ninety five percent. </em>This efficiency makes churn prediction not just a technical tool but a strategic business asset.</p><h3>Understanding and Acting on Churn Drivers</h3><p>Churn rarely occurs without warning. Behavioral signals such as decreased engagement, frequent complaints, or limited feature usage often precede customer exits. Churn models help surface these patterns in advance, allowing teams to act before the risk materializes.</p><p>This insight is valuable across several business functions. For product teams, identifying the features or touchpoints associated with high churn enables targeted improvements in design or usability. Marketing can craft messaging that resonates with high risk customers by focusing on the aspects of the product they are most likely to find valuable. Customer success and support teams can also use these insights to prioritize outreach and offer proactive assistance to those exhibiting signs of dissatisfaction.</p><p>By using churn understanding into operations, companies can adapt and create a more responsive service environment, ultimately reducing churn and improving user satisfaction over time.</p><h3>Optimizing Retention Efforts and Financial Planning</h3><p>Churn prediction also enhances how businesses allocate resources. Without it, retention strategies often rely on blanket approaches that treat all customers equally. This results in inefficient use of time, budget, and personnel. With churn scores in place, retention efforts can be prioritized toward the most at risk customers, making interventions more precise and effective. Whether through targeted discounts, personalized engagement campaigns, or prioritized support, businesses can apply the right strategy to the right customer at the right moment.</p><p>From a financial standpoint, churn models contribute to greater planning accuracy. High churn rates reduce stability in forecasting, particularly for subscription based or businesses dependent on repeat purchases. Predictions help analysts set realistic growth expectations, manage budget allocations, and prepare contingency plans with greater confidence. They also support a more predictable revenue stream, which is crucial for long term strategies.</p><p>Additionally, reduced churn naturally lowers acquisition pressure. Since acquiring new customers is often several times more expensive than retaining existing ones, a well functioning churn prediction system can directly reduce marketing and sales expenditure. Long-term customers are also more likely to contribute to organic growth through referrals and advocacy, creating further downstream benefits without added cost.</p><h3>Applicability Across Sectors</h3><p>Although the primary focus here is on retail and e-commerce, churn prediction methodologies are relevant across many industries. In telecommunications, software as a service, banking, digital media, and utility services, customer retention plays a central role in revenue stability. The economic impact of churn is consistently significant. Accurate forecasting and timely intervention are thus essential across all customer-centric industries.</p><p>‍</p><h2>How to Build a Churn Prediction Model</h2><p>A good churn model lets a company intervene (with special offers or support) before customers go, protecting revenue and growth.</p><p>Building such a model involves several clear steps: gathering the right data, turning it into useful <em>features</em>,  choosing the right algorithm, and carefully evaluating the results. Each step shapes how well the model will work in real business settings.</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1548pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/688537501865bcc1d64c5ac9_AD_4nXcBrH09ecaOCubsX_xmXhUVYBQ6KVJ1U2sMSjk4iH7GuXxoSwM9jbiWfxqT1wgdg0VhuJxIt1faq6AozTpy1_5RsF1nVqK3VPZ7osTxSGbZteW-jRNlF9wVelR4MxlNy9wFDDViFA.png""/></div></figure><h3>Data Collection</h3><p>The first step is gathering data from all relevant sources. Think of every way you interact with your customers:</p><ul role=""list""><li><strong>Customer Profile</strong>: Demographics like age, location, or account type can influence churn.</li><li><strong>Transaction History</strong>: Records of purchases, subscription changes, or payment failures.</li><li><strong>Behavioral Logs</strong>: Usage data (e.g., login frequency, pages visited, support chat logs) show engagement levels.</li><li><strong>Support &amp; Feedback</strong>: Help tickets or complaint logs can signal dissatisfaction.</li></ul><p>Combine data from systems like CRM, billing, and web analytics so each customer’s history is complete. Ensure consistent customer IDs and timestamps across sources so data aligns correctly. In practice, this means cleaning and linking tables so every purchase or support call matches the right customer record.</p><p>Once collected, look at distributions of key variables or how churners differ from others. You might find churners tend to log in less or have lower lifetime value. This initial analysis helps verify the data quality and suggests which factors are important.</p><h3>Feature Engineering</h3><p>Raw data must be transformed into features that a model can use. In business terms, features are metrics that capture customer behavior and value. Accurate churn prediction models depend heavily on the quality and relevance of the input features. These features should reflect customer behavior, engagement, financial patterns, and satisfaction over time. The goal is to extract signals that precede churn events, enabling early intervention. This section organizes the most effective feature types into categories, describing their rationale and interpretation, and implementation.</p><h4>RFM Features (Recency, Frequency, Monetary Value)</h4><p>The RFM features standing for Recency, Frequency and monetary, is one of the most widely used approaches to capture customer behavior, particularly in churn prediction and segmentation tasks. </p><p><strong>Recency </strong>captures how recently a customer engaged with the product or service. This could be days since their last login, purchase, or session. It is a critical feature because churn often follows a decline in activity. For instance, a user who hasn’t logged in for 60 days is more likely to churn than one who used the platform yesterday. Declining recency is a common early warning sign, especially for previously active users.</p><p><strong>Frequency </strong>measures how often a customer engages over a defined period, such as logins per month or purchases per quarter. High-frequency users are typically more engaged and less likely to churn, as frequent use suggests the product delivers ongoing value. A sharp drop in frequency, even if recency is recent, can still indicate waning interest.</p><p><strong>Monetary </strong>value reflects how much a customer has spent, either in total or over recent intervals. In subscription models, this could be average monthly revenue; in transactional models, cumulative spend. A decline in monetary value may signal reduced perceived value or intent to downgrade. <em>For example, smaller orders or a shift from premium to basic plans may precede churn. Conversely, high-value users may stay despite lower engagement due to their investment.</em></p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1308pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/688537501865bcc1d64c5ad2_AD_4nXe-KklldqM2JWRugcUBQxrQzg6k4XZySkyITFKlTNPQOx-aB_78fmSwXllX6R1nOkshmjRu_1JH1buhcj-g2SU-5-KIBdg0Vygji7gf-8maQGaafM14HvGoNgS63Q8Z4tSSO8_9Sw.png""/></div></figure><p>These RFM metrics can be used individually or in combination to segment users into behavioral cohorts like champions, at-risk users, or lapsed customers. Each segment can then be targeted with appropriate retention strategies.</p><h4>Engagement and Usage Behavior</h4><p>Engagement features go beyond recency and frequency to capture how users interact with the product. They track behavior over specific time windows, reflecting usage depth, session quality, and interaction diversity. This includes active usage metrics such as number of sessions, screen views, feature clicks, or time spent on the platform over the past 7 or 30 days. A decline in usage hours or session counts may signal fading interest, while an uptick may reflect renewed satisfaction after a product update or offer.</p><p>More granular signals include how often specific modules or tools are used. <em>For example, on a SaaS platform, reduced use of a core feature like dashboards might indicate declining reliance.</em> Consistency or variability in engagement is also informative. Metrics like engagement slope, which reflects momentum, or rolling averages of session count help track trends. A flat or downward slope can be an early churn signal even when activity levels are moderate.</p><p>Engagement features help models understand how embedded a product is in a user's routine. A drop in time spent or lack of exploration may indicate disengagement. When modeled well, engagement often correlates strongly with retention and gives early warning before frequency declines.</p><h4>Transactional Behavior</h4><p>Changes in financial activity are strong predictors of churn in subscriptions or e-commerce. Average order value, calculated over a recent window, reflects transaction size. A sustained drop can signal reduced interest or budget limits. Purchase frequency over a recent quarter or year helps assess customer commitment, with fewer purchases suggesting waning satisfaction.</p><p>Tracking subscription changes like downgrades, removed features, or shorter contracts is especially useful, as these often precede churn. Payment issues such as missed or delayed transactions frequently lead to involuntary churn if not resolved quickly. These features closely reflect customer intent and are essential to churn models.</p><h4>Support and Satisfaction Signals</h4><p>Customer frustration often shows up in support data. An increase in tickets, help requests, or unresolved complaints often reflects dissatisfaction. Sentiment from support transcripts, reviews, or chat logs can flag declining experiences.</p><p>Survey-based metrics like Net Promoter Score (NPS) and Customer Satisfaction (CSAT) offer structured feedback. A downward trend in these scores often signals increased churn risk. Support features help differentiate between customers who are confused but can be retained and those genuinely frustrated. Combined with engagement or RFM signals, they help models understand intent, not just usage.</p><h4>Demographic and Contextual Features</h4><p>Demographic and account features capture static traits like age, region, plan tier, device, or tenure. While not dynamic, these help contextualize churn drivers.</p><p>Customer type, subscription tier, or service bundle often correlates with churn. For instance, users on basic plans typically churn more than those on premium ones. Geographic and demographic factors like location or language highlight behavioral differences across groups. Acquisition channels such as organic search, paid ads, or referrals influence loyalty, with different sources yielding different churn rates.</p><p>These features are critical for adjusting for population effects and identifying high-risk user segments. Well-engineered features make even simple models effective by surfacing meaningful patterns in data. They support both churn prediction and the design of retention strategies.</p><h3>Model Selection</h3><p>With features ready, the next step is choosing a predictive algorithm. No single model is best in every situation. The choice depends on factors like desired interpretability and data complexity.</p><h4>Logistic Regression</h4><p>Logistic regression is a linear model that outputs a probability of churn. It is highly interpretable because each feature receives a weight indicating its impact on churn risk. It trains quickly, scales to very large datasets and integrates easily into real‑time systems. As a baseline it is often the first model to try. Its limitation is that it assumes a linear relationship between features and the log‑odds of churn, so it may underperform on complex, non‑linear patterns.</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1071pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/688537501865bcc1d64c5ac6_AD_4nXeYmvRgbZL7Qh4Rp9EkGBIklUCNJWleqUpqQVTlJ3KvuZHWFbaxgoA-OmTRdEnxToZqqDz_ByFSnUq8jgeD_HHLh_y6IvZ1ijBzb3A6Q-XKk3jAhCcBk0Bvh83vIhfvPmjCQLQO.png""/></div></figure><h4>Decision Trees</h4><p>Decision trees classify customers by creating a series of clear, rule-based decisions. These rules are easy to follow and explain to non-technical stakeholders, making trees highly interpretable. They can naturally model non-linear relationships and interactions between features. However, a single decision tree can easily overfit the training data, capturing noise instead of real patterns, which may limit its predictive accuracy on new data. Still, decision trees are a solid choice when you need transparent, easy to communicate models and want to understand the logic behind churn predictions.</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1200pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/688537501865bcc1d64c5ac3_AD_4nXd-Z2sCOcp8TYvns4fXf2-6yX31BxwtD2f1Xh5DNWf_612QgofplFbMsL2Wqd2fMknEjRsH6ye3PIrobRtvyefFlwnju75NTQ0CmvP9S0Lin8N8z_Ruz2w_-yUzQlnLhgNJey466g.png""/></div></figure><h4>Random Forests</h4><p>This model builds many decision trees and averages their predictions. Unlike traditional linear models, it excels at capturing complex, non-linear relationships between customer behaviours and churn likelihood. Random forests use ensemble approaches of multiple trees, minimizing the risk of overfitting, ensuring that the model performs well not only on training data but also on unseen customers. Each tree in the forest considers random subsets of features which makes the model robust, even when certain data points are noisy or incomplete. However, this ensemble approach reduces interpretability, it's no longer easy to say ‘this feature caused churn without additional tools like SHAP. They also require more computing resources, especially on large datasets. Still, they strike a strong balance between performance and generalizability, making them a top choice for many churn prediction systems.</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68853999eaa76c813f0cf410_AD_4nXe8tmYfpvn-7VCsQ9fT7f3syKryuqp3DFvPR-hAxXE1Nf-GL8UwcRDY05dZg8zshTaIECslmNdh2dA5cEM9okWjqfWsc0GE2RTCYb6QGGIFh6zlG6DdDKMEQwan2WSLHS3nKtDeBQ.png""/></div></figure><p>‍</p><h4>Kaplan-Meier Survival Analysis</h4><p>Survival analysis models view churn as a <em>time to event</em> process rather than a classification problem. Unlike traditional models that ask whether a customer will churn within a given window, survival models aim to estimate the probability that a customer will remain active at a future time. They also handle <em>censoring</em>, which happens when a customer hasn't churned by the end of the observation period. This approach avoids bias and gives more useful insights, like the probability a customer stays for 6, 12, or 24 months or computing the median customer lifetime. </p><p>The Kaplan–Meier (KM) estimator is a method used to estimate the <em>survival function</em>, which gives the probability that a customer is still active and has <em>not</em> churned at time t. It creates a stepwise survival curve, starting at probability 1.0 and dropping at each observed churn event. The survival function is defined as:</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:671pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/688537501865bcc1d64c5ab9_AD_4nXeLX5rgV3UfAfwyVHUCLcVba2A14gwez5blQOQsS0qpobcJplyz274u0p7C8lLNiiKjkYP7mrjW7r5Q1X2mvLxRdDEu5pg89dXIpyAC8kHR6DbZMCAIF7vbMsLU5hWFcHMYViVi0g.png""/></div></figure><p>with <em>t<sub>i</sub> </em>a time when at least one <strong>churn event</strong> occurred, <em>d<sub>i</sub> </em>the number of customers who churned at time <em>t<sub>i</sub></em>, and <em>n<sub>i</sub> </em>the number of customers who were still active (had not yet churned or been censored) just before time <em>t<sub>i</sub></em> .</p><p>The Kaplan–Meier method makes no distributional assumptions and is easy for visualizing and comparing survival curves across customer segments. It assumes that survival times are independent, meaning one person churning doesn’t directly affect another, and that censoring is uninformative, meaning customers who haven’t churned are not different from those who did at the same tenure.</p><p>‍</p><pre class=""w-code-block"" contenteditable=""false"" style=""display:block;overflow-x:auto;background:#2b2b2b;color:#f8f8f2;padding:0.5em""><code class=""language-python"" style=""white-space:pre""><span style=""color:#dcc6e0"">import</span><span> pandas </span><span style=""color:#dcc6e0"">as</span><span> pd
</span><span></span><span style=""color:#dcc6e0"">from</span><span> lifelines </span><span style=""color:#dcc6e0"">import</span><span> KaplanMeierFitter
</span><span></span><span style=""color:#d4d0ab""># Load data and preprocess </span><span>
</span><span>df = pd.read_csv(</span><span style=""color:#abe338"">""Telco-Customer-Churn.csv""</span><span>)
</span><span>df[</span><span style=""color:#abe338"">'churn_event'</span><span>] = df[</span><span style=""color:#abe338"">'Churn'</span><span>].</span><span style=""color:#f5ab35"">map</span><span>({</span><span style=""color:#abe338"">'Yes'</span><span>: </span><span style=""color:#f5ab35"">1</span><span>, </span><span style=""color:#abe338"">'No'</span><span>: </span><span style=""color:#f5ab35"">0</span><span>})
</span><span></span><span style=""color:#d4d0ab"">#Create a KaplanMeier object</span><span>
</span>kmf = KaplanMeierFitter()
<span></span><span style=""color:#d4d0ab"">#Calculate the K-M curve for all groups</span><span>
</span><span>kmf.fit(df[</span><span style=""color:#abe338"">'tenure'</span><span>],event_observed = df[</span><span style=""color:#abe338"">'churn_event'</span><span>],label = </span><span style=""color:#abe338"">""All Customers""</span><span>)
</span><span></span><span style=""color:#d4d0ab"">#Plot the curve and assign labels</span><span>
</span>kmf.plot()
<span>plt.ylabel(</span><span style=""color:#abe338"">'Probability of Customer Survival'</span><span>)
</span><span>plt.xlabel(</span><span style=""color:#abe338"">'Tenure'</span><span>)
</span><span>plt.title(</span><span style=""color:#abe338"">'Kaplan-Meier Curve'</span><span>);
</span></code></pre><p>‍</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/688539cdc66e32973b708cb5_AD_4nXfT4CS3jO3c5o5eHikM2paFoTHzwUqGX8lavtskXE8ZitwLDq7_ORcUjMm1nnPsGCNzVdlDIB_wy1p4Pe1T1UMyldxAO7x5dEiXZbsn28VjQiW_yVUnXajlvfEgASZtHqvSdnQo.png""/></div></figure><p>‍</p><p>‍</p><p>The above example uses the <strong>IBM Telco Churn</strong> dataset and estimates customer retention as a function of tenure. Sudden drop in the starting says that after one tenure only customers start churning and over time that churning rate decreases. It can be extended to compare curves across different subgroups.</p><h4>Cox Proportional Hazards Model</h4><p>The Cox Proportional Hazards is a survival regression model that relates customer features (covariates) to churn timing. The model uses customer features and finds how each factor influences churn. Whereas Kaplan-Meier is used to estimate the probability of survival, Cox Proportional Hazards is used to estimate the hazard ratio. The hazard ratio represents the difference in hazard that exists between two individuals (or groups). The hazard is essentially the inverse of survival, or the probability of failure. Kaplan-Meier estimates the probability of survival whereas Cox Proportional Hazards estimates a hazard ratio. As long as you have one, you can calculate the other. </p><p>The Cox Proportional Hazards equation states that the hazard ratio is the product of two terms: the baseline hazard and the partial hazard. </p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/688537501865bcc1d64c5ac0_AD_4nXdT54bp_ysIU5r7QhEEpsfsaC5jcIpJaAx0oFZ81NjEYfNX_TIlkN6s13d14RS9VAw1Ho5NiEvxPbMXMDgo0G3obTMd7ByIcnoHpqc01k7LFuIPV_jhex6Mg1sYE03fDotLCzElbA.png""/></div></figure><p>The baseline hazard is simply a baseline. It's the hazard that exists when each variable is set to some specific value.The partial hazard represents the change in the hazard that occurs when the value for a variable is different from the baseline. The primary assumption is proportional hazards, meaning the effect of a feature on the hazard is constant over time. </p><p>The Cox Proportional Hazards model was fitted on IBM Telco churn data to understand and assess how it performs in modelling customer churn prediction.</p><p>‍</p><pre class=""w-code-block"" contenteditable=""false"" style=""display:block;overflow-x:auto;background:#2b2b2b;color:#f8f8f2;padding:0.5em""><code class=""language-python"" style=""white-space:pre""><span style=""color:#dcc6e0"">import</span><span> pandas </span><span style=""color:#dcc6e0"">as</span><span> pd
</span><span></span><span style=""color:#dcc6e0"">from</span><span> lifelines </span><span style=""color:#dcc6e0"">import</span><span> CoxPHFitter
</span><span></span><span style=""color:#dcc6e0"">import</span><span> matplotlib.pyplot </span><span style=""color:#dcc6e0"">as</span><span> plt
</span><span>df = pd.read_csv(</span><span style=""color:#abe338"">""Telco-Customer-Churn.csv""</span><span>)
</span><span></span><span style=""color:#d4d0ab""># Preprocess features for Cox model</span><span>
</span>regression_df = datapreparation(df)
<span></span><span style=""color:#d4d0ab""># Fit Cox Proportional Hazards model</span><span>
</span>cph = CoxPHFitter()
<span>cph.fit(regression_df, duration_col=</span><span style=""color:#abe338"">'tenure'</span><span>, event_col=</span><span style=""color:#abe338"">'Churn'</span><span>)
</span><span></span><span style=""color:#d4d0ab""># Print model summary</span><span>
</span>cph.print_summary()
<span></span><span style=""color:#d4d0ab""># Plot feature coefficients</span><span>
</span><span>fig, ax = plt.subplots(figsize=(</span><span style=""color:#f5ab35"">10</span><span>, </span><span style=""color:#f5ab35"">7</span><span>))
</span>cph.plot(ax=ax)
</code></pre><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600px""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68853a0e96dfa3033e9b3e47_AD_4nXfiLfijKJvoV3cq__ppNhVe_0aZRaWU2FH0OLrTM21_fteMMAV_lyziureGn0jKCtmlEnE-UJC9e3Xvqgfc0P_XpmrMeuoJyzfUFa8EBcWVaqZsIkscHdKmLUdryUG4jatzwORuxg.png""/></div></figure><p>‍</p><p>‍</p><p> The coefficients show how each feature affects churn risk in relative terms. A log hazard ratio greater than 0 means the feature increases churn risk. <em>For example, higher monthly charges often correlate with a higher chance of churn. A log hazard ratio less than 0 means the feature reduces churn risk. </em></p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/688537501865bcc1d64c5adc_AD_4nXeuTPsb9Zm3xGsYtE0EJfx66KCwnV1jI5iT6soUTq7Wjlc8xIu5RfpKJpXPXawbuz7uz5StQZThjHCV940J2ebPAMMc3Hk9UlV4vfoNLuVRVzapZWkarODJd5smMz5-vXxhGzRGPA.png""/></div></figure><p>The plots above depict Cumulative Hazard and Survival Probability over time. The sharp drop in survival probability between 20 to 40 months suggests that many customers are leaving during this period. Similarly after around 40 months, the cumulative hazard slope becomes steeper, depicting accelerating churn risk.</p><p>When selecting a model, balance interpretability, predictive power, and how it will be used in production. Models with high interpretability are easier to audit and explain but may underperform on complex patterns. Conversely, more accurate models often require more computational resources and lack transparency, making them harder to monitor or trust without explainability tools.</p><h3>Model Evaluation and Metrics</h3><p>After training, you must <em>evaluate</em> how well the model works. In churn problems, raw accuracy (overall percent correct) can be misleading because churn events are relatively rare. For instance, if 90% of customers don’t churn, a model that predicts “no churn” for everyone is 90% accurate but completely useless.</p><p>Instead, focus on metrics that capture the business goal of catching actual churners:</p><ul role=""list""><li><strong>Precision</strong>: It tells us what fraction of the customers predicted to churn actually did. High precision means resources won’t be wasted on targeting customers who wouldn’t have churned, avoiding unnecessary allocation of resources.</li><li><strong>Recall </strong>: Of all customers who actually churned, what fraction did the model catch? High recall ensures you identify most at-risk customers. Missing a true churner (a false negative) can be very costly.</li><li><strong>F1-Score</strong>: The harmonic mean of precision and recall; it balances the two.</li><li><strong>ROC-AUC (Area Under the Curve)</strong>: This measures how well the model ranks customers by churn risk. An AUC close to 1 means the model can distinguish churners from non-churners across all thresholds.<br/><br/></li></ul><p>For churn prediction, precision and recall are often more informative than plain accuracy. You may also use a confusion matrix to see the counts of true/false positives and negatives.<em> From a business view, consider a cost sensitive example, failing to prevent a high value customer's churn (false negative) may result in a greater loss than attempting to retain a low risk customer who would have stayed anyway (false positive).</em></p><p><strong>Cross validation</strong> (testing the model on multiple train/test splits) is also important. It ensures your model’s performance is stable and not just luck on one split. Finally, Finally, metrics should be contextualized in terms of business outcomes. Rather than reporting scores, quantify how many customers the model helps retain, or how much revenue it protects compared to a random or untargeted approach. This translation from statistical evaluation to business value enables businesses to pinpoint weaknesses and take focused steps toward improving.</p><p>‍</p><h3>Model Comparison</h3><p>Different algorithms offer varying strengths in terms of interpretability, accuracy, and scalability. These trade-offs must be considered in light of business goals and the nature of available data.</p><p>When you want high interpretability, you can rely on models like logistic regression, decision trees, and survival analysis. Logistic regression provides clear feature weights and works well when the relationships are mostly linear. Decision trees are intuitive and offer rule based splits that are easy to explain. Kaplan-Meier curves and the Cox model, while part of the survival analysis family, also provide transparency either through visual churn timelines or clear coefficients that show how each factor affects churn risk.</p><p>When the data becomes more complex or the relationships are non-linear, logistic regression tends to underperform. In such cases, survival analysis and ensemble methods like random forests offer better accuracy. Random forests are particularly effective at capturing complex interactions and handling noise, with some trade off in transparency. Survival models, especially the Cox model, can still retain interpretability while adapting to more intricate data structures.</p><p>Overfitting is a challenge particularly with decision trees. Pruning and validation help, but accuracy may still be limited. Random forests reduce this risk making them more reliable, though heavier to deploy. Survival models aren’t typically used for classification, but when timing matters, they provide unique insights that are both strategic and actionable.<em>For example, Kaplan-Meier curves are straightforward plots of churn probability over time, and Cox models let you interpret each factor’s effect on churn risk.</em></p><p>Ultimately, model choice should reflect not just the structure of the data but the kind of insight the business needs whether it’s <em>who will churn</em>, <em>why they churn</em>, or <em>when they are likely to churn</em>. Model selection should align with business priorities. If transparency is essential for communication, an interpretable model such as logistic regression, Survival models or decision trees is appropriate. If the goal is to maximize predictive performance in complex data settings, ensemble models or survival models may be more effective. For large scale deployment, models must also be fast and efficient. The optimal strategy is to evaluate multiple models, assess their performance on relevant business metrics, and select the one that best aligns with ones business model.</p><h2>Using Churn Predictions for Business and Marketing Optimization</h2><p>Predictive churn models allow businesses to identify customers at risk of leaving and intervene strategically. Since retaining an existing customer is significantly more cost-effective than acquiring a new one, churn scores help prioritize efforts. After scoring, customers are segmented into risk cohorts such as high, medium, or low churn probability. Actions are then tailored to each segment. <em>For example, a segment of VIP customers showing reduced activity may be prioritized by the customer success team for retention interventions.</em></p><h3>Targeted Retention Campaigns</h3><p>Once high-risk segments are defined, proactive outreach becomes essential. These cohorts should be engaged with targeted retention campaigns that include personalized incentives and optimized offers.</p><p><strong>Personalized Incentives</strong> are based on the customer's prior behavior. For instance, customers who leave a one- or two-star review are automatically sent a coupon for their next purchase. Other forms include loyalty points, upgrade offers, or access to premium content. Such personalization makes customers feel valued and can reverse attrition.</p><p><strong>Coupon and Offer Optimization:</strong> To avoid wasting marketing spend on ineffective promotions, combine churn risk scores with “redeemability” models. Using dual prediction approaches, one predictive model identifies customers at high risk of churn, while a second model evaluates each customer's likelihood to redeem retention offers. By only sending coupons to those who were both likely to churn and likely to redeem the offer, the company cut costs and retained more customers. In practice, a retailer might first flag customers who haven’t shopped in months (higher churn risk), then filter that list by who’s historically used coupons. Only this intersection receives the coupon code. The same principle applies to free shipping, bundle deals, or trial extensions: target them with data-driven precision. This data-backed approach yields much higher ROI than blanket promotions (up to 30–50% lift in engagement is possible when targeting is precise).</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1330pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/688537501865bcc1d64c5ace_AD_4nXc1iN_r7dRy7r-hVqm5u1bqpittx5loJEXycHnfVqMJL-EK7hseoI6wfW_4TAaAXLoaEA9K1KkxetK8jJL3rG55Zn-KYwPS_Brlz8-5_OYQihErQyEeZI0CK3fKWcihKiYIS0NpoQ.png""/></div></figure><h3>Feature-Driven Engagement</h3><p>Churn models can highlight which features or behaviors contribute to customer retention. If the model shows that reduced usage of a specific feature is associated with churn, businesses can respond with targeted communications. <em>For example, users who stop using a key feature might receive an in-app message or email with a tutorial or guide explaining its value. This approach helps reestablish engagement and demonstrates the product’s relevance.</em></p><p>Additional strategies include personalized recommendations and cross-sell opportunities. If customers who previously bought some accessories stop doing so, the business might send curated suggestions to encourage repeat purchases. Similarly, content platforms can notify users when new material is available in a genre they previously consumed. These interactions are guided by model outputs that identify which features are most closely tied to retention.</p><h3>Onboarding and Support Enhancements</h3><p>Churn prediction is especially useful during onboarding. Many customers disengage shortly after sign-up if onboarding is ineffective.<em> For example, a high proportion of new users abandon products within the first week if setup is confusing or incomplete. Churn scores can help flag these users early.</em></p><p><strong>Accelerated Onboarding</strong> involves monitoring early activity. Users who stall during setup or ignore key tasks can be enrolled in onboarding programs. This may include walkthroughs, webinars, or direct outreach by the customer success team. The goal is to address early obstacles before the user disengages completely.</p><p><strong>Priority Support</strong> ensures that high risk users receive prompt assistance. When a customer with a high churn probability submits a support request or shows signs of inactivity, the system can trigger a follow-up by the support team. Timely, personalized support often resolves issues before they lead to churn.</p><h3>Loyalty and Lifecycle Programs</h3><p>Churn risk insights can inform how and when to offer rewards. Loyalty programs can be adapted to deliver value to customers who are at risk. <em>For example, if a long-time VIP customer suddenly shows churn signals (e.g. spending dips), the company might bump them up to the next loyalty tier or send an exclusive “we value you” gift.</em></p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/688537561865bcc1d64c5bdb_AD_4nXcVuyYQIHu-BlPpSJSVsuZH7YzTNwutxgHFlSCKUSETaNT5wK_D9I-WhbeTajmpSFL2jdpsCYslhhy1mUrSK8q5FR5pvycmIoJ4OrUIpyV8F25a2dYi1VXTGaKlO1IXkw7GtH1oGA.gif""/></div></figure><p>Common tactics include tiered rewards, points bonuses, and referral incentives targeted by churn score. For instance, a retailer might double the loyalty points earned on the next purchase for any customer in the high risk segment. Or a subscription service could give at risk members a free add-on (like a month of premium content) if their engagement has dropped. The goal is to reinforce the value of staying, even small perks can help retain wavering customers. Over time, these loyalty boosts translate into higher lifetime value and lower churn.</p><h3>Sales, Customer Success, and Product Strategy</h3><p>Churn scores are also used by sales and customer success teams to prioritize outreach. In B2B or high-value accounts, automated CRM alerts can notify account managers when churn risk exceeds a threshold. This allows the team to intervene with direct outreach, address concerns, and propose tailored solutions.</p><p>Product teams can use aggregated churn data to improve features and address usage gaps. If customers consistently disengage due to missing functionality, that insight can inform roadmap prioritization. For instance, if a feature is underutilized among churn-prone users, redesign or additional training might be warranted. In some cases, pricing plans may need adjustment if usage ceilings lead to customer exits.</p><h3>Example Workflows</h3><p>Churn prediction can be integrated into end-to-end workflows across various industries.</p><p>In retail e-commerce, a churn model identifies customers who have not purchased in six months. These customers are further filtered by a redemption likelihood model. Those who qualify receive targeted offers such as discounts or free shipping, resulting in higher engagement with lower promotional waste.</p><p>In subscription services, a user with falling activity is identified before a renewal date. The system delivers a message with a discounted annual plan and personalized content recommendations, increasing the chances of retention.</p><p>In B2B SaaS, declining platform usage triggers a high churn score for an enterprise client. The account manager receives an alert and schedules a check-in call to explore solutions. The feedback is also passed to the product team to inform feature development.</p><p>In financial services, credit card holders flagged as high risk receive custom offers. These may include relevant cashback categories or upgrades to better-suited plans. Offers are delivered through preferred communication channels to increase impact.</p><p>These examples illustrate how churn models support targeted, timely, and effective retention strategies. By integrating predictions into operational workflows, businesses can improve customer engagement, reduce attrition, and optimize resource allocation.</p><p>‍</p><h2>Want to do Churn Prediction or Analysis for your business?</h2><p>At Mercity AI we have been doing churn prediction as a <em>part</em> of our consultation process. Many times people come to use for something else and we end up doing churn analysis for them as a stepping stone. But looking at the insights they gain from this, we realized that this is a very powerful tool for many businesses. And now hence we are excited to work with more businesses on this.</p><p>If you are interested in performing churn analysis for your business, feel free to <a href=""https://www.mercity.ai/contacts"">reach out to us!</a></p></div>"
In-depth guide to building a custom GPT-4 chatbot on your data,custom-gpt-4-chatbot,640f56f76d313b2faa631c11,648af5c202810d848d3d1434,False,False,Thu Jun 15 2023 11:28:02 GMT+0000 (Coordinated Universal Time),Thu Sep 11 2025 17:54:25 GMT+0000 (Coordinated Universal Time),Sat Sep 13 2025 16:53:45 GMT+0000 (Coordinated Universal Time),"<p id="""">GPT-4, the latest language model by OpenAI, brings exciting advancements to chatbot technology. These intelligent agents are incredibly helpful in business, improving customer interactions, automating tasks, and boosting efficiency. They can also be used to automate customer service tasks, such as providing product information, answering FAQs, and helping customers with account setup. This can lead to increased customer satisfaction and loyalty, as well as improved sales and profits.</p><p id="""">‍</p><p id="""">Chatbots powered by GPT-4 can scale across sales, marketing, customer service, and onboarding. They understand user queries, adapt to context, and deliver personalized experiences. By leveraging the GPT-4 language model, businesses can build a powerful chatbot that can offer personalized experiences and help drive their customer relationships.&nbsp;</p><p id="""">‍</p><p id="""">In this article, we'll show you how to build a personalized GPT-4 chatbot trained on your dataset.&nbsp;</p><h2 id="""">What is GPT-4?</h2><p id=""""><a href=""https://openai.com/gpt-4"" id="""">GPT-4</a> is a large multimodal transformer model developed by OpenAI that accepts image and text inputs and emits text outputs. It shows human-level performance on various professional and academic benchmarks.&nbsp;</p><p id="""">‍</p><p id="""">GPT-4 has shown amazing <em id="""">reasoning</em> capabilities. It has passed many difficult exams like SAT and even the bar exam. These capabilities make it the best model out there. We can use GPT4 to build sales chatbots, marketing chatbots and do a ton of other business operations.</p><p id="""">‍</p><p id="""">As GPT is a <em id="""">General Purpose Technology</em> it can be used in a wide variety of tasks outside of just chatbots. It can be used to generate ad copy, and landing pages, handle sales negotiations, summarize sales calls, and a lot more. In this article, we will focus specifically on how to build a GPT-4 chatbot on a custom knowledge base.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1154px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1154px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/648af0a190b754e37d9fa3cb_dsfZRtWZyIk1x5_BWI_dD2CHJQa742mV0IoFUk2ziTpwqPPH8QaEDklmA-ndvgjPTqoAu2SCTV_20k6dhcq2fjxnTBVhHKcKsNkGXIoFSzSXKstr63gm2FDC5BypqmxwxQEKlO2gp4p_x2mMZNqSQxg.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h3 id="""">How is GPT-4 better than other GPT models?</h3><p id="""">GPT-4 promises a huge performance leap over GPT-3 and other GPT models, including an improvement in the generation of text that mimics human behavior and speed patterns. GPT-4 is able to handle language translation, text summarization, and other tasks in a more versatile and adaptable manner. GPT-4 is more reliable, creative, and able to handle much more nuanced instructions than its predecessors GPT-3 and ChatGPT.</p><p id="""">‍</p><p id="""">GPT-4’s enhanced capabilities can be leveraged for a wide range of business applications. Its improved performance in generating human-like text can be used for tasks such as content generation, customer support, and language translation. Its ability to handle tasks in a more versatile and adaptable manner can also be beneficial for businesses looking to automate processes and improve efficiency. GPT-4 is able to follow much more complex instructions compared to GPT-3 successfully.</p><p id="""">‍</p><p id="""">Here is a performance comparison between GPT-3 and GPT-4, provided in their <a href=""https://arxiv.org/abs/2303.08774"" id="""">technical report</a>:</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1447px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1447px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/648af0a1dd4bd8e86d3bc5fe_zUA5T2upSBBl3jmPU2tmh28fFwwA5xNHFM6nfpbM40RTvbBR8hUBWnxw47aGfL9NbInSJZ8W3tZse1IvepnGvMR8NAJRuQJHssIiFliQB92987CNv3xPK_xofgBgPvUd1MJGi4Rb7fI2uYiq09TFBEk.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">‍</p><h2 id="""">Why build a custom GPT-4 Chatbot?</h2><p id="""">Large Language Models or LLMs are trained on a massive dataset of text and code. They perform well for most of the general scenarios and for some very domain-specific settings too like <a href=""https://www.mercity.ai/blog-post/gpt-nlp-in-sales"" id="""">summarizing sales calls</a>. But for business use cases, they need to be fine-tuned to the user’s needs. This is usually done by modifying the prompt passed to the model, this process is called <em id="""">prompt engineering.</em> Here are some reasons why to customize base GPT-4 or GPT-3 models:</p><p id="""">‍</p><h3 id="""">GPT-4 Knowledge Base is Limited</h3><p id="""">Even though trained on massive datasets, LLMs always lack some knowledge about very specific data. Data that is not publically available is the best example of this. Data like private user information, medical documents, and confidential information are not included in the training datasets, and rightfully so. This means if you want to ask GPT questions based on <strong id="""">your</strong> customer data, it will simply fail, as it does not know of that. Or it might <em id="""">hallucinate</em>, that is to give wrong answers or replies.</p><p id="""">‍</p><p id="""">Another huge way GPT-4 is limited is that it lacks knowledge of events that occurred after September 2021. The training dataset is limited and is not updated, and neither does the model learns from user interactions. This is a huge limitation on the model’s capabilities.</p><p id="""">‍</p><p id="""">In the article, we will cover how to use your own knowledge base with GPT-4 using embeddings and prompt engineering.</p><p id="""">‍</p><h3 id="""">To reduce hallucinations</h3><p id="""">As mentioned, GPT models can hallucinate and provide wrong answers to users’ questions. This happens because models are a class of <em id="""">autoregressive models</em>. Meaning, at the core they work by predicting the next word in the conversation. This means if the model is not prompted correctly, the outputs can be very wrong.</p><p id="""">‍</p><p id="""">To reduce this issue, it is important to provide the model with the right prompts. This means providing the model with the right context and data to work with. This will help the model to better understand the context and provide more accurate answers. It is also important to monitor the model’s performance and adjust the prompts accordingly. This will help to ensure that the model is providing the right answers and reduce the chances of hallucinations.</p><p id="""">‍</p><h3 id="""">To control conversation and tonality</h3><p id="""">Sometimes it is necessary to control how the model responds and what kind of language it uses. For example, if a company wants to have a more formal conversation with its customers, it is important that we prompt the model that way. Or if you are building an e-learning platform, you want your chatbot to be helpful and have a softer tone, you want it to interact with the students in a specific way.&nbsp;</p><p id="""">‍</p><p id="""">It is also important to limit the chatbot model to specific topics, users might want to chat about many topics, but that is not good from a business perspective. If you are building a tutor chatbot, you want the conversation to be limited to the lesson plan. This can usually be prevented using prompting techniques, but there are techniques such as <em id="""">prompt injection</em> which can be used to trick the model into talking about topics it is not supposed to.</p><p>‍</p><h3 id="""">To personalize GPT to your needs</h3><p id="""">A personalized GPT model is a great tool to have in order to make sure that your conversations are tailored to your needs.&nbsp; GPT4 can be personalized to specific information that is unique to your business or industry. This allows the model to understand the context of the conversation better and can help to reduce the chances of wrong answers or hallucinations. One can personalize GPT by providing documents or data that are specific to the domain. This is important when you want to make sure that the conversation is helpful and appropriate and related to a specific topic. Personalizing GPT can also help to ensure that the conversation is more accurate and relevant to the user.</p><p>‍</p><p id="""">The personalization feature is now common among most of the products that use GPT4. Users are allowed to create a persona for their GPT model and provide it with data that is specific to their domain. This helps to make sure that the conversation is tailored to the user’s needs and that the model is able to understand the context better. For example,&nbsp; if you are a copywriter, you can provide the model with examples of your work and prompt it with various copywriting techniques to help it understand the context and generate better copy. Custom chatbots are the future.</p><p id="""">‍</p><h2 id="""">How to build a custom GPT-4 chatbot?</h2><p id="""">A custom chatbot at the core is a combination of two core things, <strong id="""">prompts</strong> and <strong id="""">providing the right context</strong>. Prompts control the model behavior and provide the model with a guideline on how to interact with the user. Context on the other hand is the knowledge base using which the model answers the user queries. Both of these components are necessary for a good chatbot and should be tailored to the specific use case. Here is the pipeline we use to build custom chatbots:</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1327px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1327px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/648af0a11c79c932f55714de_Lj31OMeSRlvW0Fl6u6dzo51mDIfRhGQIKUrOvexBys7Rw_hhTpuMBWOoUceoQu45bJEgXPfZ_-S6und-ElT4iTAsFkSUX1aJWpse8qpQFTwDQVwxvzLxoY0sIC4zoqNMvg1kCACem_EQutzgADhW8zo.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">‍</p><p id="""">Let’s break down the concepts and components required to build a custom chatbot.</p><h3 id="""">Chatbot</h3><p id="""">The chatbot is a large language model fine-tuned for chatting behavior. ChatGPT/GPT3.5, GPT-4, and LLaMa are some examples of LLMs fine-tuned for chat-based interactions. It is not necessary to use a chat fine-tuned model, but it will perform much better than using an LLM that is not. We will use GPT-4 in this article, as it is easily accessible via <a href=""https://platform.openai.com/docs/api-reference/chat"" id="""">GPT-4 API</a> provided by OpenAI.</p><p id="""">‍</p><p id="""">Chatbot here is interacting with users and providing them with relevant answers to their queries in a conversational way. It is also capable of understanding the provided context and replying accordingly. This helps the chatbot to provide more accurate answers and reduce the chances of hallucinations. Based on user interactions, the chatbot’s knowledge base can be updated with time. This helps the chatbot to provide more accurate answers over time and personalize itself to the user's needs.</p><h3 id="""">Embedding Generator</h3><p id="""">Our chatbot model needs access to proper context to answer the user questions. This is basically how you make GPT-4 personalized to your data. Embeddings are at the core of the context retrieval system for our chatbot. We convert our custom knowledge base into embeddings so that the chatbot can find the relevant information and use it in the conversation with the user.</p><p id="""">‍</p><p id="""">Embeddings allow us to map our data into a vector space, which can be used by the model to understand the context. Embeddings are learned by the LLMs during training. The core property of embeddings is that they carry the semantic meaning of the sentences in the form of vectors. This means embedding vectors of two sentences that are similar in nature such as: <em id="""">“I live in New York.” </em>and <em id="""">“I live in Los Angeles.” </em>will have their embedding vectors very similar. Whereas a sentence like <em id="""">“James has a dog.” </em>&nbsp;will have its embedding very different from the two. This property of embeddings allows us to retrieve relevant documents to answer the user query.</p><p id="""">‍</p><p id="""">We will use a custom embedding generator to generate embeddings for our data. One can use <a href=""https://openai.com/blog/introducing-text-and-code-embeddings"" id="""">OpenAI embeddings</a> or <a href=""https://www.sbert.net/"" id="""">SBERT</a> models for this generating embeddings. Also, this process can be decoupled from the rest of the pipeline.</p><h4 id="""">Embedding Large Documents</h4><p id="""">If you have a large number of documents or if your documents are too large to be passed in the context window of the model, we will have to pass them through a chunking pipeline. This will make smaller chunks of text which can then be passed to the model. This is usually done even if the documents are small. This process ensures that the model only receives the necessary information, too much information about topics not related to the query can confuse the model.</p><p id="""">‍</p><p id="""">Here’s a quick overview of our chunking pipeline:</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1306px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1306px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/648af0a1cc9c908a8a5ab79a_ohJuwsbhTZ_o8m4QZyGFxJ2oMOonFwF-tIyqsI7306PkEZwATI91wKK9ztIqiS8tfwoO4jeRILUR39-kE-GUNSksKri_U5TyZgjMTBLaoNywrLzVVnJmycCteHZ79WYHawatqZZWLel4LY4mteKzb5M.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h3 id="""">Retrieving Documents</h3><p id="""">Once we have our embeddings ready, we need to store and retrieve them properly to find the correct document or chunk of text which can help answer the user queries. As explained before, embeddings have the natural property of carrying semantic information. If the embeddings of two sentences are closer, they have similar meanings, if not, they have different meanings. We use this property of embeddings to retrieve the documents from the database. The query embedding is matched to each document embedding in the database, and the similarity is calculated between them. Based on the threshold of similarity, the interface returns the chunks of text with the most relevant document embedding which helps to answer the user queries.</p><p id="""">‍</p><p id="""">Here is a good representation by OpenAI of how embeddings work:</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:903px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""903px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/648af0a2ebc57bea4e1813d5_gcTZxQ46w7XyLACDONwwoamlYpzrrgAzcrrrlxV3t8FrFq_MgwREaRTJZnMKdtdA9H-1aZXbTzk9Xv88kChoa1Fr46isyciqMusd5Qg0cim98NQC_YlXBJo74dRAXwArR0vValem0c4qQgqCebVLeyI.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">‍</p><p id="""">To store embeddings, we use special databases called <strong id="""">Vector Databases</strong>. These databases, store vectors in a way that makes them easily searchable. Some good examples of these kinds of databases are <a href=""https://www.pinecone.io/"" id="""">Pinecone</a>, <a href=""https://weaviate.io/"" id="""">Weaviate</a>, and <a href=""https://milvus.io/"" id="""">Milvus</a>.&nbsp;</p><p id="""">‍</p><p id="""">Once we have the relevant embeddings, we retrieve the chunks of text which correspond to those embeddings. The chunks are then given to the chatbot model as the context using which it can answer the user’s queries and carry the conversation forward.</p><p id="""">‍</p><p id="""">For example, if you were building a custom chatbot for books, we will convert the book’s paragraphs into chunks and convert them into embeddings. Once we have that, we can fetch the relevant paragraphs required to answer the question asked by the user.</p><p id="""">‍</p><h3 id="""">Prompt Tuning for Language and Tonality</h3><p id="""">It is very important that the chatbot talks to the users in a specific tone and follow a specific language pattern. This is why prompt tuning is necessary. We want the chatbot to have a <em id="""">personality</em> based on the task at hand. If it is a sales chatbot we want the bot to reply in a friendly and persuasive tone. If it is a customer service chatbot, we want the bot to be more formal and helpful. We also want the chat topics to be somewhat restricted, if the chatbot is supposed to talk about issues faced by customers, we want to stop the model from talking about any other topic.</p><p id="""">‍</p><p id="""">To control the language and the topics of the chatbot, we modify the prompts accordingly. Here are some techniques we use:</p><h4 id="""">Few-Shot Prompting</h4><p id="""">The model can be provided with some examples of how the conversation should be continued in specific scenarios, it will learn and use similar mannerisms when those scenarios happen. This is one of the best ways to tune the model to your needs, the more examples you provide, the better the model responses will be.</p><p id="""">‍</p><p id="""">Here is an example of few shot prompting:</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1599px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1599px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/648af0a21c79c932f55715e6_5f2uLz6VQJyuxRDn8BQLPK4CTswOt58G8hUc8nfAMEDVDegpORwBCdmFfac7KFibnbbMpi3wdnBCdR2I15op5foUPLskV6XFbgLlpOzsEq2ualIg-7wVXs8eQCdrYCJJ_X6nc5uTZ0XKxvK7pAz-Wi8.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">‍</p><p id="""">Here we provided GPT-4 with scenarios and it was able to use it in the conversation right out of the box! The process of providing good few-shot examples can itself be automated if there are way too many examples to be provided. We can use an embedding-based pipeline for this too.</p><p id="""">‍</p><h4 id="""">Parameter Tuning</h4><p id="""">Another very important thing to do is to tune the parameters of the chatbot model itself. This part is often overlooked. All LLMs have some parameters that can be passed to control the behavior and outputs.&nbsp;</p><ul id=""""><li id=""""><strong id="""">Temperature: </strong>This parameter is used to control the <em id="""">randomness</em> of the outputs. Higher temperature values (e.g., 1.0) result in more varied and creative outputs, while lower values (e.g., 0.5) lead to more focused and deterministic responses. It is important to test the outputs of the model with a smaller temperature and then increase it slowly to find the value which works the best for you.</li><li id=""""><strong id="""">Top-P: </strong>Top-P or nucleus sampling helps in selecting how many of the tokens should be considered when predicting the next word or token. It sets a threshold value (e.g., 0.8) to consider the most probable words, balancing diversity and coherence in the generated text.</li><li id=""""><strong id="""">Maximum Length: </strong>This parameter sets the maximum allowed tokens or characters in the generated text to control its length and ensure relevance. This also depends on the use case at hand. If we are building a simple chatbot, it makes sense to use a smaller maximum length. If the chatbot is supposed to “explain” things to the user, then we are going to need a bigger maximum length value.</li><li id=""""><strong id="""">Frequency and Presence Penalties: </strong>Both of these parameters reduce repetitive words or phrases in the generated text by assigning a penalty, promoting diversity. The frequency penalty does this based on the number of times the word has already appeared in the output text. The presence penalty only takes into account the presence of the word, the frequency of the word does not matter, it is a much harder penalty.</li></ul><h3 id="""">Building a Custom Chatbot with Langchain</h3><p id="""">With new Python libraries like&nbsp; LangChain, AI developers can easily integrate Large Language Models (LLMs) like GPT-4 with external data. LangChain works by breaking down large sources of data into ""chunks"" and embedding them into a Vector Store. This Vector Store can then be queried by the LLM to generate answers based on the prompt. It basically automates the chatbot pipeline completely.</p><p id="""">‍</p><p id="""">Langchain provides developers with components like <strong id="""">index</strong>, <strong id="""">model,</strong> and <strong id="""">chain</strong> which make building custom chatbots very easy. You can read more about the core components of langchain <a href=""https://docs.langchain.com/docs/category/components"" id="""">here</a>.</p><h2 id="""">Traditional NLP Chatbots vs GPT-4</h2><p id="""">Before GPT based chatbots, more traditional techniques like sentiment analysis, keyword matching, etc were used to build chatbots. These chatbots used rule-based systems to understand the user’s query and then reply accordingly. This approach was very limited as it could only understand the queries which were predefined.&nbsp;</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1106px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1106px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/648af0a2da4e0285c330c337_U1rRnDb8bjArgbK7rWLYzhwJJZvXOG1Ep47gV2lDax0ecSkfS_HCEXY2M6HvlCqrNuUPt2qJoujwPoINWNeMz6AiwXiUWue6UtF4mruoEsrl1nRUidaalVnDLHPlfyRDR5jO6vDehEwMqbZSfyjpzbc.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">‍</p><p id="""">The classifier can be a machine learning algo like Decision Tree or a BERT based model that extracts the intent of the message and then replies from a predefined set of examples based on the intent. This approach is very limited and non-flexible. GPT models can understand user query and answer it even a solid example is not given in examples.&nbsp;</p><p id="""">‍</p><p id="""">Here are some ways how GPT-4 and other GPT based models solve limitations of the traditional chatbots:</p><p id="""">‍</p><h3 id="""">GPT models are more flexible</h3><p id="""">As mentioned above, traditional chatbots follow a rule based approach. They are not flexible and require a lot of human oversight. Businesses have to spend a lot of time and money to develop and maintain the rules. Also, the rules are often rigid and do not allow for any customization. This is not a very scalable model.</p><p id="""">‍</p><p id="""">On the other hand, GPT-4 offer a more flexible approach. These models use large transformer based networks to learn the context of the user’s query and generate appropriate responses. This allows for much more personalized replies as it can understand the context of the user’s query. It also allows for more scalability as businesses do not have to maintain the rules and can focus on other aspects of their business. These models are much more flexible and can adapt to a wide range of conversation topics and handle unexpected inputs.</p><p id="""">‍</p><h3 id="""">GPT models have a better understanding of user query</h3><p id="""">Models like GPT-4 have been trained on large datasets and are able to capture the nuances and context of the conversation, leading to more accurate and relevant responses. GPT-4 is able to comprehend the meaning behind user queries, allowing for more sophisticated and intelligent interactions with users. This improved understanding of user queries helps the model to better answer the user’s questions, providing a more natural conversation experience.</p><p id="""">‍</p><p id="""">Traditional techniques like intent-classification bots fail terribly at this because they are trained to classify what th user is saying into predefined buckets. Often it is the case that user has multiple intents within the same the message, or have a much complicated message than the model can handle. GPT-4 on the other hand “understands” what the user is trying to say, not just classify it, and proceeds accordingly.</p><h3 id="""">GPT models can be customized for any context</h3><p id="""">GPT-4 can be customized very quickly with some prompt engineering. If you are trying to build a customer support chatbot, you can provide some customer service related prompts to the model and it will quickly learn the language and tonality used in customer service. It will also learn the context of the customer service domain and be able to provide more personalized and tailored responses to customer queries. And because the context is passed to the prompt, it is super easy to change the use-case or scenario for a bot by changing what contexts we provide.</p><p id="""">‍</p><p id="""">Traditional chatbots on the other hand might require full on training for this. They need to be trained on a specific dataset for every use case and the context of the conversation has to be trained with that. This is very limiting. With GPT models the context is passed in the prompt, so the custom knowledge base can grow or shrink over time without any modifications to the model itself.</p><h2 id="""">Why use Custom Chatbots for your Business?</h2><p id="""">Custom chatbots provide a lot of benefits for businesses. They provide a more personalized and efficient customer experience by offering instant responses to user queries and automating common tasks. Custom chatbots can handle a large volume of inquiries simultaneously, reducing the need for human teams and increasing operational efficiency. Additionally, they can be integrated with existing systems and databases, allowing for seamless access to information and enabling smooth interactions with customers. Businesses can save a lot of time, reduce costs, and enhance customer satisfaction using custom chatbots.</p><h2 id="""">Want to build a Custom Personalized Chatbot?</h2><p id="""">If you are looking to build chatbots trained on custom datasets and knowledge bases, Mercity.ai can help. We specialize in developing highly tailored chatbot solutions for various industries and business domains, leveraging your specific data and industry knowledge. Whether you need a chatbot optimized for sales, customer service, or on-page ecommerce, our expertise ensures that the chatbot delivers accurate and relevant responses. Contact us today and let us create a custom chatbot solution that revolutionizes your business.</p><p id="""">‍</p>",https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6477c3c819cc27766b39c8ad_nlp-in-sales.png,Pranav,Chatbots,Learn how to build a custom GPT-4 chatbot on a knowledge base. GPT-4 models can be customized with prompt engineering to provide personalized responses to user queries in any domain.,False,"<div class=""rich-text w-richtext""><p>GPT-4, the latest language model by OpenAI, brings exciting advancements to chatbot technology. These intelligent agents are incredibly helpful in business, improving customer interactions, automating tasks, and boosting efficiency. They can also be used to automate customer service tasks, such as providing product information, answering FAQs, and helping customers with account setup. This can lead to increased customer satisfaction and loyalty, as well as improved sales and profits.</p><p>‍</p><p>Chatbots powered by GPT-4 can scale across sales, marketing, customer service, and onboarding. They understand user queries, adapt to context, and deliver personalized experiences. By leveraging the GPT-4 language model, businesses can build a powerful chatbot that can offer personalized experiences and help drive their customer relationships. </p><p>‍</p><p>In this article, we'll show you how to build a personalized GPT-4 chatbot trained on your dataset. </p><h2>What is GPT-4?</h2><p><a href=""https://openai.com/gpt-4"">GPT-4</a> is a large multimodal transformer model developed by OpenAI that accepts image and text inputs and emits text outputs. It shows human-level performance on various professional and academic benchmarks. </p><p>‍</p><p>GPT-4 has shown amazing <em>reasoning</em> capabilities. It has passed many difficult exams like SAT and even the bar exam. These capabilities make it the best model out there. We can use GPT4 to build sales chatbots, marketing chatbots and do a ton of other business operations.</p><p>‍</p><p>As GPT is a <em>General Purpose Technology</em> it can be used in a wide variety of tasks outside of just chatbots. It can be used to generate ad copy, and landing pages, handle sales negotiations, summarize sales calls, and a lot more. In this article, we will focus specifically on how to build a GPT-4 chatbot on a custom knowledge base.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1154pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/648af0a190b754e37d9fa3cb_dsfZRtWZyIk1x5_BWI_dD2CHJQa742mV0IoFUk2ziTpwqPPH8QaEDklmA-ndvgjPTqoAu2SCTV_20k6dhcq2fjxnTBVhHKcKsNkGXIoFSzSXKstr63gm2FDC5BypqmxwxQEKlO2gp4p_x2mMZNqSQxg.png""/></div></figure><h3>How is GPT-4 better than other GPT models?</h3><p>GPT-4 promises a huge performance leap over GPT-3 and other GPT models, including an improvement in the generation of text that mimics human behavior and speed patterns. GPT-4 is able to handle language translation, text summarization, and other tasks in a more versatile and adaptable manner. GPT-4 is more reliable, creative, and able to handle much more nuanced instructions than its predecessors GPT-3 and ChatGPT.</p><p>‍</p><p>GPT-4’s enhanced capabilities can be leveraged for a wide range of business applications. Its improved performance in generating human-like text can be used for tasks such as content generation, customer support, and language translation. Its ability to handle tasks in a more versatile and adaptable manner can also be beneficial for businesses looking to automate processes and improve efficiency. GPT-4 is able to follow much more complex instructions compared to GPT-3 successfully.</p><p>‍</p><p>Here is a performance comparison between GPT-3 and GPT-4, provided in their <a href=""https://arxiv.org/abs/2303.08774"">technical report</a>:</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1447pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/648af0a1dd4bd8e86d3bc5fe_zUA5T2upSBBl3jmPU2tmh28fFwwA5xNHFM6nfpbM40RTvbBR8hUBWnxw47aGfL9NbInSJZ8W3tZse1IvepnGvMR8NAJRuQJHssIiFliQB92987CNv3xPK_xofgBgPvUd1MJGi4Rb7fI2uYiq09TFBEk.png""/></div></figure><p>‍</p><h2>Why build a custom GPT-4 Chatbot?</h2><p>Large Language Models or LLMs are trained on a massive dataset of text and code. They perform well for most of the general scenarios and for some very domain-specific settings too like <a href=""https://www.mercity.ai/blog-post/gpt-nlp-in-sales"">summarizing sales calls</a>. But for business use cases, they need to be fine-tuned to the user’s needs. This is usually done by modifying the prompt passed to the model, this process is called <em>prompt engineering.</em> Here are some reasons why to customize base GPT-4 or GPT-3 models:</p><p>‍</p><h3>GPT-4 Knowledge Base is Limited</h3><p>Even though trained on massive datasets, LLMs always lack some knowledge about very specific data. Data that is not publically available is the best example of this. Data like private user information, medical documents, and confidential information are not included in the training datasets, and rightfully so. This means if you want to ask GPT questions based on <strong>your</strong> customer data, it will simply fail, as it does not know of that. Or it might <em>hallucinate</em>, that is to give wrong answers or replies.</p><p>‍</p><p>Another huge way GPT-4 is limited is that it lacks knowledge of events that occurred after September 2021. The training dataset is limited and is not updated, and neither does the model learns from user interactions. This is a huge limitation on the model’s capabilities.</p><p>‍</p><p>In the article, we will cover how to use your own knowledge base with GPT-4 using embeddings and prompt engineering.</p><p>‍</p><h3>To reduce hallucinations</h3><p>As mentioned, GPT models can hallucinate and provide wrong answers to users’ questions. This happens because models are a class of <em>autoregressive models</em>. Meaning, at the core they work by predicting the next word in the conversation. This means if the model is not prompted correctly, the outputs can be very wrong.</p><p>‍</p><p>To reduce this issue, it is important to provide the model with the right prompts. This means providing the model with the right context and data to work with. This will help the model to better understand the context and provide more accurate answers. It is also important to monitor the model’s performance and adjust the prompts accordingly. This will help to ensure that the model is providing the right answers and reduce the chances of hallucinations.</p><p>‍</p><h3>To control conversation and tonality</h3><p>Sometimes it is necessary to control how the model responds and what kind of language it uses. For example, if a company wants to have a more formal conversation with its customers, it is important that we prompt the model that way. Or if you are building an e-learning platform, you want your chatbot to be helpful and have a softer tone, you want it to interact with the students in a specific way. </p><p>‍</p><p>It is also important to limit the chatbot model to specific topics, users might want to chat about many topics, but that is not good from a business perspective. If you are building a tutor chatbot, you want the conversation to be limited to the lesson plan. This can usually be prevented using prompting techniques, but there are techniques such as <em>prompt injection</em> which can be used to trick the model into talking about topics it is not supposed to.</p><p>‍</p><h3>To personalize GPT to your needs</h3><p>A personalized GPT model is a great tool to have in order to make sure that your conversations are tailored to your needs.  GPT4 can be personalized to specific information that is unique to your business or industry. This allows the model to understand the context of the conversation better and can help to reduce the chances of wrong answers or hallucinations. One can personalize GPT by providing documents or data that are specific to the domain. This is important when you want to make sure that the conversation is helpful and appropriate and related to a specific topic. Personalizing GPT can also help to ensure that the conversation is more accurate and relevant to the user.</p><p>‍</p><p>The personalization feature is now common among most of the products that use GPT4. Users are allowed to create a persona for their GPT model and provide it with data that is specific to their domain. This helps to make sure that the conversation is tailored to the user’s needs and that the model is able to understand the context better. For example,  if you are a copywriter, you can provide the model with examples of your work and prompt it with various copywriting techniques to help it understand the context and generate better copy. Custom chatbots are the future.</p><p>‍</p><h2>How to build a custom GPT-4 chatbot?</h2><p>A custom chatbot at the core is a combination of two core things, <strong>prompts</strong> and <strong>providing the right context</strong>. Prompts control the model behavior and provide the model with a guideline on how to interact with the user. Context on the other hand is the knowledge base using which the model answers the user queries. Both of these components are necessary for a good chatbot and should be tailored to the specific use case. Here is the pipeline we use to build custom chatbots:</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1327pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/648af0a11c79c932f55714de_Lj31OMeSRlvW0Fl6u6dzo51mDIfRhGQIKUrOvexBys7Rw_hhTpuMBWOoUceoQu45bJEgXPfZ_-S6und-ElT4iTAsFkSUX1aJWpse8qpQFTwDQVwxvzLxoY0sIC4zoqNMvg1kCACem_EQutzgADhW8zo.png""/></div></figure><p>‍</p><p>Let’s break down the concepts and components required to build a custom chatbot.</p><h3>Chatbot</h3><p>The chatbot is a large language model fine-tuned for chatting behavior. ChatGPT/GPT3.5, GPT-4, and LLaMa are some examples of LLMs fine-tuned for chat-based interactions. It is not necessary to use a chat fine-tuned model, but it will perform much better than using an LLM that is not. We will use GPT-4 in this article, as it is easily accessible via <a href=""https://platform.openai.com/docs/api-reference/chat"">GPT-4 API</a> provided by OpenAI.</p><p>‍</p><p>Chatbot here is interacting with users and providing them with relevant answers to their queries in a conversational way. It is also capable of understanding the provided context and replying accordingly. This helps the chatbot to provide more accurate answers and reduce the chances of hallucinations. Based on user interactions, the chatbot’s knowledge base can be updated with time. This helps the chatbot to provide more accurate answers over time and personalize itself to the user's needs.</p><h3>Embedding Generator</h3><p>Our chatbot model needs access to proper context to answer the user questions. This is basically how you make GPT-4 personalized to your data. Embeddings are at the core of the context retrieval system for our chatbot. We convert our custom knowledge base into embeddings so that the chatbot can find the relevant information and use it in the conversation with the user.</p><p>‍</p><p>Embeddings allow us to map our data into a vector space, which can be used by the model to understand the context. Embeddings are learned by the LLMs during training. The core property of embeddings is that they carry the semantic meaning of the sentences in the form of vectors. This means embedding vectors of two sentences that are similar in nature such as: <em>“I live in New York.” </em>and <em>“I live in Los Angeles.” </em>will have their embedding vectors very similar. Whereas a sentence like <em>“James has a dog.” </em> will have its embedding very different from the two. This property of embeddings allows us to retrieve relevant documents to answer the user query.</p><p>‍</p><p>We will use a custom embedding generator to generate embeddings for our data. One can use <a href=""https://openai.com/blog/introducing-text-and-code-embeddings"">OpenAI embeddings</a> or <a href=""https://www.sbert.net/"">SBERT</a> models for this generating embeddings. Also, this process can be decoupled from the rest of the pipeline.</p><h4>Embedding Large Documents</h4><p>If you have a large number of documents or if your documents are too large to be passed in the context window of the model, we will have to pass them through a chunking pipeline. This will make smaller chunks of text which can then be passed to the model. This is usually done even if the documents are small. This process ensures that the model only receives the necessary information, too much information about topics not related to the query can confuse the model.</p><p>‍</p><p>Here’s a quick overview of our chunking pipeline:</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1306pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/648af0a1cc9c908a8a5ab79a_ohJuwsbhTZ_o8m4QZyGFxJ2oMOonFwF-tIyqsI7306PkEZwATI91wKK9ztIqiS8tfwoO4jeRILUR39-kE-GUNSksKri_U5TyZgjMTBLaoNywrLzVVnJmycCteHZ79WYHawatqZZWLel4LY4mteKzb5M.png""/></div></figure><h3>Retrieving Documents</h3><p>Once we have our embeddings ready, we need to store and retrieve them properly to find the correct document or chunk of text which can help answer the user queries. As explained before, embeddings have the natural property of carrying semantic information. If the embeddings of two sentences are closer, they have similar meanings, if not, they have different meanings. We use this property of embeddings to retrieve the documents from the database. The query embedding is matched to each document embedding in the database, and the similarity is calculated between them. Based on the threshold of similarity, the interface returns the chunks of text with the most relevant document embedding which helps to answer the user queries.</p><p>‍</p><p>Here is a good representation by OpenAI of how embeddings work:</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:903pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/648af0a2ebc57bea4e1813d5_gcTZxQ46w7XyLACDONwwoamlYpzrrgAzcrrrlxV3t8FrFq_MgwREaRTJZnMKdtdA9H-1aZXbTzk9Xv88kChoa1Fr46isyciqMusd5Qg0cim98NQC_YlXBJo74dRAXwArR0vValem0c4qQgqCebVLeyI.png""/></div></figure><p>‍</p><p>To store embeddings, we use special databases called <strong>Vector Databases</strong>. These databases, store vectors in a way that makes them easily searchable. Some good examples of these kinds of databases are <a href=""https://www.pinecone.io/"">Pinecone</a>, <a href=""https://weaviate.io/"">Weaviate</a>, and <a href=""https://milvus.io/"">Milvus</a>. </p><p>‍</p><p>Once we have the relevant embeddings, we retrieve the chunks of text which correspond to those embeddings. The chunks are then given to the chatbot model as the context using which it can answer the user’s queries and carry the conversation forward.</p><p>‍</p><p>For example, if you were building a custom chatbot for books, we will convert the book’s paragraphs into chunks and convert them into embeddings. Once we have that, we can fetch the relevant paragraphs required to answer the question asked by the user.</p><p>‍</p><h3>Prompt Tuning for Language and Tonality</h3><p>It is very important that the chatbot talks to the users in a specific tone and follow a specific language pattern. This is why prompt tuning is necessary. We want the chatbot to have a <em>personality</em> based on the task at hand. If it is a sales chatbot we want the bot to reply in a friendly and persuasive tone. If it is a customer service chatbot, we want the bot to be more formal and helpful. We also want the chat topics to be somewhat restricted, if the chatbot is supposed to talk about issues faced by customers, we want to stop the model from talking about any other topic.</p><p>‍</p><p>To control the language and the topics of the chatbot, we modify the prompts accordingly. Here are some techniques we use:</p><h4>Few-Shot Prompting</h4><p>The model can be provided with some examples of how the conversation should be continued in specific scenarios, it will learn and use similar mannerisms when those scenarios happen. This is one of the best ways to tune the model to your needs, the more examples you provide, the better the model responses will be.</p><p>‍</p><p>Here is an example of few shot prompting:</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1599pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/648af0a21c79c932f55715e6_5f2uLz6VQJyuxRDn8BQLPK4CTswOt58G8hUc8nfAMEDVDegpORwBCdmFfac7KFibnbbMpi3wdnBCdR2I15op5foUPLskV6XFbgLlpOzsEq2ualIg-7wVXs8eQCdrYCJJ_X6nc5uTZ0XKxvK7pAz-Wi8.png""/></div></figure><p>‍</p><p>Here we provided GPT-4 with scenarios and it was able to use it in the conversation right out of the box! The process of providing good few-shot examples can itself be automated if there are way too many examples to be provided. We can use an embedding-based pipeline for this too.</p><p>‍</p><h4>Parameter Tuning</h4><p>Another very important thing to do is to tune the parameters of the chatbot model itself. This part is often overlooked. All LLMs have some parameters that can be passed to control the behavior and outputs. </p><ul role=""list""><li><strong>Temperature: </strong>This parameter is used to control the <em>randomness</em> of the outputs. Higher temperature values (e.g., 1.0) result in more varied and creative outputs, while lower values (e.g., 0.5) lead to more focused and deterministic responses. It is important to test the outputs of the model with a smaller temperature and then increase it slowly to find the value which works the best for you.</li><li><strong>Top-P: </strong>Top-P or nucleus sampling helps in selecting how many of the tokens should be considered when predicting the next word or token. It sets a threshold value (e.g., 0.8) to consider the most probable words, balancing diversity and coherence in the generated text.</li><li><strong>Maximum Length: </strong>This parameter sets the maximum allowed tokens or characters in the generated text to control its length and ensure relevance. This also depends on the use case at hand. If we are building a simple chatbot, it makes sense to use a smaller maximum length. If the chatbot is supposed to “explain” things to the user, then we are going to need a bigger maximum length value.</li><li><strong>Frequency and Presence Penalties: </strong>Both of these parameters reduce repetitive words or phrases in the generated text by assigning a penalty, promoting diversity. The frequency penalty does this based on the number of times the word has already appeared in the output text. The presence penalty only takes into account the presence of the word, the frequency of the word does not matter, it is a much harder penalty.</li></ul><h3>Building a Custom Chatbot with Langchain</h3><p>With new Python libraries like  LangChain, AI developers can easily integrate Large Language Models (LLMs) like GPT-4 with external data. LangChain works by breaking down large sources of data into ""chunks"" and embedding them into a Vector Store. This Vector Store can then be queried by the LLM to generate answers based on the prompt. It basically automates the chatbot pipeline completely.</p><p>‍</p><p>Langchain provides developers with components like <strong>index</strong>, <strong>model,</strong> and <strong>chain</strong> which make building custom chatbots very easy. You can read more about the core components of langchain <a href=""https://docs.langchain.com/docs/category/components"">here</a>.</p><h2>Traditional NLP Chatbots vs GPT-4</h2><p>Before GPT based chatbots, more traditional techniques like sentiment analysis, keyword matching, etc were used to build chatbots. These chatbots used rule-based systems to understand the user’s query and then reply accordingly. This approach was very limited as it could only understand the queries which were predefined. </p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1106pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/648af0a2da4e0285c330c337_U1rRnDb8bjArgbK7rWLYzhwJJZvXOG1Ep47gV2lDax0ecSkfS_HCEXY2M6HvlCqrNuUPt2qJoujwPoINWNeMz6AiwXiUWue6UtF4mruoEsrl1nRUidaalVnDLHPlfyRDR5jO6vDehEwMqbZSfyjpzbc.png""/></div></figure><p>‍</p><p>The classifier can be a machine learning algo like Decision Tree or a BERT based model that extracts the intent of the message and then replies from a predefined set of examples based on the intent. This approach is very limited and non-flexible. GPT models can understand user query and answer it even a solid example is not given in examples. </p><p>‍</p><p>Here are some ways how GPT-4 and other GPT based models solve limitations of the traditional chatbots:</p><p>‍</p><h3>GPT models are more flexible</h3><p>As mentioned above, traditional chatbots follow a rule based approach. They are not flexible and require a lot of human oversight. Businesses have to spend a lot of time and money to develop and maintain the rules. Also, the rules are often rigid and do not allow for any customization. This is not a very scalable model.</p><p>‍</p><p>On the other hand, GPT-4 offer a more flexible approach. These models use large transformer based networks to learn the context of the user’s query and generate appropriate responses. This allows for much more personalized replies as it can understand the context of the user’s query. It also allows for more scalability as businesses do not have to maintain the rules and can focus on other aspects of their business. These models are much more flexible and can adapt to a wide range of conversation topics and handle unexpected inputs.</p><p>‍</p><h3>GPT models have a better understanding of user query</h3><p>Models like GPT-4 have been trained on large datasets and are able to capture the nuances and context of the conversation, leading to more accurate and relevant responses. GPT-4 is able to comprehend the meaning behind user queries, allowing for more sophisticated and intelligent interactions with users. This improved understanding of user queries helps the model to better answer the user’s questions, providing a more natural conversation experience.</p><p>‍</p><p>Traditional techniques like intent-classification bots fail terribly at this because they are trained to classify what th user is saying into predefined buckets. Often it is the case that user has multiple intents within the same the message, or have a much complicated message than the model can handle. GPT-4 on the other hand “understands” what the user is trying to say, not just classify it, and proceeds accordingly.</p><h3>GPT models can be customized for any context</h3><p>GPT-4 can be customized very quickly with some prompt engineering. If you are trying to build a customer support chatbot, you can provide some customer service related prompts to the model and it will quickly learn the language and tonality used in customer service. It will also learn the context of the customer service domain and be able to provide more personalized and tailored responses to customer queries. And because the context is passed to the prompt, it is super easy to change the use-case or scenario for a bot by changing what contexts we provide.</p><p>‍</p><p>Traditional chatbots on the other hand might require full on training for this. They need to be trained on a specific dataset for every use case and the context of the conversation has to be trained with that. This is very limiting. With GPT models the context is passed in the prompt, so the custom knowledge base can grow or shrink over time without any modifications to the model itself.</p><h2>Why use Custom Chatbots for your Business?</h2><p>Custom chatbots provide a lot of benefits for businesses. They provide a more personalized and efficient customer experience by offering instant responses to user queries and automating common tasks. Custom chatbots can handle a large volume of inquiries simultaneously, reducing the need for human teams and increasing operational efficiency. Additionally, they can be integrated with existing systems and databases, allowing for seamless access to information and enabling smooth interactions with customers. Businesses can save a lot of time, reduce costs, and enhance customer satisfaction using custom chatbots.</p><h2>Want to build a Custom Personalized Chatbot?</h2><p>If you are looking to build chatbots trained on custom datasets and knowledge bases, Mercity.ai can help. We specialize in developing highly tailored chatbot solutions for various industries and business domains, leveraging your specific data and industry knowledge. Whether you need a chatbot optimized for sales, customer service, or on-page ecommerce, our expertise ensures that the chatbot delivers accurate and relevant responses. Contact us today and let us create a custom chatbot solution that revolutionizes your business.</p><p>‍</p></div>"
Guide to fine-tuning LLMs using PEFT and LoRa techniques,fine-tuning-llms-using-peft-and-lora,640f56f76d313b2faa631c11,64ac7b01220d3feb89c10540,False,False,Mon Jul 10 2023 21:41:21 GMT+0000 (Coordinated Universal Time),Fri Sep 01 2023 21:31:17 GMT+0000 (Coordinated Universal Time),Fri Sep 01 2023 21:31:17 GMT+0000 (Coordinated Universal Time),"<p id="""">Large Language Models (LLMs) like GPT are getting only larger in size. Even open-source models like <a href=""https://huggingface.co/mosaicml/mpt-30b"" id="""">MPT</a> and <a href=""https://huggingface.co/tiiuae/falcon-40b"" id="""">Falcon</a> have reached 30 and 40 billion parameters respectively. With size, the capabilities and complexities of these models have also increased. But this increased complexity and model size can also create challenges. Training larger models requires more extensive data sets, and as the model grows, more parameters must be tuned. This can be very compute-heavy and as a result costly too. This is where fine-tuning comes in. Fine-tuning is a technique that allows for the re-purposing of pre-trained models and can help reduce the complexity of building larger models.&nbsp;</p><p id="""">‍</p><p id="""">In this blog, we will discuss advanced fine-tuning techniques like PEFT (Parameter Efficient Fine-Tuning) and see how they can save you a ton of time and money on training massive LLMs.</p><p id="""">‍</p><h2 id="""">What is Fine-tuning?</h2><p id="""">Fine-tuning is the process of taking a model that is already trained on some task and then tweaking it to perform a similar task. It is often used when a new dataset or task requires the model to have some modifications, or when the model is not performing well on a specific task.</p><p id="""">‍</p><p id="""">For example, a model trained to generate stories can be fine-tuned to generate poems. This is possible because the model has already learned how to generate casual language and write stories, this <em id="""">skill</em> can also be used to generate poems if the model is tweaked properly.</p><h3 id="""">How does Fine-tuning work?</h3><p id="""">As mentioned, fine-tuning is tweaking an already-trained model for some other task. The way this works is by taking the weights of the original model and adjusting them to fit a new task.</p><p id="""">‍</p><p id="""">Models when trained learn to do some specific task, for example, GPT-3 has been trained on a massive dataset and as a result, it has learned to generate stories, poems, songs, letters, and a lot of other things. One can take this ability of GPT-3 and fine-tune it on a specific task like generating answers to customer queries in a specific manner.</p><p id="""">‍</p><p id="""">There are different ways and techniques to fine-tune a model, the most popular being <em id="""">transfer learning</em>. Transfer learning comes out of the computer vision world, it is the process of freezing the weights of the initial layers of a network and only updating the weights of the later layers. This is because the lower layers, the layers closer to the input, are responsible for learning the general features of the training dataset. And the upper layers, closer to the output, learn more specific information which is directly tied to generating the correct output.</p><p id="""">‍</p><p id="""">Here is a quick visualization of how fine-tuning works:</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1438px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1438px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64ac796a220d3feb89bedcef_gksjp_6BS_dpO8KdfKa51eQ_BNcKLnmQlNcZRdg5MHOqfJoj7bMoUf3rJwUpk1fZh64R0zSo5jl1nMRzfeBQuiZ63R4rO8PsOvEWLNR3QrQCBdC8qvtRk6C0K5_0xd6zGc_zoy96cgqHz5_kSa-jQfw.gif"" id="""" width=""auto"" height=""auto"" loading=""auto"" alt=""""></div></figure><p id=""""><em id="""">Alammar, J (2018).&nbsp; </em><a href=""https://jalammar.github.io/illustrated-transformer/"" id=""""><em id="""">The Illustrated Transformer</em></a> <em id="""">[Blog post].</em></p><h3 id="""">Why use Fine-Tuning?</h3><p id="""">As the model size increases, it becomes more costly and time-consuming to train it. And with more size it requires more training data, otherwise, models usually overfit and generate poor results in a production environment. Fine-tuning allows us to not run into these issues by efficiently using a pre-trained model for our purposes. Here are some reasons why you should consider fine tuning instead of training a model from scratch:</p><h4 id="""">Larger models generalize to downstream tasks well</h4><p id="""">We all know how large models like GPT-3 and GPT-4 can perform really well on complicated tasks. This is because they have very sophisticated architectures and are trained on massive datasets, this helps them generalize on a lot of tasks really well. These models understand the underlying properties of language and that helps them learn any new tasks with minimal effort like prompt engineering.</p><p id="""">‍</p><p id="""">But if we want to use these models for some very specific tasks, like building a legal contract generator, you should probably fine-tune the model instead of using prompt engineering. This is because a model performing well in a very general task like language generation will perform well in a downstream task like generating legal contracts.</p><p id="""">‍</p><h4 id="""">Cheaper than training a whole model</h4><p id="""">As mentioned before, these large models can be very expensive to train from scratch. Also very time-consuming. It is always cheaper to train an already-trained model. This also allows you to leverage what is already out there instead of doing everything yourself. Most of the time good datasets can be very hard and time-consuming to build. Open-source models like <a href=""https://huggingface.co/mosaicml/mpt-30b"" id="""">MPT</a> and <a href=""https://ai.facebook.com/blog/large-language-model-llama-meta-ai/"" id="""">LLaMA</a> have already been trained and made sure that they work well by some of the best researchers out there. It is very easy to load and train them in a cloud infrastructure.</p><h4 id="""">Good for online training</h4><p id="""">One of the biggest challenges in AI is to keep the model up to date with the latest data. Models when deployed in production can start degrading in performance if not updated regularly. For example, if you deploy an AI model to predict customer behavior in a store, it might stop performing well once the store is restocked with products with different prices or if they introduce new products in the store. This is a classic example of how changes in data can drastically change the performance of a model.&nbsp;</p><p id="""">‍</p><p id="""">Fine-tuning can help you to keep updating the model with the latest data without having to re-train the whole model. This makes it possible to deploy models in production without much effort and cost. This is called online learning or online training and is absolutely necessary for any model in production.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1175px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1175px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64ac796a5f93c078881443f3_xoH3raA0kMV9Fg-sVtG84G5NI_irw72IksNhONqjeZAdDnkaME7av4Jiopt9zHumsz2bwosamV5Yzp59jsFZliX5K7RCoomps5tsED3FcZPpiiQ3RImJXRUfq3ycN54flyjmxJ5LunNLbrYFV7SiDos.png"" id="""" width=""auto"" height=""auto"" loading=""auto"" alt=""""></div></figure><h2 id="""">What is PEFT?</h2><p id="""">PEFT, Parameter Efficient Fine-Tuning, is a set of techniques or methods to fine-tune a large model in the most compute and time-efficient way possible, without losing any performance which you might see from full fine-tuning. This is done because with models growing bigger and bigger like <a href=""https://huggingface.co/docs/transformers/model_doc/bloom"" id="""">BLOOM</a> which has a whopping <strong id="""">176 billion </strong>parameters, it is almost impossible to finetune them without spending tens of thousands of dollars. But it is sometimes almost necessary to use such big models for better performance. This is where PEFT comes in. It helps you solve the problems faced during such big models.</p><p id="""">‍</p><p id="""">Here are some PEFT techniques:</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1322px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1322px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64ac796b64197e69fc87f670_iWe274ACYUm0_Q-aPmdQPWUSWZR4YnNs2gP7xxX2sCZh7TXPL5WWfOu4pEkJGBkNridVRNFMnkHlc3NjE9M-5SMlv9pksU1LZ_G7Jjb7OzPXNeqKpURkaGfWa5uyWL48sle16BUmXsGlTHOk8ui4xIA.png"" id="""" width=""auto"" height=""auto"" loading=""auto"" alt=""""></div></figure><h3 id="""">Why PEFT?</h3><p id="""">As mentioned above, it has become a necessity to fine-tune and use bigger models when it comes to production-grade applications. PEFT techniques allow you to fine-tune the models efficiently and save money and time as a result. This is done by fine-tuning only the most important and relevant parameters in the neural network. The techniques introduce new parameters in the network or freeze the whole model except for some parts to make it easier to train the model.</p><p id="""">‍</p><h2 id="""">Transfer Learning</h2><p id="""">Transfer learning is when we take some of the learned parameters of a model and use them for some other task. This sounds similar to fine-tuning but is different. In finetuning, we re-adjust all the parameters of the model or <em id="""">freeze</em> some of the weights and adjust the rest of the parameters. But in fine-tuning, we use some of the learned parameters from a model and use them in other networks. This gives us more flexibility in terms of what we can do. For example, we cannot change the architecture of the model when fine-tuning, this limits us in many ways. But when using transfer learning, we use only a part of the trained model, which we can then attach to any other model with any architecture.</p><h3 id="""">How Transfer Learning Works</h3><p id="""">Transfer learning has been a common practice in the computer vision world for a very long time now. This is because of the nature of the visual models and how they learn. In CNN models, the early layers extract more general features like edges and curves, whereas the later layers extract more complicated features like whole eyes and faces. This is because the receptive field of CNNs grows as they are stacked on top of each other.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:242px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""242px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64ac796a1def5997ce0baa32_U46S6iJQLnVqHJ1LGxDtJRDMWHKYb37vDR9pYrfja7281G78AxIIYeiFgXDvBkDKDAYD_wUeUKfgktRaep1HHRE1_hfXshnW0gqt8_KJQa65h3mnu5W_53FUzp_zdPkrES_9KoZfQKAkmYmP30cbw_E.png"" id="""" width=""auto"" height=""auto"" loading=""auto"" alt=""""></div></figure><p id="""">‍</p><p id="""">Let’s say for example you are trying to train a neural network to classify if a vehicle in front of you is a car or a motorbike. This is a very basic task. But let’s say you have very limited data and you don’t want to train your model too much. Here is what a basic CNN network looks like.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1452px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1452px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64ac796a1def5997ce0baa43_jab3buYuSz0FRlKFQuxdtoFlqCDAW_Nf31qJA5Taqo42KJqoWLezsdTsA8W1klbWAAvNZ1RKnHUtLX261SKOBqQB9UDZpSOQCB1ihC4qSRZyNoIIC7Psjel7XbaoFgKanUy9SkzjmgB3rUzhrATPgSk.png"" id="""" width=""auto"" height=""auto"" loading=""auto"" alt=""""></div></figure><p id="""">‍</p><p id="""">There are 2 major parts of the network here, the CNN head and the later fully connected layers. As mentioned, CNN layers extract <em id="""">representations</em> of the data which then are used by the fully connected network to classify the image. Here we can use any other CNN network trained on a similar classification problem and use that as the CNN head for this new problem.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1430px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1430px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64ac796aee80430969e1b55c_bGcQDyySuDDZfgUx_IU-dsNXrx0gtRVHQXM7zLhPiiEvSdnu10727d4iV9Tg41MMDAEY0WKpM2PT5Z3iQQ73iFxBHU9VFWFwrf1Iff2Y6rSF0_AW7cFgXjpw2G2rw8h2Y9hvuRQlxhdHZIGZwTtvvko.png"" id="""" width=""auto"" height=""auto"" loading=""auto"" alt=""""></div></figure><p id="""">‍</p><p id="""">Here as you can see, we are using transfer learning by using the weights of a network pretrained to classify the car type. We are only <em id="""">freezing</em> the first two layers of the CNN network, and leaving the latter two free to be updated during the training process. This makes sure that the CNN head of the model learns new features from the images which might be necessary for the new task we are training the model for.</p><p id="""">‍</p><p id="""">Transfer learning is also often seen in NLP tasks with LLMs where people use the encoder part of the transformer network from a pretrained model like T5 and train the later layers.</p><h2 id="""">Adapters</h2><p id="""">Adapters were one of the first parameter-efficient fine-tuning techniques released. In the <a href=""https://arxiv.org/abs/1902.00751"" id="""">paper</a>, they showed that you can add more layers to the pre-existing transformer architecture and only finetune them instead of the whole model. They showed that this technique resulted in similar performance when compared to complete fine-tuning.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:778px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""778px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64ac796b2cf93ff7ca7a4c38_NYcHjBoLPNr0WP0Le-F7x_7sJ8PXpyQRRQ0gM-0Qi7NTL-9PY3W-E_YH7QjrjAMdjG6t9LowvUg2kSaPcIv_G2MLJNakeMu8idZCwix1pPXSj-jXDQyaF_iYXOLHSEuUHDG17ZnBvfrDIwUuLmudI20.png"" id="""" width=""auto"" height=""auto"" loading=""auto"" alt=""""></div></figure><p id="""">‍</p><p id="""">On the left, there is the modified transformer architecture with added adapter layers. You can see adapter layers are added after the attention stack and the feed-forward stack. And on the right, you can see the architecture of the adapter layer itself. The adapter layer comprises a bottleneck architecture, it takes the input and narrows it down to a smaller dimension representation and then passes it through a non-linear activation function, and then scales it back up to the dimension of the input. This makes sure that the next layer in the transformer stack will be able to receive the generated output from the adapter layer.</p><p id="""">‍</p><p id="""">In the paper, the authors show that this method of fine-tuning is comparable to complete fine-tuning while consuming much less compute resources and training time. They were able to attain 0.4% of full fine-tuning on the GLUE benchmark while adding 3.6% of the parameters.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center"" data-rt-max-width=""""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64ac796bb31d4755e5b9f245_6Pf3OFK7AznckqlTNxGUyXDxbjAP-ffOHAkI-C3XjwXuQAlZ-Z16akAAd4AVHLy7ha2Uu86_-sx4dJFjPa4X04SXBRyZM13JiCZS2Yx7h1M_pr6lO_FXlO7Tvluv6EoD0u_lShCh_K1fvj9Jk2twwU4.png"" id="""" width=""auto"" height=""auto"" loading=""auto"" alt=""""></div></figure><p id="""">‍</p><h2 id="""">LoRA - Low-Rank Adaptation</h2><p id="""">LoRA is a similar strategy to Adapter layers but it aims to further reduce the number of trainable parameters. It takes a more mathematically rigorous approach. LoRA works by modifying how the updatable parameters are trained and updated in the neural network.</p><p id="""">‍</p><p id="""">Let’s explain mathematically, you can skip to the next paragraph if you are not interested. We know that the weights matrices of a pretrained neural network are full rank, meaning each weight is unique and can't be made by combining other weights. But in <a href=""https://arxiv.org/abs/2012.13255"" id="""">this</a> paper authors showed that when pretrained language models are adjusted to a new task the weights have a lower “intrinsic dimension”. Meaning, that the weights can be represented in a smaller matrix, or that it has a lower rank. This in turn means that during backpropagation, the weight update matrix has a lower rank, as most of the necessary information has already been captured by the pre-training process and only task-specific adjustments are made during fine-tuning.</p><p id="""">‍</p><p id="""">A much simpler explanation is that during finetuning only a very few weights are updated a lot as most of the learning is done during the pretraining phase of the neural network. LoRA uses this information to reduce the number of trainable parameters.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1219px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1219px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64ac796b52955d6d3a8e667c_mtT--qmnIZxHjHPCETS9PnUPkjyC_tQgV5suVwlyK4cRVckfUPo6zt2KXE2BGcO1rqQkfyhX5tR-jkJPwv7k8Km279JpDEopXqCNTqF20AmZhkxn2AIYTBkgfyyiWJ214hGAKtAuJ2EAOkgt8qJbCPM.png"" id="""" width=""auto"" height=""auto"" loading=""auto"" alt=""""></div></figure><p id="""">The image above gives a visual representation of what LoRA is doing. The Δ<em id="""">W<sub id="""">AxB </sub></em>is the weight updation matrix, these are the changes needed to be applied to the neural network in order for it to learn a new task. This matrix can be broken down into two matrices and then we can only train them and then use them to get back our weight updation matrix. As you can see in the image, the matrix is broken down into matrices with columns and rows <em id="""">r</em>, it can be understood as the <strong id="""">rank</strong> of the weight updation matrix if it was actually trained. The bigger the rank, the more parameters will be updated during training.</p><h3 id="""">Efficiency of LoRA</h3><p id="""">Authors in the paper show that LoRA can <strong id="""">outperform</strong> full finetuning <strong id="""">with only 2% of total trainable parameters</strong>.&nbsp;</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:994px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""994px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64ac796b33bf8099615d1cc4_RjrvdIVZHbUoDY64Rs67dN5VUb1eaXU1rb1AXuxh719N5v_o2KA_aDchDadXjzOstc-mNqeUfT1bXCvcq2Uzq5XxgArhUiXVKNizRlEQMqCoL176aV-RLeKaKRCcm-hrIyGsw1HNIN1y1joFMduSctw.png"" id="""" width=""auto"" height=""auto"" loading=""auto"" alt=""""></div></figure><p id="""">‍</p><p id="""">As for the number of parameters it trains, we can largely control that using the rank <em id="""">r</em> parameter. For example, let’s say the weight updation matrix has 100,000 parameters, <em id="""">A</em> being 200 and <em id="""">B</em> being 500. The weight updation matrix can be decomposed into smaller matrixes of lower dimensions, <em id="""">A</em> being <em id="""">200 x 3</em> and <em id="""">B</em> being <em id="""">3 x 500</em>. This gives us <em id="""">200 x 3 + 3 x 500 = </em><strong id=""""><em id="""">2100</em></strong> trainable parameters only, which is only <strong id="""">2.1%</strong> of the total number of parameters. This can be further reduced as we can decide to only apply LoRA to specific layers only.</p><p id="""">‍</p><p id="""">As the number of parameters trained and applied are MUCH smaller than the actual model, the files can be as small as <strong id="""">8MB</strong>. This makes loading, applying, and transferring the learned models much easier and faster.</p><p id="""">‍</p><p id="""">You can read the <a href=""https://arxiv.org/abs/2106.09685"" id="""">LoRA paper</a> if you want to learn more and do a deeper dive into the topic.</p><h3 id="""">LoRA in Stable Diffusion</h3><p id="""">One of the most interesting use cases of LoRA can be shown in image generation applications. Images have an inherent <em id="""">style</em> that can be visually seen. Instead of training massive models to get specific styles of images out of models, users can now only train LoRA weights and use them with techniques like <a href=""https://dreambooth.github.io/"" id="""">Dreambooth</a> to achieve really good quality images with a lot of customizability.</p><p id="""">‍</p><p id="""">LoRA weights can also be combined with other LoRA weights and be used in a weighted combination to generate images that carry multiple styles. You can find a ton of LoRA adapters online and load them into your models on <a href=""https://civitai.com/"" id="""">CivitAI</a>.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64ac796cb0b88a3f25a5aba9_2JwaTCH1vvT8nnzzzQ8pSTQi-bJSgHBfZORoGjOHs6QzmY8dMECiTpHz9mbVBNJuCEvnSeOEGNI1zBJI5XZfZ1tJYI4mpJ1hQykIkbqg_Y_fgfKU1QBS5E_Q7AFnegtmcoLQLdcxmqKczk4zGg17-TE.png"" id="""" width=""auto"" height=""auto"" loading=""auto"" alt=""""></div></figure><p id="""">‍</p><h2 id="""">IA3 - Infused Adapter by Inhibiting and Amplifying Inner Activations</h2><p id=""""><a href=""https://arxiv.org/abs/2205.05638"" id="""">IA3</a> is an adapter-based technique that is somewhat similar to LoRA. The goal of the authors was to replicate the advantages of ICL (in context learning or Few-Shot prompting) without the issues that come with it. ICL can get messy in terms of cost and inference as it requires prompting the model with examples. Longer length prompts require more time and computation to process. But ICL is perhaps the easiest way to get started working with models.</p><p>‍</p><p id="""">IA3 works by introducing <strong id="""">rescaling vectors</strong> that target the activations of the model. A total of 3 vectors are introduced, <em id="""">l<sub id="""">v</sub>, i<sub id="""">k, </sub></em>and <em id="""">l<sub id="""">ff</sub>. </em>These vectors target the <em id="""">value, keys</em> in the attention layer, and the <em id="""">non-linear</em> layer in the dense layers. These vectors are multiplied elementwise to the default values in the model. Once injected, these parameters are then learned during the training process, while the rest of the model remains frozen. These learned vectors essentially rescale or optimize the targeted pretrained model weights for the task at hand.</p><p>‍</p><figure class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:963px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""963px""><div><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64f258167f01443dc5873fd7_b1JmzItn0cZq5XLABg3j4GI3m45obrl0DtDIs1e3c1K5BsQmrZwHUxb1-G9W2GK65WEdXObzu-2NZ0TSzZWyaahY4Brhogg8mnkDCcKBXrjMEiobDqdW1PNZ3baXGXQcJ2m2H9QuytN5sE4D9fmiPxY.png"" id="""" width=""auto"" height=""auto"" alt="""" loading=""auto""></div></figure><p>‍</p><p id="""">So far this seems like a basic adapter type PEFT method. But that’s not all. The authors also use 3 loss terms to enhance the learning process. The 3 losses are <em id="""">L<sub id="""">LM</sub>, L<sub id="""">UL,</sub> </em>and<em id=""""> L<sub id="""">LN</sub></em>. <em id="""">L<sub id="""">LM</sub></em> is the standard cross-entropy loss, which increases the likelihood of generating the correct response. Then there is <em id="""">L<sub id="""">UL</sub></em> which is <em id="""">Unlikelihood Loss</em>. This loss term reduces the probability of incorrect outputs using Rank Classification. Finally, we have <em id="""">L<sub id="""">LN</sub></em>, which is a length-normalized loss that applies a softmax cross-entropy loss to length-normalized log probabilities of all output choices. Multiple losses are used here to ensure faster and better learning of the model. Because we are trying learn using few-shot examples, these losses are necessary.</p><p id="""">Now let’s talk about two very important concepts in IA3. Rank Classification and Length Normalization.</p><p>‍</p><p id="""">In Rank Classification a model is asked to rank a set of responses by their correctness. This is done by calculating the probability scores for the potential responses. The L<sub id="""">UL </sub>is then used to reduce the probability of the wrong responses and as a result, increase the probability of the correct response. But with Rank classification, we face a critical problem, which is that the responses with fewer tokens will rank higher, because of how probability works. A smaller amount of generated tokens ensures a higher probability as the probability of every generated token is &lt; 1. To fix this, the authors propose dividing the score of the response by the number of tokens in the response. Doing this will normalize the scores. One very important thing to note here is that normalization is done over log probabilities, not raw probabilities. Log probabilities are negative and between zero to one.</p><p>‍</p><h3 id="""">Efficiency of IA3</h3><p id="""">IA3 just like LoRA reduces the number of trainable parameters. But instead of using low-rank matrices, IA3 uses rescaling vectors. This reduces the trainable parameters to about 0.01%, compared to LoRA's &gt; 0.1%, for the T0 model trained in the paper. The frozen state of the LLM also provides us with the option of having multiple adapters for multiple use cases. Also, because the authors used element-wise multiplication, it is super easy to merge the adapter to the LLM weights because of the commutative property of multiplication.</p><p>‍</p><figure class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1247px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1247px""><div><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64f2581684d2e29d3f357ee7_nh7Jp_Bv3DSHLC3rjBFirxHpQ04gjvM-T1zn7OfHpRcwtcNf9lyampebUIWUILzJ5tegpu6-8ySzmX2nohM5ODOH_ozBNMGQNr3EbV-UpVR0jbmKe_wsFehjKJqKDqJ6XaNJuAGwVmT7zNVsOMPbrgA.png"" id="""" width=""auto"" height=""auto"" alt="""" loading=""auto""></div></figure><p>‍</p><p id="""">The above figure shows that IA3 performs better than LoRA and barely affects the FLOPs. This makes IA3 a highly efficient and desirable technique. Also because IA3 is an additive adapter technique, just like LoRA we can target specific parts of the model and decide where to introduce the rescaling vectors. This helps us reduce the training time and even more.</p><p>‍</p><p>‍</p><h2 id="""">P-Tuning</h2><p id="""">The P-tuning method aims to optimize the representation of the prompt which is passed to the model. In the <a href=""https://arxiv.org/abs/2103.10385"" id="""">P-Tuning paper</a>, the authors emphasize how prompt engineering is a very strong technique when working with large language models. The p-Tuning method builds up on top of prompt engineering and tries to further improve the effectiveness of a good prompt.</p><p id="""">‍</p><p id="""">P-Tuning works by creating a small <em id="""">encoder network</em> for your prompt that creates a <em id="""">soft prompt</em> for your passed prompt. To tune your LLM using P-tuning, you are supposed to create a <em id="""">prompt template</em> that represents your prompt. And a context <em id="""">x</em> which is used in the template to get label <em id="""">y</em>. This is the approach mentioned in the paper. The tokens used for the prompt template are trainable and learnable parameters, these are called <em id="""">pseudo tokens</em>. We also add a prompt encoder which then helps us update pseudo tokens to the specific task at hand. The prompt encoder is usually a <em id="""">bi-LSTM</em> network that learns the optimal representation of the prompt for the model and then passes the representation to it. The LSTM network is attached to the original model. Only the encoder network and the pseudo tokens are trained here, the weights of the original network remain unaffected. Once the training is done, the LSTM head is discarded as we have the <em id="""">h<sub id="""">i</sub> </em>which can be used directly.</p><p id="""">‍</p><p id="""">In short, the prompt encoder only changes the embeddings of the passed prompt to better represent the task, everything else remains unchanged.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1470px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1470px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64ac796b04cddb42676dc685_aM4QCzgu6ql_uqxJuHdVBwnMkbaY2QzJ855KSVzwnk1_BVwpnKXBZHGqo87QZMSK1b3YvLrSdxNrYqIX6HH3n9tLXJ_HsWhxqN15LgT15jB0N2I1p7OdJ2E-1rLeMfhGpSDf2qe9rD4xw__aBwns9xY.png"" id="""" width=""auto"" height=""auto"" loading=""auto"" alt=""""></div></figure><h3 id="""">Efficiency of P-Tuning</h3><p id="""">In terms of efficiency, P-tuning is just as good as any other method. In the paper, the authors show that P-Tuning was able to perform <strong id="""">better than full fine-tuning</strong> on most of the benchmarks. It can be said that P-Tuning is comparable to the full fine-tuning of large language models.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1455px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1455px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64ac796c5ab7e0b32071c2a9_8iTDr8NKlsgmLyo_TIIfUXrfiHR-cnm-8CkCbZRKhO7Otf9pdiOa_3sfkp_OXNGgkuBS54RRAJTqxSXlrwNMQFn4I1VwKxdHzNRjcFHx1O7MWT1OjklldGpZQZwIYvBQBR5dUhCIaOofJ1lm5Hpivvw.png"" id="""" width=""auto"" height=""auto"" loading=""auto"" alt=""""></div></figure><p id="""">‍</p><p id="""">But there is a core issue when it comes to P-Tuning. P-Tuning is a prompt optimization technique, it optimizes the prompt that is passed to the bigger model. This means that we are still largely based on the large model in terms of capability. If a model has not been trained on sentiment classification optimizing sentiment classification prompts using P-Tuning will not do a lot of good to the model. P-Tuning is an assistive technique. It is always very important to pick a model that can do the required task out of the box “well” with some prompt engineering, and then further optimize it.</p><p id="""">‍</p><h2 id="""">Prefix Tuning</h2><p id="""">Prefix tuning can be considered the next version of P-Tuning. The authors of P-Tuning published a paper on <a href=""https://arxiv.org/abs/2110.07602"" id="""">P-Tuning V-2</a> addressing the issues of P-Tuning. In this paper, they implemented the Prefix tuning introduced in <a href=""https://arxiv.org/abs/2101.00190"" id="""">this paper</a>. Prefix tuning and P-Tuning do not have a lot of differences but can still lead to different results. Let’s dive into a deeper explanation.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1153px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1153px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64ac796ca90f165006083da8_A_KieXmVWiCxx2HDCfnPhP7Yw67KDpTn6tqFF_8wKNMel0AfOZu3NoNHfvm23O2F8yVbjYl73WH8XLHXvlCn1MoRch5wHMlZmpEKL32wgTf9-JvVRqf8IuzuyknPLe5RWQ57I3tXFGe4dhuLu0z_Ni4.png"" id="""" width=""auto"" height=""auto"" loading=""auto"" alt=""""></div></figure><p id="""">‍</p><p id="""">In P-Tuning, we added learnable parameters only to the input embeddings but in Prefix Tuning we add them <strong id="""">to all the layers of the network</strong>. This ensures that the model itself learns more about the task it is being finetuned on. We append learnable parameters to the prompt and to every layer activation in the transformer layers. The difference from P-Tuning is that instead of completely modifying the prompt embeddings, we only add very few learnable parameters at the start of the prompt at every layer. Here’s a visual explanation:</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64ac796c5ab7e0b32071c301_LMgayME_f2cuV5YWWcSeCLRFXyU4y4mE4kbfajfq-OSTH06PnqhYgJscRCji-KLqKk8C25-ZbqR2PN15Ffr1kFT5EVGXIcF3eghgDe8A9GYeLDwXxZNZ-YhrjCYltq_qbOUrh17SmcN1BE5DczTE6KE.png"" id="""" width=""auto"" height=""auto"" loading=""auto"" alt=""""></div></figure><p id="""">‍</p><p id="""">At every layer in the transformer, we concatenate a soft prompt with the input which has learnable parameters. These learnable parameters are tuned using a very small MLP, only 2 fully connected layers. This is done because in the paper authors note that directly updating these prompt tokens is very sensitive to learning rate and initialization. The soft prompts increase the number of trainable parameters but substantially increase the learning ability of the model too. The MLP or fully connected layers can be dropped later as we only care about the soft prompts, which will be appended to the input sequences during inference and will guide the model.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1128px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1128px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64ac796cb31d4755e5b9f326_gxwL6634yC6BJhGonpxUj78cM0Z_hJM-wmxiJuhhsF9PiK1W-g_eLCpalPG3Z9vjAMsbBwyx-mnG_ALUC1qak9i0QRK0hlPeUI5V_15TdHDM6fWCNfOgujr4F6RYxH4g9znSx-DE4UGpKQAr2ItcOFU.png"" id="""" width=""auto"" height=""auto"" loading=""auto"" alt=""""></div></figure><p id="""">‍</p><h3 id="""">Efficiency of Prefix Tuning</h3><p id="""">Prefix tuning shows massive gains over P-Tuning. And as the model size increases, these gains increase too. This is perhaps because there are more trainable parameters for larger models. In the chart, you can see the authors compare the performance of P-Tuning, full finetuning, and Prefix tuning. Prefix tuning performs better than or as well as P-tuning in almost all tasks. In many cases, it performs even better than Full fine-tuning!</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1580px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1580px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64ac796c1def5997ce0bad2b_HWKJYOvpznNVd06S-B6iw8qRyrCrvv4qUwjTPN7SDhIUDvbFsNS7__2EoLWnWyM7TlwUs98vXnxnfmDORHZ6b4Rdcgyvf6roYNmFfhMZIpVxl2LmHijQgBytA9CDvJJhD-N3uteFL4f0QdXFsrCX1dA.png"" id="""" width=""auto"" height=""auto"" loading=""auto"" alt=""""></div></figure><p id="""">‍</p><p id="""">One big reason why prefix tuning works really well is that the number of trainable parameters is not limited only to the input sequence. Learnable parameters are added at every layer, making the model much more flexible. Prefix tuning, unlike P-tuning, not only affects the prompt tokens but also the model itself. This allows the model to learn more. But this approach is still largely based on the prompt. It is still suggested to take a model that can perform the task and only then optimize it, as that will lead to much better results. As for the size of parameters, the number of trained parameters increase substantially, from <strong id="""">0.01% to 0.1 to 3% parameters</strong>. But the size of parameters still remains small enough to be transferred and loaded easily and quickly.</p><p id="""">‍</p><h2 id="""">Prompt Tuning</h2><p id="""">Prompt tuning was one of the first papers to build upon the idea of finetuning only with soft prompts. The ideas of P-Tuning and Prefix Tuning come from this paper. Prompt tuning is a very simple and easy-to-implement idea. It involves prepending a specific prompt to the input and using virtual tokens or new trainable tokens for that specific prompt. These new virtual tokens can be finetuned during the process to learn a better representation of the prompt. This means that the model is tuned to understand the prompt better. Here is a comparison of prompt tuning with full fine-tuning from the paper:</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1310px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1310px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64ac796dcfdba29eb2c9287c_61MouOmSUGteW9qoekJTGOfmat0OhESch4gqkOztM_39O3bmE2GxvQ8OsHPpF06lSSQYY-zPGWhM2w3P6eQoD2OG_Q46WpR0RCAoAq0q0qZisLDvSbSRio-jJKcmO44MelRp1fhxAD9bm_g2tgfjH9k.png"" id="""" width=""auto"" height=""auto"" loading=""auto"" alt=""""></div></figure><p id="""">‍</p><p id="""">Here you can see that full model tuning requires multiple copies of the model to exist if we want to use the model for multiple tasks. But with Prompt Tuning, you only need to store the learned virtual tokens of the prompt tokens. So for example, if you use a prompt like <em id="""">“Classify this tweet: {tweet}” </em>the goal will be to learn new better embeddings for the prompt. And during inference, only these new embeddings will be used to generate the outputs. This allows the model to tune the prompt to help itself generate better outputs during inference.</p><p id="""">‍</p><h3 id="""">Efficiency of Prompt Tuning</h3><p id="""">The biggest advantage of using prompt tuning is the small size of learned parameters. The files can be in <strong id="""">KBs.</strong> As we can determine the dimension size and number of parameters to use for the new tokens, we can greatly control the number of parameters we are going to learn. In the paper, the authors show how even with a very small number of trainable tokens method performs really well. And the performance only goes up as bigger models are used. You can read the paper <a href=""https://arxiv.org/abs/2104.08691"" id="""">here</a>.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1165px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1165px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64ac796d4542b678f53df34a_T0sHFW8zg2lugHlswaWtCgjGfZ_LQMcSBr8VYCOAfwwWFzwWtVXtY13ytAUPV_8tkYklYbVGWyOLcR-ehrhKgfQx5rz4D_HZF_5Fw5ASqcYxEJuAsC7cdZRFtHdyI995PtOB41OjxUKRm8JFg-gOZPg.png"" id="""" width=""auto"" height=""auto"" loading=""auto"" alt=""""></div></figure><p id="""">‍</p><p id="""">Another big advantage is that we can use the same model <strong id="""">without any changes</strong> for multiple tasks, as the only thing being updated are the embeddings of the prompt tokens. Meaning you can use the same model for a tweet classification task and for a language generation task without any changes to the model itself, given the model is big and sophisticated enough to perform those tasks. But a big limitation is that the model itself doesn’t learn anything new. This is purely a prompt optimization task. This means if the model has never trained on a sentiment classification dataset, prompt tuning might not be of any help. It is <strong id="""">very important</strong> to note that this method optimizes the prompts, not the model. So, if you cannot handcraft a <em id="""">hard </em>prompt that can do the task relatively well, there is no use of trying to optimize for a <em id="""">soft</em> prompt using prompt optimization techniques.</p><p id="""">‍</p><h2 id="""">LoRA vs Prompt Tuning</h2><p id="""">Now we have explored various PEFT techniques. Now the question becomes whether to use an additive technique like Adapter and LoRA or you use a Prompt based technique like P-Tuning and Prefix Tuning.</p><p id="""">‍</p><p id="""">On comparing LoRA vs P-Tuning and Prefix Tuning, one can say for sure LoRA is the best strategy in terms of getting the most out of the model. But it might not be the most efficient based on your needs. If you want to <strong id="""">train</strong> the model on a much different task than what it has been trained on, LoRA is without a doubt the best strategy for tuning the model efficiently. But if your task is more or less already understood by the model, but the challenge is to properly prompt the model, then you should use Prompt Tuning techniques. Prompt Tuning doesn’t modify many parameters in the model and mainly focuses on the passed prompt instead.</p><p id="""">‍</p><p id="""">One important point to note is that LoRA decomposes the weight updation matrix into smaller rank matrices and uses them to update the weights of the model. Even though trainable parameters are low, LoRA updates all the parameters in the targeted parts of the neural network. Whereas in Prompt Tuning techniques, a few trainable parameters are added to the model, this usually helps the model adjust to and understand the task better but does not help the model learn new properties well.</p><p id="""">‍</p><h2 id="""">LoRA and PEFT in comparison to full Finetuning</h2><p id="""">PEFT, Parameter Efficient Fine Tuning, is proposed as an alternative to full Finetuning. For most of the tasks, it has already been shown in papers that PEFT techniques like LoRA are comparable to full finetuning, if not better. But, if the new task you want the model to adapt to is completely different from the tasks the model has been trained on, PEFT might not be enough for you. The limited number of trainable parameters can result in major issues in such scenarios.</p><p id="""">‍</p><p id="""">If you are trying to build a code generation model using a text-based model like LLaMA or Alpaca, you should probably consider fine-tuning the whole model instead of tuning the model using LoRA. This is because the task is too different from what the model already knows and has been trained on. Another good example of such a task is training a model, which only understands English, to generate text in the Nepali language.</p><p id="""">‍</p><h2 id="""">Why you should Fine-tune models for your business use case</h2><p id="""">Finetuning model is an important step for any business that wants to get the most out of its machine-learning applications. It allows you to customize the model to your specific use case, which can lead to improved accuracy and performance. It saves time, money, and resources by eliminating the need to build a new model from the ground up. Fine-tuning lets you optimize the use of your proprietary data, adjusting the model to better fit your available data, and even incorporating new data if needed. This ensures a more accurate model that better serves your business needs. Here are some more benefits:</p><p id="""">‍</p><ul id=""""><li id="""">Customization: Fine-tuning allows you to tailor the model to your specific needs, enhancing accuracy and performance.&nbsp;</li><li id="""">Resource Efficiency: It saves time, money, and resources by eliminating the need to build a new model from scratch.&nbsp;</li><li id="""">Performance Boost: Fine-tuning enhances the performance of the pretrained model using your unique datasets.&nbsp;</li><li id="""">Data Optimization: It lets you make the most of your data, adjusting the model to better fit your available data, and even incorporating new data if needed.</li></ul><p id="""">‍</p><p id="""">But as the size of models grows to billions of parameters fine-tuning itself can be a challenge. The&nbsp; PEFT&nbsp; techniques we discussed in this blog help to reduce the time and resources needed to fine-tune a model. It helps speed up the training process by making use of the pretrained weights and parameters and allows you to fine-tune the model more efficiently. Also, using PEFT, you can easily transfer models over the internet and even use the same model for multiple purposes. PEFT opens up a whole new world of possibilities for businesses that want to make the most of their machine-learning applications.</p><h2 id="""">Want to Train Custom LLMs with PEFT?</h2><p id="""">If you want to build or train custom LLMs or Chatbots, we can help you fine-tune them to your specific needs. We have done a ton of work on <a href=""https://www.mercity.ai/blog-post/custom-gpt-4-chatbot"" id="""">building custom chatbots</a> and training large language models. Contact us today and let us build a custom LLM that revolutionizes your business.</p><p id="""">‍</p>",https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64ac7af09311957ab6dbecc1_Peft%20finetuning.png,Pranav Patel,Large Language Models,Using Low-rank adaptation (LoRA) and other PEFT techniques can help you train LLMs and other models faster and in a much cheaper way.,True,"<div class=""rich-text w-richtext""><p>Large Language Models (LLMs) like GPT are getting only larger in size. Even open-source models like <a href=""https://huggingface.co/mosaicml/mpt-30b"">MPT</a> and <a href=""https://huggingface.co/tiiuae/falcon-40b"">Falcon</a> have reached 30 and 40 billion parameters respectively. With size, the capabilities and complexities of these models have also increased. But this increased complexity and model size can also create challenges. Training larger models requires more extensive data sets, and as the model grows, more parameters must be tuned. This can be very compute-heavy and as a result costly too. This is where fine-tuning comes in. Fine-tuning is a technique that allows for the re-purposing of pre-trained models and can help reduce the complexity of building larger models. </p><p>‍</p><p>In this blog, we will discuss advanced fine-tuning techniques like PEFT (Parameter Efficient Fine-Tuning) and see how they can save you a ton of time and money on training massive LLMs.</p><p>‍</p><h2>What is Fine-tuning?</h2><p>Fine-tuning is the process of taking a model that is already trained on some task and then tweaking it to perform a similar task. It is often used when a new dataset or task requires the model to have some modifications, or when the model is not performing well on a specific task.</p><p>‍</p><p>For example, a model trained to generate stories can be fine-tuned to generate poems. This is possible because the model has already learned how to generate casual language and write stories, this <em>skill</em> can also be used to generate poems if the model is tweaked properly.</p><h3>How does Fine-tuning work?</h3><p>As mentioned, fine-tuning is tweaking an already-trained model for some other task. The way this works is by taking the weights of the original model and adjusting them to fit a new task.</p><p>‍</p><p>Models when trained learn to do some specific task, for example, GPT-3 has been trained on a massive dataset and as a result, it has learned to generate stories, poems, songs, letters, and a lot of other things. One can take this ability of GPT-3 and fine-tune it on a specific task like generating answers to customer queries in a specific manner.</p><p>‍</p><p>There are different ways and techniques to fine-tune a model, the most popular being <em>transfer learning</em>. Transfer learning comes out of the computer vision world, it is the process of freezing the weights of the initial layers of a network and only updating the weights of the later layers. This is because the lower layers, the layers closer to the input, are responsible for learning the general features of the training dataset. And the upper layers, closer to the output, learn more specific information which is directly tied to generating the correct output.</p><p>‍</p><p>Here is a quick visualization of how fine-tuning works:</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1438pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64ac796a220d3feb89bedcef_gksjp_6BS_dpO8KdfKa51eQ_BNcKLnmQlNcZRdg5MHOqfJoj7bMoUf3rJwUpk1fZh64R0zSo5jl1nMRzfeBQuiZ63R4rO8PsOvEWLNR3QrQCBdC8qvtRk6C0K5_0xd6zGc_zoy96cgqHz5_kSa-jQfw.gif""/></div></figure><p><em>Alammar, J (2018).  </em><a href=""https://jalammar.github.io/illustrated-transformer/""><em>The Illustrated Transformer</em></a> <em>[Blog post].</em></p><h3>Why use Fine-Tuning?</h3><p>As the model size increases, it becomes more costly and time-consuming to train it. And with more size it requires more training data, otherwise, models usually overfit and generate poor results in a production environment. Fine-tuning allows us to not run into these issues by efficiently using a pre-trained model for our purposes. Here are some reasons why you should consider fine tuning instead of training a model from scratch:</p><h4>Larger models generalize to downstream tasks well</h4><p>We all know how large models like GPT-3 and GPT-4 can perform really well on complicated tasks. This is because they have very sophisticated architectures and are trained on massive datasets, this helps them generalize on a lot of tasks really well. These models understand the underlying properties of language and that helps them learn any new tasks with minimal effort like prompt engineering.</p><p>‍</p><p>But if we want to use these models for some very specific tasks, like building a legal contract generator, you should probably fine-tune the model instead of using prompt engineering. This is because a model performing well in a very general task like language generation will perform well in a downstream task like generating legal contracts.</p><p>‍</p><h4>Cheaper than training a whole model</h4><p>As mentioned before, these large models can be very expensive to train from scratch. Also very time-consuming. It is always cheaper to train an already-trained model. This also allows you to leverage what is already out there instead of doing everything yourself. Most of the time good datasets can be very hard and time-consuming to build. Open-source models like <a href=""https://huggingface.co/mosaicml/mpt-30b"">MPT</a> and <a href=""https://ai.facebook.com/blog/large-language-model-llama-meta-ai/"">LLaMA</a> have already been trained and made sure that they work well by some of the best researchers out there. It is very easy to load and train them in a cloud infrastructure.</p><h4>Good for online training</h4><p>One of the biggest challenges in AI is to keep the model up to date with the latest data. Models when deployed in production can start degrading in performance if not updated regularly. For example, if you deploy an AI model to predict customer behavior in a store, it might stop performing well once the store is restocked with products with different prices or if they introduce new products in the store. This is a classic example of how changes in data can drastically change the performance of a model. </p><p>‍</p><p>Fine-tuning can help you to keep updating the model with the latest data without having to re-train the whole model. This makes it possible to deploy models in production without much effort and cost. This is called online learning or online training and is absolutely necessary for any model in production.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1175pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64ac796a5f93c078881443f3_xoH3raA0kMV9Fg-sVtG84G5NI_irw72IksNhONqjeZAdDnkaME7av4Jiopt9zHumsz2bwosamV5Yzp59jsFZliX5K7RCoomps5tsED3FcZPpiiQ3RImJXRUfq3ycN54flyjmxJ5LunNLbrYFV7SiDos.png""/></div></figure><h2>What is PEFT?</h2><p>PEFT, Parameter Efficient Fine-Tuning, is a set of techniques or methods to fine-tune a large model in the most compute and time-efficient way possible, without losing any performance which you might see from full fine-tuning. This is done because with models growing bigger and bigger like <a href=""https://huggingface.co/docs/transformers/model_doc/bloom"">BLOOM</a> which has a whopping <strong>176 billion </strong>parameters, it is almost impossible to finetune them without spending tens of thousands of dollars. But it is sometimes almost necessary to use such big models for better performance. This is where PEFT comes in. It helps you solve the problems faced during such big models.</p><p>‍</p><p>Here are some PEFT techniques:</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1322pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64ac796b64197e69fc87f670_iWe274ACYUm0_Q-aPmdQPWUSWZR4YnNs2gP7xxX2sCZh7TXPL5WWfOu4pEkJGBkNridVRNFMnkHlc3NjE9M-5SMlv9pksU1LZ_G7Jjb7OzPXNeqKpURkaGfWa5uyWL48sle16BUmXsGlTHOk8ui4xIA.png""/></div></figure><h3>Why PEFT?</h3><p>As mentioned above, it has become a necessity to fine-tune and use bigger models when it comes to production-grade applications. PEFT techniques allow you to fine-tune the models efficiently and save money and time as a result. This is done by fine-tuning only the most important and relevant parameters in the neural network. The techniques introduce new parameters in the network or freeze the whole model except for some parts to make it easier to train the model.</p><p>‍</p><h2>Transfer Learning</h2><p>Transfer learning is when we take some of the learned parameters of a model and use them for some other task. This sounds similar to fine-tuning but is different. In finetuning, we re-adjust all the parameters of the model or <em>freeze</em> some of the weights and adjust the rest of the parameters. But in fine-tuning, we use some of the learned parameters from a model and use them in other networks. This gives us more flexibility in terms of what we can do. For example, we cannot change the architecture of the model when fine-tuning, this limits us in many ways. But when using transfer learning, we use only a part of the trained model, which we can then attach to any other model with any architecture.</p><h3>How Transfer Learning Works</h3><p>Transfer learning has been a common practice in the computer vision world for a very long time now. This is because of the nature of the visual models and how they learn. In CNN models, the early layers extract more general features like edges and curves, whereas the later layers extract more complicated features like whole eyes and faces. This is because the receptive field of CNNs grows as they are stacked on top of each other.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:242pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64ac796a1def5997ce0baa32_U46S6iJQLnVqHJ1LGxDtJRDMWHKYb37vDR9pYrfja7281G78AxIIYeiFgXDvBkDKDAYD_wUeUKfgktRaep1HHRE1_hfXshnW0gqt8_KJQa65h3mnu5W_53FUzp_zdPkrES_9KoZfQKAkmYmP30cbw_E.png""/></div></figure><p>‍</p><p>Let’s say for example you are trying to train a neural network to classify if a vehicle in front of you is a car or a motorbike. This is a very basic task. But let’s say you have very limited data and you don’t want to train your model too much. Here is what a basic CNN network looks like.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1452pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64ac796a1def5997ce0baa43_jab3buYuSz0FRlKFQuxdtoFlqCDAW_Nf31qJA5Taqo42KJqoWLezsdTsA8W1klbWAAvNZ1RKnHUtLX261SKOBqQB9UDZpSOQCB1ihC4qSRZyNoIIC7Psjel7XbaoFgKanUy9SkzjmgB3rUzhrATPgSk.png""/></div></figure><p>‍</p><p>There are 2 major parts of the network here, the CNN head and the later fully connected layers. As mentioned, CNN layers extract <em>representations</em> of the data which then are used by the fully connected network to classify the image. Here we can use any other CNN network trained on a similar classification problem and use that as the CNN head for this new problem.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1430pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64ac796aee80430969e1b55c_bGcQDyySuDDZfgUx_IU-dsNXrx0gtRVHQXM7zLhPiiEvSdnu10727d4iV9Tg41MMDAEY0WKpM2PT5Z3iQQ73iFxBHU9VFWFwrf1Iff2Y6rSF0_AW7cFgXjpw2G2rw8h2Y9hvuRQlxhdHZIGZwTtvvko.png""/></div></figure><p>‍</p><p>Here as you can see, we are using transfer learning by using the weights of a network pretrained to classify the car type. We are only <em>freezing</em> the first two layers of the CNN network, and leaving the latter two free to be updated during the training process. This makes sure that the CNN head of the model learns new features from the images which might be necessary for the new task we are training the model for.</p><p>‍</p><p>Transfer learning is also often seen in NLP tasks with LLMs where people use the encoder part of the transformer network from a pretrained model like T5 and train the later layers.</p><h2>Adapters</h2><p>Adapters were one of the first parameter-efficient fine-tuning techniques released. In the <a href=""https://arxiv.org/abs/1902.00751"">paper</a>, they showed that you can add more layers to the pre-existing transformer architecture and only finetune them instead of the whole model. They showed that this technique resulted in similar performance when compared to complete fine-tuning.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:778pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64ac796b2cf93ff7ca7a4c38_NYcHjBoLPNr0WP0Le-F7x_7sJ8PXpyQRRQ0gM-0Qi7NTL-9PY3W-E_YH7QjrjAMdjG6t9LowvUg2kSaPcIv_G2MLJNakeMu8idZCwix1pPXSj-jXDQyaF_iYXOLHSEuUHDG17ZnBvfrDIwUuLmudI20.png""/></div></figure><p>‍</p><p>On the left, there is the modified transformer architecture with added adapter layers. You can see adapter layers are added after the attention stack and the feed-forward stack. And on the right, you can see the architecture of the adapter layer itself. The adapter layer comprises a bottleneck architecture, it takes the input and narrows it down to a smaller dimension representation and then passes it through a non-linear activation function, and then scales it back up to the dimension of the input. This makes sure that the next layer in the transformer stack will be able to receive the generated output from the adapter layer.</p><p>‍</p><p>In the paper, the authors show that this method of fine-tuning is comparable to complete fine-tuning while consuming much less compute resources and training time. They were able to attain 0.4% of full fine-tuning on the GLUE benchmark while adding 3.6% of the parameters.</p><p>‍</p><figure class=""w-richtext-align-center w-richtext-figure-type-image""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64ac796bb31d4755e5b9f245_6Pf3OFK7AznckqlTNxGUyXDxbjAP-ffOHAkI-C3XjwXuQAlZ-Z16akAAd4AVHLy7ha2Uu86_-sx4dJFjPa4X04SXBRyZM13JiCZS2Yx7h1M_pr6lO_FXlO7Tvluv6EoD0u_lShCh_K1fvj9Jk2twwU4.png""/></div></figure><p>‍</p><h2>LoRA - Low-Rank Adaptation</h2><p>LoRA is a similar strategy to Adapter layers but it aims to further reduce the number of trainable parameters. It takes a more mathematically rigorous approach. LoRA works by modifying how the updatable parameters are trained and updated in the neural network.</p><p>‍</p><p>Let’s explain mathematically, you can skip to the next paragraph if you are not interested. We know that the weights matrices of a pretrained neural network are full rank, meaning each weight is unique and can't be made by combining other weights. But in <a href=""https://arxiv.org/abs/2012.13255"">this</a> paper authors showed that when pretrained language models are adjusted to a new task the weights have a lower “intrinsic dimension”. Meaning, that the weights can be represented in a smaller matrix, or that it has a lower rank. This in turn means that during backpropagation, the weight update matrix has a lower rank, as most of the necessary information has already been captured by the pre-training process and only task-specific adjustments are made during fine-tuning.</p><p>‍</p><p>A much simpler explanation is that during finetuning only a very few weights are updated a lot as most of the learning is done during the pretraining phase of the neural network. LoRA uses this information to reduce the number of trainable parameters.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1219pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64ac796b52955d6d3a8e667c_mtT--qmnIZxHjHPCETS9PnUPkjyC_tQgV5suVwlyK4cRVckfUPo6zt2KXE2BGcO1rqQkfyhX5tR-jkJPwv7k8Km279JpDEopXqCNTqF20AmZhkxn2AIYTBkgfyyiWJ214hGAKtAuJ2EAOkgt8qJbCPM.png""/></div></figure><p>The image above gives a visual representation of what LoRA is doing. The Δ<em>W<sub>AxB </sub></em>is the weight updation matrix, these are the changes needed to be applied to the neural network in order for it to learn a new task. This matrix can be broken down into two matrices and then we can only train them and then use them to get back our weight updation matrix. As you can see in the image, the matrix is broken down into matrices with columns and rows <em>r</em>, it can be understood as the <strong>rank</strong> of the weight updation matrix if it was actually trained. The bigger the rank, the more parameters will be updated during training.</p><h3>Efficiency of LoRA</h3><p>Authors in the paper show that LoRA can <strong>outperform</strong> full finetuning <strong>with only 2% of total trainable parameters</strong>. </p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:994pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64ac796b33bf8099615d1cc4_RjrvdIVZHbUoDY64Rs67dN5VUb1eaXU1rb1AXuxh719N5v_o2KA_aDchDadXjzOstc-mNqeUfT1bXCvcq2Uzq5XxgArhUiXVKNizRlEQMqCoL176aV-RLeKaKRCcm-hrIyGsw1HNIN1y1joFMduSctw.png""/></div></figure><p>‍</p><p>As for the number of parameters it trains, we can largely control that using the rank <em>r</em> parameter. For example, let’s say the weight updation matrix has 100,000 parameters, <em>A</em> being 200 and <em>B</em> being 500. The weight updation matrix can be decomposed into smaller matrixes of lower dimensions, <em>A</em> being <em>200 x 3</em> and <em>B</em> being <em>3 x 500</em>. This gives us <em>200 x 3 + 3 x 500 = </em><strong><em>2100</em></strong> trainable parameters only, which is only <strong>2.1%</strong> of the total number of parameters. This can be further reduced as we can decide to only apply LoRA to specific layers only.</p><p>‍</p><p>As the number of parameters trained and applied are MUCH smaller than the actual model, the files can be as small as <strong>8MB</strong>. This makes loading, applying, and transferring the learned models much easier and faster.</p><p>‍</p><p>You can read the <a href=""https://arxiv.org/abs/2106.09685"">LoRA paper</a> if you want to learn more and do a deeper dive into the topic.</p><h3>LoRA in Stable Diffusion</h3><p>One of the most interesting use cases of LoRA can be shown in image generation applications. Images have an inherent <em>style</em> that can be visually seen. Instead of training massive models to get specific styles of images out of models, users can now only train LoRA weights and use them with techniques like <a href=""https://dreambooth.github.io/"">Dreambooth</a> to achieve really good quality images with a lot of customizability.</p><p>‍</p><p>LoRA weights can also be combined with other LoRA weights and be used in a weighted combination to generate images that carry multiple styles. You can find a ton of LoRA adapters online and load them into your models on <a href=""https://civitai.com/"">CivitAI</a>.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64ac796cb0b88a3f25a5aba9_2JwaTCH1vvT8nnzzzQ8pSTQi-bJSgHBfZORoGjOHs6QzmY8dMECiTpHz9mbVBNJuCEvnSeOEGNI1zBJI5XZfZ1tJYI4mpJ1hQykIkbqg_Y_fgfKU1QBS5E_Q7AFnegtmcoLQLdcxmqKczk4zGg17-TE.png""/></div></figure><p>‍</p><h2>IA3 - Infused Adapter by Inhibiting and Amplifying Inner Activations</h2><p><a href=""https://arxiv.org/abs/2205.05638"">IA3</a> is an adapter-based technique that is somewhat similar to LoRA. The goal of the authors was to replicate the advantages of ICL (in context learning or Few-Shot prompting) without the issues that come with it. ICL can get messy in terms of cost and inference as it requires prompting the model with examples. Longer length prompts require more time and computation to process. But ICL is perhaps the easiest way to get started working with models.</p><p>‍</p><p>IA3 works by introducing <strong>rescaling vectors</strong> that target the activations of the model. A total of 3 vectors are introduced, <em>l<sub>v</sub>, i<sub>k, </sub></em>and <em>l<sub>ff</sub>. </em>These vectors target the <em>value, keys</em> in the attention layer, and the <em>non-linear</em> layer in the dense layers. These vectors are multiplied elementwise to the default values in the model. Once injected, these parameters are then learned during the training process, while the rest of the model remains frozen. These learned vectors essentially rescale or optimize the targeted pretrained model weights for the task at hand.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:963px""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64f258167f01443dc5873fd7_b1JmzItn0cZq5XLABg3j4GI3m45obrl0DtDIs1e3c1K5BsQmrZwHUxb1-G9W2GK65WEdXObzu-2NZ0TSzZWyaahY4Brhogg8mnkDCcKBXrjMEiobDqdW1PNZ3baXGXQcJ2m2H9QuytN5sE4D9fmiPxY.png""/></div></figure><p>‍</p><p>So far this seems like a basic adapter type PEFT method. But that’s not all. The authors also use 3 loss terms to enhance the learning process. The 3 losses are <em>L<sub>LM</sub>, L<sub>UL,</sub> </em>and<em> L<sub>LN</sub></em>. <em>L<sub>LM</sub></em> is the standard cross-entropy loss, which increases the likelihood of generating the correct response. Then there is <em>L<sub>UL</sub></em> which is <em>Unlikelihood Loss</em>. This loss term reduces the probability of incorrect outputs using Rank Classification. Finally, we have <em>L<sub>LN</sub></em>, which is a length-normalized loss that applies a softmax cross-entropy loss to length-normalized log probabilities of all output choices. Multiple losses are used here to ensure faster and better learning of the model. Because we are trying learn using few-shot examples, these losses are necessary.</p><p>Now let’s talk about two very important concepts in IA3. Rank Classification and Length Normalization.</p><p>‍</p><p>In Rank Classification a model is asked to rank a set of responses by their correctness. This is done by calculating the probability scores for the potential responses. The L<sub>UL </sub>is then used to reduce the probability of the wrong responses and as a result, increase the probability of the correct response. But with Rank classification, we face a critical problem, which is that the responses with fewer tokens will rank higher, because of how probability works. A smaller amount of generated tokens ensures a higher probability as the probability of every generated token is &lt; 1. To fix this, the authors propose dividing the score of the response by the number of tokens in the response. Doing this will normalize the scores. One very important thing to note here is that normalization is done over log probabilities, not raw probabilities. Log probabilities are negative and between zero to one.</p><p>‍</p><h3>Efficiency of IA3</h3><p>IA3 just like LoRA reduces the number of trainable parameters. But instead of using low-rank matrices, IA3 uses rescaling vectors. This reduces the trainable parameters to about 0.01%, compared to LoRA's &gt; 0.1%, for the T0 model trained in the paper. The frozen state of the LLM also provides us with the option of having multiple adapters for multiple use cases. Also, because the authors used element-wise multiplication, it is super easy to merge the adapter to the LLM weights because of the commutative property of multiplication.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1247px""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64f2581684d2e29d3f357ee7_nh7Jp_Bv3DSHLC3rjBFirxHpQ04gjvM-T1zn7OfHpRcwtcNf9lyampebUIWUILzJ5tegpu6-8ySzmX2nohM5ODOH_ozBNMGQNr3EbV-UpVR0jbmKe_wsFehjKJqKDqJ6XaNJuAGwVmT7zNVsOMPbrgA.png""/></div></figure><p>‍</p><p>The above figure shows that IA3 performs better than LoRA and barely affects the FLOPs. This makes IA3 a highly efficient and desirable technique. Also because IA3 is an additive adapter technique, just like LoRA we can target specific parts of the model and decide where to introduce the rescaling vectors. This helps us reduce the training time and even more.</p><p>‍</p><p>‍</p><h2>P-Tuning</h2><p>The P-tuning method aims to optimize the representation of the prompt which is passed to the model. In the <a href=""https://arxiv.org/abs/2103.10385"">P-Tuning paper</a>, the authors emphasize how prompt engineering is a very strong technique when working with large language models. The p-Tuning method builds up on top of prompt engineering and tries to further improve the effectiveness of a good prompt.</p><p>‍</p><p>P-Tuning works by creating a small <em>encoder network</em> for your prompt that creates a <em>soft prompt</em> for your passed prompt. To tune your LLM using P-tuning, you are supposed to create a <em>prompt template</em> that represents your prompt. And a context <em>x</em> which is used in the template to get label <em>y</em>. This is the approach mentioned in the paper. The tokens used for the prompt template are trainable and learnable parameters, these are called <em>pseudo tokens</em>. We also add a prompt encoder which then helps us update pseudo tokens to the specific task at hand. The prompt encoder is usually a <em>bi-LSTM</em> network that learns the optimal representation of the prompt for the model and then passes the representation to it. The LSTM network is attached to the original model. Only the encoder network and the pseudo tokens are trained here, the weights of the original network remain unaffected. Once the training is done, the LSTM head is discarded as we have the <em>h<sub>i</sub> </em>which can be used directly.</p><p>‍</p><p>In short, the prompt encoder only changes the embeddings of the passed prompt to better represent the task, everything else remains unchanged.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1470pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64ac796b04cddb42676dc685_aM4QCzgu6ql_uqxJuHdVBwnMkbaY2QzJ855KSVzwnk1_BVwpnKXBZHGqo87QZMSK1b3YvLrSdxNrYqIX6HH3n9tLXJ_HsWhxqN15LgT15jB0N2I1p7OdJ2E-1rLeMfhGpSDf2qe9rD4xw__aBwns9xY.png""/></div></figure><h3>Efficiency of P-Tuning</h3><p>In terms of efficiency, P-tuning is just as good as any other method. In the paper, the authors show that P-Tuning was able to perform <strong>better than full fine-tuning</strong> on most of the benchmarks. It can be said that P-Tuning is comparable to the full fine-tuning of large language models.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1455pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64ac796c5ab7e0b32071c2a9_8iTDr8NKlsgmLyo_TIIfUXrfiHR-cnm-8CkCbZRKhO7Otf9pdiOa_3sfkp_OXNGgkuBS54RRAJTqxSXlrwNMQFn4I1VwKxdHzNRjcFHx1O7MWT1OjklldGpZQZwIYvBQBR5dUhCIaOofJ1lm5Hpivvw.png""/></div></figure><p>‍</p><p>But there is a core issue when it comes to P-Tuning. P-Tuning is a prompt optimization technique, it optimizes the prompt that is passed to the bigger model. This means that we are still largely based on the large model in terms of capability. If a model has not been trained on sentiment classification optimizing sentiment classification prompts using P-Tuning will not do a lot of good to the model. P-Tuning is an assistive technique. It is always very important to pick a model that can do the required task out of the box “well” with some prompt engineering, and then further optimize it.</p><p>‍</p><h2>Prefix Tuning</h2><p>Prefix tuning can be considered the next version of P-Tuning. The authors of P-Tuning published a paper on <a href=""https://arxiv.org/abs/2110.07602"">P-Tuning V-2</a> addressing the issues of P-Tuning. In this paper, they implemented the Prefix tuning introduced in <a href=""https://arxiv.org/abs/2101.00190"">this paper</a>. Prefix tuning and P-Tuning do not have a lot of differences but can still lead to different results. Let’s dive into a deeper explanation.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1153pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64ac796ca90f165006083da8_A_KieXmVWiCxx2HDCfnPhP7Yw67KDpTn6tqFF_8wKNMel0AfOZu3NoNHfvm23O2F8yVbjYl73WH8XLHXvlCn1MoRch5wHMlZmpEKL32wgTf9-JvVRqf8IuzuyknPLe5RWQ57I3tXFGe4dhuLu0z_Ni4.png""/></div></figure><p>‍</p><p>In P-Tuning, we added learnable parameters only to the input embeddings but in Prefix Tuning we add them <strong>to all the layers of the network</strong>. This ensures that the model itself learns more about the task it is being finetuned on. We append learnable parameters to the prompt and to every layer activation in the transformer layers. The difference from P-Tuning is that instead of completely modifying the prompt embeddings, we only add very few learnable parameters at the start of the prompt at every layer. Here’s a visual explanation:</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64ac796c5ab7e0b32071c301_LMgayME_f2cuV5YWWcSeCLRFXyU4y4mE4kbfajfq-OSTH06PnqhYgJscRCji-KLqKk8C25-ZbqR2PN15Ffr1kFT5EVGXIcF3eghgDe8A9GYeLDwXxZNZ-YhrjCYltq_qbOUrh17SmcN1BE5DczTE6KE.png""/></div></figure><p>‍</p><p>At every layer in the transformer, we concatenate a soft prompt with the input which has learnable parameters. These learnable parameters are tuned using a very small MLP, only 2 fully connected layers. This is done because in the paper authors note that directly updating these prompt tokens is very sensitive to learning rate and initialization. The soft prompts increase the number of trainable parameters but substantially increase the learning ability of the model too. The MLP or fully connected layers can be dropped later as we only care about the soft prompts, which will be appended to the input sequences during inference and will guide the model.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1128pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64ac796cb31d4755e5b9f326_gxwL6634yC6BJhGonpxUj78cM0Z_hJM-wmxiJuhhsF9PiK1W-g_eLCpalPG3Z9vjAMsbBwyx-mnG_ALUC1qak9i0QRK0hlPeUI5V_15TdHDM6fWCNfOgujr4F6RYxH4g9znSx-DE4UGpKQAr2ItcOFU.png""/></div></figure><p>‍</p><h3>Efficiency of Prefix Tuning</h3><p>Prefix tuning shows massive gains over P-Tuning. And as the model size increases, these gains increase too. This is perhaps because there are more trainable parameters for larger models. In the chart, you can see the authors compare the performance of P-Tuning, full finetuning, and Prefix tuning. Prefix tuning performs better than or as well as P-tuning in almost all tasks. In many cases, it performs even better than Full fine-tuning!</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1580pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64ac796c1def5997ce0bad2b_HWKJYOvpznNVd06S-B6iw8qRyrCrvv4qUwjTPN7SDhIUDvbFsNS7__2EoLWnWyM7TlwUs98vXnxnfmDORHZ6b4Rdcgyvf6roYNmFfhMZIpVxl2LmHijQgBytA9CDvJJhD-N3uteFL4f0QdXFsrCX1dA.png""/></div></figure><p>‍</p><p>One big reason why prefix tuning works really well is that the number of trainable parameters is not limited only to the input sequence. Learnable parameters are added at every layer, making the model much more flexible. Prefix tuning, unlike P-tuning, not only affects the prompt tokens but also the model itself. This allows the model to learn more. But this approach is still largely based on the prompt. It is still suggested to take a model that can perform the task and only then optimize it, as that will lead to much better results. As for the size of parameters, the number of trained parameters increase substantially, from <strong>0.01% to 0.1 to 3% parameters</strong>. But the size of parameters still remains small enough to be transferred and loaded easily and quickly.</p><p>‍</p><h2>Prompt Tuning</h2><p>Prompt tuning was one of the first papers to build upon the idea of finetuning only with soft prompts. The ideas of P-Tuning and Prefix Tuning come from this paper. Prompt tuning is a very simple and easy-to-implement idea. It involves prepending a specific prompt to the input and using virtual tokens or new trainable tokens for that specific prompt. These new virtual tokens can be finetuned during the process to learn a better representation of the prompt. This means that the model is tuned to understand the prompt better. Here is a comparison of prompt tuning with full fine-tuning from the paper:</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1310pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64ac796dcfdba29eb2c9287c_61MouOmSUGteW9qoekJTGOfmat0OhESch4gqkOztM_39O3bmE2GxvQ8OsHPpF06lSSQYY-zPGWhM2w3P6eQoD2OG_Q46WpR0RCAoAq0q0qZisLDvSbSRio-jJKcmO44MelRp1fhxAD9bm_g2tgfjH9k.png""/></div></figure><p>‍</p><p>Here you can see that full model tuning requires multiple copies of the model to exist if we want to use the model for multiple tasks. But with Prompt Tuning, you only need to store the learned virtual tokens of the prompt tokens. So for example, if you use a prompt like <em>“Classify this tweet: {tweet}” </em>the goal will be to learn new better embeddings for the prompt. And during inference, only these new embeddings will be used to generate the outputs. This allows the model to tune the prompt to help itself generate better outputs during inference.</p><p>‍</p><h3>Efficiency of Prompt Tuning</h3><p>The biggest advantage of using prompt tuning is the small size of learned parameters. The files can be in <strong>KBs.</strong> As we can determine the dimension size and number of parameters to use for the new tokens, we can greatly control the number of parameters we are going to learn. In the paper, the authors show how even with a very small number of trainable tokens method performs really well. And the performance only goes up as bigger models are used. You can read the paper <a href=""https://arxiv.org/abs/2104.08691"">here</a>.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1165pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64ac796d4542b678f53df34a_T0sHFW8zg2lugHlswaWtCgjGfZ_LQMcSBr8VYCOAfwwWFzwWtVXtY13ytAUPV_8tkYklYbVGWyOLcR-ehrhKgfQx5rz4D_HZF_5Fw5ASqcYxEJuAsC7cdZRFtHdyI995PtOB41OjxUKRm8JFg-gOZPg.png""/></div></figure><p>‍</p><p>Another big advantage is that we can use the same model <strong>without any changes</strong> for multiple tasks, as the only thing being updated are the embeddings of the prompt tokens. Meaning you can use the same model for a tweet classification task and for a language generation task without any changes to the model itself, given the model is big and sophisticated enough to perform those tasks. But a big limitation is that the model itself doesn’t learn anything new. This is purely a prompt optimization task. This means if the model has never trained on a sentiment classification dataset, prompt tuning might not be of any help. It is <strong>very important</strong> to note that this method optimizes the prompts, not the model. So, if you cannot handcraft a <em>hard </em>prompt that can do the task relatively well, there is no use of trying to optimize for a <em>soft</em> prompt using prompt optimization techniques.</p><p>‍</p><h2>LoRA vs Prompt Tuning</h2><p>Now we have explored various PEFT techniques. Now the question becomes whether to use an additive technique like Adapter and LoRA or you use a Prompt based technique like P-Tuning and Prefix Tuning.</p><p>‍</p><p>On comparing LoRA vs P-Tuning and Prefix Tuning, one can say for sure LoRA is the best strategy in terms of getting the most out of the model. But it might not be the most efficient based on your needs. If you want to <strong>train</strong> the model on a much different task than what it has been trained on, LoRA is without a doubt the best strategy for tuning the model efficiently. But if your task is more or less already understood by the model, but the challenge is to properly prompt the model, then you should use Prompt Tuning techniques. Prompt Tuning doesn’t modify many parameters in the model and mainly focuses on the passed prompt instead.</p><p>‍</p><p>One important point to note is that LoRA decomposes the weight updation matrix into smaller rank matrices and uses them to update the weights of the model. Even though trainable parameters are low, LoRA updates all the parameters in the targeted parts of the neural network. Whereas in Prompt Tuning techniques, a few trainable parameters are added to the model, this usually helps the model adjust to and understand the task better but does not help the model learn new properties well.</p><p>‍</p><h2>LoRA and PEFT in comparison to full Finetuning</h2><p>PEFT, Parameter Efficient Fine Tuning, is proposed as an alternative to full Finetuning. For most of the tasks, it has already been shown in papers that PEFT techniques like LoRA are comparable to full finetuning, if not better. But, if the new task you want the model to adapt to is completely different from the tasks the model has been trained on, PEFT might not be enough for you. The limited number of trainable parameters can result in major issues in such scenarios.</p><p>‍</p><p>If you are trying to build a code generation model using a text-based model like LLaMA or Alpaca, you should probably consider fine-tuning the whole model instead of tuning the model using LoRA. This is because the task is too different from what the model already knows and has been trained on. Another good example of such a task is training a model, which only understands English, to generate text in the Nepali language.</p><p>‍</p><h2>Why you should Fine-tune models for your business use case</h2><p>Finetuning model is an important step for any business that wants to get the most out of its machine-learning applications. It allows you to customize the model to your specific use case, which can lead to improved accuracy and performance. It saves time, money, and resources by eliminating the need to build a new model from the ground up. Fine-tuning lets you optimize the use of your proprietary data, adjusting the model to better fit your available data, and even incorporating new data if needed. This ensures a more accurate model that better serves your business needs. Here are some more benefits:</p><p>‍</p><ul role=""list""><li>Customization: Fine-tuning allows you to tailor the model to your specific needs, enhancing accuracy and performance. </li><li>Resource Efficiency: It saves time, money, and resources by eliminating the need to build a new model from scratch. </li><li>Performance Boost: Fine-tuning enhances the performance of the pretrained model using your unique datasets. </li><li>Data Optimization: It lets you make the most of your data, adjusting the model to better fit your available data, and even incorporating new data if needed.</li></ul><p>‍</p><p>But as the size of models grows to billions of parameters fine-tuning itself can be a challenge. The  PEFT  techniques we discussed in this blog help to reduce the time and resources needed to fine-tune a model. It helps speed up the training process by making use of the pretrained weights and parameters and allows you to fine-tune the model more efficiently. Also, using PEFT, you can easily transfer models over the internet and even use the same model for multiple purposes. PEFT opens up a whole new world of possibilities for businesses that want to make the most of their machine-learning applications.</p><h2>Want to Train Custom LLMs with PEFT?</h2><p>If you want to build or train custom LLMs or Chatbots, we can help you fine-tune them to your specific needs. We have done a ton of work on <a href=""https://www.mercity.ai/blog-post/custom-gpt-4-chatbot"">building custom chatbots</a> and training large language models. Contact us today and let us build a custom LLM that revolutionizes your business.</p><p>‍</p></div>"
NLP and GPT-4 applications in Sales,gpt-nlp-in-sales,640f56f76d313b2faa631c11,6477c3e69ede7ec2c180f592,False,False,Wed May 31 2023 22:02:14 GMT+0000 (Coordinated Universal Time),Sun Jul 23 2023 20:35:07 GMT+0000 (Coordinated Universal Time),Sun Jul 23 2023 20:35:07 GMT+0000 (Coordinated Universal Time),"<p id="""">The rapid advancements in AI and Large Language Models like OpenAI’s GPT-4 and Google’s Bard, have made it possible to extract valuable insights from large corpora of data. These capabilities can be used to make better customer interactions and optimize sales processes. By leveraging Language Models and custom NLP pipelines, businesses can analyze vast amounts of data, such as customer conversations and interactions, to gain valuable insights. This enables sales teams to understand customer needs, anticipate intent, and personalize the sales experience.&nbsp;</p><p id="""">‍</p><p id="""">In this article, you will learn how LLMs like GPT and NLP can help you boost your sales.</p><h2 id="""">What is NLP?</h2><p id="""">NLP, or Natural Language Processing, is a branch of artificial intelligence that enables machines to understand and interpret human language. It encompasses a set of algorithms and techniques that allows computer programs to comprehend, analyze, and generate natural language text or speech.&nbsp;</p><p id="""">‍</p><p id="""">Large Language models are a part of NLP where we train massive neural networks to understand and perform tasks with human language. There has been a lot of development in LLMs recently.</p><p id="""">‍</p><p id="""">By leveraging advanced algorithms and machine learning models, NLP enables businesses to extract meaningful insights from vast amounts of textual data, empowering them to make data-driven decisions and drive positive outcomes.</p><h3 id="""">How can NLP help Sales processes?</h3><p id="""">As mentioned before, NLP pipelines allow computers to understand human language better. Computers can analyze a ton of information and extract key information from it. This allows sales teams to identify and understand customer needs more effectively and faster.</p><p id="""">‍</p><p id="""">NLP algorithms and LLMs can help in understanding customer intent, going beyond the literal interpretation of their words. By analyzing language patterns, sentiment, and context, NLP algorithms can discern the underlying meaning and emotions behind customer messages. This understanding of customer intent empowers sales teams to provide more relevant, targeted, and personalized responses, enhancing the overall customer experience and increasing the likelihood of successful sales engagements.</p><p id="""">‍</p><p id="""">Here are some ways NLP can help sales:</p><h4 id="""">Identifying customer needs</h4><p id="""">NLP models are really good at understanding language and inferring meaning. Even early language models like BERT, and GPT-2 can be used to extract relevant information from online reviews, surveys, comments, and feedback forms. This data can help the sales and marketing teams to understand what customers want and what are the issues they are facing.</p><p id="""">‍</p><p id="""">Larger models like GPT-4 and Claude can perform these tasks out of the box with zero-shot capabilities.</p><h4 id="""">Understanding customer intent</h4><p id="""">Language models can decipher customer intent by analyzing their language patterns, sentiment, and context. This data can be paired with their browsing behavior and pattern to understand what they are looking for and why.</p><p id="""">‍</p><p id="""">Understanding “why” or intent behind the purchase is very valuable information as it can help understand user behavior and needs. This can lead to selling more products to the same customer and giving better suggestions too.</p><h4 id="""">Personalizing the sales experience</h4><p id="""">Sales outreach is often done in a “one-to-many” way. A single piece of text, like a cold email, is taken and sent to a list of ten thousand prospects. This is not very effective.</p><p id="""">‍</p><p id="""">AI can help you personalize your sales message for every prospect. LLMs can take some information and generate or rephrase some text based on that information. You can use this to write a unique cold email for every prospect in no time.</p><p id="""">‍</p><p id="""">There are SaaS tools out there that help you personalize your cold emails by generating “personalized first lines”. This approach has shown great success and seems to significantly increase open and reply rates. Studies have shown that better first lines can <a href=""https://www.marketingdive.com/news/study-personalized-email-subject-lines-increase-open-rates-by-50/504714/"" id="""">increase open rates by 50%</a>.</p><h4 id="""">Measuring the effectiveness of sales campaigns</h4><p id="""">Once a campaign is launched the data collection becomes crucial to make sure the sales and marketing teams learn what they did right and what they did wrong. Classic NLP techniques like sentiment analysis can be used to understand qualitative data better and learn from it.</p><p id="""">‍</p><p id="""">You can collect all the campaign replies and feedback, and give it to a Langauge model like GPT4 and ask it to analyze and find patterns in it. This data can come from cold email replies, sales email threads, and even from social media campaigns.</p><p id="""">‍</p><h2 id="""">How GPT and NLP can Help Sales</h2><p id="""">Now let's do a deep dive into use cases of NLP in sales and how it can help improve sales processes and overall performance.</p><h3 id="""">Chatbot for Sales Assistance</h3><p id="""">Buying things online is great, but as online marketplaces grow, finding exactly what you are looking for is getting harder and harder. Search functionality returns thousands if not hundreds of similar products for one single search. This leads to <strong id="""">Search Abandonment</strong> by the customer.</p><p id="""">‍</p><p id="""">A <a href=""https://cloud.google.com/blog/topics/retail/search-abandonment-impacts-retail-sales-brand-loyalty"" id="""">survey by Google</a> showed that <strong id="""">77%</strong> of users avoid sites where they have experienced difficulties with search. They also showed that <strong id="""">$300B</strong> is lost every year just because of bad search experiences.</p><p id="""">‍</p><p id="""">This is where AI comes in. GPT-4 or ChatGPT-based chatbots can assist users in narrowing down the search and finding exactly what they are looking for.</p><p id="""">‍</p><p id="""">Here’s an example of a chatbot we built to assist buyers in their buying journey.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6477bd64daafea3f47f5ba42_c7ad7dfd.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">‍</p><p id="""">You can see that the chatbot is capable of asking follow-up questions and collecting relevant information required to accurately search for the product.</p><p id="""">‍</p><p id="""">Once the chatbot has collected enough information, it will use the <strong id="""">search</strong> functionality to find the product and return it:</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1028px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1028px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6477b94a54a1a2de3581822d_d4b01dce.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">‍</p><p id="""">We can have a Python pipeline to replace the <strong id="""">search(...)</strong> with actual product images and links. This allows us to integrate search and product browsing features directly into the chat experience. We can also ask ChatGPT to extract information in a JSON format which then can be used to implement as filters on the search itself.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1024px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1024px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6477b943ef72722ebff0ba42_5aa993e6.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">‍</p><h4 id="""">How to build a Sales Assistance Chatbot?</h4><p id="""">A good sales/search chatbot has to be able to be good at certain tasks. It has to be able to ask for more information and should be able to form search queries to look up products. It should also have access to product inventory and should be able to show information from the product database right in the chat for ease of use. These are the key capabilities of a good chatbot:</p><p id="""">‍</p><ul id=""""><li id=""""><strong id="""">Ask follow-up questions:</strong> Asking follow-up questions is necessary for knowledge gathering and forming a proper search query. It helps the model understand what the user wants and execute a better search query. In the example shown above, you can see it asked multiple questions before using the <strong id="""">search(...)</strong> function.</li><li id=""""><strong id="""">Form search queries: </strong>After gathering the information, the chatbot should be able to form proper search queries to look up the products in the inventory database.</li><li id=""""><strong id="""">Access to Product Inventory:</strong> After forming the search query, Chatbot should be able to access the products returned and show them in a proper manner. It should also be able to take the search further if too many products are returned.</li></ul><p id="""">‍</p><p id="""">At Mercity AI we have built our pipelines and techniques to tackle all these problems. Here is our pipeline:</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1287px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1287px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6477b94e3634a7dffddb959a_97043a18.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">Let’s go through all these components step by step.</p><h5 id="""">Chatbot</h5><p id="""">Chatbot is a large language model here. It can be GPT3 finetuned for chatting purposes, or ChatGPT GPT-4. Chat models like GPT3.5 and GPT-4 perform better as they have been pretty much trained to follow a chat-like structure.</p><p id="""">‍</p><p id="""">Chatbot here is responsible for interacting with the user and gathering information about what kind of product the user is looking for. And after that, writing a search query to search for the relevant products and return them to the user.</p><p id="""">‍</p><p id="""">Here we use a modified version of <a href=""https://arxiv.org/abs/2210.03629"" id="""">ReACT prompting</a> to give the model search capabilities. Here’s an example of original ReACT prompting:</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:818px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""818px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6477b956ef72722ebff0cf98_3392684f.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">‍</p><p id="""">Here you can see the model <em id="""">thinks</em> and uses the <strong id="""">search</strong> function and search results to give a better answer to the user.</p><p id="""">‍</p><h5 id="""">Embedding Generator</h5><p id="""">This component is responsible for taking all the pre-existing product data and converting it into embedding vectors.</p><p id="""">‍</p><p id="""">Embeddings are numerical representations of textual data, these are learned and used by AI models. These vectors are responsible for encoding and learning the <em id="""">semantic</em> meanings of the text. Because these vectors “carry” meaning, they have a property: <em id="""">Texts with similar meanings or semantics will be located closer to each other in the vector space than the texts with different meanings.</em> This property allows us to use these embeddings for <strong id="""">semantic search</strong>.</p><p id="""">‍</p><p id="""">We can use <a href=""https://openai.com/blog/introducing-text-and-code-embeddings"" id="""">OpenAI’s embeddings</a> or <a href=""https://www.sbert.net/"" id="""">Sentence Transformers</a> for embeddings.</p><p id="""">‍</p><h5 id="""">Embedding Search Pipeline</h5><p id="""">This component takes search queries from the chatbot and finds the relevant products for it.</p><p id="""">‍</p><p id="""">It works by converting the search query into embedding vectors and then using cosine similarity to find the most similar product embeddings. These product embeddings can then be tracked back to the original products, which then can be returned to the user via the chatbot.</p><p id="""">‍</p><p id="""">This graph by OpenAI shows how sentences with similar meanings and contexts are placed together and texts with different meanings are placed farther away.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:903px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""903px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6477b94aeb5a0567608aac37_c31e19b2.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">‍</p><h5 id="""">Vector Database</h5><p id="""">Vector Database here is used to store the embeddings generated by the Embedding Generator. It is responsible for the quick retrieval of these embeddings when required.</p><p id="""">‍</p><p id="""">Some popular choices of vector databases are <a href=""https://www.pinecone.io/"" id="""">Pinecone</a>, <a href=""https://milvus.io/"" id="""">Milvus</a>, and <a href=""https://weaviate.io/"" id="""">Weaviate</a>.</p><p id="""">‍</p><h4 id="""">Advantages of using a Sales Chatbot</h4><p id="""">As presented here, a sales chatbot can greatly help with product search and can make it easy for buyers to interact with the platform. They can answer questions, make recommendations, and complete transactions.&nbsp;</p><p id="""">‍</p><p id="""">Chatbots are becoming increasingly popular as a way to improve the customer shopping experience. They offer a number of benefits for both customers and businesses.</p><p id="""">‍</p><h2 id="""">Visual Product Search</h2><p id="""">Visual Product Search has emerged as a game-changing technology in the realm of e-commerce and retail. With the rise of smartphones and advancements in computer vision, consumers now have the ability to search and discover products simply by capturing or&nbsp; uploading images. This innovative approach allows users to bypass traditional keyword searches and instead rely on visual cues to find exactly what they are looking for.&nbsp;</p><p id="""">‍</p><p id=""""><a href=""https://lens.google/"" id="""">Google lens</a> is the best example of this feature being used in the wild. One can simply take an image of a dress and find it online amongst millions of products. All under a few seconds.</p><p id="""">‍</p><p id="""">Previously, if you like a dress or piece of furniture, you would have to ask or spend hours online trying to find the same product you saw. But google lens can take care of this end to end.</p><p>‍</p><p id="""">Let’s see how can you build a visual search AI for products on your own. You can see our more comprehensive coding guide to build your visual product search engine <a href=""https://www.mercity.ai/blog-post/how-to-build-a-visual-search-pipeline"" id="""">here</a>.</p><p id="""">‍</p><h3 id="""">How to Build a Visual Product Search Pipeline</h3><p id="""">A pipeline to search for similar products is not very difficult to build. So much product matching depends on the data the AI is trained on. The goal of the AI here is not to do a specific task, but to learn the similarities and differences between product images. This AI model can then be used to find products similar to the one passed by the user.</p><p id="""">‍</p><p id="""">To be more specific, we will use the AI model to get embeddings from it and then use them for similarity matching, just as we used in the chatbot pipeline to find relevant products from search queries.</p><p id="""">‍</p><p id="""">The pipeline is not very different from the data retrieval pipeline in the chatbot section:</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1211px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1211px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6477b950cf94dee6523feee1_4404e5f0.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">‍</p><p id="""">Only a very few things have changed here, mainly the <strong id="""">model</strong> and <strong id="""">how we feed the data </strong>has changed. This is because we are using the <a href=""https://openai.com/research/clip"" id="""">CLIP model</a> here, it is a special model that learns to combine both text and images in the same embedding space. Let’s take a deeper look at how the CLIP model can be used for this.</p><h4 id="""">CLIP Model</h4><p id="""">CLIP is a model by OpenAI. It has been trained specifically to learn to put together similar images AND their text descriptions closer together in the embedding space. Meaning that CLIP embedding space will understand the textual meaning of the images, as the texts which can describe the image will be located closer to the image itself.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1861px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1861px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6477ba07a4ae0a7b3f66754b_Screenshot%202023-06-01%20024953.png"" loading=""lazy"" id="""" width=""auto"" height=""auto""></div></figure><p id="""">‍</p><p id="""">This simple capability of the model allows us to create embedding spaces which can find similar products with very high accuracy. This works not only for image search but also for text search as the CLIP model learns with both image and text.</p><p id="""">‍</p><p id="""">But this is not where it ends, CLIP model can be further finetuned on custom datasets to learn more about a task specifically. This is perfect for our task.&nbsp;</p><p id="""">‍</p><p id="""">In this <a href=""https://arxiv.org/abs/2204.03972"" id="""">paper</a>, researchers trained a CLIP model to work specifically on clothes datasets. They trained the model on clothes along with their text descriptions:&nbsp;</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:685px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""685px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6477b953aa69aa2d196246a6_fb64c969.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">‍</p><p id="""">The CLIP model slowly learned to put together the image and its descriptions together in the latent space.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1511px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1511px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6477b9539635f86373d8bed4_72737ffb.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">‍</p><p id="""">You can check out FashionCLIP <a href=""https://github.com/patrickjohncyh/fashion-clip"" id="""">here</a>.</p><p id="""">‍</p><p id="""">Alternatively, MetaAI has also released a model similar to CLIP, but it can combine much more data than just text and images. This is <a href=""https://ai.facebook.com/blog/imagebind-six-modalities-binding-ai/"" id="""">ImageBind</a>. One can also use this model to achieve similar capabilities as CLIP.</p><h3 id="""">Attribute Extraction from Product Images</h3><p id="""">There are not only search but also tagging capabilities that are highly desirable from tools like these:&nbsp;</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1570px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1570px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6477b9b9e4766f017594e3d3_2f04196d.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">Attribute extraction allows us to understand the product better, and as a result, understand the customer better. This allows us to create better recommendations, provide better results, and even better search capabilities.</p><p id="""">‍</p><p id="""">This data can be further used to search or even improve customer recommendations on an e-commerce platform. Some companies offering such capabilities are: <a href=""https://www.pixyle.ai/"" id="""">Pixyle.ai</a>, and <a href=""https://www.visenze.com/"" id="""">Visenze</a>.</p><h3 id="""">Why use Visual Product Search?</h3><p id="""">Visual Product Search offers a ton of benefits that make it a valuable tool for both businesses and consumers.&nbsp;</p><p id="""">‍</p><ul id=""""><li id=""""><strong id="""">Enhanced User Experience:</strong> Visual Product Search simplifies the search process by allowing users to find products simply by uploading or capturing an image. This intuitive approach eliminates the need for keyword searches and provides a more seamless and user-friendly experience. This leads to increased customer satisfaction.</li><li id=""""><strong id="""">Improved Product Discovery:</strong> Visual Search expands the possibilities of product discovery by enabling users to explore visually similar items that they may not have found through traditional search methods. This significantly decreases search abandonment rates.</li><li id=""""><strong id="""">Competitive Advantage:</strong> By implementing Visual Product Search, businesses gain a competitive edge in the crowded e-commerce landscape. Providing an innovative and seamless search experience sets them apart from competitors and helps attract and retain customers.</li><li id=""""><strong id="""">Better Upsells: Once we have a base idea of what the user is looking for, we can sell other products or accessories to go with the product. This enables businesses to enhance upselling opportunities by offering relevant and appealing add-on products, thereby increasing the average order value and maximizing revenue potential.</strong></li></ul><p id="""">There have been studies that show that around <a href=""https://www.businesswire.com/news/home/20180829005092/en/New-Research-ViSenze-Finds-62-Percent-Generation#.W4eYrWp5Mrc.linkedin"" id="""">62% of millennials and Gen-Z want Visual search capabilities</a>. Google has also been working on using more and <a href=""https://blog.google/products/search/making-visual-content-more-useful-search/"" id="""">more visual elements</a> in its search platform. And using visual search simply allows us to shop better and provide a better experience to the customers.</p><p id="""">‍</p><h2 id="""">Summarization of Sales Calls</h2><p id="""">Sales calls can become increasingly lengthy and time-consuming, posing challenges for sales teams to effectively utilize the insights gained from these interactions. By employing automated summarization techniques, businesses can condense lengthy sales calls into concise and meaningful summaries, extracting key points, important details, and actionable insights.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1280px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1280px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6477b9c5639b36a8fd196843_96bdfa3e.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id=""""><em id="""">Source: </em><a href=""https://fireflies.ai/"" id=""""><em id="""">Firefly.ai</em></a></p><p id="""">‍</p><p id="""">Call summarization software can simplify the process for sales teams by saving time and resources. Instead of manually reviewing lengthy sales call recordings, the software condenses the most important information, allowing sales professionals to quickly access key insights. This efficiency lets teams focus on strategic tasks like follow-ups and lead nurturing. Summaries can be easily shared among team members, promoting collaboration and consistent messaging.</p><p id="""">‍</p><p id="""">Here we can use GPT3 for the summarization of these sales calls. By providing it with the text or recordings of these calls, GPT-3 can analyze and understand the conversations, picking out the important information. It then generates short summaries that capture the main points, giving sales teams a clear understanding of the conversation and helping them make better decisions.</p><p id="""">‍</p><p id="""">Here we asked GPT3 to summarize an hour-long call. We told it to give a quick summary with essential points of the conversation. You can see that GPT not only was able to compress the information in the call, but also able to extract the important points with exact figures from the call.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1568px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1568px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6477b9698083d8e8b32ca99d_0012bb56.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">‍</p><h3 id="""">How to summarize very long transcripts?</h3><p id="""">To summarize long transcripts or text using GPT3, you can use a chunking strategy. You can split your long transcript into smaller chunks, and then feed those chunks into GPT3 one by one, asking it to summarize it. Once you have the summaries of the chunks, you can prompt GPT3 to combine all those summaries into one.</p><p id="""">‍</p><p id="""">We use a chunking pipeline to create chunks of the passed text. One important property of this pipeline is that there always should be some overlap between the beginning and end of the consequent chunks. This ensures that there is no data loss between the chunks.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:812px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""812px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6477b969373f992018fac2a6_ba638308.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">‍</p><p id="""">Once we have the chunks, we can then feed them to GPT3 or GPT4 asking it to summarize and put them all together.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:910px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""910px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6477b96a373f992018fac334_4676bd0d.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h2 id="""">Sales Call Analytics</h2><p id="""">Summarization is not the only thing you might want out of your sales calls. There is so much data condensed that it can be hard to put it in a summary. We can ask GPT-3 or GPT-4 to perform all kinds of tasks. Here are some examples:</p><h3 id="""">Extract Questions, Topics, and Tasks from Calls</h3><p id="""">GPT and other models can be used to extract important information from call transcripts. All we need is a prompt to pass the call transcript. And if the transcript doesn’t fit the context length limits, we can do chunking and combining here just as we did in the summarization process.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1579px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1579px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6477b97007ef964e432a80de_da85c204.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">‍</p><h3 id="""">Review Sales Performance</h3><p id="""">GPT3 and GPT-4 can be used to give valuable reviews and gain insights from sales calls. You can put in your transcript and ask GPT3 to give a “detailed analysis” of the salesman and it will.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1584px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1584px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6477b97cb5611714adcdf106_753991a1.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">‍</p><p id="""">This capability can be used to evaluate large sales teams and improve their performance as a result. Salespeople can also practice selling with ChatGPT to improve themselves.</p><p id="""">‍</p><h3 id="""">Sentiment Analysis</h3><p id="""">GPT models can also be used to do sentiment analysis based on what the people on the calls said. Here’s how you do it:</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1179px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1179px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6477b96e9635f86373d8d2da_4f1e7b24.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">‍</p><p id="""">The prompt here has been modified to output one single label for every speaker. We can ask it to answer in a more detailed manner too:</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1571px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1571px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6477b970b8a5a55dc07709e8_1ff6cd07.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">‍</p><h2 id="""">Sales Message Writing Assistant</h2><p id="""">In the competitive world of sales, the ability to communicate persuasively and convey the right message is crucial for success. A sales message writing assistant can be really powerful in optimizing sales communication. Such an assistant can provide real-time suggestions, improvements, and guidance to craft compelling messages. It helps professionals improve their messaging by optimizing the choice of words, tone, and structure, enabling them to create persuasive messages that resonate with potential customers.</p><p id="""">‍</p><h3 id="""">How can an AI Sales Assitant help?</h3><p id="""">Let’s go over some ways AI can help you improve your sales.</p><h4 id="""">Improve your Sales messages</h4><p id="""">An AI assistant can help you improve your sales messages and interactions with your customers by improving your language. Here is how you do that:</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6477b97a2bd445882fe81959_3edf7f64.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">‍</p><p id="""">Here we can see that without a very complex prompt, GPT4 is able to rephrase the message with better language.</p><p id="""">‍</p><p id="""">You can check out our <a href=""https://www.mercity.ai/blog-post/ai-to-write-better-sales-messages"" id="""">detailed guide</a> on how to build a custom model to rewrite sales messages for simplicity and more persuasion.</p><h4 id="""">Recommending Cross and Upsells</h4><p id="""">Having an AI assistant while you go on your sales calls or interact with customers over text can help you recommend what products you can sell them. More importantly, it can suggest you right upsells and cross-sells to maximize the sales potential.</p><p id="""">‍</p><p id="""">Here we gave GPT4 a sales conversation and asked it what products from the product list can be upsold to the customer. This can be integrated directly into the sales platform to provide suggestions in real time.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6477b97c85267f51c6931aa4_512b132f.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h4 id="""">Extract Valuable Data</h4><p id="""">GPT4 can be used to extract valuable information from sales conversations, here’s how:</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1583px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1583px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6477b98054a1a2de3581a49a_379c2269.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">‍</p><p id="""">Here we have used a very weak prompt to extract <em id="""">dynamic</em> information from a sales conversation. We can use similar prompts to extract <strong id="""">specific</strong> information from a conversation. This can be customer data, user behavior, user preferences, etc.</p><h2 id="""">Key data extraction from Sales Documentation</h2><p id="""">AI methods can also be used to extract information from massive documents. This is different from summarization. Summarization is dynamic by nature, here we can extract specific data and work with that. This can be used to analyze specific bits or to extract specific information from long documents, calls, email chains, etc.</p><p id="""">‍</p><p id="""">The extracted information can then be used to maintain CRMs and improve sales performance. This also helps in maintaining information across different mediums.</p><p id="""">‍</p><h3 id="""">Extracting data from Chat Conversations and Emails</h3><p id="""">Conversation analytics can be a huge help in understanding customer interactions and extracting valuable insights. AI techniques can be employed to analyze chat conversations and emails, extracting important data such as customer preferences, sentiment analysis, frequently asked questions, and emerging trends. This data collected over a long period of time and for a big user base can prove really valuable.</p><p id="""">‍</p><p id="""">Here’s how you extract key data from chat conversations:</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1562px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1562px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6477b981b3e5eca0fff89e63_0215b86b.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">‍</p><p id="""">Here we use GPT3 to extract valuable info and specific data from an internet chat between a customer and a salesperson. We have extracted names, objects of interest, and specific reasons for the customer to not buy. This is valuable information for the sales agents on the field who interact with the customers face to face. This information can be used to follow up and do targeted outreach toward this customer.</p><p id="""">‍</p><h3 id="""">Sentiment Analysis on Customer Feedback</h3><p id="""">By leveraging advanced natural language processing algorithms, businesses can uncover customer preferences, sentiment analysis frequently asked questions, and emerging trends from these interactions. This data can be used to understand what customers want and what their major issues are and solve them accordingly.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1025px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1025px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6477b981e4766f017594b283_f786b0fa.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">‍</p><p id="""">Here we asked GPT4 to analyze the past reviews and it returned a quick summary of how customers are reacting to the product. It also returns information about the issues, this can be used to make the product better.</p><p id="""">‍</p><h2 id="""">Personalized Sales Outreach with GPT4</h2><p id="""">GPT-4 can enhance personalized sales outreach by using its advanced language processing capabilities. Businesses can create customized messages, emails, and pitches that resonate with each prospect. By understanding customer needs and preferences, GPT-4 empowers sales teams to deliver targeted and persuasive communications. Using GPT-4 for personalized outreach can boost customer engagement, build trust, and drive sales growth.</p><p id="""">‍</p><p id="""">Moreover, GPT-4 can help sales teams save time and increase efficiency. Automated processes can be used to generate personalized messages quickly and accurately. This can help sales teams reach more prospects in less time, resulting in higher conversion rates. GPT-4 can be integrated into existing sales pipelines to help sales teams maximize their outreach efforts and drive better results.&nbsp;</p><p id="""">‍</p><h3 id="""">Personalizing Emails using GPT4</h3><p id="""">As mentioned before, people often use AI to personalize the first lines of emails for better open rates. But what if you could use AI to personalize the entire email? GPT-4 can help you do that. GPT-4 can generate emails that are tailored to each individual customer, based on their preferences and needs. This can help sales teams build trust and increase engagement with their prospects.</p><p id="""">‍</p><p id="""">Here we are going to generate cold emails for a customer based on their reviews left on the website for a specific product. We pass the review left by the customer to GPT and ask it to write an email specific to the issue faced by them. This can help bring lost customers back to the platform.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1017px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1017px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6477b982f60e5d53ecfac3f7_66dae82d.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">‍</p><p id="""">Prompts can be further modified to generate more specific emails and responses.</p><p id="""">‍</p><p id="""">We can use the same technique to generate a text message using GPT4.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1041px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1041px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/6477b9838712346994811e72_5a8bb121.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">Similarly, GPT4 can also be used to generate newsletters and other marketing material which can be targeted specifically to individual customers or customer segments.</p><p id="""">‍</p><h2 id="""">Enhance Sales Performance with NLP and Cutting-Edge Language Models</h2><p id="""">Discover the incredible potential of NLP-powered large language models like GPT-3, GPT-4, Claude, and LlaMA in revolutionizing your sales strategies. With their advanced language processing capabilities, these models can help you optimize customer interactions, drive conversions, and boost sales growth. <a href=""https://www.mercity.ai/contacts"" id="""">Reach out to us</a> today and unlock the full power of NLP in your sales journey.</p><p id="""">‍</p>",https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/648af64564f542cb9faea366_nlp-in-sales.png,Pranav,AI,Learn how to leverage OpenAI's GPT-4 and NLP techniques to improve sales processes and boost your conversions.,False,"<div class=""rich-text w-richtext""><p>The rapid advancements in AI and Large Language Models like OpenAI’s GPT-4 and Google’s Bard, have made it possible to extract valuable insights from large corpora of data. These capabilities can be used to make better customer interactions and optimize sales processes. By leveraging Language Models and custom NLP pipelines, businesses can analyze vast amounts of data, such as customer conversations and interactions, to gain valuable insights. This enables sales teams to understand customer needs, anticipate intent, and personalize the sales experience. </p><p>‍</p><p>In this article, you will learn how LLMs like GPT and NLP can help you boost your sales.</p><h2>What is NLP?</h2><p>NLP, or Natural Language Processing, is a branch of artificial intelligence that enables machines to understand and interpret human language. It encompasses a set of algorithms and techniques that allows computer programs to comprehend, analyze, and generate natural language text or speech. </p><p>‍</p><p>Large Language models are a part of NLP where we train massive neural networks to understand and perform tasks with human language. There has been a lot of development in LLMs recently.</p><p>‍</p><p>By leveraging advanced algorithms and machine learning models, NLP enables businesses to extract meaningful insights from vast amounts of textual data, empowering them to make data-driven decisions and drive positive outcomes.</p><h3>How can NLP help Sales processes?</h3><p>As mentioned before, NLP pipelines allow computers to understand human language better. Computers can analyze a ton of information and extract key information from it. This allows sales teams to identify and understand customer needs more effectively and faster.</p><p>‍</p><p>NLP algorithms and LLMs can help in understanding customer intent, going beyond the literal interpretation of their words. By analyzing language patterns, sentiment, and context, NLP algorithms can discern the underlying meaning and emotions behind customer messages. This understanding of customer intent empowers sales teams to provide more relevant, targeted, and personalized responses, enhancing the overall customer experience and increasing the likelihood of successful sales engagements.</p><p>‍</p><p>Here are some ways NLP can help sales:</p><h4>Identifying customer needs</h4><p>NLP models are really good at understanding language and inferring meaning. Even early language models like BERT, and GPT-2 can be used to extract relevant information from online reviews, surveys, comments, and feedback forms. This data can help the sales and marketing teams to understand what customers want and what are the issues they are facing.</p><p>‍</p><p>Larger models like GPT-4 and Claude can perform these tasks out of the box with zero-shot capabilities.</p><h4>Understanding customer intent</h4><p>Language models can decipher customer intent by analyzing their language patterns, sentiment, and context. This data can be paired with their browsing behavior and pattern to understand what they are looking for and why.</p><p>‍</p><p>Understanding “why” or intent behind the purchase is very valuable information as it can help understand user behavior and needs. This can lead to selling more products to the same customer and giving better suggestions too.</p><h4>Personalizing the sales experience</h4><p>Sales outreach is often done in a “one-to-many” way. A single piece of text, like a cold email, is taken and sent to a list of ten thousand prospects. This is not very effective.</p><p>‍</p><p>AI can help you personalize your sales message for every prospect. LLMs can take some information and generate or rephrase some text based on that information. You can use this to write a unique cold email for every prospect in no time.</p><p>‍</p><p>There are SaaS tools out there that help you personalize your cold emails by generating “personalized first lines”. This approach has shown great success and seems to significantly increase open and reply rates. Studies have shown that better first lines can <a href=""https://www.marketingdive.com/news/study-personalized-email-subject-lines-increase-open-rates-by-50/504714/"">increase open rates by 50%</a>.</p><h4>Measuring the effectiveness of sales campaigns</h4><p>Once a campaign is launched the data collection becomes crucial to make sure the sales and marketing teams learn what they did right and what they did wrong. Classic NLP techniques like sentiment analysis can be used to understand qualitative data better and learn from it.</p><p>‍</p><p>You can collect all the campaign replies and feedback, and give it to a Langauge model like GPT4 and ask it to analyze and find patterns in it. This data can come from cold email replies, sales email threads, and even from social media campaigns.</p><p>‍</p><h2>How GPT and NLP can Help Sales</h2><p>Now let's do a deep dive into use cases of NLP in sales and how it can help improve sales processes and overall performance.</p><h3>Chatbot for Sales Assistance</h3><p>Buying things online is great, but as online marketplaces grow, finding exactly what you are looking for is getting harder and harder. Search functionality returns thousands if not hundreds of similar products for one single search. This leads to <strong>Search Abandonment</strong> by the customer.</p><p>‍</p><p>A <a href=""https://cloud.google.com/blog/topics/retail/search-abandonment-impacts-retail-sales-brand-loyalty"">survey by Google</a> showed that <strong>77%</strong> of users avoid sites where they have experienced difficulties with search. They also showed that <strong>$300B</strong> is lost every year just because of bad search experiences.</p><p>‍</p><p>This is where AI comes in. GPT-4 or ChatGPT-based chatbots can assist users in narrowing down the search and finding exactly what they are looking for.</p><p>‍</p><p>Here’s an example of a chatbot we built to assist buyers in their buying journey.</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6477bd64daafea3f47f5ba42_c7ad7dfd.png""/></div></figure><p>‍</p><p>You can see that the chatbot is capable of asking follow-up questions and collecting relevant information required to accurately search for the product.</p><p>‍</p><p>Once the chatbot has collected enough information, it will use the <strong>search</strong> functionality to find the product and return it:</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1028pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6477b94a54a1a2de3581822d_d4b01dce.png""/></div></figure><p>‍</p><p>We can have a Python pipeline to replace the <strong>search(...)</strong> with actual product images and links. This allows us to integrate search and product browsing features directly into the chat experience. We can also ask ChatGPT to extract information in a JSON format which then can be used to implement as filters on the search itself.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1024pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6477b943ef72722ebff0ba42_5aa993e6.png""/></div></figure><p>‍</p><h4>How to build a Sales Assistance Chatbot?</h4><p>A good sales/search chatbot has to be able to be good at certain tasks. It has to be able to ask for more information and should be able to form search queries to look up products. It should also have access to product inventory and should be able to show information from the product database right in the chat for ease of use. These are the key capabilities of a good chatbot:</p><p>‍</p><ul role=""list""><li><strong>Ask follow-up questions:</strong> Asking follow-up questions is necessary for knowledge gathering and forming a proper search query. It helps the model understand what the user wants and execute a better search query. In the example shown above, you can see it asked multiple questions before using the <strong>search(...)</strong> function.</li><li><strong>Form search queries: </strong>After gathering the information, the chatbot should be able to form proper search queries to look up the products in the inventory database.</li><li><strong>Access to Product Inventory:</strong> After forming the search query, Chatbot should be able to access the products returned and show them in a proper manner. It should also be able to take the search further if too many products are returned.</li></ul><p>‍</p><p>At Mercity AI we have built our pipelines and techniques to tackle all these problems. Here is our pipeline:</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1287pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6477b94e3634a7dffddb959a_97043a18.png""/></div></figure><p>Let’s go through all these components step by step.</p><h5>Chatbot</h5><p>Chatbot is a large language model here. It can be GPT3 finetuned for chatting purposes, or ChatGPT GPT-4. Chat models like GPT3.5 and GPT-4 perform better as they have been pretty much trained to follow a chat-like structure.</p><p>‍</p><p>Chatbot here is responsible for interacting with the user and gathering information about what kind of product the user is looking for. And after that, writing a search query to search for the relevant products and return them to the user.</p><p>‍</p><p>Here we use a modified version of <a href=""https://arxiv.org/abs/2210.03629"">ReACT prompting</a> to give the model search capabilities. Here’s an example of original ReACT prompting:</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:818pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6477b956ef72722ebff0cf98_3392684f.png""/></div></figure><p>‍</p><p>Here you can see the model <em>thinks</em> and uses the <strong>search</strong> function and search results to give a better answer to the user.</p><p>‍</p><h5>Embedding Generator</h5><p>This component is responsible for taking all the pre-existing product data and converting it into embedding vectors.</p><p>‍</p><p>Embeddings are numerical representations of textual data, these are learned and used by AI models. These vectors are responsible for encoding and learning the <em>semantic</em> meanings of the text. Because these vectors “carry” meaning, they have a property: <em>Texts with similar meanings or semantics will be located closer to each other in the vector space than the texts with different meanings.</em> This property allows us to use these embeddings for <strong>semantic search</strong>.</p><p>‍</p><p>We can use <a href=""https://openai.com/blog/introducing-text-and-code-embeddings"">OpenAI’s embeddings</a> or <a href=""https://www.sbert.net/"">Sentence Transformers</a> for embeddings.</p><p>‍</p><h5>Embedding Search Pipeline</h5><p>This component takes search queries from the chatbot and finds the relevant products for it.</p><p>‍</p><p>It works by converting the search query into embedding vectors and then using cosine similarity to find the most similar product embeddings. These product embeddings can then be tracked back to the original products, which then can be returned to the user via the chatbot.</p><p>‍</p><p>This graph by OpenAI shows how sentences with similar meanings and contexts are placed together and texts with different meanings are placed farther away.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:903pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6477b94aeb5a0567608aac37_c31e19b2.png""/></div></figure><p>‍</p><h5>Vector Database</h5><p>Vector Database here is used to store the embeddings generated by the Embedding Generator. It is responsible for the quick retrieval of these embeddings when required.</p><p>‍</p><p>Some popular choices of vector databases are <a href=""https://www.pinecone.io/"">Pinecone</a>, <a href=""https://milvus.io/"">Milvus</a>, and <a href=""https://weaviate.io/"">Weaviate</a>.</p><p>‍</p><h4>Advantages of using a Sales Chatbot</h4><p>As presented here, a sales chatbot can greatly help with product search and can make it easy for buyers to interact with the platform. They can answer questions, make recommendations, and complete transactions. </p><p>‍</p><p>Chatbots are becoming increasingly popular as a way to improve the customer shopping experience. They offer a number of benefits for both customers and businesses.</p><p>‍</p><h2>Visual Product Search</h2><p>Visual Product Search has emerged as a game-changing technology in the realm of e-commerce and retail. With the rise of smartphones and advancements in computer vision, consumers now have the ability to search and discover products simply by capturing or  uploading images. This innovative approach allows users to bypass traditional keyword searches and instead rely on visual cues to find exactly what they are looking for. </p><p>‍</p><p><a href=""https://lens.google/"">Google lens</a> is the best example of this feature being used in the wild. One can simply take an image of a dress and find it online amongst millions of products. All under a few seconds.</p><p>‍</p><p>Previously, if you like a dress or piece of furniture, you would have to ask or spend hours online trying to find the same product you saw. But google lens can take care of this end to end.</p><p>‍</p><p>Let’s see how can you build a visual search AI for products on your own. You can see our more comprehensive coding guide to build your visual product search engine <a href=""https://www.mercity.ai/blog-post/how-to-build-a-visual-search-pipeline"">here</a>.</p><p>‍</p><h3>How to Build a Visual Product Search Pipeline</h3><p>A pipeline to search for similar products is not very difficult to build. So much product matching depends on the data the AI is trained on. The goal of the AI here is not to do a specific task, but to learn the similarities and differences between product images. This AI model can then be used to find products similar to the one passed by the user.</p><p>‍</p><p>To be more specific, we will use the AI model to get embeddings from it and then use them for similarity matching, just as we used in the chatbot pipeline to find relevant products from search queries.</p><p>‍</p><p>The pipeline is not very different from the data retrieval pipeline in the chatbot section:</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1211pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6477b950cf94dee6523feee1_4404e5f0.png""/></div></figure><p>‍</p><p>Only a very few things have changed here, mainly the <strong>model</strong> and <strong>how we feed the data </strong>has changed. This is because we are using the <a href=""https://openai.com/research/clip"">CLIP model</a> here, it is a special model that learns to combine both text and images in the same embedding space. Let’s take a deeper look at how the CLIP model can be used for this.</p><h4>CLIP Model</h4><p>CLIP is a model by OpenAI. It has been trained specifically to learn to put together similar images AND their text descriptions closer together in the embedding space. Meaning that CLIP embedding space will understand the textual meaning of the images, as the texts which can describe the image will be located closer to the image itself.</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1861pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6477ba07a4ae0a7b3f66754b_Screenshot%202023-06-01%20024953.png""/></div></figure><p>‍</p><p>This simple capability of the model allows us to create embedding spaces which can find similar products with very high accuracy. This works not only for image search but also for text search as the CLIP model learns with both image and text.</p><p>‍</p><p>But this is not where it ends, CLIP model can be further finetuned on custom datasets to learn more about a task specifically. This is perfect for our task. </p><p>‍</p><p>In this <a href=""https://arxiv.org/abs/2204.03972"">paper</a>, researchers trained a CLIP model to work specifically on clothes datasets. They trained the model on clothes along with their text descriptions: </p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:685pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6477b953aa69aa2d196246a6_fb64c969.png""/></div></figure><p>‍</p><p>The CLIP model slowly learned to put together the image and its descriptions together in the latent space.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1511pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6477b9539635f86373d8bed4_72737ffb.png""/></div></figure><p>‍</p><p>You can check out FashionCLIP <a href=""https://github.com/patrickjohncyh/fashion-clip"">here</a>.</p><p>‍</p><p>Alternatively, MetaAI has also released a model similar to CLIP, but it can combine much more data than just text and images. This is <a href=""https://ai.facebook.com/blog/imagebind-six-modalities-binding-ai/"">ImageBind</a>. One can also use this model to achieve similar capabilities as CLIP.</p><h3>Attribute Extraction from Product Images</h3><p>There are not only search but also tagging capabilities that are highly desirable from tools like these: </p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1570pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6477b9b9e4766f017594e3d3_2f04196d.png""/></div></figure><p>Attribute extraction allows us to understand the product better, and as a result, understand the customer better. This allows us to create better recommendations, provide better results, and even better search capabilities.</p><p>‍</p><p>This data can be further used to search or even improve customer recommendations on an e-commerce platform. Some companies offering such capabilities are: <a href=""https://www.pixyle.ai/"">Pixyle.ai</a>, and <a href=""https://www.visenze.com/"">Visenze</a>.</p><h3>Why use Visual Product Search?</h3><p>Visual Product Search offers a ton of benefits that make it a valuable tool for both businesses and consumers. </p><p>‍</p><ul role=""list""><li><strong>Enhanced User Experience:</strong> Visual Product Search simplifies the search process by allowing users to find products simply by uploading or capturing an image. This intuitive approach eliminates the need for keyword searches and provides a more seamless and user-friendly experience. This leads to increased customer satisfaction.</li><li><strong>Improved Product Discovery:</strong> Visual Search expands the possibilities of product discovery by enabling users to explore visually similar items that they may not have found through traditional search methods. This significantly decreases search abandonment rates.</li><li><strong>Competitive Advantage:</strong> By implementing Visual Product Search, businesses gain a competitive edge in the crowded e-commerce landscape. Providing an innovative and seamless search experience sets them apart from competitors and helps attract and retain customers.</li><li><strong>Better Upsells: Once we have a base idea of what the user is looking for, we can sell other products or accessories to go with the product. This enables businesses to enhance upselling opportunities by offering relevant and appealing add-on products, thereby increasing the average order value and maximizing revenue potential.</strong></li></ul><p>There have been studies that show that around <a href=""https://www.businesswire.com/news/home/20180829005092/en/New-Research-ViSenze-Finds-62-Percent-Generation#.W4eYrWp5Mrc.linkedin"">62% of millennials and Gen-Z want Visual search capabilities</a>. Google has also been working on using more and <a href=""https://blog.google/products/search/making-visual-content-more-useful-search/"">more visual elements</a> in its search platform. And using visual search simply allows us to shop better and provide a better experience to the customers.</p><p>‍</p><h2>Summarization of Sales Calls</h2><p>Sales calls can become increasingly lengthy and time-consuming, posing challenges for sales teams to effectively utilize the insights gained from these interactions. By employing automated summarization techniques, businesses can condense lengthy sales calls into concise and meaningful summaries, extracting key points, important details, and actionable insights.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1280pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6477b9c5639b36a8fd196843_96bdfa3e.png""/></div></figure><p><em>Source: </em><a href=""https://fireflies.ai/""><em>Firefly.ai</em></a></p><p>‍</p><p>Call summarization software can simplify the process for sales teams by saving time and resources. Instead of manually reviewing lengthy sales call recordings, the software condenses the most important information, allowing sales professionals to quickly access key insights. This efficiency lets teams focus on strategic tasks like follow-ups and lead nurturing. Summaries can be easily shared among team members, promoting collaboration and consistent messaging.</p><p>‍</p><p>Here we can use GPT3 for the summarization of these sales calls. By providing it with the text or recordings of these calls, GPT-3 can analyze and understand the conversations, picking out the important information. It then generates short summaries that capture the main points, giving sales teams a clear understanding of the conversation and helping them make better decisions.</p><p>‍</p><p>Here we asked GPT3 to summarize an hour-long call. We told it to give a quick summary with essential points of the conversation. You can see that GPT not only was able to compress the information in the call, but also able to extract the important points with exact figures from the call.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1568pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6477b9698083d8e8b32ca99d_0012bb56.png""/></div></figure><p>‍</p><h3>How to summarize very long transcripts?</h3><p>To summarize long transcripts or text using GPT3, you can use a chunking strategy. You can split your long transcript into smaller chunks, and then feed those chunks into GPT3 one by one, asking it to summarize it. Once you have the summaries of the chunks, you can prompt GPT3 to combine all those summaries into one.</p><p>‍</p><p>We use a chunking pipeline to create chunks of the passed text. One important property of this pipeline is that there always should be some overlap between the beginning and end of the consequent chunks. This ensures that there is no data loss between the chunks.</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:812pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6477b969373f992018fac2a6_ba638308.png""/></div></figure><p>‍</p><p>Once we have the chunks, we can then feed them to GPT3 or GPT4 asking it to summarize and put them all together.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:910pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6477b96a373f992018fac334_4676bd0d.png""/></div></figure><h2>Sales Call Analytics</h2><p>Summarization is not the only thing you might want out of your sales calls. There is so much data condensed that it can be hard to put it in a summary. We can ask GPT-3 or GPT-4 to perform all kinds of tasks. Here are some examples:</p><h3>Extract Questions, Topics, and Tasks from Calls</h3><p>GPT and other models can be used to extract important information from call transcripts. All we need is a prompt to pass the call transcript. And if the transcript doesn’t fit the context length limits, we can do chunking and combining here just as we did in the summarization process.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1579pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6477b97007ef964e432a80de_da85c204.png""/></div></figure><p>‍</p><h3>Review Sales Performance</h3><p>GPT3 and GPT-4 can be used to give valuable reviews and gain insights from sales calls. You can put in your transcript and ask GPT3 to give a “detailed analysis” of the salesman and it will.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1584pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6477b97cb5611714adcdf106_753991a1.png""/></div></figure><p>‍</p><p>This capability can be used to evaluate large sales teams and improve their performance as a result. Salespeople can also practice selling with ChatGPT to improve themselves.</p><p>‍</p><h3>Sentiment Analysis</h3><p>GPT models can also be used to do sentiment analysis based on what the people on the calls said. Here’s how you do it:</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1179pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6477b96e9635f86373d8d2da_4f1e7b24.png""/></div></figure><p>‍</p><p>The prompt here has been modified to output one single label for every speaker. We can ask it to answer in a more detailed manner too:</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1571pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6477b970b8a5a55dc07709e8_1ff6cd07.png""/></div></figure><p>‍</p><h2>Sales Message Writing Assistant</h2><p>In the competitive world of sales, the ability to communicate persuasively and convey the right message is crucial for success. A sales message writing assistant can be really powerful in optimizing sales communication. Such an assistant can provide real-time suggestions, improvements, and guidance to craft compelling messages. It helps professionals improve their messaging by optimizing the choice of words, tone, and structure, enabling them to create persuasive messages that resonate with potential customers.</p><p>‍</p><h3>How can an AI Sales Assitant help?</h3><p>Let’s go over some ways AI can help you improve your sales.</p><h4>Improve your Sales messages</h4><p>An AI assistant can help you improve your sales messages and interactions with your customers by improving your language. Here is how you do that:</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6477b97a2bd445882fe81959_3edf7f64.png""/></div></figure><p>‍</p><p>Here we can see that without a very complex prompt, GPT4 is able to rephrase the message with better language.</p><p>‍</p><p>You can check out our <a href=""https://www.mercity.ai/blog-post/ai-to-write-better-sales-messages"">detailed guide</a> on how to build a custom model to rewrite sales messages for simplicity and more persuasion.</p><h4>Recommending Cross and Upsells</h4><p>Having an AI assistant while you go on your sales calls or interact with customers over text can help you recommend what products you can sell them. More importantly, it can suggest you right upsells and cross-sells to maximize the sales potential.</p><p>‍</p><p>Here we gave GPT4 a sales conversation and asked it what products from the product list can be upsold to the customer. This can be integrated directly into the sales platform to provide suggestions in real time.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6477b97c85267f51c6931aa4_512b132f.png""/></div></figure><h4>Extract Valuable Data</h4><p>GPT4 can be used to extract valuable information from sales conversations, here’s how:</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1583pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6477b98054a1a2de3581a49a_379c2269.png""/></div></figure><p>‍</p><p>Here we have used a very weak prompt to extract <em>dynamic</em> information from a sales conversation. We can use similar prompts to extract <strong>specific</strong> information from a conversation. This can be customer data, user behavior, user preferences, etc.</p><h2>Key data extraction from Sales Documentation</h2><p>AI methods can also be used to extract information from massive documents. This is different from summarization. Summarization is dynamic by nature, here we can extract specific data and work with that. This can be used to analyze specific bits or to extract specific information from long documents, calls, email chains, etc.</p><p>‍</p><p>The extracted information can then be used to maintain CRMs and improve sales performance. This also helps in maintaining information across different mediums.</p><p>‍</p><h3>Extracting data from Chat Conversations and Emails</h3><p>Conversation analytics can be a huge help in understanding customer interactions and extracting valuable insights. AI techniques can be employed to analyze chat conversations and emails, extracting important data such as customer preferences, sentiment analysis, frequently asked questions, and emerging trends. This data collected over a long period of time and for a big user base can prove really valuable.</p><p>‍</p><p>Here’s how you extract key data from chat conversations:</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1562pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6477b981b3e5eca0fff89e63_0215b86b.png""/></div></figure><p>‍</p><p>Here we use GPT3 to extract valuable info and specific data from an internet chat between a customer and a salesperson. We have extracted names, objects of interest, and specific reasons for the customer to not buy. This is valuable information for the sales agents on the field who interact with the customers face to face. This information can be used to follow up and do targeted outreach toward this customer.</p><p>‍</p><h3>Sentiment Analysis on Customer Feedback</h3><p>By leveraging advanced natural language processing algorithms, businesses can uncover customer preferences, sentiment analysis frequently asked questions, and emerging trends from these interactions. This data can be used to understand what customers want and what their major issues are and solve them accordingly.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1025pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6477b981e4766f017594b283_f786b0fa.png""/></div></figure><p>‍</p><p>Here we asked GPT4 to analyze the past reviews and it returned a quick summary of how customers are reacting to the product. It also returns information about the issues, this can be used to make the product better.</p><p>‍</p><h2>Personalized Sales Outreach with GPT4</h2><p>GPT-4 can enhance personalized sales outreach by using its advanced language processing capabilities. Businesses can create customized messages, emails, and pitches that resonate with each prospect. By understanding customer needs and preferences, GPT-4 empowers sales teams to deliver targeted and persuasive communications. Using GPT-4 for personalized outreach can boost customer engagement, build trust, and drive sales growth.</p><p>‍</p><p>Moreover, GPT-4 can help sales teams save time and increase efficiency. Automated processes can be used to generate personalized messages quickly and accurately. This can help sales teams reach more prospects in less time, resulting in higher conversion rates. GPT-4 can be integrated into existing sales pipelines to help sales teams maximize their outreach efforts and drive better results. </p><p>‍</p><h3>Personalizing Emails using GPT4</h3><p>As mentioned before, people often use AI to personalize the first lines of emails for better open rates. But what if you could use AI to personalize the entire email? GPT-4 can help you do that. GPT-4 can generate emails that are tailored to each individual customer, based on their preferences and needs. This can help sales teams build trust and increase engagement with their prospects.</p><p>‍</p><p>Here we are going to generate cold emails for a customer based on their reviews left on the website for a specific product. We pass the review left by the customer to GPT and ask it to write an email specific to the issue faced by them. This can help bring lost customers back to the platform.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1017pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6477b982f60e5d53ecfac3f7_66dae82d.png""/></div></figure><p>‍</p><p>Prompts can be further modified to generate more specific emails and responses.</p><p>‍</p><p>We can use the same technique to generate a text message using GPT4.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1041pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6477b9838712346994811e72_5a8bb121.png""/></div></figure><p>Similarly, GPT4 can also be used to generate newsletters and other marketing material which can be targeted specifically to individual customers or customer segments.</p><p>‍</p><h2>Enhance Sales Performance with NLP and Cutting-Edge Language Models</h2><p>Discover the incredible potential of NLP-powered large language models like GPT-3, GPT-4, Claude, and LlaMA in revolutionizing your sales strategies. With their advanced language processing capabilities, these models can help you optimize customer interactions, drive conversions, and boost sales growth. <a href=""https://www.mercity.ai/contacts"">Reach out to us</a> today and unlock the full power of NLP in your sales journey.</p><p>‍</p></div>"
Guide to building an Enterprise Grade Customer Support Chatbot Using LLMs,guide-to-building-an-enterprise-grade-customer-support-chatbot-using-llms,640f56f76d313b2faa631c11,67fe5a33e4374f290f89c03e,False,False,Tue Apr 15 2025 13:08:03 GMT+0000 (Coordinated Universal Time),Tue Apr 15 2025 13:08:03 GMT+0000 (Coordinated Universal Time),Tue Apr 15 2025 13:08:03 GMT+0000 (Coordinated Universal Time),"<p id="""">‍</p><p id="""">Customers today want quick and personalized support, leading businesses to use AI chatbots for better service. Large Language Models (LLMs) make this possible by allowing chatbots to respond instantly, understand context, and improve over time. Companies like <a href=""https://support.zendesk.com/hc/en-us/articles/6059285322522-About-generative-AI-features-in-Zendesk"" id="""">Zendesk</a> and <a href=""https://www.ndtv.com/india-news/dukaan-ceo-replaces-90-of-customer-support-staff-with-ai-chatbot-internet-angry-4197641"" id="""">Dukaan</a> have enhanced customer support by using AI to manage common questions and provide reliable assistance. Unlike basic bots, LLM-powered chatbots can understand natural language and adjust to different queries, making interactions more helpful. However, businesses need to balance automation with human oversight, fine-tune responses for accuracy, and ensure data security to build a successful AI-driven customer support system.</p><p id="""">This guide will explore how to build an effective LLM-based chatbot, covering key concepts, best practices, and implementation strategies for a smarter customer support system.</p><h2 id="""">Why Traditional Customer Support is No Longer Enough</h2><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/67fe5726312dbe40824e60e2_AD_4nXefJjl89qY-H4ILU9poWm1pKRuXEwJehhzrRNPwLyx1WfsEUnz6LbkhKJ0eRatrtC8VUSxMHqvFHkBmoTrQnAMk2P2xfKjm2IXd7op4e0kOVbZ-ngabvxjEMcYMJP_CiEY4IzkS.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">Traditional customer support relies on human agents to answer queries, which can be slow and inefficient, especially when there is a high volume of requests. Long wait times frustrate customers, and businesses struggle to scale their support teams without significant costs. Inconsistencies in responses and human errors also impact customer satisfaction. Additionally, customer expectations have increased, with people wanting instant and personalized assistance, something traditional support methods cannot always provide.</p><p id="""">‍</p><h2 id="""">Why Use Large Language Models?</h2><p id="""">‍</p><p id="""">Large Language Models (LLMs) help solve these problems by automating responses, understanding customer intent, and providing quick and accurate answers. They improve efficiency by handling repetitive queries, allowing human agents to focus on complex issues that require judgment and personalized attention.&nbsp;</p><p id="""">Below are four key factors that make LLMs an essential tool for modern customer support, particularly in handling FAQs and complex issues.</p><p id="""">‍</p><h3 id="""">Fast Response Time</h3><p id="""">Long wait times, especially during peak hours, frustrate customers. Traditional support relies on human agents, creating bottlenecks in handling customer queries. LLM-powered chatbots handle multiple queries instantly, understanding natural language, extracting intent, and generating accurate responses within seconds. This reduces wait times, improves efficiency, and enhances customer satisfaction.</p><p id="""">‍</p><h3 id="""">Personalized Responses</h3><p id="""">Generic responses fail to meet customer expectations, making personalized inquiries essential. LLMs personalize interactions by analyzing user history, preferences, and past conversations, generating context-aware responses with a human-like tone. <a href=""https://www.zendesk.com/in/newsroom/articles/relate2024-anthropic-aws/"" id="""">Zendesk</a>, for example, integrates Anthropic’s Claude 3 models to deliver empathetic, real-time responses, reducing wait times and improving customer satisfaction. AI-driven personalization makes customers feel valued, strengthening loyalty.</p><p id="""">&nbsp;</p><h3 id="""">Consistent and Accurate Support</h3><p id="""">Human agents often provide inconsistent resolutions to inquiries, leading to confusion. LLMs ensure standardized, data-driven responses based on company knowledge and past interactions. They refine answers over time, improving accuracy and trust. <a href=""https://www.zendesk.com/in/newsroom/articles/relate2024-anthropic-aws/"" id="""">Zendesk’s</a> CX data, combined with Anthropic’s AI models and AWS, enables customized, conversational support, minimizing misinformation and enhancing reliability.</p><p id="""">‍</p><h3 id="""">Cost Efficiency and Scalability</h3><p id="""">Hiring and training large support teams is expensive. LLMs reduce costs by automating repetitive queries and handling high volumes of customer inquiries efficiently. They provide 24/7 service, minimizing human dependency. Companies like <a href=""https://arxiv.org/pdf/2405.00801"" id="""">Comcast</a> use AI-powered features like “Ask Me Anything” (AMA) to assist agents in real-time, cutting conversation time by 10% and saving millions annually.</p><p id="""">‍</p><h2 id="""">Must-Have Features for a Customer Support Assistant</h2><p id="""">‍</p><h3 id="""">Personalization</h3><p id="""">A key feature for any customer support assistant is its ability to adapt responses to individual customer needs. It analyzes customer data such as purchase history, previous interactions, and preferences to tailor responses that feel relevant and personalized. This helps in resolving issues more effectively while also building a sense of trust and rapport with customers.</p><p id="""">An AI system that delivers personalized support can dynamically adjust its tone to match the customer's mood. It can also provide product recommendations or offer custom solutions based on the customer's unique context. This makes interactions more engaging and improves the overall customer experience.</p><p id="""">‍</p><h3 id="""">Sentiment Analysis &amp; Tagging&nbsp;</h3><p id="""">Understanding a customer's emotional state is important for providing effective support. Sentiment analysis helps by detecting emotions in customer messages, whether they show frustration, satisfaction, or confusion. It automatically tags interactions based on sentiment, helping the system identify critical issues that need immediate attention.</p><p id="""">This feature enables the assistant to respond in an empathetic way, reducing tension and improving the overall experience. If a customer is frustrated, the system can prioritize their issue or escalate it to a human agent. This ensures that urgent cases are handled quickly while maintaining high customer satisfaction.</p><p id="""">‍</p><h3 id="""">Escalation Management</h3><p id="""">While AI can handle many routine inquiries, some situations require human judgment. An effective support assistant must identify queries that exceed its capabilities or involve complex issues. It should be able to recognize when automation is not enough and ensure a smooth transition to a human agent. This prevents miscommunication and frustration for customers.</p><p id="""">The system must also maintain conversation history so customers do not have to repeat themselves. Proper escalation management improves service quality by ensuring difficult problems get the attention they need. Customers feel supported throughout their journey, even when AI cannot provide the final solution.</p><p id="""">‍</p><h3 id="""">Multi-Platform Support&nbsp;</h3><p id="""">Customers today connect with businesses across multiple platforms such as WhatsApp, Facebook, and other messaging apps. A reliable customer support assistant must integrate with these channels to provide a seamless experience. This ensures that customers receive the same level of service, regardless of how they choose to reach out.</p><p id="""">Multi-platform support makes it easier for customers to communicate using their preferred medium, reducing frustration and improving accessibility. It also helps businesses centralize interactions, making it more efficient to track, analyze, and respond to customer queries across different platforms.</p><p id="""">‍</p><h2 id="""">Building an Intelligent LLM-Powered Customer Support Chatbot</h2><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/67fe5726dec17beba3bc1c66_AD_4nXcuOI38RxSDrYb2ycKegRUFA4k6tyy4ewHHXRaQlp_tJrnGLWOH96feFDdBY_ZV8WLKCc5nHTS7OVlQdGDN5jxU3Y4Caa2a-scqlKAjA5RXmAXRiSlG8OQxCylsWu9Y6fO5QG4l2Q.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><p id="""">Implementing a robust AI-driven customer support chatbot requires a well-architected system that integrates large language models (LLMs), databases, APIs, and external customer support tools. The effectiveness of the chatbot depends on how seamlessly it processes queries, retrieves relevant information, and escalates unresolved issues to human agents. A strong technical foundation ensures efficiency, scalability, and reliability in customer interactions.</p><p id="""">‍</p><h3 id="""">Core Architecture and Key Components</h3><p id="""">‍</p><p id="""">A modern customer support chatbot functions as an intelligent interface between users and enterprise systems, providing accurate, contextual, and real-time assistance. It integrates LLMs, databases, and external tools to enhance efficiency.&nbsp;&nbsp;</p><p id="""">The chatbot features a conversational interface for seamless user interaction, an LLM processing engine for intent recognition, and a Retrieval-Augmented Generation (RAG) module for precise responses. It leverages an embedding generator and vector database for efficient search, prompt optimization for consistency, and an API layer for enterprise integration. Additionally, an escalation mechanism ensures smooth handoffs to human agents when needed, delivering personalized and efficient customer support.</p><h3 id="""">Chatbot</h3><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1110px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1110px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/67fe5725010cc3f2e6a6e067_AD_4nXdAhG_2fdRJW-qaIMO2_DEM_8KQCQZ577Hg9XKqx2hOcZ5m5U_fBcVWTlo9dJqq9Y9dD92OOv-UbfEMmf8bH2vB4wvH2i6Czpo_yxCn9CqG-K_6vheLnYVnLlpjpuYR-JJvj7aRjg.jpeg"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><p id="""">A chatbot serves as the primary interface between users and customer support systems. It understands user queries, processes requests, and delivers relevant responses. Unlike traditional scripted bots, an LLM-powered chatbot adapts to natural language, refines its answers over time, and offers a conversational experience. It also manages tasks such as ticket creation, order tracking, and escalation handling, streamlining customer support operations while reducing human workload.</p><p id="""">‍</p><h3 id="""">Embedding Generator</h3><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/67fe5726ef8f791c34cea36d_AD_4nXdwRDF6IDN9uDe-zEl22eWIKbPsaEfJ4lCW7NnUZi8KT_qpEgaSJ0ZX-sfxOFH19W1TxEq5diPXI-Fc_u4xTdIbkJxgxweCqM9Y5uF6Bc5Gjvbvbt8S_rOi59UVOTrGt85fJCRpZA.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">The embedding generator plays a crucial role in transforming user queries into numerical representations, allowing the system to perform efficient searches. By converting text into vector embeddings, the chatbot can retrieve relevant knowledge base articles, previous interactions, or other resources stored in vector databases. This process enhances the chatbot’s ability to understand intent, find the most relevant information, and generate accurate responses, making customer interactions smoother and more effective.</p><p id="""">‍</p><h3 id="""">Retrieving Knowledge for Context-Aware Responses</h3><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/67fe5726b6570bdcf2ac3911_AD_4nXcVLQygMEcPhyzHOnubPKsROhomDVaVRFB2az2-Z9-Kxvmk-ZBP53by_eYcZKYUjYai-Y5BPEPK2jQf6kKsdFw5I53G7ihzIRWriGDlTx7oWln66xuJZNZoxjvd31Neh2alaP44.jpeg"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">A chatbot must do more than provide scripted answers; it must pull in real-time, relevant data to generate useful responses. Retrieval-Augmented Generation (RAG) helps by pulling knowledge from verified sources like company databases, past tickets, and external APIs. This ensures the chatbot provides responses based on facts rather than generic or outdated information, increasing reliability and accuracy.</p><p id="""">To process queries efficiently, the chatbot converts customer questions into structured formats using models like BERT or OpenAI’s Ada. These models allow the chatbot to search its knowledge base using both keywords and semantic understanding. This hybrid search method helps the chatbot find the most relevant and precise answers, improving customer interactions.</p><p id="""">The chatbot also integrates retrieved information into responses before sending them to the customer. This process, called contextual augmentation, ensures the answers are based on real-time, updated data. Additionally, the chatbot stays connected to databases like PostgreSQL and CRM tools like <a href=""https://developer.zendesk.com/api-reference/"" id="""">Zendesk</a>, <a href=""https://desk.zoho.com/DeskAPIDocument"" id="""">ZohoDesk</a>, and <a href=""https://support.happyfox.com/kb/article/360-api-for-happyfox/"" id="""">HappyFox</a> allowing it to fetch live updates about orders, tickets, or account information, ensuring customers receive accurate and timely support.</p><h3 id="""">Interaction Between LLM, Databases, APIs, and External Tools</h3><p id="""">‍</p><p id="""">For a customer support chatbot to function efficiently, it must integrate seamlessly with multiple data sources, ensuring real-time, accurate, and contextual responses. Large Language Models (LLMs) process user queries by analyzing the intent, extracting key entities, and determining the most relevant information needed for a response. However, instead of relying solely on pre-trained knowledge, the chatbot enhances its accuracy by pulling data from structured sources such as relational databases, CRM systems, and ticketing platforms. This integration allows the chatbot to retrieve customer history, ongoing support tickets, and relevant company policies, ensuring responses are tailored to individual user needs. By connecting to external knowledge bases, the chatbot can dynamically update its responses, reducing misinformation and improving reliability.</p><p id="""">APIs act as the bridge between the chatbot and enterprise systems, facilitating the smooth exchange of information between different platforms. For instance, when a customer asks about their order status, the chatbot queries an order management database via an API call and retrieves real-time updates. Similarly, ticketing system integration allows the chatbot to create, modify, or track customer issues without requiring manual intervention. External tools such as analytics dashboards, workflow automation platforms, and logging systems help refine chatbot interactions, allowing businesses to optimize performance over time. This interconnected ecosystem ensures that the chatbot does not operate in isolation but instead functions as an intelligent assistant capable of retrieving, processing, and delivering precise information from various data sources.</p><p id="""">‍</p><h3 id="""">Optimizing Language and Tonality for Customer Engagement</h3><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1348px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1348px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/67fe5726026386cb249807d4_AD_4nXeyK1i_vCiMQoMkN6ac10i8PGkuPcWiMvbSn7y8ePgTDSS375jLv2FHoQt7ePfFc8PynbB5lIQbkDw6wHB5deO5SfV1f8mhKx06e1qev8FwaOuPGthSUl5GP62gb9FQ-cXRoFBH.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><p id="""">For a chatbot to provide a smooth and professional customer experience, its responses must match the company’s tone and style. This ensures that interactions feel natural, maintaining consistency across all customer communications. Businesses should fine-tune their models to align with their brand voice and adjust responses to different customer moods and situations.</p><p id="""">Optimizing language involves training the chatbot to understand and adapt to formality levels based on the customer’s tone. It should be capable of providing friendly, professional, or empathetic responses as needed. Additionally, sentiment adaptation ensures that the chatbot recognizes frustration, urgency, or satisfaction in customer messages and responds appropriately, enhancing engagement and trust.</p><h3 id="""">Integrating with Existing Ticketing Systems</h3><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1374px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1374px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/67fe5726a4f3920785c5ea70_AD_4nXc93Jy9wImPIGh8ccYHeUlOdKdAR_R5W-qLsOEMVRZGVBtzsQms-Y-85sCmAeL6XbBCsTxZD-QJ628YkcqGP2-4y8of2jE3Iq7lFFA_-sMucPi5fh7bMTSRwXuyadjvYNwVzki-_g.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">A ticketing system is a centralized platform used by businesses to track, manage, and resolve customer queries efficiently. It helps support teams organize customer requests, ensuring that every issue is documented and assigned to the right team for resolution. These systems improve communication between customers and businesses by providing a structured approach to handling inquiries, tracking progress, and ensuring timely follow-ups.</p><p id="""">Integrating a chatbot with a ticketing system automates issue tracking and resolution. When a customer raises a query, the chatbot can generate a ticket, categorize the issue based on urgency, and assign it to the appropriate team.&nbsp;</p><p id=""""><a href=""https://huggingface.co/blog/rlhf"" id="""">Reinforcement Learning with Human Feedback (RLHF)</a> uses curated datasets and customer feedback to refine chatbot responses. By continuously learning from past interactions, the chatbot improves its accuracy and relevance over time, leading to better customer engagement.</p><p id="""">‍</p><h3 id="""">Pulling in Data from External Sources</h3><p id="""">To enhance personalization and efficiency, chatbots can pull in data from external sources such as CRM systems and ticketing platforms. By accessing past interactions, purchase history, and ongoing support tickets, AI chatbots provide more informed and relevant responses.&nbsp;</p><p id="""">Integration with platforms like Salesforce and HubSpot enables real-time data retrieval, ensuring customers receive personalized solutions. This approach reduces redundancy, improves response accuracy, and enhances customer satisfaction by making support interactions more context-aware.</p><h3 id="""">Selecting the Right Open-Source LLM for Scalability</h3><p id="""">Choosing the right large language model (LLM) is critical for balancing performance, scalability, and cost when building a customer support chatbot. The selection process should consider:</p><ul id=""""><li id="""">Accuracy &amp; Context Retention – Models like LLaMA 3, Falcon, and Mistral excel at generating high-quality responses with improved contextual awareness.</li><li id="""">Latency &amp; Compute Efficiency – Smaller, optimized models like FastChat and GPT-3.5 Turbo provide faster real-time responses, ensuring a seamless customer experience.</li><li id="""">Fine-Tuning Capabilities – Open-source LLMs allow businesses to train models on industry-specific data, improving chatbot accuracy for niche use cases.</li><li id="""">Security &amp; Data Privacy – Deploying models in a self-hosted or private cloud environment (e.g., LLaMA 3 deployments) ensures compliance with data privacy regulations while preventing sensitive information from being exposed to third-party AI services.</li></ul><p id="""">‍</p><h2 id="""">How to Handle Query Escalation to Senior Support Staff</h2><p id="""">While AI chatbots can handle many customer queries, some issues require human expertise. Escalating complex or sensitive queries to senior support staff ensures customers receive accurate solutions without frustration.</p><h3 id="""">Steps for Effective Query Escalation:</h3><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1266px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1266px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/67fe5726f5474898f175f17a_AD_4nXf_loW8DVi5YCeB0flG5PLAvthjk9MgZcNWC9fff14oEhts_ibreYcASDGn43mVGWgffYDrG35PC996A784XwTQ9y9aFon-WVypDaZW07oH3USjB-q02ykHfSqgj9txNXYrWoJiXg.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><ol id=""""><li id=""""><strong id="""">Identify Complex Queries</strong>: The chatbot should detect when an issue is beyond its capability using sentiment analysis, named entity recognition (NER), and intent classification. By analyzing the tone, keywords, and complexity of customer messages, the chatbot can determine whether a human intervention is required. Machine learning models such as BERT or GPT can be trained to recognize negative sentiment, ambiguous queries, or repeated unsuccessful responses as triggers for escalation.</li></ol><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1128px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1128px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/67fe5726a4f3920785c5ea6c_AD_4nXfcOPQRV5j12OtHu6jeNXEi_MUV4ltp9MGerrdDnef7dwpJ5RqvA1zTwMH5ttIqh2lbZNDJkCWfQU4NJutCWGmVhQk6Tg5MDqC-dU1Upi3TwxS3dupfntnjVRUQi-CIPUxJLNE7.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><ol start=""2"" id=""""><li id=""""><strong id="""">Capture Query Context: </strong>Before escalation, the chatbot should collect all relevant details, such as customer history, past tickets, conversation context, and any troubleshooting steps already taken. This requires integrating with CRM systems and databases through API calls. Additionally, storing interactions in vector databases ensures that past conversations can be quickly retrieved for reference.</li></ol><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1008px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1008px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/67fe5726ec61dbdc82bd750d_AD_4nXdZ6-2NZ2-QzjYQHBTrptc-Y_OKkJpp5tpxSaf8mwilHqqLDzxYRykYH0QcBPAG7F5-zTLejnrIwRh0v1hHxu_CYdshgH0KyAghg3FSFogyLIIm6xbXUUpyGgpwIWljnmy76ie6.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><ol start=""3"" id=""""><li id=""""><strong id="""">Assign to the Right Team:</strong> Queries should be categorized based on predefined criteria, such as issue type (technical support, billing, complaints, etc.), priority level, and complexity. Using routing algorithms and classification models, the system can automatically assign the issue to the most suitable team or agent. Integration with workforce management systems can also ensure optimal workload distribution.</li></ol><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1188px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1188px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/67fe5725efdc6823759d80fb_AD_4nXeeb9Bu5Wxq6SUQkS4m9ilUvEzl5-Pq42Et15NK508r6Ck4p_tzkuHKsXrJNs-CfbXrnvrH5FwdjK6WWZxLHFEXiS4JAScVzs7TOhLfvjSyCeRc5I86JB1Q4n3Auh3yRuRr1rCfwA.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><ol start=""4"" id=""""><li id=""""><strong id="""">Seamless Handoff:</strong> The chatbot should generate a structured summary of the conversation, including timestamps, previous responses, collected customer information, and detected intent. This information should be passed to the human agent via a ticketing system or live chat interface. Using WebSockets or webhook-based real-time communication, the transition should be instantaneous, allowing agents to pick up conversations without requiring customers to repeat themselves.</li></ol><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1030px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1030px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/67fe57260209fb4eb0ca39ac_AD_4nXe2ec8CNf7ortC-y_XeBX9Qm-oFr-CXoObysMtoTx-wWMuZFcuIdPsW2SVfdXEY4GtBVvy0w6wXRWmWujb_CtmAm3DD8TyPbx9LtfI2tmkrQMoZUJP69-EPMDf2xnriv3PeiQUxJQ.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><ol start=""5"" id=""""><li id=""""><strong id="""">Post-Resolution Learning: </strong>The chatbot should continuously improve by analyzing escalated cases. Using feedback loops, supervised learning, and reinforcement learning techniques, the chatbot can learn from resolved queries to improve its response accuracy. Customer feedback, agent annotations, and sentiment analysis on post-interaction surveys can further refine intent recognition and future chatbot interactions.</li></ol><p id="""">‍</p><h2 id="""">How Intent Extraction Improves Chatbot Responses and Customer Satisfaction</h2><p id="""">‍</p><p id="""">Intent extraction plays a crucial role in enhancing chatbot accuracy by ensuring that user queries are understood correctly. By leveraging machine learning techniques such as deep neural networks and attention mechanisms, chatbots can precisely classify user intents, reducing misinterpretations. These models analyze the structure of a query, extract relevant keywords, and determine the user’s actual intent rather than relying on predefined rules. Fine-tuning pre-trained models on domain-specific datasets further enhances accuracy, enabling the chatbot to provide responses that are not only contextually relevant but also aligned with industry-specific knowledge. This results in improved customer interactions, as users receive more precise and helpful responses tailored to their queries.&nbsp;&nbsp;</p><p id="""">‍</p><p id="""">Efficiency is another key advantage of intent extraction, as real-time processing allows chatbots to recognize and respond to user queries instantly. By utilizing vectorized text embeddings and frameworks like <a href=""https://github.com/facebookresearch/faiss"" id="""">FAISS (Facebook AI Similarity Search)</a> Chatbots can quickly retrieve the closest matching intents from a database. This significantly reduces response times, ensuring that users receive answers without delay. Additionally, caching frequently asked queries in Redis or Memcached further enhances efficiency by reducing the need for repetitive processing. These optimizations collectively contribute to a seamless user experience, where customers receive quick, relevant, and well-structured responses.&nbsp;&nbsp;</p><p id="""">‍</p><p id="""">Beyond accuracy and efficiency, intent extraction also improves personalization in chatbot interactions. By integrating contextual understanding, chatbots can analyze past conversations, user preferences, and behavior patterns to tailor their responses accordingly. Advanced deep learning architectures such as Long Short-Term Memory (LSTM) networks and transformer-based models help chatbots retain contextual memory, making interactions feel more natural and engaging. Sentiment analysis further refines chatbot responses by adjusting the tone based on customer emotions ensuring that responses are empathetic, professional, or friendly depending on the context. With these capabilities, businesses can offer more personalized and satisfying customer support, ultimately improving engagement and customer loyalty.</p><p id="""">‍</p><h3 id="""">Tools for Accurate Intent Detection</h3><h4 id="""">spaCy &amp; NLTK (Natural Language Processing Libraries)</h4><ul id=""""><li id="""">Used for text preprocessing, including tokenization, stemming, and Named Entity Recognition (NER).</li><li id="""">Helps break down raw text into structured components, making it easier for machine learning models to analyze.</li></ul><h4 id="""">BERT, RoBERTa &amp; LLaMA (Pre-trained Transformer Models)</h4><ul id=""""><li id="""">These deep learning models analyze entire sentences rather than isolated words, improving intent recognition.</li><li id="""">Fine-tuning them with domain-specific data enhances accuracy and ensures chatbot responses align with business needs.</li></ul><h4 id="""">Support Vector Machines (SVM) &amp; Recurrent Neural Networks (RNNs)</h4><ul id=""""><li id="""">SVM is a traditional ML algorithm that classifies user intents by analyzing structured text features.</li><li id="""">RNNs, including LSTM (Long Short-Term Memory) networks, are useful for maintaining context in sequential conversations.</li></ul><h4 id="""">TensorFlow &amp; PyTorch (Deep Learning Frameworks)</h4><ul id=""""><li id="""">Used to train and fine-tune intent classification models, enabling real-time chatbot deployment.</li><li id="""">Helps in developing custom AI models that adapt to unique business needs.</li></ul><h4 id="""">FAISS (Facebook AI Similarity Search)</h4><ul id=""""><li id="""">A vector search framework that enables fast intent retrieval by finding the most relevant query match.</li><li id="""">Reduces latency, ensuring chatbots respond in real-time with high accuracy.</li></ul><h4 id="""">Redis &amp; Memcached (Caching Solutions)</h4><ul id=""""><li id="""">Stores frequently asked queries to reduce repetitive processing and improve chatbot response speed.</li><li id="""">Enhances chatbot efficiency by retrieving precomputed answers almost instantly.</li></ul><p id="""">‍</p><h2 id="""">Managing Customer Support Data</h2><p id="""">‍</p><p id="""">Effective customer support data management ensures seamless operations, security, and scalability. A multi-tenant database architecture allows multiple customers to use the same system while keeping their data isolated, making it ideal for SaaS-based solutions. Security features such as row-level security (RLS) and attribute-based access control (ABAC) protect sensitive information, while database sharding and caching (Redis, Memcached) optimize performance. Cloud platforms like AWS RDS, Google Cloud Spanner, and Azure Cosmos DB offer built-in support for cost-effective scalability. Additionally, robust backup strategies, including point-in-time recovery (PITR) and automated failover (Amazon S3, Google Cloud Storage), ensure high availability and data resilience.</p><p id="""">Real-time data handling enhances customer support efficiency by enabling instant updates and adaptive responses. Event-driven architectures like Apache Kafka and RabbitMQ facilitate live data synchronization across systems, reducing delays in ticket management and response handling. AI-powered models such as OpenAI’s GPT and Google’s BERT enhance chatbot interactions by providing context-aware responses, while integrations with platforms like Zendesk, Freshdesk, and ServiceNow streamline automated ticket updates. Additionally, vector search frameworks (FAISS, Pinecone) improve query accuracy, reducing response time and enhancing user experience. By implementing these technologies, businesses can create a scalable, real-time, and highly efficient customer support system.</p><p id="""">‍</p><h2 id="""">Deployment &amp; Production: Getting Your Support Assistant Live</h2><h3 id="""">Deploying Your Fine-Tuned LLM with MosaicML</h3><p id="""">Deploying a fine-tuned LLM with <a href=""https://huggingface.co/mosaicml"" id="""">MosaicML</a> involves configuring the model for inference, optimizing deployment pipelines, and testing performance before production. The process begins with setting up the training environment using PyTorch Lightning or TensorFlow, ensuring the model weights are properly stored in an accessible cloud storage solution like AWS S3 or Google Cloud Storage. Once fine-tuned, the model is exported to a TorchScript or ONNXformat for efficient inference. Deployment pipelines are established using MLflow or KServe, allowing dynamic scaling based on user demand. Testing inference speed and accuracy with A/B testing frameworks ensures performance before rolling out the assistant.</p><h3 id="""">Docker Containers: How Containerization Ensures Easy Scaling and Management</h3><p id="""">Using Docker containers enables seamless deployment of LLM-based support assistants, ensuring consistency across different environments. By packaging the model and all its dependencies within a lightweight container, businesses can deploy chatbots without worrying about compatibility issues. Kubernetes (K8s) further enhances scalability by orchestrating multiple instances of the chatbot, automatically scaling up during peak usage. Containerized environments allow for zero-downtime updates, where newer versions of the chatbot can be deployed alongside existing ones using rolling updates and blue-green deployment strategies.</p><h3 id="""">Authentication &amp; Authorization: Securing Your Support Assistant System</h3><p id="""">Security is crucial for AI-powered customer support assistants, as they often handle sensitive customer data. Implementing OAuth 2.0, JWT-based authentication, and role-based access control (RBAC) ensures secure API communications and prevents unauthorized access. Data encryption techniques such as AES-256 and TLS 1.2+ protect stored and transmitted data. Additionally, compliance with GDPR and CCPA regulations ensures that customer data privacy is maintained. Secure logging and monitoring using ELK Stack (Elasticsearch, Logstash, Kibana) help detect unauthorized activities and ensure system integrity.</p><h3 id="""">Cloud Services: Choosing the Right Cloud Infrastructure for Scalability and Performance</h3><p id="""">Choosing the right cloud provider ensures the chatbot operates efficiently under varying workloads. AWS Lambda (serverless architecture) can be used for event-driven chatbot execution, reducing costs by only using compute power when needed. Google Cloud Run and Azure Functions offer similar capabilities. For high-performance inference, NVIDIA GPUs with TensorRT optimization are preferred, while TPUs (Tensor Processing Units) can significantly speed up model computations. A multi-cloud strategy leveraging Kubernetes Federation ensures redundancy, minimizing downtime and enhancing availability.</p><h2 id="""">Why You Should Integrate AI into Your Customer Support Pipeline</h2><h3 id="""">How AI-Driven Support Assistants Are the Future of Customer Service</h3><p id="""">AI-powered customer support assistants provide 24/7 availability, ensuring immediate responses to customer queries without human intervention. They enhance scalability by handling thousands of queries simultaneously, reducing customer wait times. Advanced retrieval-augmented generation (RAG) models allow AI assistants to pull information from dynamic knowledge bases, ensuring responses remain up to date. AI-driven solutions reduce operational costs by automating repetitive tasks and allowing human agents to focus on complex issues.</p><h3 id="""">The Benefits of a Personalized Approach for Both Customers and Businesses</h3><p id="""">Personalized AI-driven support assistants improve user engagement by tailoring responses based on customer behavior, preferences, and past interactions. Context-aware AI assistants analyze historical chat records to provide relevant suggestions, improving user satisfaction. Businesses benefit from increased customer retention rates, enhanced service quality, and valuable insights from AI-driven analytics. Integration with CRM systems like Salesforce, HubSpot, and Zoho ensures seamless personalization, improving overall efficiency and customer experience. Transform your customer support with AI today! </p><p>‍</p><p>‍</p><p id="""">Contact <a href=""http://mercity.ai"" id="""">Mercity.ai</a> for intelligent, scalable, and personalized AI-driven support solutions!</p><p id="""">‍</p><p>‍</p><p>‍</p>",https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/67fe59e0c751ab61c59f32dd_customer%20support%20bot.png,Mathavan,LLMs,Comprehensive guide to building an enterprise grade customer support bot using LLMs and advanced escalation and ticketing systems.,False,"<div class=""rich-text w-richtext""><p>‍</p><p>Customers today want quick and personalized support, leading businesses to use AI chatbots for better service. Large Language Models (LLMs) make this possible by allowing chatbots to respond instantly, understand context, and improve over time. Companies like <a href=""https://support.zendesk.com/hc/en-us/articles/6059285322522-About-generative-AI-features-in-Zendesk"">Zendesk</a> and <a href=""https://www.ndtv.com/india-news/dukaan-ceo-replaces-90-of-customer-support-staff-with-ai-chatbot-internet-angry-4197641"">Dukaan</a> have enhanced customer support by using AI to manage common questions and provide reliable assistance. Unlike basic bots, LLM-powered chatbots can understand natural language and adjust to different queries, making interactions more helpful. However, businesses need to balance automation with human oversight, fine-tune responses for accuracy, and ensure data security to build a successful AI-driven customer support system.</p><p>This guide will explore how to build an effective LLM-based chatbot, covering key concepts, best practices, and implementation strategies for a smarter customer support system.</p><h2>Why Traditional Customer Support is No Longer Enough</h2><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/67fe5726312dbe40824e60e2_AD_4nXefJjl89qY-H4ILU9poWm1pKRuXEwJehhzrRNPwLyx1WfsEUnz6LbkhKJ0eRatrtC8VUSxMHqvFHkBmoTrQnAMk2P2xfKjm2IXd7op4e0kOVbZ-ngabvxjEMcYMJP_CiEY4IzkS.png""/></div></figure><p>Traditional customer support relies on human agents to answer queries, which can be slow and inefficient, especially when there is a high volume of requests. Long wait times frustrate customers, and businesses struggle to scale their support teams without significant costs. Inconsistencies in responses and human errors also impact customer satisfaction. Additionally, customer expectations have increased, with people wanting instant and personalized assistance, something traditional support methods cannot always provide.</p><p>‍</p><h2>Why Use Large Language Models?</h2><p>‍</p><p>Large Language Models (LLMs) help solve these problems by automating responses, understanding customer intent, and providing quick and accurate answers. They improve efficiency by handling repetitive queries, allowing human agents to focus on complex issues that require judgment and personalized attention. </p><p>Below are four key factors that make LLMs an essential tool for modern customer support, particularly in handling FAQs and complex issues.</p><p>‍</p><h3>Fast Response Time</h3><p>Long wait times, especially during peak hours, frustrate customers. Traditional support relies on human agents, creating bottlenecks in handling customer queries. LLM-powered chatbots handle multiple queries instantly, understanding natural language, extracting intent, and generating accurate responses within seconds. This reduces wait times, improves efficiency, and enhances customer satisfaction.</p><p>‍</p><h3>Personalized Responses</h3><p>Generic responses fail to meet customer expectations, making personalized inquiries essential. LLMs personalize interactions by analyzing user history, preferences, and past conversations, generating context-aware responses with a human-like tone. <a href=""https://www.zendesk.com/in/newsroom/articles/relate2024-anthropic-aws/"">Zendesk</a>, for example, integrates Anthropic’s Claude 3 models to deliver empathetic, real-time responses, reducing wait times and improving customer satisfaction. AI-driven personalization makes customers feel valued, strengthening loyalty.</p><p> </p><h3>Consistent and Accurate Support</h3><p>Human agents often provide inconsistent resolutions to inquiries, leading to confusion. LLMs ensure standardized, data-driven responses based on company knowledge and past interactions. They refine answers over time, improving accuracy and trust. <a href=""https://www.zendesk.com/in/newsroom/articles/relate2024-anthropic-aws/"">Zendesk’s</a> CX data, combined with Anthropic’s AI models and AWS, enables customized, conversational support, minimizing misinformation and enhancing reliability.</p><p>‍</p><h3>Cost Efficiency and Scalability</h3><p>Hiring and training large support teams is expensive. LLMs reduce costs by automating repetitive queries and handling high volumes of customer inquiries efficiently. They provide 24/7 service, minimizing human dependency. Companies like <a href=""https://arxiv.org/pdf/2405.00801"">Comcast</a> use AI-powered features like “Ask Me Anything” (AMA) to assist agents in real-time, cutting conversation time by 10% and saving millions annually.</p><p>‍</p><h2>Must-Have Features for a Customer Support Assistant</h2><p>‍</p><h3>Personalization</h3><p>A key feature for any customer support assistant is its ability to adapt responses to individual customer needs. It analyzes customer data such as purchase history, previous interactions, and preferences to tailor responses that feel relevant and personalized. This helps in resolving issues more effectively while also building a sense of trust and rapport with customers.</p><p>An AI system that delivers personalized support can dynamically adjust its tone to match the customer's mood. It can also provide product recommendations or offer custom solutions based on the customer's unique context. This makes interactions more engaging and improves the overall customer experience.</p><p>‍</p><h3>Sentiment Analysis &amp; Tagging </h3><p>Understanding a customer's emotional state is important for providing effective support. Sentiment analysis helps by detecting emotions in customer messages, whether they show frustration, satisfaction, or confusion. It automatically tags interactions based on sentiment, helping the system identify critical issues that need immediate attention.</p><p>This feature enables the assistant to respond in an empathetic way, reducing tension and improving the overall experience. If a customer is frustrated, the system can prioritize their issue or escalate it to a human agent. This ensures that urgent cases are handled quickly while maintaining high customer satisfaction.</p><p>‍</p><h3>Escalation Management</h3><p>While AI can handle many routine inquiries, some situations require human judgment. An effective support assistant must identify queries that exceed its capabilities or involve complex issues. It should be able to recognize when automation is not enough and ensure a smooth transition to a human agent. This prevents miscommunication and frustration for customers.</p><p>The system must also maintain conversation history so customers do not have to repeat themselves. Proper escalation management improves service quality by ensuring difficult problems get the attention they need. Customers feel supported throughout their journey, even when AI cannot provide the final solution.</p><p>‍</p><h3>Multi-Platform Support </h3><p>Customers today connect with businesses across multiple platforms such as WhatsApp, Facebook, and other messaging apps. A reliable customer support assistant must integrate with these channels to provide a seamless experience. This ensures that customers receive the same level of service, regardless of how they choose to reach out.</p><p>Multi-platform support makes it easier for customers to communicate using their preferred medium, reducing frustration and improving accessibility. It also helps businesses centralize interactions, making it more efficient to track, analyze, and respond to customer queries across different platforms.</p><p>‍</p><h2>Building an Intelligent LLM-Powered Customer Support Chatbot</h2><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/67fe5726dec17beba3bc1c66_AD_4nXcuOI38RxSDrYb2ycKegRUFA4k6tyy4ewHHXRaQlp_tJrnGLWOH96feFDdBY_ZV8WLKCc5nHTS7OVlQdGDN5jxU3Y4Caa2a-scqlKAjA5RXmAXRiSlG8OQxCylsWu9Y6fO5QG4l2Q.png""/></div></figure><p>‍</p><p>Implementing a robust AI-driven customer support chatbot requires a well-architected system that integrates large language models (LLMs), databases, APIs, and external customer support tools. The effectiveness of the chatbot depends on how seamlessly it processes queries, retrieves relevant information, and escalates unresolved issues to human agents. A strong technical foundation ensures efficiency, scalability, and reliability in customer interactions.</p><p>‍</p><h3>Core Architecture and Key Components</h3><p>‍</p><p>A modern customer support chatbot functions as an intelligent interface between users and enterprise systems, providing accurate, contextual, and real-time assistance. It integrates LLMs, databases, and external tools to enhance efficiency.  </p><p>The chatbot features a conversational interface for seamless user interaction, an LLM processing engine for intent recognition, and a Retrieval-Augmented Generation (RAG) module for precise responses. It leverages an embedding generator and vector database for efficient search, prompt optimization for consistency, and an API layer for enterprise integration. Additionally, an escalation mechanism ensures smooth handoffs to human agents when needed, delivering personalized and efficient customer support.</p><h3>Chatbot</h3><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1110pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/67fe5725010cc3f2e6a6e067_AD_4nXdAhG_2fdRJW-qaIMO2_DEM_8KQCQZ577Hg9XKqx2hOcZ5m5U_fBcVWTlo9dJqq9Y9dD92OOv-UbfEMmf8bH2vB4wvH2i6Czpo_yxCn9CqG-K_6vheLnYVnLlpjpuYR-JJvj7aRjg.jpeg""/></div></figure><p>‍</p><p>A chatbot serves as the primary interface between users and customer support systems. It understands user queries, processes requests, and delivers relevant responses. Unlike traditional scripted bots, an LLM-powered chatbot adapts to natural language, refines its answers over time, and offers a conversational experience. It also manages tasks such as ticket creation, order tracking, and escalation handling, streamlining customer support operations while reducing human workload.</p><p>‍</p><h3>Embedding Generator</h3><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/67fe5726ef8f791c34cea36d_AD_4nXdwRDF6IDN9uDe-zEl22eWIKbPsaEfJ4lCW7NnUZi8KT_qpEgaSJ0ZX-sfxOFH19W1TxEq5diPXI-Fc_u4xTdIbkJxgxweCqM9Y5uF6Bc5Gjvbvbt8S_rOi59UVOTrGt85fJCRpZA.png""/></div></figure><p>The embedding generator plays a crucial role in transforming user queries into numerical representations, allowing the system to perform efficient searches. By converting text into vector embeddings, the chatbot can retrieve relevant knowledge base articles, previous interactions, or other resources stored in vector databases. This process enhances the chatbot’s ability to understand intent, find the most relevant information, and generate accurate responses, making customer interactions smoother and more effective.</p><p>‍</p><h3>Retrieving Knowledge for Context-Aware Responses</h3><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/67fe5726b6570bdcf2ac3911_AD_4nXcVLQygMEcPhyzHOnubPKsROhomDVaVRFB2az2-Z9-Kxvmk-ZBP53by_eYcZKYUjYai-Y5BPEPK2jQf6kKsdFw5I53G7ihzIRWriGDlTx7oWln66xuJZNZoxjvd31Neh2alaP44.jpeg""/></div></figure><p>A chatbot must do more than provide scripted answers; it must pull in real-time, relevant data to generate useful responses. Retrieval-Augmented Generation (RAG) helps by pulling knowledge from verified sources like company databases, past tickets, and external APIs. This ensures the chatbot provides responses based on facts rather than generic or outdated information, increasing reliability and accuracy.</p><p>To process queries efficiently, the chatbot converts customer questions into structured formats using models like BERT or OpenAI’s Ada. These models allow the chatbot to search its knowledge base using both keywords and semantic understanding. This hybrid search method helps the chatbot find the most relevant and precise answers, improving customer interactions.</p><p>The chatbot also integrates retrieved information into responses before sending them to the customer. This process, called contextual augmentation, ensures the answers are based on real-time, updated data. Additionally, the chatbot stays connected to databases like PostgreSQL and CRM tools like <a href=""https://developer.zendesk.com/api-reference/"">Zendesk</a>, <a href=""https://desk.zoho.com/DeskAPIDocument"">ZohoDesk</a>, and <a href=""https://support.happyfox.com/kb/article/360-api-for-happyfox/"">HappyFox</a> allowing it to fetch live updates about orders, tickets, or account information, ensuring customers receive accurate and timely support.</p><h3>Interaction Between LLM, Databases, APIs, and External Tools</h3><p>‍</p><p>For a customer support chatbot to function efficiently, it must integrate seamlessly with multiple data sources, ensuring real-time, accurate, and contextual responses. Large Language Models (LLMs) process user queries by analyzing the intent, extracting key entities, and determining the most relevant information needed for a response. However, instead of relying solely on pre-trained knowledge, the chatbot enhances its accuracy by pulling data from structured sources such as relational databases, CRM systems, and ticketing platforms. This integration allows the chatbot to retrieve customer history, ongoing support tickets, and relevant company policies, ensuring responses are tailored to individual user needs. By connecting to external knowledge bases, the chatbot can dynamically update its responses, reducing misinformation and improving reliability.</p><p>APIs act as the bridge between the chatbot and enterprise systems, facilitating the smooth exchange of information between different platforms. For instance, when a customer asks about their order status, the chatbot queries an order management database via an API call and retrieves real-time updates. Similarly, ticketing system integration allows the chatbot to create, modify, or track customer issues without requiring manual intervention. External tools such as analytics dashboards, workflow automation platforms, and logging systems help refine chatbot interactions, allowing businesses to optimize performance over time. This interconnected ecosystem ensures that the chatbot does not operate in isolation but instead functions as an intelligent assistant capable of retrieving, processing, and delivering precise information from various data sources.</p><p>‍</p><h3>Optimizing Language and Tonality for Customer Engagement</h3><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1348pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/67fe5726026386cb249807d4_AD_4nXeyK1i_vCiMQoMkN6ac10i8PGkuPcWiMvbSn7y8ePgTDSS375jLv2FHoQt7ePfFc8PynbB5lIQbkDw6wHB5deO5SfV1f8mhKx06e1qev8FwaOuPGthSUl5GP62gb9FQ-cXRoFBH.png""/></div></figure><p>‍</p><p>For a chatbot to provide a smooth and professional customer experience, its responses must match the company’s tone and style. This ensures that interactions feel natural, maintaining consistency across all customer communications. Businesses should fine-tune their models to align with their brand voice and adjust responses to different customer moods and situations.</p><p>Optimizing language involves training the chatbot to understand and adapt to formality levels based on the customer’s tone. It should be capable of providing friendly, professional, or empathetic responses as needed. Additionally, sentiment adaptation ensures that the chatbot recognizes frustration, urgency, or satisfaction in customer messages and responds appropriately, enhancing engagement and trust.</p><h3>Integrating with Existing Ticketing Systems</h3><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1374pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/67fe5726a4f3920785c5ea70_AD_4nXc93Jy9wImPIGh8ccYHeUlOdKdAR_R5W-qLsOEMVRZGVBtzsQms-Y-85sCmAeL6XbBCsTxZD-QJ628YkcqGP2-4y8of2jE3Iq7lFFA_-sMucPi5fh7bMTSRwXuyadjvYNwVzki-_g.png""/></div></figure><p>A ticketing system is a centralized platform used by businesses to track, manage, and resolve customer queries efficiently. It helps support teams organize customer requests, ensuring that every issue is documented and assigned to the right team for resolution. These systems improve communication between customers and businesses by providing a structured approach to handling inquiries, tracking progress, and ensuring timely follow-ups.</p><p>Integrating a chatbot with a ticketing system automates issue tracking and resolution. When a customer raises a query, the chatbot can generate a ticket, categorize the issue based on urgency, and assign it to the appropriate team. </p><p><a href=""https://huggingface.co/blog/rlhf"">Reinforcement Learning with Human Feedback (RLHF)</a> uses curated datasets and customer feedback to refine chatbot responses. By continuously learning from past interactions, the chatbot improves its accuracy and relevance over time, leading to better customer engagement.</p><p>‍</p><h3>Pulling in Data from External Sources</h3><p>To enhance personalization and efficiency, chatbots can pull in data from external sources such as CRM systems and ticketing platforms. By accessing past interactions, purchase history, and ongoing support tickets, AI chatbots provide more informed and relevant responses. </p><p>Integration with platforms like Salesforce and HubSpot enables real-time data retrieval, ensuring customers receive personalized solutions. This approach reduces redundancy, improves response accuracy, and enhances customer satisfaction by making support interactions more context-aware.</p><h3>Selecting the Right Open-Source LLM for Scalability</h3><p>Choosing the right large language model (LLM) is critical for balancing performance, scalability, and cost when building a customer support chatbot. The selection process should consider:</p><ul role=""list""><li>Accuracy &amp; Context Retention – Models like LLaMA 3, Falcon, and Mistral excel at generating high-quality responses with improved contextual awareness.</li><li>Latency &amp; Compute Efficiency – Smaller, optimized models like FastChat and GPT-3.5 Turbo provide faster real-time responses, ensuring a seamless customer experience.</li><li>Fine-Tuning Capabilities – Open-source LLMs allow businesses to train models on industry-specific data, improving chatbot accuracy for niche use cases.</li><li>Security &amp; Data Privacy – Deploying models in a self-hosted or private cloud environment (e.g., LLaMA 3 deployments) ensures compliance with data privacy regulations while preventing sensitive information from being exposed to third-party AI services.</li></ul><p>‍</p><h2>How to Handle Query Escalation to Senior Support Staff</h2><p>While AI chatbots can handle many customer queries, some issues require human expertise. Escalating complex or sensitive queries to senior support staff ensures customers receive accurate solutions without frustration.</p><h3>Steps for Effective Query Escalation:</h3><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1266pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/67fe5726f5474898f175f17a_AD_4nXf_loW8DVi5YCeB0flG5PLAvthjk9MgZcNWC9fff14oEhts_ibreYcASDGn43mVGWgffYDrG35PC996A784XwTQ9y9aFon-WVypDaZW07oH3USjB-q02ykHfSqgj9txNXYrWoJiXg.png""/></div></figure><ol role=""list""><li><strong>Identify Complex Queries</strong>: The chatbot should detect when an issue is beyond its capability using sentiment analysis, named entity recognition (NER), and intent classification. By analyzing the tone, keywords, and complexity of customer messages, the chatbot can determine whether a human intervention is required. Machine learning models such as BERT or GPT can be trained to recognize negative sentiment, ambiguous queries, or repeated unsuccessful responses as triggers for escalation.</li></ol><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1128pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/67fe5726a4f3920785c5ea6c_AD_4nXfcOPQRV5j12OtHu6jeNXEi_MUV4ltp9MGerrdDnef7dwpJ5RqvA1zTwMH5ttIqh2lbZNDJkCWfQU4NJutCWGmVhQk6Tg5MDqC-dU1Upi3TwxS3dupfntnjVRUQi-CIPUxJLNE7.png""/></div></figure><p>‍</p><ol role=""list"" start=""2""><li><strong>Capture Query Context: </strong>Before escalation, the chatbot should collect all relevant details, such as customer history, past tickets, conversation context, and any troubleshooting steps already taken. This requires integrating with CRM systems and databases through API calls. Additionally, storing interactions in vector databases ensures that past conversations can be quickly retrieved for reference.</li></ol><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1008pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/67fe5726ec61dbdc82bd750d_AD_4nXdZ6-2NZ2-QzjYQHBTrptc-Y_OKkJpp5tpxSaf8mwilHqqLDzxYRykYH0QcBPAG7F5-zTLejnrIwRh0v1hHxu_CYdshgH0KyAghg3FSFogyLIIm6xbXUUpyGgpwIWljnmy76ie6.png""/></div></figure><p>‍</p><ol role=""list"" start=""3""><li><strong>Assign to the Right Team:</strong> Queries should be categorized based on predefined criteria, such as issue type (technical support, billing, complaints, etc.), priority level, and complexity. Using routing algorithms and classification models, the system can automatically assign the issue to the most suitable team or agent. Integration with workforce management systems can also ensure optimal workload distribution.</li></ol><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1188pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/67fe5725efdc6823759d80fb_AD_4nXeeb9Bu5Wxq6SUQkS4m9ilUvEzl5-Pq42Et15NK508r6Ck4p_tzkuHKsXrJNs-CfbXrnvrH5FwdjK6WWZxLHFEXiS4JAScVzs7TOhLfvjSyCeRc5I86JB1Q4n3Auh3yRuRr1rCfwA.png""/></div></figure><p>‍</p><ol role=""list"" start=""4""><li><strong>Seamless Handoff:</strong> The chatbot should generate a structured summary of the conversation, including timestamps, previous responses, collected customer information, and detected intent. This information should be passed to the human agent via a ticketing system or live chat interface. Using WebSockets or webhook-based real-time communication, the transition should be instantaneous, allowing agents to pick up conversations without requiring customers to repeat themselves.</li></ol><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1030pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/67fe57260209fb4eb0ca39ac_AD_4nXe2ec8CNf7ortC-y_XeBX9Qm-oFr-CXoObysMtoTx-wWMuZFcuIdPsW2SVfdXEY4GtBVvy0w6wXRWmWujb_CtmAm3DD8TyPbx9LtfI2tmkrQMoZUJP69-EPMDf2xnriv3PeiQUxJQ.png""/></div></figure><p>‍</p><ol role=""list"" start=""5""><li><strong>Post-Resolution Learning: </strong>The chatbot should continuously improve by analyzing escalated cases. Using feedback loops, supervised learning, and reinforcement learning techniques, the chatbot can learn from resolved queries to improve its response accuracy. Customer feedback, agent annotations, and sentiment analysis on post-interaction surveys can further refine intent recognition and future chatbot interactions.</li></ol><p>‍</p><h2>How Intent Extraction Improves Chatbot Responses and Customer Satisfaction</h2><p>‍</p><p>Intent extraction plays a crucial role in enhancing chatbot accuracy by ensuring that user queries are understood correctly. By leveraging machine learning techniques such as deep neural networks and attention mechanisms, chatbots can precisely classify user intents, reducing misinterpretations. These models analyze the structure of a query, extract relevant keywords, and determine the user’s actual intent rather than relying on predefined rules. Fine-tuning pre-trained models on domain-specific datasets further enhances accuracy, enabling the chatbot to provide responses that are not only contextually relevant but also aligned with industry-specific knowledge. This results in improved customer interactions, as users receive more precise and helpful responses tailored to their queries.  </p><p>‍</p><p>Efficiency is another key advantage of intent extraction, as real-time processing allows chatbots to recognize and respond to user queries instantly. By utilizing vectorized text embeddings and frameworks like <a href=""https://github.com/facebookresearch/faiss"">FAISS (Facebook AI Similarity Search)</a> Chatbots can quickly retrieve the closest matching intents from a database. This significantly reduces response times, ensuring that users receive answers without delay. Additionally, caching frequently asked queries in Redis or Memcached further enhances efficiency by reducing the need for repetitive processing. These optimizations collectively contribute to a seamless user experience, where customers receive quick, relevant, and well-structured responses.  </p><p>‍</p><p>Beyond accuracy and efficiency, intent extraction also improves personalization in chatbot interactions. By integrating contextual understanding, chatbots can analyze past conversations, user preferences, and behavior patterns to tailor their responses accordingly. Advanced deep learning architectures such as Long Short-Term Memory (LSTM) networks and transformer-based models help chatbots retain contextual memory, making interactions feel more natural and engaging. Sentiment analysis further refines chatbot responses by adjusting the tone based on customer emotions ensuring that responses are empathetic, professional, or friendly depending on the context. With these capabilities, businesses can offer more personalized and satisfying customer support, ultimately improving engagement and customer loyalty.</p><p>‍</p><h3>Tools for Accurate Intent Detection</h3><h4>spaCy &amp; NLTK (Natural Language Processing Libraries)</h4><ul role=""list""><li>Used for text preprocessing, including tokenization, stemming, and Named Entity Recognition (NER).</li><li>Helps break down raw text into structured components, making it easier for machine learning models to analyze.</li></ul><h4>BERT, RoBERTa &amp; LLaMA (Pre-trained Transformer Models)</h4><ul role=""list""><li>These deep learning models analyze entire sentences rather than isolated words, improving intent recognition.</li><li>Fine-tuning them with domain-specific data enhances accuracy and ensures chatbot responses align with business needs.</li></ul><h4>Support Vector Machines (SVM) &amp; Recurrent Neural Networks (RNNs)</h4><ul role=""list""><li>SVM is a traditional ML algorithm that classifies user intents by analyzing structured text features.</li><li>RNNs, including LSTM (Long Short-Term Memory) networks, are useful for maintaining context in sequential conversations.</li></ul><h4>TensorFlow &amp; PyTorch (Deep Learning Frameworks)</h4><ul role=""list""><li>Used to train and fine-tune intent classification models, enabling real-time chatbot deployment.</li><li>Helps in developing custom AI models that adapt to unique business needs.</li></ul><h4>FAISS (Facebook AI Similarity Search)</h4><ul role=""list""><li>A vector search framework that enables fast intent retrieval by finding the most relevant query match.</li><li>Reduces latency, ensuring chatbots respond in real-time with high accuracy.</li></ul><h4>Redis &amp; Memcached (Caching Solutions)</h4><ul role=""list""><li>Stores frequently asked queries to reduce repetitive processing and improve chatbot response speed.</li><li>Enhances chatbot efficiency by retrieving precomputed answers almost instantly.</li></ul><p>‍</p><h2>Managing Customer Support Data</h2><p>‍</p><p>Effective customer support data management ensures seamless operations, security, and scalability. A multi-tenant database architecture allows multiple customers to use the same system while keeping their data isolated, making it ideal for SaaS-based solutions. Security features such as row-level security (RLS) and attribute-based access control (ABAC) protect sensitive information, while database sharding and caching (Redis, Memcached) optimize performance. Cloud platforms like AWS RDS, Google Cloud Spanner, and Azure Cosmos DB offer built-in support for cost-effective scalability. Additionally, robust backup strategies, including point-in-time recovery (PITR) and automated failover (Amazon S3, Google Cloud Storage), ensure high availability and data resilience.</p><p>Real-time data handling enhances customer support efficiency by enabling instant updates and adaptive responses. Event-driven architectures like Apache Kafka and RabbitMQ facilitate live data synchronization across systems, reducing delays in ticket management and response handling. AI-powered models such as OpenAI’s GPT and Google’s BERT enhance chatbot interactions by providing context-aware responses, while integrations with platforms like Zendesk, Freshdesk, and ServiceNow streamline automated ticket updates. Additionally, vector search frameworks (FAISS, Pinecone) improve query accuracy, reducing response time and enhancing user experience. By implementing these technologies, businesses can create a scalable, real-time, and highly efficient customer support system.</p><p>‍</p><h2>Deployment &amp; Production: Getting Your Support Assistant Live</h2><h3>Deploying Your Fine-Tuned LLM with MosaicML</h3><p>Deploying a fine-tuned LLM with <a href=""https://huggingface.co/mosaicml"">MosaicML</a> involves configuring the model for inference, optimizing deployment pipelines, and testing performance before production. The process begins with setting up the training environment using PyTorch Lightning or TensorFlow, ensuring the model weights are properly stored in an accessible cloud storage solution like AWS S3 or Google Cloud Storage. Once fine-tuned, the model is exported to a TorchScript or ONNXformat for efficient inference. Deployment pipelines are established using MLflow or KServe, allowing dynamic scaling based on user demand. Testing inference speed and accuracy with A/B testing frameworks ensures performance before rolling out the assistant.</p><h3>Docker Containers: How Containerization Ensures Easy Scaling and Management</h3><p>Using Docker containers enables seamless deployment of LLM-based support assistants, ensuring consistency across different environments. By packaging the model and all its dependencies within a lightweight container, businesses can deploy chatbots without worrying about compatibility issues. Kubernetes (K8s) further enhances scalability by orchestrating multiple instances of the chatbot, automatically scaling up during peak usage. Containerized environments allow for zero-downtime updates, where newer versions of the chatbot can be deployed alongside existing ones using rolling updates and blue-green deployment strategies.</p><h3>Authentication &amp; Authorization: Securing Your Support Assistant System</h3><p>Security is crucial for AI-powered customer support assistants, as they often handle sensitive customer data. Implementing OAuth 2.0, JWT-based authentication, and role-based access control (RBAC) ensures secure API communications and prevents unauthorized access. Data encryption techniques such as AES-256 and TLS 1.2+ protect stored and transmitted data. Additionally, compliance with GDPR and CCPA regulations ensures that customer data privacy is maintained. Secure logging and monitoring using ELK Stack (Elasticsearch, Logstash, Kibana) help detect unauthorized activities and ensure system integrity.</p><h3>Cloud Services: Choosing the Right Cloud Infrastructure for Scalability and Performance</h3><p>Choosing the right cloud provider ensures the chatbot operates efficiently under varying workloads. AWS Lambda (serverless architecture) can be used for event-driven chatbot execution, reducing costs by only using compute power when needed. Google Cloud Run and Azure Functions offer similar capabilities. For high-performance inference, NVIDIA GPUs with TensorRT optimization are preferred, while TPUs (Tensor Processing Units) can significantly speed up model computations. A multi-cloud strategy leveraging Kubernetes Federation ensures redundancy, minimizing downtime and enhancing availability.</p><h2>Why You Should Integrate AI into Your Customer Support Pipeline</h2><h3>How AI-Driven Support Assistants Are the Future of Customer Service</h3><p>AI-powered customer support assistants provide 24/7 availability, ensuring immediate responses to customer queries without human intervention. They enhance scalability by handling thousands of queries simultaneously, reducing customer wait times. Advanced retrieval-augmented generation (RAG) models allow AI assistants to pull information from dynamic knowledge bases, ensuring responses remain up to date. AI-driven solutions reduce operational costs by automating repetitive tasks and allowing human agents to focus on complex issues.</p><h3>The Benefits of a Personalized Approach for Both Customers and Businesses</h3><p>Personalized AI-driven support assistants improve user engagement by tailoring responses based on customer behavior, preferences, and past interactions. Context-aware AI assistants analyze historical chat records to provide relevant suggestions, improving user satisfaction. Businesses benefit from increased customer retention rates, enhanced service quality, and valuable insights from AI-driven analytics. Integration with CRM systems like Salesforce, HubSpot, and Zoho ensures seamless personalization, improving overall efficiency and customer experience. Transform your customer support with AI today! </p><p>‍</p><p>‍</p><p>Contact <a href=""http://mercity.ai"">Mercity.ai</a> for intelligent, scalable, and personalized AI-driven support solutions!</p><p>‍</p><p>‍</p><p>‍</p></div>"
Comprehensive Guide to Chain-of-Thought Prompting ,guide-to-chain-of-thought-prompting,640f56f76d313b2faa631c11,64f232ac890b774214e3e738,False,False,Fri Sep 01 2023 18:51:24 GMT+0000 (Coordinated Universal Time),Fri Sep 12 2025 23:11:07 GMT+0000 (Coordinated Universal Time),Sat Sep 13 2025 16:53:45 GMT+0000 (Coordinated Universal Time),"<h1 id="""">How Chain-of-Thought (CoT) Prompting Makes LLMs Better At Reasoning?</h1><p id="""">Scaling up large language models (LLMs) has shown good results in sentiment analysis and machine translation, even without any examples. However, they fail in complex multi-step problems such as arithmetic and commonsense reasoning. To address this, LLMs can either be fine-tuned for a particular task or taught with few-shot prompting. However, both of these methods have limitations. Fine-tuning is costly for creating high-quality reasoning, while only few-shot prompting is not effective enough for the task.</p><p id="""">Chain-of-Thought (CoT) prompting can address both of these problems. In this article, we will explore CoT prompting and how implementing it can upskill your business.</p><h2 id="""">What is Prompt Engineering?</h2><p id="""">Prompt engineering is the practice of writing well-structured and carefully crafted prompts that can be better interpreted by a generative AI model. A prompt tells the LLM what task to perform and what kind of output to generate. It can contain instruction, context, input data, and output indicators. Using prompt engineering, we can use LLMs to carry out various tasks, from simple question answering to complex creative text generation. It is based on an emergent property, in-context learning, allowing LLMs to learn from prompts. Prompt engineering improves the performance of LLMs on the task at hand. It uses zero-shot, few-shot, active and CoT prompting as discussed ahead.</p><p id="""">We also have a blog on <a href=""https://www.mercity.ai/blog-post/advanced-prompt-engineering-techniques#want-to-write-high-quality-prompts-for-llms"" id="""">advanced prompt engineering</a>.</p><h3 id="""">Zero-shot Prompting</h3><p id="""">In zero-shot prompting, we provide the LLM a prompt that describes the task, but the prompt does not provide any examples for the task. The LLM is then asked to generate a response to the prompt. It improves the flexibility and generalization of LLMs. It can be used to train LLMs on several tasks, without having to collect training data for each task. For example, ChatGPT can write a poem on prompt engineering without giving any examples of how to write a poem. However, zero-shot prompting is limited for complex tasks.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1021px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1021px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a6be268c58e672f2c732_ea4a7243.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><h3 id="""">Few-shot Prompting</h3><p id="""">Few-shot prompting can provide demonstrations to steer the model to better performance. It is a technique for providing LLMs with a few examples of the desired output, in addition to the prompt. The examples help the model to better understand the task and to generate more accurate and informative responses. We should provide vast and different examples to the model, instead of multiple similar examples. It ensures the model learns as much as possible about the task. Standard few-shot prompting is a good technique for many tasks, but not reliable for complex reasoning tasks. Therefore, more advanced prompting techniques, such as chain-of-thought, active prompting, and fine-tuning, are needed.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:936px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""936px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a6be268c58e672f2c735_b7362b56.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><h3 id="""">Active Prompting</h3><p id=""""><a href=""https://github.com/shizhediao/active-prompt"" id="""">Active prompting</a> improves the performance of LLMs on complex tasks by iteratively providing them with feedback on their responses. This feedback can help the LLMs to learn from their mistakes and to generate more accurate and informative responses. It provides the LLM with a prompt and a few examples of the desired output. The LLM then generates a response. The response is then evaluated by a human evaluator, who provides feedback to the LLM on the accuracy and informativeness of the response. The LLM then uses this feedback to improve its response generation capabilities. This process repeats until the LLM can generate responses that are accurate and informative enough to satisfy the human evaluator.</p><p id=""""><a href=""https://arxiv.org/pdf/2302.12246.pdf"" id="""">Active prompting</a></p><p id="""">is important for CoT prompting as it identifies important questions for annotation, minimizes human annotation efforts, and improves the accuracy and informativeness of CoT prompts. The following figure shows active prompting with CoT to improve performance. It is a four-stage process that involves estimating the uncertainty of a question by querying an LLM multiple times, selecting the most uncertain questions for annotation by ranking them, annotating them with detailed feedback from human evaluators, inferring the answers to new questions by using the LLM to generate answers and the feedback from the annotation step to improve the quality.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:939px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""939px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a6be268c58e672f2c738_38f81e06.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><h2 id="""">What is Chain-of-Thought Prompting?</h2><p id=""""><a href=""https://arxiv.org/pdf/2201.11903.pdf"" id="""">Chain-of-Thought prompting</a> is a prompt engineering technique through which we force LLMs to output a sequence of intermediate steps that lead to the desired answer. It improves the reasoning abilities of LLMs. It is beneficial because it allows the model to focus on solving one step at a time, rather than having to consider the entire problem all at once. It can be especially helpful for complex problems that would be difficult or impossible to solve in a single step. It provides an interpretable window into the behavior of the model. We can see how the model arrived at its answer by following the sequence of steps that it took.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:862px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""862px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a6be268c58e672f2c741_3402878e.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">CoT prompting can be used with LLMs with a large set of parameters (~100 B parameters) for several reasoning tasks, including math word problems, commonsense reasoning, and symbolic manipulation. For example, using <a href=""https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html"" id="""">CoT prompting</a> in the PaLM model instead of standard few-shots, improved the performance in the GSM8K benchmark from 17.9% to 58.1%. CoT prompting can be readily elicited in sufficiently large language models without any special training or fine-tuning of the model. It makes CoT prompting a scalable and accessible technique.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:411px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""411px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a6be268c58e672f2c744_7e42e126.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><h3 id="""">Few-Shot CoT</h3><p id="""">Few-shot prompting prompts the LLM with a question and the answer. Then, the LLM is provided with a few examples of how to solve similar problems. The examples are presented in a way that encourages the LLM to reason about the problem and come up with a chain of thought that leads to the answer.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a6be268c58e672f2c73b_d69047d3.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id=""""><a href=""https://arxiv.org/abs/2305.14045"" id="""">Few-shot CoT</a> is a more effective technique for improving the reasoning abilities of LLMs than the few-shot baseline because it provides LLMs with examples of similar problems. It can be more complex to implement than a few-shot baseline because it requires the creation of example prompts. However, the benefits of few-shot CoT outweigh the additional complexity.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:994px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""994px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a6be268c58e672f2c73e_04c2ed6b.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><h3 id="""">Zero-Shot CoT</h3><p id=""""><a href=""https://arxiv.org/abs/2205.11916"" id="""">Zero-shot CoT</a> involves adding ""Let's think step by step"" to the original prompt. It extracts reasoning and answers using two prompts.</p><ul id=""""><li id="""">Reasoning extraction: In this step, the language model thinks about the question and comes up with a chain of reasoning that leads to the answer. For this, we give the language model a prompt that includes the question and a trigger sentence ""<strong id="""">Let's think step by step</strong>."" The language model will then generate a sentence that explains how it arrived at the answer.</li></ul><ul id=""""><li id="""">Answer extraction: In the second step, we extract the final answer from the language model's response. We concatenate the prompt, the generated sentence, and a trigger sentence, ""<strong id="""">The answer is</strong>"". It tells the language model to give us the answer. The language model will then generate a sentence that contains the answer to the question.</li></ul><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:852px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""852px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a732f6c79420bbf1232c_f20c0952.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">In contrast to this, the zero-shot baseline uses prompts like ""The answer is"" for answer extraction. Few-shot prompting, whether standard or CoT, avoids the need for such answer-extraction prompts by designing example answers to end in the correct formats.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:802px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""802px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a732f6c79420bbf1232f_15f147a5.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">On comparing zero-shot CoT to two other methods for evaluating the zero-shot reasoning abilities of LLMs, researchers found that zero-shot-CoT outperforms the other methods on various reasoning tasks. If you are looking for a smaller model trained on CoT prompting, consider the Flan-T5 model. It can be used for zero-shot NLP tasks including text summarization, natural language inference, translation, and common sense reasoning.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:712px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""712px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a732f6c79420bbf12332_4ec38025.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><h2 id="""">When Does CoT Emerge?</h2><p id="""">CoT reasoning is an <a href=""https://web.stanford.edu/class/cs224v/lectures/jason-wei-emergence-talk-stanford.pdf"" id="""">emergent ability</a> of LLMs that may arise due to <a href=""https://arxiv.org/abs/2210.11416"" id="""">scaling models</a> over 100 billion parameters. It does not positively impact performance for smaller LLMs and only yields performance gains when used with models of this size. There are two reasons for it. Firstly, smaller LLMs are not able to generate long chains of thought that are both fluent and logical. This leads to lower performance than standard prompting. Secondly, CoT reasoning is more effective for more complicated problems. It requires the LLM to be able to identify the key steps involved in solving a problem and then generate a chain of thoughts that leads to the solution. Smaller LLMs may not be able to do this as effectively as larger LLMs.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:802px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""802px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a732f6c79420bbf12335_ba0ffc6d.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">Another reason for the emergence of CoT reasoning in large LLMs may be due to their pre-training data. Larger LLMs are typically trained on massive datasets that include step-by-step reasoning, which could help them to develop the ability to reason in a chain-of-thought fashion. Instruction-following does not seem to be necessary for CoT capabilities, as zero-shot and few-shot CoT reasoning was shown using LLMs that were not fine-tuned to follow instructions. However, instruction-following could possibly improve the quality of CoT reasoning. Ultimately, more research is needed to determine the exact cause of the emergence of CoT reasoning in large LLMs.</p><p id="""">‍</p><h2 id="""">How To Perform CoT Prompting?</h2><p id="""">To perform Chain of Thought prompting you just need to append “<strong id="""">Let’s think step by step</strong>” at the end of your prompt. This forces the model to think in steps and break down the problem in steps or smaller parts. Here’s an example of what happens when don’t use and do use Chain of Thought prompting:</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a7419d734a87bb467ac5_1ca195eb.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1598px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1598px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a7419d734a87bb467ac8_3bb2d506.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">Here you can see how using Chain of thought makes LLM return a better more sophisticated and correct output. The prompt without thinking in steps immediately results in a wrong answer.</p><p id="""">If you have a rather strict problem that you know can only be solved with a specific set of reasoning patterns, that’s where you would use Few Shot COT. You can provide some examples of reasoning steps required for your specific set of problems and then the LLM will attempt to solve the given problem using similar steps. Or you can use this technique to solve the problem in a specific method for your users. For example, if students are going to be using your app, you might want to use few-shot-CoT to solve problems in a fun, simple and easy to understand way.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1030px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1030px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a7419d734a87bb467acb_3973fe94.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">These few shot examples should showcase the intermediate steps and the final solution. Once you have developed the chain of thought prompts and examples, you can incorporate them into the model. Finally, test the model and iterate on the chain of thought prompts and examples until the model's performance is satisfactory.</p><h2 id="""">Key Aspects of CoT Prompting</h2><p id="""">In this section, we will explore crucial dimensions of CoT prompting impacting its performance and reliability in large language models. We will delve into how sensitivity, self-consistency, robustness, and coherence play pivotal roles in shaping the effectiveness of CoT prompting technique.</p><h3 id="""">Self-consistency</h3><p id=""""><a href=""https://arxiv.org/pdf/2203.11171.pdf"" id="""">Self-consistency</a> is a technique for improving the performance of language models on tasks that require multi-step reasoning. In the context of chain-of-thought prompting, self-consistency can be used to improve the performance of the model by sampling multiple, diverse chains of thought for the same problem. The model can then be trained to select the most consistent answer from these chains of thought.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:841px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""841px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a7419d734a87bb467ace_02b580bb.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">Self-consistency significantly boosts the performance of CoT prompting on many popular arithmetic and commonsense reasoning benchmarks. For example, on the GSM8K benchmark, self-consistency increased the performance of CoT prompting by 17.9%. On the SVAMP benchmark, by 11.0%. And on the AQuA benchmark, by 12.2%. It is an entirely unsupervised technique that works off-the-shelf with pre-trained language models. It requires no additional human annotation and avoids any other training, models, or fine-tuning. It is robust to sampling strategies and parameters. On varying T in temperature sampling, k in top-k sampling, and p in nucleus sampling strategies over PaLM-540B, self-consistency consistently improved performance.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:754px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""754px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a7419d734a87bb467ad1_edccafa6.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><h3 id="""">Robustness</h3><p id="""">The researchers conducted experiments with three different sets of chain-of-thought annotations, each written by a different annotator. They found that CoT prompting consistently outperformed the standard baseline, regardless of the annotator. This suggests that CoT prompting is not dependent on a particular linguistic style. The researchers also conducted experiments with exemplars randomly sampled from the GSM8K training set, an independent source. They found that CoT prompting with these exemplars performed comparably to CoT prompting with manually written exemplars. This suggests that CoT prompting is not dependent on the specific exemplars that are used.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:657px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""657px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a7429d734a87bb467ad5_2ba7207c.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">The researchers also conducted experiments with varying numbers of exemplars. They found that CoT prompting remained robust to varying numbers of exemplars. This suggests that CoT prompting does not require a large number of exemplars to be effective. The researchers conducted experiments with a variety of language models, including LaMDA 137B. They found that CoT prompting was effective with all of these language models. This suggests that CoT prompting is not dependent on the specific language model that is used. Overall, the results of these experiments suggest that CoT prompting is a robust technique for improving the performance of language models on a variety of tasks. It is not dependent on a particular linguistic style, annotator, set of exemplars, or language model.</p><h3 id="""">Sensitivity</h3><p id=""""><a href=""https://arxiv.org/pdf/2104.08786.pdf"" id="""">Sensitivity</a> in CoT prompting refers to the extent to which the performance of the model is affected by the design of the prompts. If the prompts are not well-designed, then the model's performance may deteriorate. The prompts should be clear, concise, and easy for the model to understand. Avoid using jargon or technical terms that the model may not be familiar with. The prompts should be matched to the specific task that the model is trying to solve. If the prompts are not matched to the task, then the model may not be able to generate the correct answer. The more complex the task, the more sensitive the model may be to the design of the prompts.</p><p id="""">The performance of few-shot CoT deteriorated when the prompt example question types and task question types were unmatched. This suggests that few-shot CoT is highly sensitive to the design of the prompts and that the prompts need to be carefully matched to the specific task to achieve good performance.</p><h3 id="""">Coherence</h3><p id=""""><a href=""https://aclanthology.org/2023.acl-long.153.pdf"" id="""">Coherence</a> refers to the extent to which the steps of a CoT rationale are in the correct order. This means that later steps should not be preconditions for earlier steps, and earlier steps should not be based on later steps. For example, a rationale where ""32 + 42 = 74"" appears before the introduction of ""32"" or ""42"", would not have coherence. This is because the equation ""32 + 42 = 74"" is a later step that depends on the earlier steps of introducing the numbers ""32"" and ""42.""</p><p id="""">The researchers designed a set of ablation settings to examine the impact of coherence on different components of a CoT-like rationale. Ablation settings are a way of testing the importance of different parts of a system by removing them and observing the impact on the system's performance. It was found that coherence was important for all components of a CoT-like rationale. When coherence was removed, the performance of the system deteriorated.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:828px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""828px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a7429d734a87bb467ad8_53d6eee7.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">The researchers also found that the coherence of language templates is particularly important for the performance of CoT prompting. Language templates are the phrases that are used to connect the different steps of a CoT rationale. If the language templates are not coherent, then the model may not be able to understand the rationale and generate the correct answer.</p><h2 id="""">Types of Chain-of-Thought Prompting</h2><p id="""">Within the realm of chain-of-thought (CoT) prompting, two notable variations emerge as impactful strategies: multimodal CoT and least to most prompting. Let us explore these techniques in detail.</p><h3 id="""">Multi-modal CoT</h3><p id="""">Traditional CoT focuses on the language modality, which means that it only uses text to provide the model with a context for reasoning. <a href=""https://arxiv.org/pdf/2302.00923.pdf"" id="""">Multimodal CoT</a> incorporates text and vision into a two-stage framework. The first step involves rationale generation based on multimodal information. This means that the model is provided with both text and images, and it is then asked to generate a rationale that explains how the text and images are related. The second phase of the framework is answer inference. This is where the model uses the informative rationale that it generated in the first step to infer the correct answer to the question.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:999px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""999px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a75bd66134431539f318_c06330ee.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">1B multimodal CoT outperforms GPT-3.5 by 16 percentage points (75.17% to 91.68% accuracy) and surpasses human performance on the ScienceQA benchmark. Among the 8 question classes, our model improved performance from 67.43% to 88.80% for questions with paired images. Methods such as UnifiedQA and GPT-3.5, use image captions to understand what the image shows, however, using image features was more effective. Future studies could improve CoT reasoning by using better image features, adding common sense knowledge, and filtering out irrelevant information.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:961px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""961px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a75bd66134431539f31b_93c3ddb5.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">The only thing that you might need to consider when also using images is computational cost, as using CoT with images will increase the number of tokens your model is consuming AND the number of tokens the model ends up generating. You can check out this <a href=""https://arxiv.org/abs/2503.12605"" id="""">detailed survey</a> if you want to learn more about applications of CoT with Image Inputs.</p><p id="""">Another very interesting and important parallel to CoT for images is how it can be used to focus on different parts of the images. You can simply prompt for it, but if your requirements are more advanced, we suggest checking out this paper: <a href=""https://arxiv.org/pdf/2411.16044v4"" id="""">ZoomEye</a>. This method enhances Multimodal LLMs by using tree or chain-based zooming-in methods and gives amazing results. Worth checking out if you are strongly dependent on CoT for your image based prompts.</p><h3 id="""">Least-to-Most Prompting</h3><p id="""">Chain-of-thought prompting is a powerful technique for natural language reasoning, but it can struggle with tasks that require solving problems that are harder than the examples shown in the prompts. To address this challenge, we propose a novel prompting strategy called least-to-most prompting.</p><p id=""""><a href=""https://arxiv.org/pdf/2205.10625.pdf"" id="""">Least-to-most prompting</a> works by breaking down a complex problem into a series of simpler subproblems, and then solving them in sequence. Each subproblem is facilitated by the answers to the previous subproblems. For example, to solve a math word problem, we might first query the language model to decompose the problem into subproblems, such as ""What is the cost of the first item?"" and ""What is the total cost?"" We would then query the language model to sequentially solve the subproblems, using the answers to the previous subproblems to inform our queries.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:826px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""826px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a75cd66134431539f339_1081a457.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">Least-to-most prompting generalizes to more difficult problems on symbolic manipulation, compositional generalization, and math reasoning tasks. GPT-3 code-davinci-002 with least-to-most prompting can solve SCAN with 99% accuracy using 14 exemplars, while chain-of-thought prompting only gets 16% accuracy. The table below shows the accuracies of different prompting methods on the subset of GSM8K and DROP benchmarks containing only numerical problems. The base language model is code-davinci-002.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:658px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""658px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a75cd66134431539f33c_474af1b7.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">The table below shows the accuracies of different prompting methods on the last-letter-concatenation task.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:598px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""598px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a75cd66134431539f345_4a92f15d.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><h2 id="""">Auto-CoT</h2><p id=""""><a href=""https://arxiv.org/abs/2210.03493"" id="""">Auto-CoT</a> is a way to automatically create demonstrations with questions and reasoning chains. It uses large language models to generate reasoning chains for each demonstration, using the prompt ""Let's think step by step."" Auto-CoT has two main steps. First, it partitions the questions in a given dataset into a few clusters. Then, it selects a representative question from each group and uses Zero-Shot-CoT with simple heuristics to generate a reasoning chain. The diversity of the demonstration questions is important for reducing the number of mistakes that Zero-Shot-CoT makes in the reasoning chain. By clustering the questions into a few groups, Auto-CoT can ensure that each demonstration is representative of a different type of question. This helps to reduce the chances that Zero-Shot-CoT will make mistakes in the reasoning chain.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:960px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""960px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a75cd66134431539f33f_2083e010.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id=""""><a href=""https://github.com/amazon-science/auto-cot"" id="""">Auto-CoT</a> was tested on 10 reasoning tasks, including arithmetic reasoning (MultiArith, GSM8K, AQUA-RAT, SVAMP), commonsense reasoning (CSQA, StrategyQA), and symbolic reasoning (Last Letter Concatenation, Coin Flip). Auto-CoT consistently matched or exceeded the performance of Manual-CoT in GPT-3.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:927px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""927px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a75cd66134431539f342_7ecb233e.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">Here is a comparison of Auto-CoT with four baseline methods: Zero-Shot, Zero-Shot-CoT, Few-Shot, and Manual-CoT.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:927px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""927px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a75cd66134431539f342_7ecb233e.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><h2 id="""">Applications of CoT</h2><p id="""">Applications of CoT are in various domains, including arithmetic, commonsense, symbolic reasoning, natural language inference, and question answering. CoT prompts offer capabilities to LLMs to address complex problems across these areas.</p><h3 id="""">Arithmetic Reasoning</h3><p id="""">Chain of thought (CoT) prompting, when used with a 540B parameter language model, has comparable performance with task-specific fine tuned models on various tasks, including arithmetic reasoning. Solving math word problems is a challenging task for language models. To evaluate LLMs on the ability to solve math problems, two benchmarks, MultiArith and GSM8K, are used. Standard prompting shows relatively flat scaling curves for these benchmarks, meaning increasing model size does not substantially improve performance. However, when using CoT prompting, increasing model scale significantly improves performance, especially for large model sizes.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:804px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""804px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a7d56a829551a9eccbd4_80b4f84d.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">PaLM, a 540B parameter language model, combined with CoT prompting, achieves a state-of-the-art performance of 58% on the GSM8K benchmark. Self-consistency techniques further improve CoT prompting performance, reaching 74% accuracy on GSM8K. CoT prompting results in a state of the art in math word problem-solving, surpassing fine-tuned GPT-3 baselines.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1176px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1176px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a7d56a829551a9eccbda_500c0b5b.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><h3 id="""">Commonsense Reasoning</h3><p id="""">Chain-of-thought prompting can also be used for commonsense reasoning tasks. Such tasks require reasoning about physical and human interactions based on general knowledge. Commonsense reasoning is challenging for current natural language understanding systems. CoT prompting is evaluated on commonsense reasoning benchmarks such as CommonsenseQA, StrategyQA, date understanding, and sports understanding. Performance on these tasks generally improves with an increase in model size. CoT prompting provides small improvements over it. CoT prompting is most effective in improving performance on sports understanding tasks.</p><p id="""">PaLM 540B with CoT outperformed an unaided sports enthusiast with a score of 95% vs. 84% and the prior state-of-the-art on StrategyQA with a score of 75.6% vs. 69.4% and sports understanding with 95.4% vs. 84%. But, minimal improvement is seen in CommonsenseQA (CSQA).</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:924px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""924px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a7d56a829551a9eccbd1_a076e14f.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><h3 id="""">Symbolic reasoning</h3><p id="""">Chain-of-thought prompting enables language models to perform symbolic reasoning tasks that are difficult with standard prompting. It also supports length generalization, allowing models to handle inference-time inputs longer than those seen in few-shot exemplars. During research, to test CoT prompting, two toy tasks were used for evaluation. The first was the last letter concatenation, where the model concatenates the last letters of words in a name. And the second was coin flip, where the model determines if a coin remains heads up after people flip it or not.</p><p id="""">In-domain and out-of-domain test sets were used to evaluate the performance of PaLM 540B with chain-of-thought prompting (CoT) and standard prompting on these two tasks. For in-domain evaluations, the examples had the same number of steps as the training/few-shot exemplars. For out-of-domain evaluations, the examples had more steps than those in the exemplars.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:324px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""324px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a7d56a829551a9eccbd7_45ca8d61.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">PaLM 540B with CoT achieved almost 100% solve rates for in-domain evaluations. Standard prompting failed for both tasks in both in-domain and out-of-domain evaluations. CoT prompting resulted in improved performance, but it was lower than in in-domain evaluations.</p><h3 id="""">Question Answering</h3><p id="""">CoT prompting improves question answering (QA) by decomposing complex questions or prompts into a sequence of simpler, logical steps. This approach helps the language model understand the structure of the question and the relationships between its components. Each step focuses on a specific aspect of the question, helping the model to identify relevant information more effectively. CoT encourages the model to perform multi-hop reasoning, where it iteratively gathers and combines information from different sources or documents. This enables the model to perform improved inference and connect separate pieces of knowledge to arrive at an accurate answer. By explicitly specifying reasoning steps, CoT prompts can help prevent common errors or biases that language models might introduce when answering complex questions. Additionally, CoT prompts allows users to understand how the model arrived at a particular response.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:631px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""631px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a7d56a829551a9eccbe5_30133278.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><h2 id="""">Advanced CoT Variants</h2><p id="""">After the core CoT technique, many sophisticated variants improving upon it have come out over the years. Some optimize for faster problem solving, some for more accurate and more in-depth. Let’s look at a few of these advanced Chain of Thought prompting techniques that can be used to boost accuracy and speed, too!</p><p id="""">In our experience, these are very strong and niche techniques that you should explore only when dealing with very specific issues. CoT and simply running the same problem through multiple different types of prompts often solves many issues. It is recommended to explore the low hanging fruits before diving deeper into these sophisticated techniques.</p><h3 id="""">Tree of Thoughts</h3><p id="""">CoT follows a linear approach where each new word or idea is linked directly to the one before it, forming a chain. It represents a sequential thought organization. <a href=""https://arxiv.org/abs/2305.08291"" id="""">Tree of Thoughts (ToT)</a>, on the other hand, adopts a hierarchical approach. Ideas are organized into a tree-like structure, with each idea branching off into multiple related ideas. It represents a more complex and branching thought organization.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:918px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""918px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a7d56a829551a9eccbe8_dd9f16ae.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">CoT models, like GPT-3, are generally good at generating coherent and contextually relevant text over short spans. ToT models, such as Transformer models, are often better at maintaining coherence over longer texts and can keep track of multiple related ideas at once. CoT models are simpler in structure and are computationally less intensive compared to ToT models because of the latter’s hierarchical nature. Also, ToT introduces the concept of a ""ToT Controller"" trained through reinforcement learning (RL). This controller can potentially learn from new data or self-play, allowing the ToT system to evolve and acquire new knowledge even with a fixed language model.</p><p id="""">CoT-SC (Self-consistency with a Chain of Thoughts) uses a simple prompting technique. It doesn't explicitly mention the use of search algorithms. ToT employs search algorithms like breadth-first search (BFS) and depth-first search (DFS) to enable systematic exploration of thoughts. It uses these algorithms in conjunction with the tree structure for problem-solving. Hence, ToT outperforms other methods significantly.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:964px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""964px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a7d56a829551a9eccbde_62a08931.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">You can choose CoT for simpler, shorter texts, and ToT can be more appropriate for complex, longer texts and problem-solving tasks.</p><p id="""">‍</p><h3 id="""">Graph of Thoughts</h3><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a81f77d8b9e652c31502_b1ef4946.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id=""""><a href=""https://arxiv.org/abs/2308.09687"" id="""">Graph of Thoughts</a> goes beyond ToT's tree structure by letting LLM thoughts connect in a web instead of a tree. In ToT, thoughts can only branch out from a single parent (like a family tree). GoT allows thoughts to have multiple parents, meaning several different reasoning paths can combine into one new thought. This lets the LLM cover more ground quickly and perform more sophisticated reasoning operations by following different paths. GoT works through three main operations.</p><ul id=""""><li id=""""><strong id="""">Aggregation:</strong> Multiple thoughts merge into one (k→1 transformation)</li></ul><ul id=""""><li id=""""><strong id="""">Generation:</strong> Single thought branches into multiple (1→k transformation)</li></ul><ul id=""""><li id=""""><strong id="""">Refinement:</strong> Thought loops back on itself for iterative improvement (1→1 with self-edge)</li></ul><p id="""">These operations are controlled by a Graph of Operations (GoO), which is basically a recipe that tells the system what operations to perform and in what order.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a81f77d8b9e652c314ff_af108bcc.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">For sorting 128 numbers, <strong id="""">GoT reduces errors by 62% compared to ToT while costing 31% less</strong>. It achieves this by breaking the list into smaller chunks, sorting each chunk separately, then merging them back together level by level, like organizing papers by first sorting them into piles, then combining those piles systematically. The whole process takes about 55 steps across 7 merging levels. In set intersection tasks (finding common elements between sets), GoT gets perfect accuracy while ToT makes 4 or more errors.</p><p id="""">The key technical advantage is the latency-volume tradeoff. Volume means how many previous thoughts contributed to the final answer. GoT keeps latency at O(log k N) while maintaining volume N, meaning all intermediate thoughts can potentially influence the final result. ToT only achieves O(log k N) for both, limiting the amount of information that flows to the final answer. But both algorithms are highly parallel and are fine in terms of how much time they take to run in our experience.</p><p id="""">GoT works best for problems that naturally split into parts that need to be solved separately, then combined. However, GoT adds unnecessary complexity for straightforward problems that don't need multiple paths or merging. It also costs more than simpler methods due to multiple LLM calls. One good application of GoT we can identify is Contract evaluation and analysis, especially for complex ones. We have worked with these, and we tend to run multiple prompts to analyze the contract from different aspects like technical, sales, IP, etc. That is good, but GoT can do that out of the box without multiple prompts and sophisticated architectures.</p><h3 id="""">Layer of Thoughts</h3><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1494px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1494px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a82077d8b9e652c3152b_cf8da515.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">LoT builds on Graph of Thoughts by organizing the reasoning process into distinct layers. Instead of letting thoughts connect freely like in GoT, LoT structures them hierarchically. Thoughts in layer 1 must be completed before layer 2 begins, and layer 2's output feeds into layer 3. Each layer contains two types of thoughts: layer thoughts that manage the overall process for that level, and option thoughts that explore different solutions within that layer. Each layer acts as a filter with specific criteria. Layer 1 might filter by basic keywords, layer 2 by semantic meaning, and layer 3 by final validation.</p><p id="""">LoT uses several aggregation methods to combine results within a layer. The ""all"" metric requires documents to pass every criterion (like requiring both a password AND fingerprint). The ""at-least-k"" metric is more flexible—documents need to pass at least k criteria out of the total. The ""locally-better"" metric keeps documents that aren't beaten on every criterion by another document. The ""max-count"" metric simply counts how many criteria each document passes and ranks them accordingly.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a82077d8b9e652c31506_8545694c.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">In testing on Japanese Civil Law retrieval, LoT achieved an F2 score of 0.835, beating the best systems from COLIEE 2024 competition. It maintained precision at 0.838 while keeping recall at 0.839—a balance that simpler approaches couldn't achieve. When they removed the semantic filtering layer, recall jumped to 0.885 but precision crashed to 0.432. One extreme case had the LLM marking 400 articles as relevant when only 2 actually were. The layered filtering prevented these false positives.</p><p id="""">LoT works best when you have clear filtering criteria that can be arranged from broad to specific. Legal document retrieval is perfect first filter by relevant keywords, then by legal concepts, then confirm relevance to the specific query. It struggles when criteria don't have a natural hierarchy or when the problem needs thoughts to influence each other across layers (which GoT handles better through arbitrary connections).</p><h2 id="""">CoT vs. Other Methods</h2><p id="""">In this section, we go into a detailed comparison of CoT prompting with other methods, specifically Standard and Tree of Thought Prompting. Evaluating their strengths and limitations offers valuable insights into selecting the most suitable approach for your business applications.</p><h3 id="""">CoT vs. Standard Prompting</h3><p id="""">Standard Prompting uses input-output pairs as examples. The pairs are formatted as questions and answers. The model predicts answers based on these pairs. It is limited in handling multi-step reasoning tasks effectively. But is suitable for straightforward tasks, such as single-turn questions. It demands fewer computational resources. It commonly uses single-shot prompts for training and tends to require more data to fine-tune for complex tasks. Standard prompting may not exhibit significant performance improvements with the model scale.</p><p id=""""><a href=""https://github.com/FranxYao/chain-of-thought-hub"" id="""">CoT prompting</a> involves generating intermediate reasoning steps. These steps precede providing a final answer. It excels at complex reasoning, enabling models to think step by step. It is versatile and applicable to a wide range of tasks requiring intricate reasoning. It requires training on sequences of prompts and efficiently utilizes data for multi-step reasoning. It demonstrates enhanced performance with larger models and, thus, requires more computational power. It excels in complex reasoning benchmarks and tasks that demand multi-step problem-solving.</p><p id=""""><strong id="""">Comparison on MAWPS Benchmark:</strong></p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:636px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""636px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a82077d8b9e652c3150c_cbb81dd8.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id=""""><strong id="""">Comparison on Length Generalization Task:</strong></p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:781px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""781px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a82077d8b9e652c3150f_26414907.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">You can choose Standard Prompting for straightforward tasks and CoT Prompting, as the superior choice, for applications requiring deep, multi-step reasoning and interpretability. An open-source repository of data and tools related to CoT reasoning is available on <a href=""https://github.com/OpenBioLink/ThoughtSource"" id="""">GitHub</a>. It has datasets on various tasks such as math problems and common sense reasoning. It also has a community forum for discussions.</p><h3 id="""">CoT vs. ReAct Prompting</h3><p id="""">CoT forces the model to generate an internal, sequential reasoning trace before providing a final answer. <a href=""https://arxiv.org/abs/2210.03629"" id="""">ReAct</a>, on the other hand, synergizes reasoning and acting by generating interleaved reasoning traces and task-specific actions.</p><p id="""">The fundamental difference lies in their interaction with information. CoT operates as a static chain, where it thinks a lot and then gives an output. This makes it highly susceptible to fact hallucination and error propagation, as its reasoning is not grounded. ReAct works differently. The <strong id="""">Action</strong> step allows the LLM to interface with external tools. The subsequent <strong id="""">Observation</strong> grounds the model's next <strong id="""">Thought</strong>, allowing it to create, maintain, and adjust plans based on real-world feedback.</p><p id="""">CoT excels at tasks requiring purely internal, sequential logic with a fixed context, such as multi-step arithmetic. Its linear structure is efficient for problems that do not require external validation.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a82077d8b9e652c31518_f9f2809f.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">ReAct is designed for dynamic tasks where the environment is interactive or information is incomplete. On knowledge-intensive tasks like HotpotQA, ReAct overcomes CoT's hallucination issues. On decision-making benchmarks like ALFWorld, ReAct outperforms imitation learning methods by an absolute success rate of 34% with only one or two in-context examples, demonstrating its ability to handle long-horizon tasks.</p><p id=""""><em id="""">Use CoT for self-contained problems solvable with internal knowledge and linear logic. Use ReAct for dynamic tasks that require grounding in external information, tool use, or interaction with an environment.</em></p><p id="""">You can read our <a href=""https://www.mercity.ai/blog-post/react-prompting-and-react-based-agentic-systems"" id="""">Extended Report on React prompting</a> to learn more about it.</p><h3 id="""">CoT vs Reflexion</h3><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a82077d8b9e652c31512_1a0c45f5.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">Chain of Thought is a static, single-turn reasoning process. <a href=""https://arxiv.org/abs/2303.11366"" id="""">Reflexion</a>, on the other hand, is a dynamic, multi-trial framework for agent improvement through verbal reinforcement. While CoT generates a linear sequence of thoughts to solve a problem in one attempt, Reflexion uses a learning loop. This loop involves three models: an Actor that generates actions, an Evaluator that scores the outcome, and a Self-Reflection model that converts task feedback into natural language summaries.</p><p id="""">The main improvement is the learning mechanism. CoT has no way to learn from failure, a wrong answer in one attempt does not inform the next. Reflexion uses failures by storing feedback in a memory buffer, reinforcing the agent for upcoming trials. This converts environmental feedback scalar values, compiler errors, etc.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a82077d8b9e652c31515_e5959e48.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">Reflexion significantly outperforms static approaches on tasks requiring iterative refinement. On the HumanEval coding benchmark, a Reflexion agent achieves 91% pass@1 accuracy, surpassing a strong GPT-4 baseline (80%). For the AlfWorld sequential decision-making task, it improves success rates by an absolute 22% over a ReAct baseline across 12 trials. On the HotPotQA reasoning task, it boosts accuracy by 20% by learning from initial failures. <em id="""">An ablation study shows that simply providing episodic memory of past trajectories is less effective than the full self-reflection loop, demonstrating that focused, verbal reinforcement is the key contributor.</em></p><p id=""""><strong id="""">It is also very important to note that Reflexion can simply be combined with CoT or ReACT to boost the performance of your preexisting prompts.</strong></p><p id="""">Use CoT for problems where a correct line of reasoning can be generated in a single pass. Reflexion is designed for complex tasks that benefit from trial-and-error, allowing an agent to systematically learn from its mistakes across multiple attempts in coding, reasoning, or decision-making.</p><h2 id="""">Want to write high-quality production-grade prompts for your LLMs?</h2><p id="""">If you are looking to leverage prompt engineering for your business needs, we can help. We are a team of AI engineers with extensive experience in prompt engineering techniques like Chain-of-Thought, ReAct, etc. <a href=""https://www.mercity.ai/contacts"" id="""">Contact us</a> today and let us apply CoT applications to elevate your business.</p>",https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64f22a88aad6914b12dacbb4_chain%20of%20thought.png,Maithili Badhan,Prompt Engineering,Learn how you can use Chain of Thought (CoT) prompting to boost LLM performance at reasoning tasks. ,False,"<div class=""rich-text w-richtext""><h1>How Chain-of-Thought (CoT) Prompting Makes LLMs Better At Reasoning?</h1><p>Scaling up large language models (LLMs) has shown good results in sentiment analysis and machine translation, even without any examples. However, they fail in complex multi-step problems such as arithmetic and commonsense reasoning. To address this, LLMs can either be fine-tuned for a particular task or taught with few-shot prompting. However, both of these methods have limitations. Fine-tuning is costly for creating high-quality reasoning, while only few-shot prompting is not effective enough for the task.</p><p>Chain-of-Thought (CoT) prompting can address both of these problems. In this article, we will explore CoT prompting and how implementing it can upskill your business.</p><h2>What is Prompt Engineering?</h2><p>Prompt engineering is the practice of writing well-structured and carefully crafted prompts that can be better interpreted by a generative AI model. A prompt tells the LLM what task to perform and what kind of output to generate. It can contain instruction, context, input data, and output indicators. Using prompt engineering, we can use LLMs to carry out various tasks, from simple question answering to complex creative text generation. It is based on an emergent property, in-context learning, allowing LLMs to learn from prompts. Prompt engineering improves the performance of LLMs on the task at hand. It uses zero-shot, few-shot, active and CoT prompting as discussed ahead.</p><p>We also have a blog on <a href=""https://www.mercity.ai/blog-post/advanced-prompt-engineering-techniques#want-to-write-high-quality-prompts-for-llms"">advanced prompt engineering</a>.</p><h3>Zero-shot Prompting</h3><p>In zero-shot prompting, we provide the LLM a prompt that describes the task, but the prompt does not provide any examples for the task. The LLM is then asked to generate a response to the prompt. It improves the flexibility and generalization of LLMs. It can be used to train LLMs on several tasks, without having to collect training data for each task. For example, ChatGPT can write a poem on prompt engineering without giving any examples of how to write a poem. However, zero-shot prompting is limited for complex tasks.</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1021pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a6be268c58e672f2c732_ea4a7243.png""/></div></figure><h3>Few-shot Prompting</h3><p>Few-shot prompting can provide demonstrations to steer the model to better performance. It is a technique for providing LLMs with a few examples of the desired output, in addition to the prompt. The examples help the model to better understand the task and to generate more accurate and informative responses. We should provide vast and different examples to the model, instead of multiple similar examples. It ensures the model learns as much as possible about the task. Standard few-shot prompting is a good technique for many tasks, but not reliable for complex reasoning tasks. Therefore, more advanced prompting techniques, such as chain-of-thought, active prompting, and fine-tuning, are needed.</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:936pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a6be268c58e672f2c735_b7362b56.png""/></div></figure><h3>Active Prompting</h3><p><a href=""https://github.com/shizhediao/active-prompt"">Active prompting</a> improves the performance of LLMs on complex tasks by iteratively providing them with feedback on their responses. This feedback can help the LLMs to learn from their mistakes and to generate more accurate and informative responses. It provides the LLM with a prompt and a few examples of the desired output. The LLM then generates a response. The response is then evaluated by a human evaluator, who provides feedback to the LLM on the accuracy and informativeness of the response. The LLM then uses this feedback to improve its response generation capabilities. This process repeats until the LLM can generate responses that are accurate and informative enough to satisfy the human evaluator.</p><p><a href=""https://arxiv.org/pdf/2302.12246.pdf"">Active prompting</a></p><p>is important for CoT prompting as it identifies important questions for annotation, minimizes human annotation efforts, and improves the accuracy and informativeness of CoT prompts. The following figure shows active prompting with CoT to improve performance. It is a four-stage process that involves estimating the uncertainty of a question by querying an LLM multiple times, selecting the most uncertain questions for annotation by ranking them, annotating them with detailed feedback from human evaluators, inferring the answers to new questions by using the LLM to generate answers and the feedback from the annotation step to improve the quality.</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:939pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a6be268c58e672f2c738_38f81e06.png""/></div></figure><h2>What is Chain-of-Thought Prompting?</h2><p><a href=""https://arxiv.org/pdf/2201.11903.pdf"">Chain-of-Thought prompting</a> is a prompt engineering technique through which we force LLMs to output a sequence of intermediate steps that lead to the desired answer. It improves the reasoning abilities of LLMs. It is beneficial because it allows the model to focus on solving one step at a time, rather than having to consider the entire problem all at once. It can be especially helpful for complex problems that would be difficult or impossible to solve in a single step. It provides an interpretable window into the behavior of the model. We can see how the model arrived at its answer by following the sequence of steps that it took.</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:862pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a6be268c58e672f2c741_3402878e.png""/></div></figure><p>CoT prompting can be used with LLMs with a large set of parameters (~100 B parameters) for several reasoning tasks, including math word problems, commonsense reasoning, and symbolic manipulation. For example, using <a href=""https://ai.googleblog.com/2022/05/language-models-perform-reasoning-via.html"">CoT prompting</a> in the PaLM model instead of standard few-shots, improved the performance in the GSM8K benchmark from 17.9% to 58.1%. CoT prompting can be readily elicited in sufficiently large language models without any special training or fine-tuning of the model. It makes CoT prompting a scalable and accessible technique.</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:411pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a6be268c58e672f2c744_7e42e126.png""/></div></figure><h3>Few-Shot CoT</h3><p>Few-shot prompting prompts the LLM with a question and the answer. Then, the LLM is provided with a few examples of how to solve similar problems. The examples are presented in a way that encourages the LLM to reason about the problem and come up with a chain of thought that leads to the answer.</p><figure class=""w-richtext-align-center w-richtext-figure-type-image""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a6be268c58e672f2c73b_d69047d3.png""/></div></figure><p><a href=""https://arxiv.org/abs/2305.14045"">Few-shot CoT</a> is a more effective technique for improving the reasoning abilities of LLMs than the few-shot baseline because it provides LLMs with examples of similar problems. It can be more complex to implement than a few-shot baseline because it requires the creation of example prompts. However, the benefits of few-shot CoT outweigh the additional complexity.</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:994pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a6be268c58e672f2c73e_04c2ed6b.png""/></div></figure><h3>Zero-Shot CoT</h3><p><a href=""https://arxiv.org/abs/2205.11916"">Zero-shot CoT</a> involves adding ""Let's think step by step"" to the original prompt. It extracts reasoning and answers using two prompts.</p><ul role=""list""><li>Reasoning extraction: In this step, the language model thinks about the question and comes up with a chain of reasoning that leads to the answer. For this, we give the language model a prompt that includes the question and a trigger sentence ""<strong>Let's think step by step</strong>."" The language model will then generate a sentence that explains how it arrived at the answer.</li></ul><ul role=""list""><li>Answer extraction: In the second step, we extract the final answer from the language model's response. We concatenate the prompt, the generated sentence, and a trigger sentence, ""<strong>The answer is</strong>"". It tells the language model to give us the answer. The language model will then generate a sentence that contains the answer to the question.</li></ul><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:852pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a732f6c79420bbf1232c_f20c0952.png""/></div></figure><p>In contrast to this, the zero-shot baseline uses prompts like ""The answer is"" for answer extraction. Few-shot prompting, whether standard or CoT, avoids the need for such answer-extraction prompts by designing example answers to end in the correct formats.</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:802pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a732f6c79420bbf1232f_15f147a5.png""/></div></figure><p>On comparing zero-shot CoT to two other methods for evaluating the zero-shot reasoning abilities of LLMs, researchers found that zero-shot-CoT outperforms the other methods on various reasoning tasks. If you are looking for a smaller model trained on CoT prompting, consider the Flan-T5 model. It can be used for zero-shot NLP tasks including text summarization, natural language inference, translation, and common sense reasoning.</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:712pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a732f6c79420bbf12332_4ec38025.png""/></div></figure><h2>When Does CoT Emerge?</h2><p>CoT reasoning is an <a href=""https://web.stanford.edu/class/cs224v/lectures/jason-wei-emergence-talk-stanford.pdf"">emergent ability</a> of LLMs that may arise due to <a href=""https://arxiv.org/abs/2210.11416"">scaling models</a> over 100 billion parameters. It does not positively impact performance for smaller LLMs and only yields performance gains when used with models of this size. There are two reasons for it. Firstly, smaller LLMs are not able to generate long chains of thought that are both fluent and logical. This leads to lower performance than standard prompting. Secondly, CoT reasoning is more effective for more complicated problems. It requires the LLM to be able to identify the key steps involved in solving a problem and then generate a chain of thoughts that leads to the solution. Smaller LLMs may not be able to do this as effectively as larger LLMs.</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:802pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a732f6c79420bbf12335_ba0ffc6d.png""/></div></figure><p>Another reason for the emergence of CoT reasoning in large LLMs may be due to their pre-training data. Larger LLMs are typically trained on massive datasets that include step-by-step reasoning, which could help them to develop the ability to reason in a chain-of-thought fashion. Instruction-following does not seem to be necessary for CoT capabilities, as zero-shot and few-shot CoT reasoning was shown using LLMs that were not fine-tuned to follow instructions. However, instruction-following could possibly improve the quality of CoT reasoning. Ultimately, more research is needed to determine the exact cause of the emergence of CoT reasoning in large LLMs.</p><p>‍</p><h2>How To Perform CoT Prompting?</h2><p>To perform Chain of Thought prompting you just need to append “<strong>Let’s think step by step</strong>” at the end of your prompt. This forces the model to think in steps and break down the problem in steps or smaller parts. Here’s an example of what happens when don’t use and do use Chain of Thought prompting:</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a7419d734a87bb467ac5_1ca195eb.png""/></div></figure><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1598pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a7419d734a87bb467ac8_3bb2d506.png""/></div></figure><p>Here you can see how using Chain of thought makes LLM return a better more sophisticated and correct output. The prompt without thinking in steps immediately results in a wrong answer.</p><p>If you have a rather strict problem that you know can only be solved with a specific set of reasoning patterns, that’s where you would use Few Shot COT. You can provide some examples of reasoning steps required for your specific set of problems and then the LLM will attempt to solve the given problem using similar steps. Or you can use this technique to solve the problem in a specific method for your users. For example, if students are going to be using your app, you might want to use few-shot-CoT to solve problems in a fun, simple and easy to understand way.</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1030pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a7419d734a87bb467acb_3973fe94.png""/></div></figure><p>These few shot examples should showcase the intermediate steps and the final solution. Once you have developed the chain of thought prompts and examples, you can incorporate them into the model. Finally, test the model and iterate on the chain of thought prompts and examples until the model's performance is satisfactory.</p><h2>Key Aspects of CoT Prompting</h2><p>In this section, we will explore crucial dimensions of CoT prompting impacting its performance and reliability in large language models. We will delve into how sensitivity, self-consistency, robustness, and coherence play pivotal roles in shaping the effectiveness of CoT prompting technique.</p><h3>Self-consistency</h3><p><a href=""https://arxiv.org/pdf/2203.11171.pdf"">Self-consistency</a> is a technique for improving the performance of language models on tasks that require multi-step reasoning. In the context of chain-of-thought prompting, self-consistency can be used to improve the performance of the model by sampling multiple, diverse chains of thought for the same problem. The model can then be trained to select the most consistent answer from these chains of thought.</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:841pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a7419d734a87bb467ace_02b580bb.png""/></div></figure><p>Self-consistency significantly boosts the performance of CoT prompting on many popular arithmetic and commonsense reasoning benchmarks. For example, on the GSM8K benchmark, self-consistency increased the performance of CoT prompting by 17.9%. On the SVAMP benchmark, by 11.0%. And on the AQuA benchmark, by 12.2%. It is an entirely unsupervised technique that works off-the-shelf with pre-trained language models. It requires no additional human annotation and avoids any other training, models, or fine-tuning. It is robust to sampling strategies and parameters. On varying T in temperature sampling, k in top-k sampling, and p in nucleus sampling strategies over PaLM-540B, self-consistency consistently improved performance.</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:754pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a7419d734a87bb467ad1_edccafa6.png""/></div></figure><h3>Robustness</h3><p>The researchers conducted experiments with three different sets of chain-of-thought annotations, each written by a different annotator. They found that CoT prompting consistently outperformed the standard baseline, regardless of the annotator. This suggests that CoT prompting is not dependent on a particular linguistic style. The researchers also conducted experiments with exemplars randomly sampled from the GSM8K training set, an independent source. They found that CoT prompting with these exemplars performed comparably to CoT prompting with manually written exemplars. This suggests that CoT prompting is not dependent on the specific exemplars that are used.</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:657pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a7429d734a87bb467ad5_2ba7207c.png""/></div></figure><p>The researchers also conducted experiments with varying numbers of exemplars. They found that CoT prompting remained robust to varying numbers of exemplars. This suggests that CoT prompting does not require a large number of exemplars to be effective. The researchers conducted experiments with a variety of language models, including LaMDA 137B. They found that CoT prompting was effective with all of these language models. This suggests that CoT prompting is not dependent on the specific language model that is used. Overall, the results of these experiments suggest that CoT prompting is a robust technique for improving the performance of language models on a variety of tasks. It is not dependent on a particular linguistic style, annotator, set of exemplars, or language model.</p><h3>Sensitivity</h3><p><a href=""https://arxiv.org/pdf/2104.08786.pdf"">Sensitivity</a> in CoT prompting refers to the extent to which the performance of the model is affected by the design of the prompts. If the prompts are not well-designed, then the model's performance may deteriorate. The prompts should be clear, concise, and easy for the model to understand. Avoid using jargon or technical terms that the model may not be familiar with. The prompts should be matched to the specific task that the model is trying to solve. If the prompts are not matched to the task, then the model may not be able to generate the correct answer. The more complex the task, the more sensitive the model may be to the design of the prompts.</p><p>The performance of few-shot CoT deteriorated when the prompt example question types and task question types were unmatched. This suggests that few-shot CoT is highly sensitive to the design of the prompts and that the prompts need to be carefully matched to the specific task to achieve good performance.</p><h3>Coherence</h3><p><a href=""https://aclanthology.org/2023.acl-long.153.pdf"">Coherence</a> refers to the extent to which the steps of a CoT rationale are in the correct order. This means that later steps should not be preconditions for earlier steps, and earlier steps should not be based on later steps. For example, a rationale where ""32 + 42 = 74"" appears before the introduction of ""32"" or ""42"", would not have coherence. This is because the equation ""32 + 42 = 74"" is a later step that depends on the earlier steps of introducing the numbers ""32"" and ""42.""</p><p>The researchers designed a set of ablation settings to examine the impact of coherence on different components of a CoT-like rationale. Ablation settings are a way of testing the importance of different parts of a system by removing them and observing the impact on the system's performance. It was found that coherence was important for all components of a CoT-like rationale. When coherence was removed, the performance of the system deteriorated.</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:828pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a7429d734a87bb467ad8_53d6eee7.png""/></div></figure><p>The researchers also found that the coherence of language templates is particularly important for the performance of CoT prompting. Language templates are the phrases that are used to connect the different steps of a CoT rationale. If the language templates are not coherent, then the model may not be able to understand the rationale and generate the correct answer.</p><h2>Types of Chain-of-Thought Prompting</h2><p>Within the realm of chain-of-thought (CoT) prompting, two notable variations emerge as impactful strategies: multimodal CoT and least to most prompting. Let us explore these techniques in detail.</p><h3>Multi-modal CoT</h3><p>Traditional CoT focuses on the language modality, which means that it only uses text to provide the model with a context for reasoning. <a href=""https://arxiv.org/pdf/2302.00923.pdf"">Multimodal CoT</a> incorporates text and vision into a two-stage framework. The first step involves rationale generation based on multimodal information. This means that the model is provided with both text and images, and it is then asked to generate a rationale that explains how the text and images are related. The second phase of the framework is answer inference. This is where the model uses the informative rationale that it generated in the first step to infer the correct answer to the question.</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:999pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a75bd66134431539f318_c06330ee.png""/></div></figure><p>1B multimodal CoT outperforms GPT-3.5 by 16 percentage points (75.17% to 91.68% accuracy) and surpasses human performance on the ScienceQA benchmark. Among the 8 question classes, our model improved performance from 67.43% to 88.80% for questions with paired images. Methods such as UnifiedQA and GPT-3.5, use image captions to understand what the image shows, however, using image features was more effective. Future studies could improve CoT reasoning by using better image features, adding common sense knowledge, and filtering out irrelevant information.</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:961pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a75bd66134431539f31b_93c3ddb5.png""/></div></figure><p>The only thing that you might need to consider when also using images is computational cost, as using CoT with images will increase the number of tokens your model is consuming AND the number of tokens the model ends up generating. You can check out this <a href=""https://arxiv.org/abs/2503.12605"">detailed survey</a> if you want to learn more about applications of CoT with Image Inputs.</p><p>Another very interesting and important parallel to CoT for images is how it can be used to focus on different parts of the images. You can simply prompt for it, but if your requirements are more advanced, we suggest checking out this paper: <a href=""https://arxiv.org/pdf/2411.16044v4"">ZoomEye</a>. This method enhances Multimodal LLMs by using tree or chain-based zooming-in methods and gives amazing results. Worth checking out if you are strongly dependent on CoT for your image based prompts.</p><h3>Least-to-Most Prompting</h3><p>Chain-of-thought prompting is a powerful technique for natural language reasoning, but it can struggle with tasks that require solving problems that are harder than the examples shown in the prompts. To address this challenge, we propose a novel prompting strategy called least-to-most prompting.</p><p><a href=""https://arxiv.org/pdf/2205.10625.pdf"">Least-to-most prompting</a> works by breaking down a complex problem into a series of simpler subproblems, and then solving them in sequence. Each subproblem is facilitated by the answers to the previous subproblems. For example, to solve a math word problem, we might first query the language model to decompose the problem into subproblems, such as ""What is the cost of the first item?"" and ""What is the total cost?"" We would then query the language model to sequentially solve the subproblems, using the answers to the previous subproblems to inform our queries.</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:826pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a75cd66134431539f339_1081a457.png""/></div></figure><p>Least-to-most prompting generalizes to more difficult problems on symbolic manipulation, compositional generalization, and math reasoning tasks. GPT-3 code-davinci-002 with least-to-most prompting can solve SCAN with 99% accuracy using 14 exemplars, while chain-of-thought prompting only gets 16% accuracy. The table below shows the accuracies of different prompting methods on the subset of GSM8K and DROP benchmarks containing only numerical problems. The base language model is code-davinci-002.</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:658pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a75cd66134431539f33c_474af1b7.png""/></div></figure><p>The table below shows the accuracies of different prompting methods on the last-letter-concatenation task.</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:598pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a75cd66134431539f345_4a92f15d.png""/></div></figure><h2>Auto-CoT</h2><p><a href=""https://arxiv.org/abs/2210.03493"">Auto-CoT</a> is a way to automatically create demonstrations with questions and reasoning chains. It uses large language models to generate reasoning chains for each demonstration, using the prompt ""Let's think step by step."" Auto-CoT has two main steps. First, it partitions the questions in a given dataset into a few clusters. Then, it selects a representative question from each group and uses Zero-Shot-CoT with simple heuristics to generate a reasoning chain. The diversity of the demonstration questions is important for reducing the number of mistakes that Zero-Shot-CoT makes in the reasoning chain. By clustering the questions into a few groups, Auto-CoT can ensure that each demonstration is representative of a different type of question. This helps to reduce the chances that Zero-Shot-CoT will make mistakes in the reasoning chain.</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:960pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a75cd66134431539f33f_2083e010.png""/></div></figure><p><a href=""https://github.com/amazon-science/auto-cot"">Auto-CoT</a> was tested on 10 reasoning tasks, including arithmetic reasoning (MultiArith, GSM8K, AQUA-RAT, SVAMP), commonsense reasoning (CSQA, StrategyQA), and symbolic reasoning (Last Letter Concatenation, Coin Flip). Auto-CoT consistently matched or exceeded the performance of Manual-CoT in GPT-3.</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:927pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a75cd66134431539f342_7ecb233e.png""/></div></figure><p>Here is a comparison of Auto-CoT with four baseline methods: Zero-Shot, Zero-Shot-CoT, Few-Shot, and Manual-CoT.</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:927pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a75cd66134431539f342_7ecb233e.png""/></div></figure><h2>Applications of CoT</h2><p>Applications of CoT are in various domains, including arithmetic, commonsense, symbolic reasoning, natural language inference, and question answering. CoT prompts offer capabilities to LLMs to address complex problems across these areas.</p><h3>Arithmetic Reasoning</h3><p>Chain of thought (CoT) prompting, when used with a 540B parameter language model, has comparable performance with task-specific fine tuned models on various tasks, including arithmetic reasoning. Solving math word problems is a challenging task for language models. To evaluate LLMs on the ability to solve math problems, two benchmarks, MultiArith and GSM8K, are used. Standard prompting shows relatively flat scaling curves for these benchmarks, meaning increasing model size does not substantially improve performance. However, when using CoT prompting, increasing model scale significantly improves performance, especially for large model sizes.</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:804pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a7d56a829551a9eccbd4_80b4f84d.png""/></div></figure><p>PaLM, a 540B parameter language model, combined with CoT prompting, achieves a state-of-the-art performance of 58% on the GSM8K benchmark. Self-consistency techniques further improve CoT prompting performance, reaching 74% accuracy on GSM8K. CoT prompting results in a state of the art in math word problem-solving, surpassing fine-tuned GPT-3 baselines.</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1176pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a7d56a829551a9eccbda_500c0b5b.png""/></div></figure><h3>Commonsense Reasoning</h3><p>Chain-of-thought prompting can also be used for commonsense reasoning tasks. Such tasks require reasoning about physical and human interactions based on general knowledge. Commonsense reasoning is challenging for current natural language understanding systems. CoT prompting is evaluated on commonsense reasoning benchmarks such as CommonsenseQA, StrategyQA, date understanding, and sports understanding. Performance on these tasks generally improves with an increase in model size. CoT prompting provides small improvements over it. CoT prompting is most effective in improving performance on sports understanding tasks.</p><p>PaLM 540B with CoT outperformed an unaided sports enthusiast with a score of 95% vs. 84% and the prior state-of-the-art on StrategyQA with a score of 75.6% vs. 69.4% and sports understanding with 95.4% vs. 84%. But, minimal improvement is seen in CommonsenseQA (CSQA).</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:924pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a7d56a829551a9eccbd1_a076e14f.png""/></div></figure><h3>Symbolic reasoning</h3><p>Chain-of-thought prompting enables language models to perform symbolic reasoning tasks that are difficult with standard prompting. It also supports length generalization, allowing models to handle inference-time inputs longer than those seen in few-shot exemplars. During research, to test CoT prompting, two toy tasks were used for evaluation. The first was the last letter concatenation, where the model concatenates the last letters of words in a name. And the second was coin flip, where the model determines if a coin remains heads up after people flip it or not.</p><p>In-domain and out-of-domain test sets were used to evaluate the performance of PaLM 540B with chain-of-thought prompting (CoT) and standard prompting on these two tasks. For in-domain evaluations, the examples had the same number of steps as the training/few-shot exemplars. For out-of-domain evaluations, the examples had more steps than those in the exemplars.</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:324pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a7d56a829551a9eccbd7_45ca8d61.png""/></div></figure><p>PaLM 540B with CoT achieved almost 100% solve rates for in-domain evaluations. Standard prompting failed for both tasks in both in-domain and out-of-domain evaluations. CoT prompting resulted in improved performance, but it was lower than in in-domain evaluations.</p><h3>Question Answering</h3><p>CoT prompting improves question answering (QA) by decomposing complex questions or prompts into a sequence of simpler, logical steps. This approach helps the language model understand the structure of the question and the relationships between its components. Each step focuses on a specific aspect of the question, helping the model to identify relevant information more effectively. CoT encourages the model to perform multi-hop reasoning, where it iteratively gathers and combines information from different sources or documents. This enables the model to perform improved inference and connect separate pieces of knowledge to arrive at an accurate answer. By explicitly specifying reasoning steps, CoT prompts can help prevent common errors or biases that language models might introduce when answering complex questions. Additionally, CoT prompts allows users to understand how the model arrived at a particular response.</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:631pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a7d56a829551a9eccbe5_30133278.png""/></div></figure><h2>Advanced CoT Variants</h2><p>After the core CoT technique, many sophisticated variants improving upon it have come out over the years. Some optimize for faster problem solving, some for more accurate and more in-depth. Let’s look at a few of these advanced Chain of Thought prompting techniques that can be used to boost accuracy and speed, too!</p><p>In our experience, these are very strong and niche techniques that you should explore only when dealing with very specific issues. CoT and simply running the same problem through multiple different types of prompts often solves many issues. It is recommended to explore the low hanging fruits before diving deeper into these sophisticated techniques.</p><h3>Tree of Thoughts</h3><p>CoT follows a linear approach where each new word or idea is linked directly to the one before it, forming a chain. It represents a sequential thought organization. <a href=""https://arxiv.org/abs/2305.08291"">Tree of Thoughts (ToT)</a>, on the other hand, adopts a hierarchical approach. Ideas are organized into a tree-like structure, with each idea branching off into multiple related ideas. It represents a more complex and branching thought organization.</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:918pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a7d56a829551a9eccbe8_dd9f16ae.png""/></div></figure><p>CoT models, like GPT-3, are generally good at generating coherent and contextually relevant text over short spans. ToT models, such as Transformer models, are often better at maintaining coherence over longer texts and can keep track of multiple related ideas at once. CoT models are simpler in structure and are computationally less intensive compared to ToT models because of the latter’s hierarchical nature. Also, ToT introduces the concept of a ""ToT Controller"" trained through reinforcement learning (RL). This controller can potentially learn from new data or self-play, allowing the ToT system to evolve and acquire new knowledge even with a fixed language model.</p><p>CoT-SC (Self-consistency with a Chain of Thoughts) uses a simple prompting technique. It doesn't explicitly mention the use of search algorithms. ToT employs search algorithms like breadth-first search (BFS) and depth-first search (DFS) to enable systematic exploration of thoughts. It uses these algorithms in conjunction with the tree structure for problem-solving. Hence, ToT outperforms other methods significantly.</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:964pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a7d56a829551a9eccbde_62a08931.png""/></div></figure><p>You can choose CoT for simpler, shorter texts, and ToT can be more appropriate for complex, longer texts and problem-solving tasks.</p><p>‍</p><h3>Graph of Thoughts</h3><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a81f77d8b9e652c31502_b1ef4946.png""/></div></figure><p><a href=""https://arxiv.org/abs/2308.09687"">Graph of Thoughts</a> goes beyond ToT's tree structure by letting LLM thoughts connect in a web instead of a tree. In ToT, thoughts can only branch out from a single parent (like a family tree). GoT allows thoughts to have multiple parents, meaning several different reasoning paths can combine into one new thought. This lets the LLM cover more ground quickly and perform more sophisticated reasoning operations by following different paths. GoT works through three main operations.</p><ul role=""list""><li><strong>Aggregation:</strong> Multiple thoughts merge into one (k→1 transformation)</li></ul><ul role=""list""><li><strong>Generation:</strong> Single thought branches into multiple (1→k transformation)</li></ul><ul role=""list""><li><strong>Refinement:</strong> Thought loops back on itself for iterative improvement (1→1 with self-edge)</li></ul><p>These operations are controlled by a Graph of Operations (GoO), which is basically a recipe that tells the system what operations to perform and in what order.</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a81f77d8b9e652c314ff_af108bcc.png""/></div></figure><p>For sorting 128 numbers, <strong>GoT reduces errors by 62% compared to ToT while costing 31% less</strong>. It achieves this by breaking the list into smaller chunks, sorting each chunk separately, then merging them back together level by level, like organizing papers by first sorting them into piles, then combining those piles systematically. The whole process takes about 55 steps across 7 merging levels. In set intersection tasks (finding common elements between sets), GoT gets perfect accuracy while ToT makes 4 or more errors.</p><p>The key technical advantage is the latency-volume tradeoff. Volume means how many previous thoughts contributed to the final answer. GoT keeps latency at O(log k N) while maintaining volume N, meaning all intermediate thoughts can potentially influence the final result. ToT only achieves O(log k N) for both, limiting the amount of information that flows to the final answer. But both algorithms are highly parallel and are fine in terms of how much time they take to run in our experience.</p><p>GoT works best for problems that naturally split into parts that need to be solved separately, then combined. However, GoT adds unnecessary complexity for straightforward problems that don't need multiple paths or merging. It also costs more than simpler methods due to multiple LLM calls. One good application of GoT we can identify is Contract evaluation and analysis, especially for complex ones. We have worked with these, and we tend to run multiple prompts to analyze the contract from different aspects like technical, sales, IP, etc. That is good, but GoT can do that out of the box without multiple prompts and sophisticated architectures.</p><h3>Layer of Thoughts</h3><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1494pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a82077d8b9e652c3152b_cf8da515.png""/></div></figure><p>LoT builds on Graph of Thoughts by organizing the reasoning process into distinct layers. Instead of letting thoughts connect freely like in GoT, LoT structures them hierarchically. Thoughts in layer 1 must be completed before layer 2 begins, and layer 2's output feeds into layer 3. Each layer contains two types of thoughts: layer thoughts that manage the overall process for that level, and option thoughts that explore different solutions within that layer. Each layer acts as a filter with specific criteria. Layer 1 might filter by basic keywords, layer 2 by semantic meaning, and layer 3 by final validation.</p><p>LoT uses several aggregation methods to combine results within a layer. The ""all"" metric requires documents to pass every criterion (like requiring both a password AND fingerprint). The ""at-least-k"" metric is more flexible—documents need to pass at least k criteria out of the total. The ""locally-better"" metric keeps documents that aren't beaten on every criterion by another document. The ""max-count"" metric simply counts how many criteria each document passes and ranks them accordingly.</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a82077d8b9e652c31506_8545694c.png""/></div></figure><p>In testing on Japanese Civil Law retrieval, LoT achieved an F2 score of 0.835, beating the best systems from COLIEE 2024 competition. It maintained precision at 0.838 while keeping recall at 0.839—a balance that simpler approaches couldn't achieve. When they removed the semantic filtering layer, recall jumped to 0.885 but precision crashed to 0.432. One extreme case had the LLM marking 400 articles as relevant when only 2 actually were. The layered filtering prevented these false positives.</p><p>LoT works best when you have clear filtering criteria that can be arranged from broad to specific. Legal document retrieval is perfect first filter by relevant keywords, then by legal concepts, then confirm relevance to the specific query. It struggles when criteria don't have a natural hierarchy or when the problem needs thoughts to influence each other across layers (which GoT handles better through arbitrary connections).</p><h2>CoT vs. Other Methods</h2><p>In this section, we go into a detailed comparison of CoT prompting with other methods, specifically Standard and Tree of Thought Prompting. Evaluating their strengths and limitations offers valuable insights into selecting the most suitable approach for your business applications.</p><h3>CoT vs. Standard Prompting</h3><p>Standard Prompting uses input-output pairs as examples. The pairs are formatted as questions and answers. The model predicts answers based on these pairs. It is limited in handling multi-step reasoning tasks effectively. But is suitable for straightforward tasks, such as single-turn questions. It demands fewer computational resources. It commonly uses single-shot prompts for training and tends to require more data to fine-tune for complex tasks. Standard prompting may not exhibit significant performance improvements with the model scale.</p><p><a href=""https://github.com/FranxYao/chain-of-thought-hub"">CoT prompting</a> involves generating intermediate reasoning steps. These steps precede providing a final answer. It excels at complex reasoning, enabling models to think step by step. It is versatile and applicable to a wide range of tasks requiring intricate reasoning. It requires training on sequences of prompts and efficiently utilizes data for multi-step reasoning. It demonstrates enhanced performance with larger models and, thus, requires more computational power. It excels in complex reasoning benchmarks and tasks that demand multi-step problem-solving.</p><p><strong>Comparison on MAWPS Benchmark:</strong></p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:636pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a82077d8b9e652c3150c_cbb81dd8.png""/></div></figure><p><strong>Comparison on Length Generalization Task:</strong></p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:781pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a82077d8b9e652c3150f_26414907.png""/></div></figure><p>You can choose Standard Prompting for straightforward tasks and CoT Prompting, as the superior choice, for applications requiring deep, multi-step reasoning and interpretability. An open-source repository of data and tools related to CoT reasoning is available on <a href=""https://github.com/OpenBioLink/ThoughtSource"">GitHub</a>. It has datasets on various tasks such as math problems and common sense reasoning. It also has a community forum for discussions.</p><h3>CoT vs. ReAct Prompting</h3><p>CoT forces the model to generate an internal, sequential reasoning trace before providing a final answer. <a href=""https://arxiv.org/abs/2210.03629"">ReAct</a>, on the other hand, synergizes reasoning and acting by generating interleaved reasoning traces and task-specific actions.</p><p>The fundamental difference lies in their interaction with information. CoT operates as a static chain, where it thinks a lot and then gives an output. This makes it highly susceptible to fact hallucination and error propagation, as its reasoning is not grounded. ReAct works differently. The <strong>Action</strong> step allows the LLM to interface with external tools. The subsequent <strong>Observation</strong> grounds the model's next <strong>Thought</strong>, allowing it to create, maintain, and adjust plans based on real-world feedback.</p><p>CoT excels at tasks requiring purely internal, sequential logic with a fixed context, such as multi-step arithmetic. Its linear structure is efficient for problems that do not require external validation.</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a82077d8b9e652c31518_f9f2809f.png""/></div></figure><p>ReAct is designed for dynamic tasks where the environment is interactive or information is incomplete. On knowledge-intensive tasks like HotpotQA, ReAct overcomes CoT's hallucination issues. On decision-making benchmarks like ALFWorld, ReAct outperforms imitation learning methods by an absolute success rate of 34% with only one or two in-context examples, demonstrating its ability to handle long-horizon tasks.</p><p><em>Use CoT for self-contained problems solvable with internal knowledge and linear logic. Use ReAct for dynamic tasks that require grounding in external information, tool use, or interaction with an environment.</em></p><p>You can read our <a href=""https://www.mercity.ai/blog-post/react-prompting-and-react-based-agentic-systems"">Extended Report on React prompting</a> to learn more about it.</p><h3>CoT vs Reflexion</h3><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a82077d8b9e652c31512_1a0c45f5.png""/></div></figure><p>Chain of Thought is a static, single-turn reasoning process. <a href=""https://arxiv.org/abs/2303.11366"">Reflexion</a>, on the other hand, is a dynamic, multi-trial framework for agent improvement through verbal reinforcement. While CoT generates a linear sequence of thoughts to solve a problem in one attempt, Reflexion uses a learning loop. This loop involves three models: an Actor that generates actions, an Evaluator that scores the outcome, and a Self-Reflection model that converts task feedback into natural language summaries.</p><p>The main improvement is the learning mechanism. CoT has no way to learn from failure, a wrong answer in one attempt does not inform the next. Reflexion uses failures by storing feedback in a memory buffer, reinforcing the agent for upcoming trials. This converts environmental feedback scalar values, compiler errors, etc.</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c4a82077d8b9e652c31515_e5959e48.png""/></div></figure><p>Reflexion significantly outperforms static approaches on tasks requiring iterative refinement. On the HumanEval coding benchmark, a Reflexion agent achieves 91% pass@1 accuracy, surpassing a strong GPT-4 baseline (80%). For the AlfWorld sequential decision-making task, it improves success rates by an absolute 22% over a ReAct baseline across 12 trials. On the HotPotQA reasoning task, it boosts accuracy by 20% by learning from initial failures. <em>An ablation study shows that simply providing episodic memory of past trajectories is less effective than the full self-reflection loop, demonstrating that focused, verbal reinforcement is the key contributor.</em></p><p><strong>It is also very important to note that Reflexion can simply be combined with CoT or ReACT to boost the performance of your preexisting prompts.</strong></p><p>Use CoT for problems where a correct line of reasoning can be generated in a single pass. Reflexion is designed for complex tasks that benefit from trial-and-error, allowing an agent to systematically learn from its mistakes across multiple attempts in coding, reasoning, or decision-making.</p><h2>Want to write high-quality production-grade prompts for your LLMs?</h2><p>If you are looking to leverage prompt engineering for your business needs, we can help. We are a team of AI engineers with extensive experience in prompt engineering techniques like Chain-of-Thought, ReAct, etc. <a href=""https://www.mercity.ai/contacts"">Contact us</a> today and let us apply CoT applications to elevate your business.</p></div>"
In-depth guide to fine-tuning LLMs with LoRA and QLoRA,guide-to-fine-tuning-llms-with-lora-and-qlora,640f56f76d313b2faa631c11,65358fa3a1841692bce53406,False,False,Sun Oct 22 2023 21:09:55 GMT+0000 (Coordinated Universal Time),Thu May 22 2025 22:06:25 GMT+0000 (Coordinated Universal Time),Thu May 22 2025 22:08:22 GMT+0000 (Coordinated Universal Time),"<p id="""">Language Models like GPT-4 have become the de facto standard in the NLP industry for building products and applications. These models are capable of performing a plethora of tasks and can easily adapt to new tasks using <a href=""https://www.mercity.ai/blog-post/advanced-prompt-engineering-techniques"" id="""">Prompt Engineering Techniques</a>. But these models also present a massive challenge around training. Massive models like GPT-4 cost millions of dollars to train, hence we use smaller models in production settings.&nbsp;</p><p id="""">‍</p><p id="""">But smaller models on the other hand cannot generalize to multiple tasks, and we end up having multiple models for multiple tasks of multiple users. This is where PEFT techniques like LoRA come in, these techniques allow you to train large models much more efficiently compared to fully finetuning them. In this blog, we will walk through LoRA, QLoRA, and other popular techniques that emerged specifically from LoRA.</p><h2 id="""">What is PEFT Finetuning?</h2><p id=""""><a href=""https://www.mercity.ai/blog-post/fine-tuning-llms-using-peft-and-lora"" id="""">PEFT Finetuning</a> is Parameter Efficient Fine Tuning, a set of fine-tuning techniques that allows you to fine-tune and train models much more efficiently than normal training. PEFT techniques usually work by reducing the number of trainable parameters in a neural network. The most famous and in-use PEFT techniques are Prefix Tuning, P-tuning, LoRA, etc. LoRA is perhaps the most used one. LoRA also has many variants like QLoRA and LongLoRA, which have their own applications.</p><h3 id="""">Why use PEFT Finetuning?</h3><p id="""">There are many reasons to use PEFT techniques, they have become the go-to way to finetune LLMs and other models. But here are some reasons why even enterprises and large businesses like to use these approaches.</p><h4 id="""">Saves Time</h4><p id="""">As the number of trainable parameters decreases, you have to spend less time on training. But that’s only one part of it. With less trainable parameters, you can train models much faster, and as a result test models much faster too. With more time on your hands, you can spend it on testing different models, different datasets different techniques, and whatnot.</p><p id="""">‍</p><p id="""">Also, with more time you can train your models for much longer periods of time leading to a much lower loss, along with an increased batch size as PEFT techniques are heavily optimized for memory usage.</p><h4 id="""">Saves Money</h4><p id="""">This goes without saying but PEFT can save you a ton of money on computing costs. As mentioned, because of heavy memory optimizations, you don’t need to rent instances with high amounts of VRAM, as you can fit bigger batches in smaller VRAM. This saves money on the compute and allows you to train on much bigger datasets leveraging the advantage of fitting in larger batch sizes.</p><p id="""">‍</p><h4 id="""">Easily build Multi-Tenancy architecture services</h4><p id="""">As said, because LLMs have become so large and complicated to serve and handle, it’s almost impossible to train specialized models for users. If you have multiple users, you can either take on the complicated task of finetuning a new model every time a new user comes in, or you can just finetune the same model on the new data for the new user. Both approaches have their own set of issues, if you finetune a new model for every user, it will lead to better accuracy but then you have to handle massive models, load them into memory, store them, process them, etc, it’s an architecture hell and can cause big issues if you make a small mistake. Training the same model for all users is much easier, but then the model accuracy drops significantly.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/65358e23a1841692bce3fdde_pZkMHXa1_5JDK-50qaL33QmXp3A_QHlenbmoeLOLZjeAwjV_4JMC258-7ft_nQP0SWwDiGjpdWAwzXoHdH9e24xopk6UPhjVfKqcpGpgmDDJ5KVju6bhb7i1z7qa8SO3MW2RPjQalY3yC66Yx3arBco.png"" id="""" width=""auto"" height=""auto"" alt="""" loading=""auto""></div></figure><p id="""">‍</p><p id="""">PEFT finetuning on the other hand takes the best of both worlds and lets you build small <strong id="""">adapters</strong> that you can pair with models and get customized results. These adapters can be finetuned for specific datasets or specific users. These adapters are very small, 6MB-8MB, and you only need to apply these adapters to the large model, which is much faster to do in a production environment.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/65358e237ecc65ca6bff3e86_pIAt6GubybV8S1uucMNhmD6r3YgBFMH10yfupxAN--0sMvjakr6kerdmhrxFpzh4NySfz83mDUHEXQXSVRCZ9cYGUJkraPYJVxNdq63WzpO30yHRr7jNQBfh9fJ0oxD7jzVPUTHZpQY536m2xoLn_D4.png"" id="""" width=""auto"" height=""auto"" alt="""" loading=""auto""></div></figure><h2 id="""">LoRA Finetuning</h2><p id="""">LoRA is the most popular and perhaps the most used PEFT technique, but was released back in 2021 in <a href=""https://arxiv.org/abs/2012.13255"" id="""">this</a> paper. LoRA is more of an adapter approach, where it introduces new parameters into the model to train the model through these new parameters. The trick is in how the new params are introduced and merged back into the model, <strong id="""">without</strong> increasing the total number of params in the model.</p><h3 id="""">How LoRA works?</h3><p id="""">As mentioned before, LoRA is an adapter-based approach, but new parameters are added only for the training step, they are not introduced as a part of the model. This keeps the model size completely the same and still offers the flexibility of parameter-efficient finetuning. Here’s a more detailed explanation of how it works.</p><p id="""">‍</p><p id="""">LoRA works by breaking down the weight update matrix into smaller matrices and using them to train the model. Take a look at the diagram below, the ΔW<sub id="""">AxB </sub>is the weight update matrix, the matrix of <em id="""">learned</em> changes from backpropagation, this is the same size as the number of parameters we need to update to finetune our model. This matrix, or any matrix, can be represented as a set of smaller matrices, presented here as A and B with <em id="""">r</em> as their rank. The <em id="""">r</em> parameter controls the size of the smaller matrices.</p><p id="""">‍</p><p id="""">These smaller matrices can then be used to train the model using normal backpropagation but updating the parameters in the smaller matrices rather than updating directly in the model. We basically learn the ΔW through the smaller matrices. These smaller matrices can then be multiplied together to get back the original matrix. As these matrices are much smaller, this process uses fewer parameters and as a result much fewer computation resources. This also results in smaller checkpoints as you don’t have to store the whole model, but just the smaller matrices.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1219px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1219px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/65358e2365a382945ff60dd2_oqMjFlY7MeAQFHy2IvqYbKYLaAHWFt2mxQXorwdv0ohg-OhpmF5X5MltxKvE6CgXW9OAyUFplEMKFxGR01DBUQEivGUX44l4tzKQoQWBniwVz5bGR8awfBBJhlOwJ-tqQXPdoAd86E2Ys-GBSCPvdmk.png"" id="""" width=""auto"" height=""auto"" alt="""" loading=""auto""></div></figure><p id="""">‍</p><h3 id="""">LoRA finetuning with HuggingFace</h3><p id="""">To implement LoRA finetuning with HuggingFace, you need to use the <a href=""https://pypi.org/project/peft/"" id="""">PEFT library</a> to inject the LoRA adapters into the model and use them as the update matrices.</p><p id="""">‍</p><div data-rt-embed-type='true'><pre>
<code class=""language-py"">
from transformers import AutoModelForCausalLM
from peft import get_peft_config, get_peft_model, LoraConfig, TaskType

model = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map=""auto"", trust_remote_code=True) # load the model

peft_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM, inference_mode=False, r=32, lora_alpha=16, lora_dropout=0.1,
    target_modules=['query_key_value'] # optional, you can target specific layers using this
) # create LoRA config for the finetuning


model = get_peft_model(model, peft_config) # create a model ready for LoRA finetuning

model.print_trainable_parameters() 
# trainable params: 9,437,184 || all params: 6,931,162,432 || trainable%: 0.13615586263611604
</code>
</pre></div><p id="""">‍</p><p id="""">Once this is done, you can train the model as you would normally do. But this time it will take much less time and compute resources as it normally does.</p><h2 id="""">QLoRA Finetuning</h2><p id="""">QLoRA is a finetuning technique that combines a high-precision computing technique with a low-precision storage method. This helps keep the model size small while still making sure the model is still highly performant and accurate.</p><h3 id="""">How does QLoRA work?</h3><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/65358e232cf7056045158369_kAizskrUxOXNivr5h5E4TCPBYNX_J8JTGZKYgd7lVTFSGPKHrZKvQZp1faQG5KfNmIsf_G-m2QzJf6Z5wuaPpURbeXo4VioqFcL8aNHccc3Pzwzlf8QLXI9maqVYxgBnRBOiUYDxKRbUex2iXTODbMA.png"" id="""" width=""auto"" height=""auto"" alt="""" loading=""auto""></div></figure><p id="""">QLoRA works by introducing 3 new concepts that help to reduce memory while keeping the same quality performance. These are <strong id="""">4-bit Normal Float</strong>, <strong id="""">Double Quantization,</strong> and <strong id="""">Paged Optimizers</strong>. Let’s talk about these 3 very important concepts in detail.</p><h4 id="""">4-Bit Normal Float</h4><p id="""">4-bit NormalFloat or NF is a new <em id="""">information-theoretically optimal</em> data type built on top of Quantile Quantization techniques. 4-Bit NF works by estimating the 2<sup id="""">k</sup> + 1 (k is the number of bits) quantiles in a 0 to 1 distribution, then normalizing its values into the [-1, 1] range. Once we have that, we can also normalize our neural network weights into the [-1, 1] range and then quantize into the quantiles we got from step 2.</p><p id="""">‍</p><p id="""">Here’s an example of what quantile quantization looks like</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1125px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1125px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/65358e23515cf733600388f4_7xYu7pLVD0_QDWRkB3GtH8WC4HZIyQBrfYpeESnmcLUQltE1FpKFno3USa6R6NhgHD11UOTssJbikqIa2hSB4P_2okBSyVO8pxxmDZbJ40LKbp323ayLshQ4XE59BYV_2Q5lYycQFrnKfFWS_CmCeKQ.png"" id="""" width=""auto"" height=""auto"" alt="""" loading=""auto""></div></figure><p id="""">You can see that there are “buckets” or “bins” of data where the data is quantized. Both the numbers 2 and 3 fall into the same quantile, 2. This quantization process allows you to use fewer numbers by “rounding off” to the nearest quantile.</p><h4 id="""">Double Dequantization</h4><p id="""">Double quantization is the process of quantizing the quantization constants used during the quantization process in the 4-bit NF quantization. This is not important, but can save 0.5 bits per parameter on average, as mentioned in the paper. This helps with the process because QLoRA uses Block-wise k-bit Quantization, meaning instead of quantizing all the weights together, we create multiple chunks or blocks of weights which are then quantized independently.</p><p id="""">‍</p><p id="""">This block-wise method leads to multiple quantization constants being created, which then once again can be quantized to save additional space. This is okay because the number of quantization constants is low and hence not a lot of computing or storage is required.</p><h4 id="""">Error Reduction with LoRA Tuning</h4><p id="""">As shown before, quantile quantization creates buckets or bins for a large range of numbers. This process leads to placing multiple different numbers into the same buckets, for example, two numbers 2 and 3 can be converted into 3 during the quantization process. This leads to an error of 1 being introduced during the dequantization of weights.</p><p id="""">‍</p><p id="""">Here is a graphical representation of these errors on a larger distribution of weights of a neural network.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1361px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1361px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/65358e23a61fd7828ed079ca_MZdJdD5ihusfmdnu1iO-YUGQ0KaM4PZXWCFu5z4hOi6k8bCR41fMHVU-JNKu11M9t9BOrVq-cYwZysc0xwIt5LV5SGafLhggn-mvA_p3aPB9bc2Yzjpx0qALyVguZ23AEWYXnx5LYvyxIWNxwPMH4dc.png"" id="""" width=""auto"" height=""auto"" alt="""" loading=""auto""></div></figure><p id="""">‍</p><p id="""">This error is why QLoRA is more of a finetuning mechanism than a standalone quantization strategy. Although it can be used for 4-bit inference. When fine-tuning with QLoRA we use the <a href=""https://www.mercity.ai/blog-post/fine-tuning-llms-using-peft-and-lora#lora---low-rank-adaptation"" id="""">LoRA tuning mechanism</a> of creating 2 smaller weight update matrices and then using them to update the weights of the neural network. Here, we keep the LoRA matrices in a higher precision format, like <a href=""https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus"" id="""">brain float 16</a> or float 16 and during the backpropagation and forward pass the weights of the network are also de-quantized. So the actual training is still happening in higher precision formats, but the storage is still in lower precision. This causes quantization errors to emerge, but the model training itself is able to compensate for these inefficiencies in the quantization process.</p><p id="""">‍</p><p id="""">Hence, the LoRA training with higher precision helps the model learn about and reduce the quantization errors.</p><h3 id="""">How is QLoRA different from LoRA?</h3><p id="""">QLoRA and LoRA both are finetuning techniques, but QLoRA uses LoRA as an accessory to fix the errors introduced during the quantization errors. LoRA in itself is more of a standalone finetuning technique.</p><h3 id="""">QLoRA finetuning with HuggingFace</h3><p id="""">To do QLoRA finetuning with HuggingFace, you need to install both the <a href=""https://pypi.org/project/bitsandbytes/"" id="""">BitsandBytes library</a> and the PEFT library. The BitsandBytes library takes care of the 4-bit quantization and the whole low-precision storage and high-precision compute part. The PEFT library will be used for the LoRA finetuning part.</p><p id="""">‍</p><div data-rt-embed-type='true'><pre>
<code class=""language-py"">
import torch
from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

model_id = ""EleutherAI/gpt-neox-20b""
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type=""nf4"",
    bnb_4bit_compute_dtype=torch.bfloat16
) # setup bits and bytes config

model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={"""":0})

model.gradient_checkpointing_enable()
model = prepare_model_for_kbit_training(model) # prepares the whole model for kbit training

config = LoraConfig(
    r=8, 
    lora_alpha=32, 
    target_modules=[""query_key_value""], 
    lora_dropout=0.05, 
    bias=""none"", 
    task_type=""CAUSAL_LM""
)

model = get_peft_model(model, config) # Now you get a model ready for QLoRA training
</code>
</pre></div><p id="""">‍</p><p id="""">And then once again you can move to normal training using the HF trainer. Check out this <a href=""https://colab.research.google.com/drive/1VoYNfYDKcKRQRor98Zbf2-9VQTtGJ24k?usp=sharing#scrollTo=FCc64bfnmd3j"" id="""">colab notebook</a> as a guide for QLoRA training.</p><h2 id="""">QLoRA vs Standard Finetuning</h2><p id="""">In the paper, researchers provide a very detailed comparison between QLoRA, LoRA, and Full Finetuning of a network.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1190px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1190px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/65358e23a61fd7828ed079d3_iFgh5dLJlkXuia76nlzEeGFJaHuZk0d27_qMY4dIPcigc3lBDNZnnyybIXrjzCFDJ4aOLC5OU5eUa3m5anHnq7F70bNGG8ql9DMFjaHT2nlgBAqqft5ojwrnILo1Q1UfdbtGqth8lFumRthtk281fvA.png"" id="""" width=""auto"" height=""auto"" alt="""" loading=""auto""></div></figure><p id="""">As you can see in the above table, there is no loss of performance whatsoever in the T5 model family upon training with QLoRA and even with Double Quantization, we don’t see any major differences. One significant difference is the number of LoRA adapters required. In the paper, the authors mention that they needed more LoRA adapters for QLoRA finetuning, compared to normal LoRA finetuning. The authors suggest applying the LoRA adapters on all the linear transformer blocks along with the query, key, and value layers.</p><p id="""">‍</p><p id="""">Even for the much bigger language models, performance remains the same:</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1186px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1186px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/65358e233b0593b2e2dce709_-cLgG3H7dbGRuFqh3m0OHfHHqnXNC9qoGEk7ANjagwAiex64QT1ayNQ86Jcfa1qOjuSqSy8BzVVdsBXyKDNMGlm_X1hCnhyNnCpY_W6ycUn2UYvYT0kgvNlcmnXulLvRbyGDn64YoexaSMMYTr4-7Mk.png"" id="""" width=""auto"" height=""auto"" alt="""" loading=""auto""></div></figure><p id="""">Hence the authors trained a model on top of the base LLaMA models, <a href=""https://huggingface.co/timdettmers/guanaco-65b"" id="""">Guanaco</a>. This is a 33 billion parameter model, trained on the OASST1 dataset. At the time of its release, it became a state-of-the-art model and achieved <strong id="""">99.3%</strong> performance relative to ChatGPT. Even though other models have lesser models like Vicuna 13B, and Guanaco33B because of the use of 4-bit precision used less memory than the 13B model.</p><p id="""">‍</p><h2 id="""">Other variants of LoRA finetuning</h2><h3 id="""">QA LoRA</h3><p id="""">QA LoRA is another fine-tuning technique built on top of QLoRA, introduced in <a href=""https://arxiv.org/abs/2310.03270"" id="""">this</a> paper. QALoRA was mainly released for finetuning diffusion models, but can easily be generalized for training any type of models, just like LoRA.</p><p id="""">‍</p><p id="""">The difference between QLoRA and QALoRA is that QALoRA is <em id="""">quantization aware</em> meaning the weights of the LoRA adapters are also quantized along with the weights of the model during the finetuning process. This helps in more efficient training as there is no need for the conversion step to update the models during the backpropagation process.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1332px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1332px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/65358e23d7d446708a1eec24_mR-H3Wh2jk0zRKaY7FQCRQANOZNaQNV62RwDzf28bcUQBHHg5ei75Q0M1lHLqL-VTAjiWCHGhzzwUhFbZYso-uVZ-sTabL78mpR3eEAmLAZNEmthk8n9oDaCPKdf4r0KaZjLPs3sbxBn-fF0r7Ggfuk.png"" id="""" width=""auto"" height=""auto"" alt="""" loading=""auto""></div></figure><p id="""">‍</p><h3 id="""">LongLoRA</h3><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/65358e2317daafaf293fb79c_wSYLsK_nzksoc9Wzl4UjQW68U8F451DuQCVh7_TzN_icR9iRnzOx0v26S0rEcMgbDgYprqBBcoU8FQhqGo9j-9w9WHr-O04EBoukXwwVXAa3GFkm4te8ElCs4nHEEh1T-xurHGQRQiVzfLjCz8b0VD8.png"" id="""" width=""auto"" height=""auto"" alt="""" loading=""auto""></div></figure><p id="""">‍</p><p id=""""><a href=""https://arxiv.org/abs/2309.12307"" id="""">LongLoRA</a> is yet another variation of the LoRA finetuning technique, but this technique is specifically for training longer context models. LongLoRA works by using something called <strong id="""">SHIFT SHORT ATTENTION</strong>. This method creates chunks or groups of the tokens, in which the attention is calculated independently. This method allows LongLoRA to scale to a much longer context because of the distributed workload.</p><p id="""">‍</p><p id="""">Along with this, the authors show the need to use the LoRA on the normalization and the embedding layers too for this method to work properly.</p><p id="""">‍</p><h3 id="""">AdaLoRA</h3><p id="""">In traditional LoRA, you fix a rank r for every layer. But different layers contribute differently to model performance.&nbsp;</p><p id="""">What <a href=""https://arxiv.org/abs/2303.10512"" id="""">AdaLoRA </a>(Adaptive LoRA) does is dynamically allocate the parameter budget i.e. rank to weight matrices based on their importance during training. This avoids the uniform rank allocation of LoRA, which can be suboptimal for layers of varying significance.&nbsp; Important layers (usually the top transformer layers) receive higher ranks, while less critical ones are pruned.</p><p id="""">It does so by mimicking the SVD (Singular value decomposition) algorithm for the weight matrices and updating the weights through their singular values:</p><figure class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:637px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""637px""><div><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/682f9f271dfebe56a0445a06_AD_4nXf5OKq5Se4xLnekVb5kZrxEK298VQiyXn_CTw6pqd7Q31KoAq1o-8F2kwncU9jbeSC-w5_nFtCyqq_l-xPwGA1_LyMWJBXR1SbGbTvIeEbXOG2IcJsyoK3V2x88J8RXyfi8002iDw.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p>‍</p><h3 id="""">DoRA</h3><p id=""""><a href=""https://arxiv.org/pdf/2402.09353"" id="""">DoRA </a>decomposes high-rank LoRA layers into structured single-rank components, allowing for dynamic pruning of parameter budget based on their importance to specific tasks during training, which makes the most of the limited parameter budget. It introduces r′ pairs of single-rank matrices, each acting as a LoRA component. During training, DoRA evaluates the contribution of each component to the overall performance and prunes components with smaller contributions</p><figure class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1196px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1196px""><div><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/682f9f270f51d8769902a253_AD_4nXe557d4mk2wd2AUnZITtRY6PcjRtnLzKQyj8HX7EBHfMUIblwR4b4UTCZHa6OC9hiDUjehweOJejBytIw_KydBH2Ca3OMIda3CfHphTc56BPiDh3RdUpUsm0VUEiB5t9BqS8kyz.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">Instead of using a standard LoRA layer, DoRA reinterprets it as:</p><figure class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1046px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1046px""><div><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/682f9f27cc46e85b462743ad_AD_4nXc41tOO91LNa9L7XwHMl_2MzsU1-LAi4kUNx18N6Jr261oEwRVATpTTJcOgXnf_TQjj5moXLVR7BXVk5BfQYQAASi0QuTvUUs5HHOaTttwSkYZWxpdol5KIf6mU25tk4095c7Allg.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">Here r′ represents the number of LoRA components. A LoRA component is a triplet of A<sub id="""">i</sub>, B<sub id="""">i</sub>, and c<sub id="""">i</sub>, where A<sub id="""">i</sub> and B<sub id="""">i</sub> are single-rank matrices, shaped as d×1 and 1×d respectively. c<sub id="""">i</sub> is a scalar used for pruning the component, it is set to 0 if the component is to be pruned.&nbsp;</p><p id="""">DoRA can directly be integrated by the following modifications in the LoRAconfig:</p><p>‍</p><div data-rt-embed-type='true'><pre>
<code class=""language-py"">

config = LoraConfig(
    r=8, 
    lora_alpha=32, 
    target_modules=[""query_key_value""], 
    lora_dropout=0.05, 
    bias=""none"", 
    task_type=""CAUSAL_LM"",
    use_dora=True # simply pass True here to use DoRA
)

model = get_peft_model(model, config) # Now you get a model ready for QLoRA training
</code>
</pre></div><figure class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1325px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1325px""><div><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/682f9f27b92cc05d5f1d923c_AD_4nXc0Fa-KzkKwd7i273CUbdvH-gO434ojJq89CJOzJK_B-1dfcunK2eeaRZKUA34mLGhOoVxa_uaItndXPy6YPNO40GryYXRn4AbvHLRJ2q7_yGzsLBxlkMwFuvxFtGLbZdr5Xresew.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p>‍</p><h2 id="""">Modern Quantization Strategies (05-2025)</h2><p>Here are some modern quantization strategies that tend to work really well.</p><h3 id="""">AWQ (Activation-aware Weight Quantization)</h3><p id=""""><a href=""https://arxiv.org/pdf/2306.00978"" id="""">AWQ</a> is a post-training quantization (PTQ) technique designed to compress large language models (LLMs) into low-bit formats—specifically 4-bit—without compromising performance. Unlike traditional quantization methods that treat all weights uniformly, AWQ intelligently identifies and preserves a small subset of <em id="""">salient</em> weights that are critical for maintaining model accuracy.&nbsp;</p><p id="""">What sets AWQ apart is its approach, rather than analyzing weights directly, it evaluates activation distributions to determine which weight channels are most important. This method eliminates the need for backpropagation or reconstruction, making the process both efficient and highly generalizable across different domains and modalities.</p><p id="""">AWQ consistently delivers strong results across various benchmarks, including instruction tuned and multimodal LLMs. For example, it enables efficient 4-bit inference on large models like LLaMA-2 70B, achieving over 3× speedup compared to FP16 implementations—on both desktop and mobile GPUs.</p><p>‍</p><h3 id="""">AffineQuant</h3><p id=""""><a href=""https://arxiv.org/html/2403.12544v1"" id="""">AffineQuant</a> introduces a novel approach to post-training quantization (PTQ) by leveraging affine transformations to optimize the distributions of both weights and activations. Unlike conventional methods that rely on simple scaling, AffineQuant uses transformation matrices to more accurately align pre- and post-quantization outputs, effectively minimizing quantization errors.</p><p id="""">By applying an affine transformation matrix, it adjusts the data distribution to better fit the quantization function, reducing quantization errors. To ensure that the transformation is reversible, AffineQuant employs a gradual mask optimization method, initially focusing on diagonal elements and gradually extending to other elements (ensuring invertability). This approach aligns with the Levy-Desplanques theorem, guaranteeing the invertibility of the transformation.</p><p>‍</p><figure class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:839px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""839px""><div><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/682f9f27e7a75d2528741bcb_AD_4nXdC0zY3zTJnXsuoNbGX8QC3a8vn1U4b1r8UJ3Zec42Lu0D5b3j2uQTwzp8krhvewwp6LYLqr6tnQ5egEdqBVWlncPbRRvoadQXzPBN3bEPj8Wn7advMgYKhbBo0FjAg-UdxqzLO.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">AffineQuant sets new standards for PTQ in LLMs. For instance, it achieves a C4 perplexity of 15.76 on the LLaMA2-7B model using W4A4 quantization—surpassing previous methods like OmniQuant. On zero-shot tasks, it reaches an impressive average accuracy of 58.61% with 4/4-bit quantization on LLaMA-30B, marking a notable leap in performance over existing techniques.</p><h3 id="""">Dynamic 4-bit Quantization</h3><p id=""""><a href=""https://unsloth.ai/blog/dynamic-4bit"" id="""">Dynamic 4-Bit Quantization</a>, as implemented by <a href=""https://huggingface.co/collections/unsloth/unsloth-4-bit-dynamic-quants-67503bb873f89e15276c44e7"" id="""">Unsloth</a>, offers a flexible approach to quantizing LLMs. Unlike static quantization methods, dynamic quantization adjusts to the model's precision levels on the fly during runtime, ensuring better performance and resource usage.</p><p id="""">Built on top of BitsandBytes (BnB) 4-bit quantization, this dynamic strategy offers visible accuracy improvements while using &lt;<strong id="""">10%</strong> more VRAM compared to standard BnB 4-bit implementations</p><p>‍</p><p id="""">Unsloth's dynamic 4-bit quantization has shown to provide accurate and reliable results, often outperforming standard 4-bit quantization methods. For instance, while standard quantization may degrade model performance, Unsloth's approach maintains high accuracy, reducing memory usage significantly. For example, compressing Llama 3.2 Vision (11B) from 20GB to just 6.5GB while maintaining its high accuracy.</p><p>‍</p><h2 id="""">How finetuning improves model performance for your business</h2><p id="""">If you are considering finetuning a model for your business, you are probably correct. But data becomes an important part of the process, take a look at our <a href=""https://www.mercity.ai/blog-post/using-chatgpt-to-build-synthetic-datasets"" id="""">synthetic data generation guide</a>. However, there are many ways fine-tuned models can improve the performance of your business by providing customization. Finetuning the models helps you customize the model to your specific needs and knowledge. You can use an RAG pipeline to customize the model, but sometimes the knowledge is so vast that embedding and similarity searches are not enough, that’s where customization via finetuning comes in.</p><p id="""">‍</p><p id="""">If you need finetuned models for your business, <a href=""https://www.mercity.ai/contacts"" id="""">reach out</a>, don’t be shy. We have a ton of experience in finetuning all kinds of models, from small T5 models to massive models like Falcon180B. We have seen it all.</p><p id="""">‍</p>",https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/65358e7d97cd72e210afa0bd_lora-qlora.png,Pranav Patel,PEFT Finetuning,In this blog we provide detailed explanation of how QLoRA works and how you can use it in hugging face to finetune your models. We also touch on the lastest quantization and LoRA based training methods!,False,"<div class=""rich-text w-richtext""><p>Language Models like GPT-4 have become the de facto standard in the NLP industry for building products and applications. These models are capable of performing a plethora of tasks and can easily adapt to new tasks using <a href=""https://www.mercity.ai/blog-post/advanced-prompt-engineering-techniques"">Prompt Engineering Techniques</a>. But these models also present a massive challenge around training. Massive models like GPT-4 cost millions of dollars to train, hence we use smaller models in production settings. </p><p>‍</p><p>But smaller models on the other hand cannot generalize to multiple tasks, and we end up having multiple models for multiple tasks of multiple users. This is where PEFT techniques like LoRA come in, these techniques allow you to train large models much more efficiently compared to fully finetuning them. In this blog, we will walk through LoRA, QLoRA, and other popular techniques that emerged specifically from LoRA.</p><h2>What is PEFT Finetuning?</h2><p><a href=""https://www.mercity.ai/blog-post/fine-tuning-llms-using-peft-and-lora"">PEFT Finetuning</a> is Parameter Efficient Fine Tuning, a set of fine-tuning techniques that allows you to fine-tune and train models much more efficiently than normal training. PEFT techniques usually work by reducing the number of trainable parameters in a neural network. The most famous and in-use PEFT techniques are Prefix Tuning, P-tuning, LoRA, etc. LoRA is perhaps the most used one. LoRA also has many variants like QLoRA and LongLoRA, which have their own applications.</p><h3>Why use PEFT Finetuning?</h3><p>There are many reasons to use PEFT techniques, they have become the go-to way to finetune LLMs and other models. But here are some reasons why even enterprises and large businesses like to use these approaches.</p><h4>Saves Time</h4><p>As the number of trainable parameters decreases, you have to spend less time on training. But that’s only one part of it. With less trainable parameters, you can train models much faster, and as a result test models much faster too. With more time on your hands, you can spend it on testing different models, different datasets different techniques, and whatnot.</p><p>‍</p><p>Also, with more time you can train your models for much longer periods of time leading to a much lower loss, along with an increased batch size as PEFT techniques are heavily optimized for memory usage.</p><h4>Saves Money</h4><p>This goes without saying but PEFT can save you a ton of money on computing costs. As mentioned, because of heavy memory optimizations, you don’t need to rent instances with high amounts of VRAM, as you can fit bigger batches in smaller VRAM. This saves money on the compute and allows you to train on much bigger datasets leveraging the advantage of fitting in larger batch sizes.</p><p>‍</p><h4>Easily build Multi-Tenancy architecture services</h4><p>As said, because LLMs have become so large and complicated to serve and handle, it’s almost impossible to train specialized models for users. If you have multiple users, you can either take on the complicated task of finetuning a new model every time a new user comes in, or you can just finetune the same model on the new data for the new user. Both approaches have their own set of issues, if you finetune a new model for every user, it will lead to better accuracy but then you have to handle massive models, load them into memory, store them, process them, etc, it’s an architecture hell and can cause big issues if you make a small mistake. Training the same model for all users is much easier, but then the model accuracy drops significantly.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/65358e23a1841692bce3fdde_pZkMHXa1_5JDK-50qaL33QmXp3A_QHlenbmoeLOLZjeAwjV_4JMC258-7ft_nQP0SWwDiGjpdWAwzXoHdH9e24xopk6UPhjVfKqcpGpgmDDJ5KVju6bhb7i1z7qa8SO3MW2RPjQalY3yC66Yx3arBco.png""/></div></figure><p>‍</p><p>PEFT finetuning on the other hand takes the best of both worlds and lets you build small <strong>adapters</strong> that you can pair with models and get customized results. These adapters can be finetuned for specific datasets or specific users. These adapters are very small, 6MB-8MB, and you only need to apply these adapters to the large model, which is much faster to do in a production environment.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/65358e237ecc65ca6bff3e86_pIAt6GubybV8S1uucMNhmD6r3YgBFMH10yfupxAN--0sMvjakr6kerdmhrxFpzh4NySfz83mDUHEXQXSVRCZ9cYGUJkraPYJVxNdq63WzpO30yHRr7jNQBfh9fJ0oxD7jzVPUTHZpQY536m2xoLn_D4.png""/></div></figure><h2>LoRA Finetuning</h2><p>LoRA is the most popular and perhaps the most used PEFT technique, but was released back in 2021 in <a href=""https://arxiv.org/abs/2012.13255"">this</a> paper. LoRA is more of an adapter approach, where it introduces new parameters into the model to train the model through these new parameters. The trick is in how the new params are introduced and merged back into the model, <strong>without</strong> increasing the total number of params in the model.</p><h3>How LoRA works?</h3><p>As mentioned before, LoRA is an adapter-based approach, but new parameters are added only for the training step, they are not introduced as a part of the model. This keeps the model size completely the same and still offers the flexibility of parameter-efficient finetuning. Here’s a more detailed explanation of how it works.</p><p>‍</p><p>LoRA works by breaking down the weight update matrix into smaller matrices and using them to train the model. Take a look at the diagram below, the ΔW<sub>AxB </sub>is the weight update matrix, the matrix of <em>learned</em> changes from backpropagation, this is the same size as the number of parameters we need to update to finetune our model. This matrix, or any matrix, can be represented as a set of smaller matrices, presented here as A and B with <em>r</em> as their rank. The <em>r</em> parameter controls the size of the smaller matrices.</p><p>‍</p><p>These smaller matrices can then be used to train the model using normal backpropagation but updating the parameters in the smaller matrices rather than updating directly in the model. We basically learn the ΔW through the smaller matrices. These smaller matrices can then be multiplied together to get back the original matrix. As these matrices are much smaller, this process uses fewer parameters and as a result much fewer computation resources. This also results in smaller checkpoints as you don’t have to store the whole model, but just the smaller matrices.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1219pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/65358e2365a382945ff60dd2_oqMjFlY7MeAQFHy2IvqYbKYLaAHWFt2mxQXorwdv0ohg-OhpmF5X5MltxKvE6CgXW9OAyUFplEMKFxGR01DBUQEivGUX44l4tzKQoQWBniwVz5bGR8awfBBJhlOwJ-tqQXPdoAd86E2Ys-GBSCPvdmk.png""/></div></figure><p>‍</p><h3>LoRA finetuning with HuggingFace</h3><p>To implement LoRA finetuning with HuggingFace, you need to use the <a href=""https://pypi.org/project/peft/"">PEFT library</a> to inject the LoRA adapters into the model and use them as the update matrices.</p><p>‍</p><div class=""w-embed""><pre>
<code class=""language-py"">
from transformers import AutoModelForCausalLM
from peft import get_peft_config, get_peft_model, LoraConfig, TaskType

model = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map=""auto"", trust_remote_code=True) # load the model

peft_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM, inference_mode=False, r=32, lora_alpha=16, lora_dropout=0.1,
    target_modules=['query_key_value'] # optional, you can target specific layers using this
) # create LoRA config for the finetuning


model = get_peft_model(model, peft_config) # create a model ready for LoRA finetuning

model.print_trainable_parameters() 
# trainable params: 9,437,184 || all params: 6,931,162,432 || trainable%: 0.13615586263611604
</code>
</pre></div><p>‍</p><p>Once this is done, you can train the model as you would normally do. But this time it will take much less time and compute resources as it normally does.</p><h2>QLoRA Finetuning</h2><p>QLoRA is a finetuning technique that combines a high-precision computing technique with a low-precision storage method. This helps keep the model size small while still making sure the model is still highly performant and accurate.</p><h3>How does QLoRA work?</h3><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/65358e232cf7056045158369_kAizskrUxOXNivr5h5E4TCPBYNX_J8JTGZKYgd7lVTFSGPKHrZKvQZp1faQG5KfNmIsf_G-m2QzJf6Z5wuaPpURbeXo4VioqFcL8aNHccc3Pzwzlf8QLXI9maqVYxgBnRBOiUYDxKRbUex2iXTODbMA.png""/></div></figure><p>QLoRA works by introducing 3 new concepts that help to reduce memory while keeping the same quality performance. These are <strong>4-bit Normal Float</strong>, <strong>Double Quantization,</strong> and <strong>Paged Optimizers</strong>. Let’s talk about these 3 very important concepts in detail.</p><h4>4-Bit Normal Float</h4><p>4-bit NormalFloat or NF is a new <em>information-theoretically optimal</em> data type built on top of Quantile Quantization techniques. 4-Bit NF works by estimating the 2<sup>k</sup> + 1 (k is the number of bits) quantiles in a 0 to 1 distribution, then normalizing its values into the [-1, 1] range. Once we have that, we can also normalize our neural network weights into the [-1, 1] range and then quantize into the quantiles we got from step 2.</p><p>‍</p><p>Here’s an example of what quantile quantization looks like</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1125pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/65358e23515cf733600388f4_7xYu7pLVD0_QDWRkB3GtH8WC4HZIyQBrfYpeESnmcLUQltE1FpKFno3USa6R6NhgHD11UOTssJbikqIa2hSB4P_2okBSyVO8pxxmDZbJ40LKbp323ayLshQ4XE59BYV_2Q5lYycQFrnKfFWS_CmCeKQ.png""/></div></figure><p>You can see that there are “buckets” or “bins” of data where the data is quantized. Both the numbers 2 and 3 fall into the same quantile, 2. This quantization process allows you to use fewer numbers by “rounding off” to the nearest quantile.</p><h4>Double Dequantization</h4><p>Double quantization is the process of quantizing the quantization constants used during the quantization process in the 4-bit NF quantization. This is not important, but can save 0.5 bits per parameter on average, as mentioned in the paper. This helps with the process because QLoRA uses Block-wise k-bit Quantization, meaning instead of quantizing all the weights together, we create multiple chunks or blocks of weights which are then quantized independently.</p><p>‍</p><p>This block-wise method leads to multiple quantization constants being created, which then once again can be quantized to save additional space. This is okay because the number of quantization constants is low and hence not a lot of computing or storage is required.</p><h4>Error Reduction with LoRA Tuning</h4><p>As shown before, quantile quantization creates buckets or bins for a large range of numbers. This process leads to placing multiple different numbers into the same buckets, for example, two numbers 2 and 3 can be converted into 3 during the quantization process. This leads to an error of 1 being introduced during the dequantization of weights.</p><p>‍</p><p>Here is a graphical representation of these errors on a larger distribution of weights of a neural network.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1361pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/65358e23a61fd7828ed079ca_MZdJdD5ihusfmdnu1iO-YUGQ0KaM4PZXWCFu5z4hOi6k8bCR41fMHVU-JNKu11M9t9BOrVq-cYwZysc0xwIt5LV5SGafLhggn-mvA_p3aPB9bc2Yzjpx0qALyVguZ23AEWYXnx5LYvyxIWNxwPMH4dc.png""/></div></figure><p>‍</p><p>This error is why QLoRA is more of a finetuning mechanism than a standalone quantization strategy. Although it can be used for 4-bit inference. When fine-tuning with QLoRA we use the <a href=""https://www.mercity.ai/blog-post/fine-tuning-llms-using-peft-and-lora#lora---low-rank-adaptation"">LoRA tuning mechanism</a> of creating 2 smaller weight update matrices and then using them to update the weights of the neural network. Here, we keep the LoRA matrices in a higher precision format, like <a href=""https://cloud.google.com/blog/products/ai-machine-learning/bfloat16-the-secret-to-high-performance-on-cloud-tpus"">brain float 16</a> or float 16 and during the backpropagation and forward pass the weights of the network are also de-quantized. So the actual training is still happening in higher precision formats, but the storage is still in lower precision. This causes quantization errors to emerge, but the model training itself is able to compensate for these inefficiencies in the quantization process.</p><p>‍</p><p>Hence, the LoRA training with higher precision helps the model learn about and reduce the quantization errors.</p><h3>How is QLoRA different from LoRA?</h3><p>QLoRA and LoRA both are finetuning techniques, but QLoRA uses LoRA as an accessory to fix the errors introduced during the quantization errors. LoRA in itself is more of a standalone finetuning technique.</p><h3>QLoRA finetuning with HuggingFace</h3><p>To do QLoRA finetuning with HuggingFace, you need to install both the <a href=""https://pypi.org/project/bitsandbytes/"">BitsandBytes library</a> and the PEFT library. The BitsandBytes library takes care of the 4-bit quantization and the whole low-precision storage and high-precision compute part. The PEFT library will be used for the LoRA finetuning part.</p><p>‍</p><div class=""w-embed""><pre>
<code class=""language-py"">
import torch
from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

model_id = ""EleutherAI/gpt-neox-20b""
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type=""nf4"",
    bnb_4bit_compute_dtype=torch.bfloat16
) # setup bits and bytes config

model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={"""":0})

model.gradient_checkpointing_enable()
model = prepare_model_for_kbit_training(model) # prepares the whole model for kbit training

config = LoraConfig(
    r=8, 
    lora_alpha=32, 
    target_modules=[""query_key_value""], 
    lora_dropout=0.05, 
    bias=""none"", 
    task_type=""CAUSAL_LM""
)

model = get_peft_model(model, config) # Now you get a model ready for QLoRA training
</code>
</pre></div><p>‍</p><p>And then once again you can move to normal training using the HF trainer. Check out this <a href=""https://colab.research.google.com/drive/1VoYNfYDKcKRQRor98Zbf2-9VQTtGJ24k?usp=sharing#scrollTo=FCc64bfnmd3j"">colab notebook</a> as a guide for QLoRA training.</p><h2>QLoRA vs Standard Finetuning</h2><p>In the paper, researchers provide a very detailed comparison between QLoRA, LoRA, and Full Finetuning of a network.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1190pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/65358e23a61fd7828ed079d3_iFgh5dLJlkXuia76nlzEeGFJaHuZk0d27_qMY4dIPcigc3lBDNZnnyybIXrjzCFDJ4aOLC5OU5eUa3m5anHnq7F70bNGG8ql9DMFjaHT2nlgBAqqft5ojwrnILo1Q1UfdbtGqth8lFumRthtk281fvA.png""/></div></figure><p>As you can see in the above table, there is no loss of performance whatsoever in the T5 model family upon training with QLoRA and even with Double Quantization, we don’t see any major differences. One significant difference is the number of LoRA adapters required. In the paper, the authors mention that they needed more LoRA adapters for QLoRA finetuning, compared to normal LoRA finetuning. The authors suggest applying the LoRA adapters on all the linear transformer blocks along with the query, key, and value layers.</p><p>‍</p><p>Even for the much bigger language models, performance remains the same:</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1186pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/65358e233b0593b2e2dce709_-cLgG3H7dbGRuFqh3m0OHfHHqnXNC9qoGEk7ANjagwAiex64QT1ayNQ86Jcfa1qOjuSqSy8BzVVdsBXyKDNMGlm_X1hCnhyNnCpY_W6ycUn2UYvYT0kgvNlcmnXulLvRbyGDn64YoexaSMMYTr4-7Mk.png""/></div></figure><p>Hence the authors trained a model on top of the base LLaMA models, <a href=""https://huggingface.co/timdettmers/guanaco-65b"">Guanaco</a>. This is a 33 billion parameter model, trained on the OASST1 dataset. At the time of its release, it became a state-of-the-art model and achieved <strong>99.3%</strong> performance relative to ChatGPT. Even though other models have lesser models like Vicuna 13B, and Guanaco33B because of the use of 4-bit precision used less memory than the 13B model.</p><p>‍</p><h2>Other variants of LoRA finetuning</h2><h3>QA LoRA</h3><p>QA LoRA is another fine-tuning technique built on top of QLoRA, introduced in <a href=""https://arxiv.org/abs/2310.03270"">this</a> paper. QALoRA was mainly released for finetuning diffusion models, but can easily be generalized for training any type of models, just like LoRA.</p><p>‍</p><p>The difference between QLoRA and QALoRA is that QALoRA is <em>quantization aware</em> meaning the weights of the LoRA adapters are also quantized along with the weights of the model during the finetuning process. This helps in more efficient training as there is no need for the conversion step to update the models during the backpropagation process.</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1332pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/65358e23d7d446708a1eec24_mR-H3Wh2jk0zRKaY7FQCRQANOZNaQNV62RwDzf28bcUQBHHg5ei75Q0M1lHLqL-VTAjiWCHGhzzwUhFbZYso-uVZ-sTabL78mpR3eEAmLAZNEmthk8n9oDaCPKdf4r0KaZjLPs3sbxBn-fF0r7Ggfuk.png""/></div></figure><p>‍</p><h3>LongLoRA</h3><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/65358e2317daafaf293fb79c_wSYLsK_nzksoc9Wzl4UjQW68U8F451DuQCVh7_TzN_icR9iRnzOx0v26S0rEcMgbDgYprqBBcoU8FQhqGo9j-9w9WHr-O04EBoukXwwVXAa3GFkm4te8ElCs4nHEEh1T-xurHGQRQiVzfLjCz8b0VD8.png""/></div></figure><p>‍</p><p><a href=""https://arxiv.org/abs/2309.12307"">LongLoRA</a> is yet another variation of the LoRA finetuning technique, but this technique is specifically for training longer context models. LongLoRA works by using something called <strong>SHIFT SHORT ATTENTION</strong>. This method creates chunks or groups of the tokens, in which the attention is calculated independently. This method allows LongLoRA to scale to a much longer context because of the distributed workload.</p><p>‍</p><p>Along with this, the authors show the need to use the LoRA on the normalization and the embedding layers too for this method to work properly.</p><p>‍</p><h3>AdaLoRA</h3><p>In traditional LoRA, you fix a rank r for every layer. But different layers contribute differently to model performance. </p><p>What <a href=""https://arxiv.org/abs/2303.10512"">AdaLoRA </a>(Adaptive LoRA) does is dynamically allocate the parameter budget i.e. rank to weight matrices based on their importance during training. This avoids the uniform rank allocation of LoRA, which can be suboptimal for layers of varying significance.  Important layers (usually the top transformer layers) receive higher ranks, while less critical ones are pruned.</p><p>It does so by mimicking the SVD (Singular value decomposition) algorithm for the weight matrices and updating the weights through their singular values:</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:637px""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/682f9f271dfebe56a0445a06_AD_4nXf5OKq5Se4xLnekVb5kZrxEK298VQiyXn_CTw6pqd7Q31KoAq1o-8F2kwncU9jbeSC-w5_nFtCyqq_l-xPwGA1_LyMWJBXR1SbGbTvIeEbXOG2IcJsyoK3V2x88J8RXyfi8002iDw.png""/></div></figure><p>‍</p><h3>DoRA</h3><p><a href=""https://arxiv.org/pdf/2402.09353"">DoRA </a>decomposes high-rank LoRA layers into structured single-rank components, allowing for dynamic pruning of parameter budget based on their importance to specific tasks during training, which makes the most of the limited parameter budget. It introduces r′ pairs of single-rank matrices, each acting as a LoRA component. During training, DoRA evaluates the contribution of each component to the overall performance and prunes components with smaller contributions</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1196px""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/682f9f270f51d8769902a253_AD_4nXe557d4mk2wd2AUnZITtRY6PcjRtnLzKQyj8HX7EBHfMUIblwR4b4UTCZHa6OC9hiDUjehweOJejBytIw_KydBH2Ca3OMIda3CfHphTc56BPiDh3RdUpUsm0VUEiB5t9BqS8kyz.png""/></div></figure><p>Instead of using a standard LoRA layer, DoRA reinterprets it as:</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1046px""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/682f9f27cc46e85b462743ad_AD_4nXc41tOO91LNa9L7XwHMl_2MzsU1-LAi4kUNx18N6Jr261oEwRVATpTTJcOgXnf_TQjj5moXLVR7BXVk5BfQYQAASi0QuTvUUs5HHOaTttwSkYZWxpdol5KIf6mU25tk4095c7Allg.png""/></div></figure><p>Here r′ represents the number of LoRA components. A LoRA component is a triplet of A<sub>i</sub>, B<sub>i</sub>, and c<sub>i</sub>, where A<sub>i</sub> and B<sub>i</sub> are single-rank matrices, shaped as d×1 and 1×d respectively. c<sub>i</sub> is a scalar used for pruning the component, it is set to 0 if the component is to be pruned. </p><p>DoRA can directly be integrated by the following modifications in the LoRAconfig:</p><p>‍</p><div class=""w-embed""><pre>
<code class=""language-py"">

config = LoraConfig(
    r=8, 
    lora_alpha=32, 
    target_modules=[""query_key_value""], 
    lora_dropout=0.05, 
    bias=""none"", 
    task_type=""CAUSAL_LM"",
    use_dora=True # simply pass True here to use DoRA
)

model = get_peft_model(model, config) # Now you get a model ready for QLoRA training
</code>
</pre></div><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1325px""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/682f9f27b92cc05d5f1d923c_AD_4nXc0Fa-KzkKwd7i273CUbdvH-gO434ojJq89CJOzJK_B-1dfcunK2eeaRZKUA34mLGhOoVxa_uaItndXPy6YPNO40GryYXRn4AbvHLRJ2q7_yGzsLBxlkMwFuvxFtGLbZdr5Xresew.png""/></div></figure><p>‍</p><h2>Modern Quantization Strategies (05-2025)</h2><p>Here are some modern quantization strategies that tend to work really well.</p><h3>AWQ (Activation-aware Weight Quantization)</h3><p><a href=""https://arxiv.org/pdf/2306.00978"">AWQ</a> is a post-training quantization (PTQ) technique designed to compress large language models (LLMs) into low-bit formats—specifically 4-bit—without compromising performance. Unlike traditional quantization methods that treat all weights uniformly, AWQ intelligently identifies and preserves a small subset of <em>salient</em> weights that are critical for maintaining model accuracy. </p><p>What sets AWQ apart is its approach, rather than analyzing weights directly, it evaluates activation distributions to determine which weight channels are most important. This method eliminates the need for backpropagation or reconstruction, making the process both efficient and highly generalizable across different domains and modalities.</p><p>AWQ consistently delivers strong results across various benchmarks, including instruction tuned and multimodal LLMs. For example, it enables efficient 4-bit inference on large models like LLaMA-2 70B, achieving over 3× speedup compared to FP16 implementations—on both desktop and mobile GPUs.</p><p>‍</p><h3>AffineQuant</h3><p><a href=""https://arxiv.org/html/2403.12544v1"">AffineQuant</a> introduces a novel approach to post-training quantization (PTQ) by leveraging affine transformations to optimize the distributions of both weights and activations. Unlike conventional methods that rely on simple scaling, AffineQuant uses transformation matrices to more accurately align pre- and post-quantization outputs, effectively minimizing quantization errors.</p><p>By applying an affine transformation matrix, it adjusts the data distribution to better fit the quantization function, reducing quantization errors. To ensure that the transformation is reversible, AffineQuant employs a gradual mask optimization method, initially focusing on diagonal elements and gradually extending to other elements (ensuring invertability). This approach aligns with the Levy-Desplanques theorem, guaranteeing the invertibility of the transformation.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:839px""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/682f9f27e7a75d2528741bcb_AD_4nXdC0zY3zTJnXsuoNbGX8QC3a8vn1U4b1r8UJ3Zec42Lu0D5b3j2uQTwzp8krhvewwp6LYLqr6tnQ5egEdqBVWlncPbRRvoadQXzPBN3bEPj8Wn7advMgYKhbBo0FjAg-UdxqzLO.png""/></div></figure><p>AffineQuant sets new standards for PTQ in LLMs. For instance, it achieves a C4 perplexity of 15.76 on the LLaMA2-7B model using W4A4 quantization—surpassing previous methods like OmniQuant. On zero-shot tasks, it reaches an impressive average accuracy of 58.61% with 4/4-bit quantization on LLaMA-30B, marking a notable leap in performance over existing techniques.</p><h3>Dynamic 4-bit Quantization</h3><p><a href=""https://unsloth.ai/blog/dynamic-4bit"">Dynamic 4-Bit Quantization</a>, as implemented by <a href=""https://huggingface.co/collections/unsloth/unsloth-4-bit-dynamic-quants-67503bb873f89e15276c44e7"">Unsloth</a>, offers a flexible approach to quantizing LLMs. Unlike static quantization methods, dynamic quantization adjusts to the model's precision levels on the fly during runtime, ensuring better performance and resource usage.</p><p>Built on top of BitsandBytes (BnB) 4-bit quantization, this dynamic strategy offers visible accuracy improvements while using &lt;<strong>10%</strong> more VRAM compared to standard BnB 4-bit implementations</p><p>‍</p><p>Unsloth's dynamic 4-bit quantization has shown to provide accurate and reliable results, often outperforming standard 4-bit quantization methods. For instance, while standard quantization may degrade model performance, Unsloth's approach maintains high accuracy, reducing memory usage significantly. For example, compressing Llama 3.2 Vision (11B) from 20GB to just 6.5GB while maintaining its high accuracy.</p><p>‍</p><h2>How finetuning improves model performance for your business</h2><p>If you are considering finetuning a model for your business, you are probably correct. But data becomes an important part of the process, take a look at our <a href=""https://www.mercity.ai/blog-post/using-chatgpt-to-build-synthetic-datasets"">synthetic data generation guide</a>. However, there are many ways fine-tuned models can improve the performance of your business by providing customization. Finetuning the models helps you customize the model to your specific needs and knowledge. You can use an RAG pipeline to customize the model, but sometimes the knowledge is so vast that embedding and similarity searches are not enough, that’s where customization via finetuning comes in.</p><p>‍</p><p>If you need finetuned models for your business, <a href=""https://www.mercity.ai/contacts"">reach out</a>, don’t be shy. We have a ton of experience in finetuning all kinds of models, from small T5 models to massive models like Falcon180B. We have seen it all.</p><p>‍</p></div>"
Comprehensive Guide to Integrating Tools and APIs with Language Models,guide-to-integrating-tools-and-apis-with-language-models,640f56f76d313b2faa631c11,650f67dcea23e1c17aacc9b5,False,False,Sat Sep 23 2023 22:34:04 GMT+0000 (Coordinated Universal Time),Mon Oct 16 2023 20:59:09 GMT+0000 (Coordinated Universal Time),Mon Oct 16 2023 20:59:16 GMT+0000 (Coordinated Universal Time),"<p id="""">With a sudden rise of LLMs like GPT-4, we are seeing a massive rise in productivity. Language models have more utility than ever. Everyone is using them and they are everywhere. This raises the need to integrate LLMs with external tools and APIs. ChatGPT with extensions solves this to some extent, but not fully.</p><p id="""">‍</p><p id="""">Issues like integrating ChatGPT with private tools are still not fully solved. This requires building your own framework for integration and then using ChatGPT. In this article we will cover how you can integrate tools and third-party APIs with GPT-4 using function calling, prompting techniques, and other methods.</p><p id="""">‍</p><h2 id="""">Why do we need to integrate tools and APIs with LLMs</h2><p id="""">Language Models are obviously going to be anywhere. I like to call them “a unified UI” for a thousand tools. We have to use hundreds if not thousands of applications on a monthly basis to do everyday things. Using tools like Excel, Email Systems, CRMs, Project management tools, etc., can add so much friction to doing something as simple as replying to an email. If we can have an LLM tightly integrated with these tools, we can just write single-line prompts and LLM does all the tasks. Here are some reasons why we need to integrate tools into LLMs and how it would help</p><h3 id="""">Using Private LLMs with Private Data</h3><p id="""">As mentioned before, it is almost necessary to use LLMs, and more so with private data. Industries like Healthcare, Oil and Gas, and Government agencies cannot provide their data to OpenAI at all, they need to use self-hosted solutions using models like LLaMA and Falcon. These self-hosted solutions don’t have access to plugins like ChatGPT does. They need to build their own pipelines and plugins. This is difficult and time-consuming.</p><p id="""">‍</p><p id="""">However if done properly, using self-hosted LLMs with private data can solve many issues and can boost productivity. For example, in the healthcare industry, integrating LLMs with electronic health records (EHRs) can assist doctors and medical professionals in analyzing patient data and providing more accurate diagnoses. This can save time and improve patient outcomes.</p><h3 id="""">Higher Utility</h3><p id="""">Integrating tools with LLMs can also increase the utility of these tools AND the LLMs. With LLMs, users can perform tasks more efficiently and accurately, reducing the need for manual labor and multiple tools. Just a single command to an LLM and everything gets taken care of. This can lead to cost savings for businesses and increased productivity for individuals.&nbsp;</p><h3 id="""">Better, More Accurate Responses</h3><p id="""">A simple yet VERY effective reason to integrate tools with LLMs is to increase the quality of the response. LLMs on their own are simply <em id="""">text-generation machines</em>, very powerful, but they lack proper context. This means you can ask questions like “How to write a good proposal?” but you cannot ask “How to write a good proposal to sell my services to Mercity?”, this is because the LLM has no context of what your services are and who Mercity is. This is where the need for custom context arises.</p><p id="""">‍</p><p id="""">Answering questions based on private data is actually much simpler. We have written an excellent guide on how to <a href=""https://www.mercity.ai/blog-post/custom-gpt-4-chatbot"" id="""">integrate custom private data with GPT-4</a>. You can check it out. We use an embeddings-based retrieval system to extract the relevant chunks of text to answer questions based on private data.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1327px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1327px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/650f66ff38e964525a33753d_It9AmDibaYSRHYjW2gvzPFSjMdgtYflkC3jljDbGfGB-EGCrnxYsGtgRWCP6d0rulXolHBBDKJ2moUaHOfIAtn5IM3A8xItozOZBdAUuO4oU3MewKH_mW9AV4x0vDDlWfiUdRVekEDVi4DRoZpeViJI.png"" id="""" width=""auto"" height=""auto"" alt="""" loading=""auto""></div></figure><h3 id="""">Single Interface for Multiple Tools</h3><p id="""">As mentioned before, LLMs can act as a unified UI for multiple tools. This is becoming necessary because the number of tools we use is increasing rapidly. Also, the information and knowledge are completely spread out on different platforms. LLMs can reduce the friction of these multi-platform tasks. Language models like GPT-4 are smart enough to string function calls together and use multiple tools in a chain, collecting data, and planning and executing the given task.</p><h3 id="""">‍</h3><p id="""">For example, if you need to <a href=""https://www.mercity.ai/blog-post/gpt-nlp-in-sales#sales-call-analytics"" id="""">extract meeting notes from sales calls</a>, write a proposal and send over it to the client. An LLM can find the meeting transcripts, extract notes from them, write a proposal, and have it ready for your review. This will save around 1~2 hours of time. Once the LLM has drafted a proposal, all you need to do is make any changes necessary and once done, you are ready to send it. So all you need to do is review the generation of the model, while the model takes care of the research and compiling the proposal, which takes more time.</p><h3 id="""">‍</h3><h2 id="""">How to Integrate Tools and APIs with LLMs</h2><p id="""">At Mercity, we have built our own pipelines for tool integration. At the core is an LLM, and the Tool-use Prompt. Note that LLM here can be any capable instruction following the Language Model. GPT-4 is the smartest and the best model out there, but bigger models like LLaMa 70B and Falcon 180B can also be used here. Models just have to be smart enough to follow the prompts and should be able to generate sophisticated outputs.</p><h3 id="""">‍</h3><p id="""">Let’s break this pipeline down step by step.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/650f6700a38686bf04abf9ae_oYM1xj0eyB4-37H6LC2k60wZN98clF_vgT6B7Rv4NTkmMqppzhWmw6fOeFw-Vp5m_daHM0Mas4j3DSf5dkVXQQTq_77WGU0GlopNEGhB5Cg-J26wDv6syQqfOdzoUUVfAr2z9a5m_iunOX2nM4oAsJk.png"" id="""" width=""auto"" height=""auto"" alt="""" loading=""auto""></div></figure><p id="""">‍</p><h3 id="""">Large Language Model</h3><p id="""">A Language model like GPT-4 is at the core of it all, it acts as the user interface for the tools and the APIs we want to integrate. The model doesn’t necessarily need to be finetuned for chatting, but it would be better if it is. In our findings, we have noticed that larger models work better for these applications, simply because they are better instruction followers and are much better at maintaining multi-step conversations without losing the nature of the conversation. Smaller models like LLaMa-13B can be finetuned to a great degree to follow specific tool use prompts using <a href=""https://www.mercity.ai/blog-post/fine-tuning-llms-using-peft-and-lora"" id="""">PEFT Techniques</a>. Specifically, techniques like Prefix Tuning and IA3 are very popular for tuning LLMs with smaller datasets.</p><h3 id="""">‍</h3><p id="""">Once the LLM is selected and is validated to follow instructions properly, we can tightly integrate it with a Tool Database and Tool-Use Prompt.</p><h4 id="""">API Tool Database</h4><p id="""">A tool database is simply a collection of all the tools you might have or want to use with the language model. This can simply be a list of tools and APIs in text or can be a much more sophisticated dynamically fetched pipeline. Most of the time, we use simple text, with the name and description of the API along with how to use it and when, and provide it to the LLM in the form of a tool library.</p><h3 id="""">‍</h3><p id="""">When providing API, we abstract it as a function call and use the arguments to construct a schema for the API call.</p><h3 id="""">‍</h3><p id="""">Here’s what a demo tool library would look like:</p><p id="""">‍</p><div data-rt-embed-type='true'><div style=""background-color: #2A2A2B;padding: 2.5%;"">

<code>
<i>

Tool Library:
<br/> <br/>
- web_search(query) - Use this function to find information on the Internet. It's a general search function that can be applied to almost any topic. You pass the query string here. Make sure your queries are precise.
<br/> <br/>
- embedding_database_search(query) - This function is specifically designed for retrieving information. You can use this tool over others to find information about very specific personal topics. In `query` pass the information you want to find about a topic.
<br/> <br/>
- wikipedia_search(query) - Use this function when the information needed can be found on Wikipedia. It serves as a direct conduit to this comprehensive knowledge base. This provides extensive knowledge on a specific topic. Use this function accordingly.

</i>
</code>
</div></div><p id="""">‍</p><h4 id="""">Tool Use Prompt</h4><p id="""">Once we have the tool library ready, we can put it in the tool use prompt and explain to the LLM how to call and use tools. This is a very important part of the pipeline as this determines exactly how and when the language model will use the tool. There are multiple prompting techniques that can be used here, but at Mercity we like to use our own self-built prompts. We will take a deeper look at the prompting techniques for tool use now.</p><h3 id="""">Prompting for Tool Using and API Calling</h3><p id="""">As said above, this is perhaps the most important part. LLMs need to be prompted properly on how to use the tools you have provided and when to use them. We need to craft the perfect prompt for this. There are many ways to do this, but we like to use the most basic technique.</p><h4 id="""">‍</h4><p id="""">We simply provide the <em id="""">tool library</em> to the model and ask the model to output in a very specific format so that we can parse and use the tool when needed. This is what the prompt looks like when combined with the aforementioned tool library:</p><p id="""">‍</p><div data-rt-embed-type='true'><div style=""background-color: #2A2A2B;padding: 2.5%;"">

<code>
<i>

You are a masterful tool user, you must assist the user with their queries, but you must also use the provided tools when necessary.
<br/> <br/>
You must reply in a concise and simple way and must always follow the provided rules.
<br/> <br/>
===========================================================
<br/> <br/>
Tool Library:
<br/> <br/>
- web_search(query) - Use this function to find information on the Internet. It's a general search function that can be applied to almost any topic. You pass the query string here. Make sure your queries are precise.
<br/> <br/>
- embedding_database_search(query) - This function is specifically designed for retrieving information. You can use this tool over others to find information about very specific personal topics. In `query` pass the information you want to find about pets in the specified categories.
<br/> <br/>
- wikipedia_search(query) - Use this function when the information needed can be found on Wikipedia. It serves as a direct conduit to this comprehensive knowledge base. This provides extensive knowledge on a specific topic. Use this function accordingly.
<br/> <br/>
<br/> <br/>
===========================================================
<br/> <br/>
To use these tools you can output func_name(query) in middle of generation or ONLY output the function call.
<br/> <br/>
Example outputs:
<br/> <br/>
- The current president of the United States of America is web_search(""Who is the current president of United States"")
<br/> <br/>
- wikipedia_search(""Joe Biden"") = You can output like this when user wants extensive detail on a specific topic or person.
<br/> <br/>
<br/> <br/>
===========================================================
<br/> <br/>
Note that you must always follow the provided rules and output in the given manner. Using a function is not always necessary, use only when needed.

</i>
</code>
</div></div><p id="""">‍</p><p id="""">This is what the outputs look like from this prompt:</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/650f66ff4065734724fd4745_NOBxGTkIvbHJamqYaS47q6gzW_qSrAf2HYNkOUn35qTFond-XCLfAvHAbcCZdLFRodgMZPQbd37SU-mzi1amLyQlLsq4ftZ3AomS8CzvKOqO_1YnNIwaQw-hbOt2KF4tSukggX707J3p2npRfYjunpI.png"" id="""" width=""auto"" height=""auto"" alt="""" loading=""auto""></div></figure><p id="""">‍</p><p id="""">You can see that the model was able to properly identify when it needed to call the function. It did not call the function when I asked it about the topics I could write on, but did call the function when it was absolutely needed.</p><p id="""">‍</p><p id="""">Even more so, it was properly able to identify when it needed to query my personal documents and was able to write an excellent query to use the embedding search tool with:</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1517px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1517px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/650f66ff33137a6ce375b155_V2ZjK0TqDsWAjtI7bFJXWo1KaT2loYn1AijL1jz_ixQx3DkoyzT7S4N_2pAJhv_RUzsedckHcvvLjigtOQ9ecJanIAMgFS-th94o_onXMktEqT7ZvjlRNSaCGclBRhvifXO4_vWEVksoyDufkQMaR6Y.png"" id="""" width=""auto"" height=""auto"" alt="""" loading=""auto""></div></figure><p id="""">‍</p><p id="""">This method of prompting is extremely easy and works beautifully. In our experiments, we have seen some issues arise when you try to combine this method with already very long prompts. With a longer prompt, it gets extremely hard to make the model output in the proper format, and the accuracy of tool use drops. But these issues are largely only seen with GPT3.5, and not with GPT-4. GPT-4 is much better at following complicated formats and instructions.</p><h4 id="""">React Based Prompting</h4><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:818px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""818px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/650f66ff788d34c59019f6d3_W6QEEql3bAz9ukUQTh7VmDibP_f54v7jVRlAS1nHS9Ov7-WQI_rd6SOa-ktEXLpvJFdebV-vqqvcgvKp5tQZcpzUG_XLWpKQ50JgChkpYlDPiXmS9qcpZfDM4hAKJpMs4S2e9IGENRlxdTwxM-ahfE4.png"" id="""" width=""auto"" height=""auto"" alt="""" loading=""auto""></div></figure><p id="""">‍</p><p id="""">There is another better prompting method called <a href=""https://arxiv.org/abs/2210.03629"" id="""">ReAct</a>. This method has been popular lately. ReAct breaks down the LLM outputs into 3 aspects, <strong id="""">Thought</strong>, <strong id="""">Act,</strong> and <strong id="""">Observation</strong>. Here is a breakdown of these parts:</p><p id="""">‍</p><ul id=""""><li id=""""><strong id="""">Thought:</strong> This is the part where LLM THINKS what it needs to do. The model analyzes the input and generates somewhat of a plan on what do to.</li><li id=""""><strong id="""">Act:</strong> This is the action part. Based on the <strong id="""">thought</strong> the LLM now acts. This can be using a tool or calling an API or interacting with something else. Or this can be left blank if needed.</li><li id=""""><strong id="""">Observation:</strong> In the end, based on the thoughts and the output of the Action, an observation is made. This observation can be an answer to a question or starting of yet another ReAct chain.</li></ul><p id="""">‍</p><p id="""">Here is an application of this prompt:</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/650f67009c7332290b91eba8_yPyjYkUsWn8KYAHAKRf2aOBm7_qUm2KDBpl1UvruX2si1nD-UehscOMADJFEI1eGq9AU8PGYPO7pCJ8QjjIfYr6ctzca3bnu87dTxexFIQkMB_Q5vkE6flfDU34E96xhflswPs7xyoWaKxCpYzMAAwY.png"" id="""" width=""auto"" height=""auto"" alt="""" loading=""auto""></div></figure><p id="""">‍</p><p id="""">You can see that LLM was able to correctly identify the need for the tool and call it accordingly. ReAct prompting is better than the basic prompting we showed above because this allows the model to analyze the input before providing an output, and this boosts the accuracy. The only downside is a bit increase in token usage, but the increase in accuracy and control over outputs makes that worth it.</p><p id="""">‍</p><p id="""">Here is the SYSTEM prompt we use:</p><p id="""">‍</p><div data-rt-embed-type='true'><div style=""background-color: #2A2A2B;padding: 2.5%;"">

<code>
<i>
<br/> <br/>
You are a masterful tool user, you must assist the user with their queries, but you must also use the provided tools when necessary.
<br/> <br/>
You must reply in a concise and simple way and must always follow the provided rules.
<br/> <br/>
===========================================================
<br/> <br/>
Tool Library:
<br/> <br/>
- web_search(query) - Use this function to find information on the Internet. It's a general search function that can be applied to almost any topic. You pass the query string here. Make sure your queries are precise.
<br/> <br/>
- embedding_database_search(query) - This function is specifically designed for retrieving information. You can use this tool over others to find information about very specific personal topics. In `query` pass the information you want to find about pets in the specified categories.
<br/> <br/>
- wikipedia_search(query) - Use this function when the information needed can be found on Wikipedia. It serves as a direct conduit to this comprehensive knowledge base. This provides extensive knowledge on a specific topic. Use this function accordingly.
<br/> <br/>
<br/> <br/>
===========================================================
<br/> <br/>
This is the format you need to output in:
<br/> <br/>
Thought: THINK AND ANALYZE THE INPUT, GOAL AND SITUATION
Act: If you need to call a tool or use a function, you can do it here: func(query). If no need to use a tool, leave this empty. The output of the function will be provided here.
Observation: Based on the Thoughts and results of the Act, provide a reply. If you are using a tool, no need to output this.
<br/> <br/>
Example Acts to use tools:
<br/> <br/>
- The current president of the United States of America is web_search(""Who is the current president of United States"")
<br/> <br/>
- wikipedia_search(""Joe Biden"") = You can output like this when the user wants extensive detail on a specific topic or person.
<br/> <br/>
<br/> <br/>
===========================================================
<br/> <br/>
Note that you must always follow the provided rules and output in the given manner. Using a function is not always necessary, use it only when needed.

</i>
</code>
</div></div><p id="""">‍</p><h3 id="""">Function Calling</h3><p id=""""><a href=""https://openai.com/blog/function-calling-and-other-api-updates"" id="""">Function calling</a> is a feature released by OpenAI. This allows you to integrate your Chat GPT models like GPT-3.5 and GPT-4 directly with the functions you want to call. You can provide the schema of your functions or APIs and the model will use the provided functions when needed.</p><h3 id="""">‍</h3><p id="""">Function Calling is the go-to way and probably the first step to take if you are looking to integrate an API with your GPT-4 or GPT-3.5.</p><h3 id="""">‍</h3><p id="""">Here is an example of how you are supposed to pass the schema of your function to the model input:</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1137px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1137px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/650f66ff65468e3379420799_SXL1TZz_vD5NS3fGxzbM0mK3Sfe0JSz8WjpnNVeJkJFAa4lfoTIivla7F20_Bn5gRtBvwFKdZ4sYBNlLIj6kZGF25fiED2D3_7lF8icBySCm6AxT_Nvdsjp8kurMzCJZVtm8sXiZSLWYfozQ5-hWJkM.png"" id="""" width=""auto"" height=""auto"" alt="""" loading=""auto""></div></figure><p id="""">‍</p><p id="""">The model will use the function as needed:</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1140px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1140px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/650f66ff0ef65200ed45d160_bT3AbWWWY4uZRlhFk9xRpCYUkJvys8Ap9iEpoVqhzeLEPERvFwNHEtLSuyqNuBlLT8I-tp1xDrlCnW-W0CDlTaS3YpBrj4lk9_mhQasexQ3Lo1ya1Fkch7w3pepzr_FQ_IpPfTR8iJpnXPCKWhtH1Y4.png"" id="""" width=""auto"" height=""auto"" alt="""" loading=""auto""></div></figure><p id="""">‍</p><h4 id="""">How effective is Function Calling for Tool Use?</h4><p id="""">In our experience, we have found that OpenAI models usually work well with function calling. But as the number of functions grows and you try to add functions that are more custom to your needs, the quality drops quickly and drastically. Also, we have found that the token usage also increases greatly, this is because OpenAI adds the prompts to the system prompt, and the JSON schema takes up a lot of tokens when compared to ReAct or simple tool use prompting.</p><p id="""">‍</p><p id="""">Many users have also reported that models sometimes hallucinate and output random function names. Here is a good forum post to read that shows how unreliable function calling is: <a href=""https://community.openai.com/t/function-calling-very-unreliable/268439"" id="""">Function Calling Very Unreliable</a>.</p><p id="""">‍</p><h2 id="""">Training LLMs to Use Tools</h2><h3 id="""">Toolformer</h3><p id=""""><a href=""https://arxiv.org/abs/2302.04761"" id="""">Toolformer </a>is a model by Meta trained to decide which API to call and then call it. Meta trained this model specifically for tool use and has shown great results. The model calls the functions and stops the generation, then the tool use pipeline provides an answer and the generation continues.</p><p id="""">‍</p><p id="""">This approach even though simple, is quite effective. But has major issues. For example, this approach works as long as the responses from the tools are short. If the responses grow in length, the quality of outputs will stop dropping. And most of the time the responses from the tools are going to be long and complicated, this can lead to context overflow and model forgetting what was being talked about originally.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:770px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""770px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/650f66ffa38686bf04abf980_mt9P0TbtdfrbjbG4kK-GDHjbDTiMxx9ElrWke0dc4X-83xdMBsaTGHTMclTEp0-GHR6clMt9UTe8dm3j7M9PlqKv-JvfOeeefnbXWO37OlL3kpdOZtlZeNO5RBp7YS-Agg3uxfzVF2ewI37XlwNuTqs.png"" id="""" width=""auto"" height=""auto"" alt="""" loading=""auto""></div></figure><h3 id="""">Gorilla LLM</h3><p id=""""><a href=""https://gorilla.cs.berkeley.edu/"" id="""">Gorilla LLM</a> is a large language model coming out of Microsoft and UC Berkeley that can generate API calls from natural language queries. Gorilla LLM can understand and use over 1,600 APIs from various domains, such as machine learning, cloud computing, and web development.</p><p id="""">‍</p><p id="""">Gorilla LLM is trained on three massive machine learning hub datasets: Torch Hub, TensorFlow Hub, and HuggingFace. It also uses a document retriever to adapt to changes in the API documentation and provide accurate and up-to-date results. Gorilla LLM outperforms other large language models, such as GPT-4, Chat-GPT, and Claude, in writing API calls.&nbsp;</p><p id="""">‍</p><div data-rt-embed-type='true'><iframe width=""960"" height=""515"" style=""display:block;"" src=""https://www.youtube.com/embed/RMgM3tPTpXI?si=zPO5gsP8npqLtIbX"" title=""YouTube video player"" frameborder=""0"" allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"" allowfullscreen></iframe></div><p>‍</p><h2 id="""">Use Cases of LLMs integrated with APIs and other Tools</h2><p id="""">Now that we have discussed how to connect APIs and Tools with LLMs, let’s talk about some of the use cases we have for this.</p><h3 id="""">Integrating with Email Service Providers</h3><p id="""">This is perhaps the most obvious one. Email inboxes have become very messy with hundreds of emails coming in every day. This makes it incredibly difficult to process information properly and reply to them timely. We already have spam filters, but they do not help clean up the mess we have in our inboxes.</p><p>‍</p><p id="""">LLMs can be paired together with these inboxes to read your emails and provide summaries, prioritize what emails to reply and even reply to your emails if allowed to. Even very simple, 3 billion parameter models can be deployed to take care of these tasks.</p><p>‍</p><p id="""">You can build a private assistant to take care of your emails end to end using an LLM connected with GMail API, or via IMAP and SMTP.</p><h3 id="""">Integrating with CRM Systems</h3><p id="""">Customer Relationship Management tools are extremely messy. Multiple teams use it, from sales to marketing to support and whatnot. CRMs are used for multiple things like storing customer data, call transcripts, feedback, and a ton of other data. And this data needs to be shared across teams. Hence, maintaining CRMs can be very complicated for everyone.</p><p>‍</p><p id="""">LLMs can be integrated with CRM APIs to simplify a ton of workflows, for example, LLMs can <a href=""https://www.mercity.ai/blog-post/gpt-nlp-in-sales#sales-call-analytics"" id="""">generate meeting notes</a>, which can save salespeople a ton of time. It can extract valuable insights from support and customer meetings for marketing teams and put everything in a proper, consumable format.</p><p>‍</p><p id="""">LLMs can compress information spread in CRMs and generate simple reports for pretty much anything or any specific customer you have.</p><h3 id="""">Integrating with CMS</h3><p id="""">Similar to CRMs, content management systems, and pipelines also have multiple functionalities, from creating and writing content to editing and to SEO optimization and whatnot. Language models can easily be integrated with every bit of these pipelines.</p><p>‍</p><p id="""">LLMs can be used to generate content, edit, and remove any unnecessary parts. LLM agents can also be deployed to plan, and generate content outlines, and then go ahead and generate the actual content and publish it.</p><p>‍</p><p id="""">WordPress APIs are one of the best and easiest to integrate with LLMs as you can access almost all parts of the pipeline.</p><p>‍</p><h2 id="""">Challenges of Integrating Tools with LLMs</h2><p id="""">Even though we have outlined many approaches to integrate LLMs with tools, there are still many challenges that make this task difficult. Let’s go over them.</p><h3 id="""">Context Overflow</h3><p id="""">As seen with Toolformer and OpenAI function calling, context overflow is a big issue. This happens because we need to prompt the model with the tools we want to integrate. This means we need to add the tool names, descriptions of the tools, examples of how to use it when to use it, and more details. This can lead to major issues like a reduction in the output length because the prompt itself is so long Or a significant increase in token usage costs.</p><h3 id="""">Accuracy drops as the number of tools grows</h3><p id="""">This is pretty evident. As the number of tools integrated with an LLM increases, maintaining accuracy and efficiency can become a difficult task. If your tools are very similar to each other, or if you don’t provide good enough examples, the model can get confused and call the wrong functions at times. Or not call a function at all! This can be fixed with better prompting.</p><h3 id="""">Latency</h3><p id="""">Latency is a significant challenge when integrating tools with LLMs. The time it takes for data to be processed and for results to be produced from the tools. High latency can lead to delays in decision-making and can negatively impact the user experience. This is particularly problematic in real-time applications, where delays of even a few seconds can have significant issues.</p><h3 id="""">Trust</h3><p id="""">This is not a huge issue, but if you are using the model to generate code or to act on your behalf, you need to trust the model. If the model, for example, replies incorrectly or deletes the wrong files from your folder, it can cause major problems. This can simply be fixed by making sure there is a human in the loop and reviewing the steps taken by the LLM.</p><p id="""">‍</p><h2 id="""">Want to integrate your tools with LLMs?</h2><p id="""">If you want to integrate your own Email Systems, CRMs, or any other APIs or internal or external tools with private or public LLMs, <a href=""https://www.mercity.ai/contacts"" id="""">contact us</a>. We have a ton of experience building applications that require a tight integration of tools and models like LLaMA, GPT-4, etc.7</p><p id="""">‍</p>",https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/650f6755ec1eba8db2438a6b_tool%20use.png,Pranav Patel,LLMs,Learn how to integrate tools with public or private language models (LLMs) using sophisticated techniques.,False,"<div class=""rich-text w-richtext""><p>With a sudden rise of LLMs like GPT-4, we are seeing a massive rise in productivity. Language models have more utility than ever. Everyone is using them and they are everywhere. This raises the need to integrate LLMs with external tools and APIs. ChatGPT with extensions solves this to some extent, but not fully.</p><p>‍</p><p>Issues like integrating ChatGPT with private tools are still not fully solved. This requires building your own framework for integration and then using ChatGPT. In this article we will cover how you can integrate tools and third-party APIs with GPT-4 using function calling, prompting techniques, and other methods.</p><p>‍</p><h2>Why do we need to integrate tools and APIs with LLMs</h2><p>Language Models are obviously going to be anywhere. I like to call them “a unified UI” for a thousand tools. We have to use hundreds if not thousands of applications on a monthly basis to do everyday things. Using tools like Excel, Email Systems, CRMs, Project management tools, etc., can add so much friction to doing something as simple as replying to an email. If we can have an LLM tightly integrated with these tools, we can just write single-line prompts and LLM does all the tasks. Here are some reasons why we need to integrate tools into LLMs and how it would help</p><h3>Using Private LLMs with Private Data</h3><p>As mentioned before, it is almost necessary to use LLMs, and more so with private data. Industries like Healthcare, Oil and Gas, and Government agencies cannot provide their data to OpenAI at all, they need to use self-hosted solutions using models like LLaMA and Falcon. These self-hosted solutions don’t have access to plugins like ChatGPT does. They need to build their own pipelines and plugins. This is difficult and time-consuming.</p><p>‍</p><p>However if done properly, using self-hosted LLMs with private data can solve many issues and can boost productivity. For example, in the healthcare industry, integrating LLMs with electronic health records (EHRs) can assist doctors and medical professionals in analyzing patient data and providing more accurate diagnoses. This can save time and improve patient outcomes.</p><h3>Higher Utility</h3><p>Integrating tools with LLMs can also increase the utility of these tools AND the LLMs. With LLMs, users can perform tasks more efficiently and accurately, reducing the need for manual labor and multiple tools. Just a single command to an LLM and everything gets taken care of. This can lead to cost savings for businesses and increased productivity for individuals. </p><h3>Better, More Accurate Responses</h3><p>A simple yet VERY effective reason to integrate tools with LLMs is to increase the quality of the response. LLMs on their own are simply <em>text-generation machines</em>, very powerful, but they lack proper context. This means you can ask questions like “How to write a good proposal?” but you cannot ask “How to write a good proposal to sell my services to Mercity?”, this is because the LLM has no context of what your services are and who Mercity is. This is where the need for custom context arises.</p><p>‍</p><p>Answering questions based on private data is actually much simpler. We have written an excellent guide on how to <a href=""https://www.mercity.ai/blog-post/custom-gpt-4-chatbot"">integrate custom private data with GPT-4</a>. You can check it out. We use an embeddings-based retrieval system to extract the relevant chunks of text to answer questions based on private data.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1327pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/650f66ff38e964525a33753d_It9AmDibaYSRHYjW2gvzPFSjMdgtYflkC3jljDbGfGB-EGCrnxYsGtgRWCP6d0rulXolHBBDKJ2moUaHOfIAtn5IM3A8xItozOZBdAUuO4oU3MewKH_mW9AV4x0vDDlWfiUdRVekEDVi4DRoZpeViJI.png""/></div></figure><h3>Single Interface for Multiple Tools</h3><p>As mentioned before, LLMs can act as a unified UI for multiple tools. This is becoming necessary because the number of tools we use is increasing rapidly. Also, the information and knowledge are completely spread out on different platforms. LLMs can reduce the friction of these multi-platform tasks. Language models like GPT-4 are smart enough to string function calls together and use multiple tools in a chain, collecting data, and planning and executing the given task.</p><h3>‍</h3><p>For example, if you need to <a href=""https://www.mercity.ai/blog-post/gpt-nlp-in-sales#sales-call-analytics"">extract meeting notes from sales calls</a>, write a proposal and send over it to the client. An LLM can find the meeting transcripts, extract notes from them, write a proposal, and have it ready for your review. This will save around 1~2 hours of time. Once the LLM has drafted a proposal, all you need to do is make any changes necessary and once done, you are ready to send it. So all you need to do is review the generation of the model, while the model takes care of the research and compiling the proposal, which takes more time.</p><h3>‍</h3><h2>How to Integrate Tools and APIs with LLMs</h2><p>At Mercity, we have built our own pipelines for tool integration. At the core is an LLM, and the Tool-use Prompt. Note that LLM here can be any capable instruction following the Language Model. GPT-4 is the smartest and the best model out there, but bigger models like LLaMa 70B and Falcon 180B can also be used here. Models just have to be smart enough to follow the prompts and should be able to generate sophisticated outputs.</p><h3>‍</h3><p>Let’s break this pipeline down step by step.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/650f6700a38686bf04abf9ae_oYM1xj0eyB4-37H6LC2k60wZN98clF_vgT6B7Rv4NTkmMqppzhWmw6fOeFw-Vp5m_daHM0Mas4j3DSf5dkVXQQTq_77WGU0GlopNEGhB5Cg-J26wDv6syQqfOdzoUUVfAr2z9a5m_iunOX2nM4oAsJk.png""/></div></figure><p>‍</p><h3>Large Language Model</h3><p>A Language model like GPT-4 is at the core of it all, it acts as the user interface for the tools and the APIs we want to integrate. The model doesn’t necessarily need to be finetuned for chatting, but it would be better if it is. In our findings, we have noticed that larger models work better for these applications, simply because they are better instruction followers and are much better at maintaining multi-step conversations without losing the nature of the conversation. Smaller models like LLaMa-13B can be finetuned to a great degree to follow specific tool use prompts using <a href=""https://www.mercity.ai/blog-post/fine-tuning-llms-using-peft-and-lora"">PEFT Techniques</a>. Specifically, techniques like Prefix Tuning and IA3 are very popular for tuning LLMs with smaller datasets.</p><h3>‍</h3><p>Once the LLM is selected and is validated to follow instructions properly, we can tightly integrate it with a Tool Database and Tool-Use Prompt.</p><h4>API Tool Database</h4><p>A tool database is simply a collection of all the tools you might have or want to use with the language model. This can simply be a list of tools and APIs in text or can be a much more sophisticated dynamically fetched pipeline. Most of the time, we use simple text, with the name and description of the API along with how to use it and when, and provide it to the LLM in the form of a tool library.</p><h3>‍</h3><p>When providing API, we abstract it as a function call and use the arguments to construct a schema for the API call.</p><h3>‍</h3><p>Here’s what a demo tool library would look like:</p><p>‍</p><div class=""w-embed""><div style=""background-color: #2A2A2B;padding: 2.5%;"">
<code>
<i>

Tool Library:
<br/> <br/>
- web_search(query) - Use this function to find information on the Internet. It's a general search function that can be applied to almost any topic. You pass the query string here. Make sure your queries are precise.
<br/> <br/>
- embedding_database_search(query) - This function is specifically designed for retrieving information. You can use this tool over others to find information about very specific personal topics. In `query` pass the information you want to find about a topic.
<br/> <br/>
- wikipedia_search(query) - Use this function when the information needed can be found on Wikipedia. It serves as a direct conduit to this comprehensive knowledge base. This provides extensive knowledge on a specific topic. Use this function accordingly.

</i>
</code>
</div></div><p>‍</p><h4>Tool Use Prompt</h4><p>Once we have the tool library ready, we can put it in the tool use prompt and explain to the LLM how to call and use tools. This is a very important part of the pipeline as this determines exactly how and when the language model will use the tool. There are multiple prompting techniques that can be used here, but at Mercity we like to use our own self-built prompts. We will take a deeper look at the prompting techniques for tool use now.</p><h3>Prompting for Tool Using and API Calling</h3><p>As said above, this is perhaps the most important part. LLMs need to be prompted properly on how to use the tools you have provided and when to use them. We need to craft the perfect prompt for this. There are many ways to do this, but we like to use the most basic technique.</p><h4>‍</h4><p>We simply provide the <em>tool library</em> to the model and ask the model to output in a very specific format so that we can parse and use the tool when needed. This is what the prompt looks like when combined with the aforementioned tool library:</p><p>‍</p><div class=""w-embed""><div style=""background-color: #2A2A2B;padding: 2.5%;"">
<code>
<i>

You are a masterful tool user, you must assist the user with their queries, but you must also use the provided tools when necessary.
<br/> <br/>
You must reply in a concise and simple way and must always follow the provided rules.
<br/> <br/>
===========================================================
<br/> <br/>
Tool Library:
<br/> <br/>
- web_search(query) - Use this function to find information on the Internet. It's a general search function that can be applied to almost any topic. You pass the query string here. Make sure your queries are precise.
<br/> <br/>
- embedding_database_search(query) - This function is specifically designed for retrieving information. You can use this tool over others to find information about very specific personal topics. In `query` pass the information you want to find about pets in the specified categories.
<br/> <br/>
- wikipedia_search(query) - Use this function when the information needed can be found on Wikipedia. It serves as a direct conduit to this comprehensive knowledge base. This provides extensive knowledge on a specific topic. Use this function accordingly.
<br/> <br/>
<br/> <br/>
===========================================================
<br/> <br/>
To use these tools you can output func_name(query) in middle of generation or ONLY output the function call.
<br/> <br/>
Example outputs:
<br/> <br/>
- The current president of the United States of America is web_search(""Who is the current president of United States"")
<br/> <br/>
- wikipedia_search(""Joe Biden"") = You can output like this when user wants extensive detail on a specific topic or person.
<br/> <br/>
<br/> <br/>
===========================================================
<br/> <br/>
Note that you must always follow the provided rules and output in the given manner. Using a function is not always necessary, use only when needed.

</i>
</code>
</div></div><p>‍</p><p>This is what the outputs look like from this prompt:</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/650f66ff4065734724fd4745_NOBxGTkIvbHJamqYaS47q6gzW_qSrAf2HYNkOUn35qTFond-XCLfAvHAbcCZdLFRodgMZPQbd37SU-mzi1amLyQlLsq4ftZ3AomS8CzvKOqO_1YnNIwaQw-hbOt2KF4tSukggX707J3p2npRfYjunpI.png""/></div></figure><p>‍</p><p>You can see that the model was able to properly identify when it needed to call the function. It did not call the function when I asked it about the topics I could write on, but did call the function when it was absolutely needed.</p><p>‍</p><p>Even more so, it was properly able to identify when it needed to query my personal documents and was able to write an excellent query to use the embedding search tool with:</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1517pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/650f66ff33137a6ce375b155_V2ZjK0TqDsWAjtI7bFJXWo1KaT2loYn1AijL1jz_ixQx3DkoyzT7S4N_2pAJhv_RUzsedckHcvvLjigtOQ9ecJanIAMgFS-th94o_onXMktEqT7ZvjlRNSaCGclBRhvifXO4_vWEVksoyDufkQMaR6Y.png""/></div></figure><p>‍</p><p>This method of prompting is extremely easy and works beautifully. In our experiments, we have seen some issues arise when you try to combine this method with already very long prompts. With a longer prompt, it gets extremely hard to make the model output in the proper format, and the accuracy of tool use drops. But these issues are largely only seen with GPT3.5, and not with GPT-4. GPT-4 is much better at following complicated formats and instructions.</p><h4>React Based Prompting</h4><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:818pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/650f66ff788d34c59019f6d3_W6QEEql3bAz9ukUQTh7VmDibP_f54v7jVRlAS1nHS9Ov7-WQI_rd6SOa-ktEXLpvJFdebV-vqqvcgvKp5tQZcpzUG_XLWpKQ50JgChkpYlDPiXmS9qcpZfDM4hAKJpMs4S2e9IGENRlxdTwxM-ahfE4.png""/></div></figure><p>‍</p><p>There is another better prompting method called <a href=""https://arxiv.org/abs/2210.03629"">ReAct</a>. This method has been popular lately. ReAct breaks down the LLM outputs into 3 aspects, <strong>Thought</strong>, <strong>Act,</strong> and <strong>Observation</strong>. Here is a breakdown of these parts:</p><p>‍</p><ul role=""list""><li><strong>Thought:</strong> This is the part where LLM THINKS what it needs to do. The model analyzes the input and generates somewhat of a plan on what do to.</li><li><strong>Act:</strong> This is the action part. Based on the <strong>thought</strong> the LLM now acts. This can be using a tool or calling an API or interacting with something else. Or this can be left blank if needed.</li><li><strong>Observation:</strong> In the end, based on the thoughts and the output of the Action, an observation is made. This observation can be an answer to a question or starting of yet another ReAct chain.</li></ul><p>‍</p><p>Here is an application of this prompt:</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/650f67009c7332290b91eba8_yPyjYkUsWn8KYAHAKRf2aOBm7_qUm2KDBpl1UvruX2si1nD-UehscOMADJFEI1eGq9AU8PGYPO7pCJ8QjjIfYr6ctzca3bnu87dTxexFIQkMB_Q5vkE6flfDU34E96xhflswPs7xyoWaKxCpYzMAAwY.png""/></div></figure><p>‍</p><p>You can see that LLM was able to correctly identify the need for the tool and call it accordingly. ReAct prompting is better than the basic prompting we showed above because this allows the model to analyze the input before providing an output, and this boosts the accuracy. The only downside is a bit increase in token usage, but the increase in accuracy and control over outputs makes that worth it.</p><p>‍</p><p>Here is the SYSTEM prompt we use:</p><p>‍</p><div class=""w-embed""><div style=""background-color: #2A2A2B;padding: 2.5%;"">
<code>
<i>
<br/> <br/>
You are a masterful tool user, you must assist the user with their queries, but you must also use the provided tools when necessary.
<br/> <br/>
You must reply in a concise and simple way and must always follow the provided rules.
<br/> <br/>
===========================================================
<br/> <br/>
Tool Library:
<br/> <br/>
- web_search(query) - Use this function to find information on the Internet. It's a general search function that can be applied to almost any topic. You pass the query string here. Make sure your queries are precise.
<br/> <br/>
- embedding_database_search(query) - This function is specifically designed for retrieving information. You can use this tool over others to find information about very specific personal topics. In `query` pass the information you want to find about pets in the specified categories.
<br/> <br/>
- wikipedia_search(query) - Use this function when the information needed can be found on Wikipedia. It serves as a direct conduit to this comprehensive knowledge base. This provides extensive knowledge on a specific topic. Use this function accordingly.
<br/> <br/>
<br/> <br/>
===========================================================
<br/> <br/>
This is the format you need to output in:
<br/> <br/>
Thought: THINK AND ANALYZE THE INPUT, GOAL AND SITUATION
Act: If you need to call a tool or use a function, you can do it here: func(query). If no need to use a tool, leave this empty. The output of the function will be provided here.
Observation: Based on the Thoughts and results of the Act, provide a reply. If you are using a tool, no need to output this.
<br/> <br/>
Example Acts to use tools:
<br/> <br/>
- The current president of the United States of America is web_search(""Who is the current president of United States"")
<br/> <br/>
- wikipedia_search(""Joe Biden"") = You can output like this when the user wants extensive detail on a specific topic or person.
<br/> <br/>
<br/> <br/>
===========================================================
<br/> <br/>
Note that you must always follow the provided rules and output in the given manner. Using a function is not always necessary, use it only when needed.

</i>
</code>
</div></div><p>‍</p><h3>Function Calling</h3><p><a href=""https://openai.com/blog/function-calling-and-other-api-updates"">Function calling</a> is a feature released by OpenAI. This allows you to integrate your Chat GPT models like GPT-3.5 and GPT-4 directly with the functions you want to call. You can provide the schema of your functions or APIs and the model will use the provided functions when needed.</p><h3>‍</h3><p>Function Calling is the go-to way and probably the first step to take if you are looking to integrate an API with your GPT-4 or GPT-3.5.</p><h3>‍</h3><p>Here is an example of how you are supposed to pass the schema of your function to the model input:</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1137pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/650f66ff65468e3379420799_SXL1TZz_vD5NS3fGxzbM0mK3Sfe0JSz8WjpnNVeJkJFAa4lfoTIivla7F20_Bn5gRtBvwFKdZ4sYBNlLIj6kZGF25fiED2D3_7lF8icBySCm6AxT_Nvdsjp8kurMzCJZVtm8sXiZSLWYfozQ5-hWJkM.png""/></div></figure><p>‍</p><p>The model will use the function as needed:</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1140pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/650f66ff0ef65200ed45d160_bT3AbWWWY4uZRlhFk9xRpCYUkJvys8Ap9iEpoVqhzeLEPERvFwNHEtLSuyqNuBlLT8I-tp1xDrlCnW-W0CDlTaS3YpBrj4lk9_mhQasexQ3Lo1ya1Fkch7w3pepzr_FQ_IpPfTR8iJpnXPCKWhtH1Y4.png""/></div></figure><p>‍</p><h4>How effective is Function Calling for Tool Use?</h4><p>In our experience, we have found that OpenAI models usually work well with function calling. But as the number of functions grows and you try to add functions that are more custom to your needs, the quality drops quickly and drastically. Also, we have found that the token usage also increases greatly, this is because OpenAI adds the prompts to the system prompt, and the JSON schema takes up a lot of tokens when compared to ReAct or simple tool use prompting.</p><p>‍</p><p>Many users have also reported that models sometimes hallucinate and output random function names. Here is a good forum post to read that shows how unreliable function calling is: <a href=""https://community.openai.com/t/function-calling-very-unreliable/268439"">Function Calling Very Unreliable</a>.</p><p>‍</p><h2>Training LLMs to Use Tools</h2><h3>Toolformer</h3><p><a href=""https://arxiv.org/abs/2302.04761"">Toolformer </a>is a model by Meta trained to decide which API to call and then call it. Meta trained this model specifically for tool use and has shown great results. The model calls the functions and stops the generation, then the tool use pipeline provides an answer and the generation continues.</p><p>‍</p><p>This approach even though simple, is quite effective. But has major issues. For example, this approach works as long as the responses from the tools are short. If the responses grow in length, the quality of outputs will stop dropping. And most of the time the responses from the tools are going to be long and complicated, this can lead to context overflow and model forgetting what was being talked about originally.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:770pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/650f66ffa38686bf04abf980_mt9P0TbtdfrbjbG4kK-GDHjbDTiMxx9ElrWke0dc4X-83xdMBsaTGHTMclTEp0-GHR6clMt9UTe8dm3j7M9PlqKv-JvfOeeefnbXWO37OlL3kpdOZtlZeNO5RBp7YS-Agg3uxfzVF2ewI37XlwNuTqs.png""/></div></figure><h3>Gorilla LLM</h3><p><a href=""https://gorilla.cs.berkeley.edu/"">Gorilla LLM</a> is a large language model coming out of Microsoft and UC Berkeley that can generate API calls from natural language queries. Gorilla LLM can understand and use over 1,600 APIs from various domains, such as machine learning, cloud computing, and web development.</p><p>‍</p><p>Gorilla LLM is trained on three massive machine learning hub datasets: Torch Hub, TensorFlow Hub, and HuggingFace. It also uses a document retriever to adapt to changes in the API documentation and provide accurate and up-to-date results. Gorilla LLM outperforms other large language models, such as GPT-4, Chat-GPT, and Claude, in writing API calls. </p><p>‍</p><div class=""w-embed w-iframe""><iframe allow=""accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"" allowfullscreen="""" frameborder=""0"" height=""515"" src=""https://www.youtube.com/embed/RMgM3tPTpXI?si=zPO5gsP8npqLtIbX"" style=""display:block;"" title=""YouTube video player"" width=""960""></iframe></div><p>‍</p><h2>Use Cases of LLMs integrated with APIs and other Tools</h2><p>Now that we have discussed how to connect APIs and Tools with LLMs, let’s talk about some of the use cases we have for this.</p><h3>Integrating with Email Service Providers</h3><p>This is perhaps the most obvious one. Email inboxes have become very messy with hundreds of emails coming in every day. This makes it incredibly difficult to process information properly and reply to them timely. We already have spam filters, but they do not help clean up the mess we have in our inboxes.</p><p>‍</p><p>LLMs can be paired together with these inboxes to read your emails and provide summaries, prioritize what emails to reply and even reply to your emails if allowed to. Even very simple, 3 billion parameter models can be deployed to take care of these tasks.</p><p>‍</p><p>You can build a private assistant to take care of your emails end to end using an LLM connected with GMail API, or via IMAP and SMTP.</p><h3>Integrating with CRM Systems</h3><p>Customer Relationship Management tools are extremely messy. Multiple teams use it, from sales to marketing to support and whatnot. CRMs are used for multiple things like storing customer data, call transcripts, feedback, and a ton of other data. And this data needs to be shared across teams. Hence, maintaining CRMs can be very complicated for everyone.</p><p>‍</p><p>LLMs can be integrated with CRM APIs to simplify a ton of workflows, for example, LLMs can <a href=""https://www.mercity.ai/blog-post/gpt-nlp-in-sales#sales-call-analytics"">generate meeting notes</a>, which can save salespeople a ton of time. It can extract valuable insights from support and customer meetings for marketing teams and put everything in a proper, consumable format.</p><p>‍</p><p>LLMs can compress information spread in CRMs and generate simple reports for pretty much anything or any specific customer you have.</p><h3>Integrating with CMS</h3><p>Similar to CRMs, content management systems, and pipelines also have multiple functionalities, from creating and writing content to editing and to SEO optimization and whatnot. Language models can easily be integrated with every bit of these pipelines.</p><p>‍</p><p>LLMs can be used to generate content, edit, and remove any unnecessary parts. LLM agents can also be deployed to plan, and generate content outlines, and then go ahead and generate the actual content and publish it.</p><p>‍</p><p>WordPress APIs are one of the best and easiest to integrate with LLMs as you can access almost all parts of the pipeline.</p><p>‍</p><h2>Challenges of Integrating Tools with LLMs</h2><p>Even though we have outlined many approaches to integrate LLMs with tools, there are still many challenges that make this task difficult. Let’s go over them.</p><h3>Context Overflow</h3><p>As seen with Toolformer and OpenAI function calling, context overflow is a big issue. This happens because we need to prompt the model with the tools we want to integrate. This means we need to add the tool names, descriptions of the tools, examples of how to use it when to use it, and more details. This can lead to major issues like a reduction in the output length because the prompt itself is so long Or a significant increase in token usage costs.</p><h3>Accuracy drops as the number of tools grows</h3><p>This is pretty evident. As the number of tools integrated with an LLM increases, maintaining accuracy and efficiency can become a difficult task. If your tools are very similar to each other, or if you don’t provide good enough examples, the model can get confused and call the wrong functions at times. Or not call a function at all! This can be fixed with better prompting.</p><h3>Latency</h3><p>Latency is a significant challenge when integrating tools with LLMs. The time it takes for data to be processed and for results to be produced from the tools. High latency can lead to delays in decision-making and can negatively impact the user experience. This is particularly problematic in real-time applications, where delays of even a few seconds can have significant issues.</p><h3>Trust</h3><p>This is not a huge issue, but if you are using the model to generate code or to act on your behalf, you need to trust the model. If the model, for example, replies incorrectly or deletes the wrong files from your folder, it can cause major problems. This can simply be fixed by making sure there is a human in the loop and reviewing the steps taken by the LLM.</p><p>‍</p><h2>Want to integrate your tools with LLMs?</h2><p>If you want to integrate your own Email Systems, CRMs, or any other APIs or internal or external tools with private or public LLMs, <a href=""https://www.mercity.ai/contacts"">contact us</a>. We have a ton of experience building applications that require a tight integration of tools and models like LLaMA, GPT-4, etc.7</p><p>‍</p></div>"
Comprehensive guide to Large Scale IDP Systems,guide-to-large-scale-idp-systems,640f56f76d313b2faa631c11,6802d60f28ec83296f64bea2,False,False,Fri Apr 18 2025 22:45:35 GMT+0000 (Coordinated Universal Time),Fri Apr 18 2025 22:45:35 GMT+0000 (Coordinated Universal Time),Fri Apr 18 2025 22:45:35 GMT+0000 (Coordinated Universal Time),"<p id="""">Extracting any value from this kind of data at scale is incredibly challenging. This is where modern IDP systems help, they turn messy documents into clean and usable data.</p><p id="""">However, to see why IDP is such a leap forward, let's first understand the traditional approaches to document processing.</p><h2 id="""">How Document Processing worked before IDP&nbsp;</h2><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6802d371852569fbd7e89ae1_AD_4nXdLvW4PbfYtNGQztLZkqZWkJRVYEMkojAAFC__rf_Av3h25nm0mt0FsyIvhI97obfTdLOG1NaaErFJfMRw3eCLYt6T0v4k3BA5wBhxg95N4PL1Oaq4fatNSSK-6Xaf_epUJNVu2.jpeg"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">The idea of automating document processing isn’t new. For years, businesses have long relied on software to speed up document handling and reduce manual work. One of the earliest solutions built for this was <strong id="""">Automated Document Processing (ADP)</strong>.</p><p id="""">‍</p><p id="""">Traditional ADP systems rely on fixed rules, predefined templates, and consistent formats to extract data from structured or semi-structured documents. They perform reasonably well when the layout is predictable, like invoices or application forms that follow the same structure every time.</p><p id="""">In practice, this means someone manually creates templates for each document type. Extraction rules are written to pull out specific fields like names, dates, or totals. But the moment the layout shifts, even slightly, those rules start to break. Any change means going back in to fix or rebuild the logic.</p><p id="""">This makes ADP workable only in tightly controlled environments. It struggles with variability. It can’t deal with documents that are unstructured or inconsistent. And most critically, it doesn’t learn or adapt. Once deployed, it stays static.&nbsp;</p><p id="""">But real-world documents are rarely that neat. Layouts change. Content shifts. And this level of messiness quickly overwhelms systems that rely on rigid rules.</p><p id="""">That’s where <strong id="""">Intelligent Document Processing (IDP)</strong> comes in, a system built to handle the complexity that ADP can’t.</p><h2 id="""">What is Intelligent Document Processing (IDP) ?&nbsp;</h2><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""center"" data-rt-max-width=""1600px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6802d373ad9e71e88726cafc_AD_4nXeFf9dBbq7oD7bx5pKHvXal9q5CW2es7ZA3dfUU2WmwLUElFhmEpSpTQCNu6rGtjoV-xoE2_m96OLNgaFd9GdH2UnICMT6Soo_xNZxaWUDyteVw0oCKYyL5da_MhykK5-UPL503fg.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">Intelligent Document Processing (IDP) is a method for extracting, classifying, and processing information from documents using techniques such as Optical Character Recognition (OCR), Natural Language Processing (NLP), Machine Learning, and in some cases, Robotic Process Automation (RPA).&nbsp;</p><p id="""">Unlike traditional rule-based systems, IDP is designed to handle documents with inconsistent formats, varied structures, and unstructured content without manual intervention or rigid templates.</p><p id="""">It starts with OCR, which converts scanned images and PDFs into machine-readable text. NLP models then interpret the language and context, identifying key fields, sections, or entities. Deep Learning/Machine learning steps in to recognize patterns across different document types and adapt as more data flows through the system. In many setups, RPA is also used to automate the actions that follow—like updating databases, triggering workflows, or sending responses.</p><p id="""">Together, these components allow IDP to work in messy, real-world conditions where structure is the exception, not the rule. It scales better, fails less often, and removes the need for constant rule maintenance, making it far more robust than traditional approaches like ADP.</p><p id="""">But things get trickier when documents get longer.</p><h2 id="""">Challenges in Large-Scale IDP Systems&nbsp;</h2><p id="""">Large documents like contracts, policy manuals, financial reports, legal case files, bring a different level of complexity. It’s not just about pulling out text. It’s about capturing structure, preserving context, and maintaining consistency across hundreds of pages.</p><p id="""">Here’s where things start to break.</p><h3 id="""">Complex Layouts</h3><p id="""">Large documents come with a structure that’s hard to ignore—multi-column layouts, tables, sidebars, footnotes, headers, stamps. OCR alone treats all of this as flat text. That means merged columns, broken tables, and lost context.</p><p id="""">Layout-aware models like <a href=""https://huggingface.co/docs/transformers/en/model_doc/layoutlm"" id=""""><strong id="""">LayoutLM</strong></a> and <a href=""https://arxiv.org/abs/2106.11539"" id=""""><strong id="""">DocFormer</strong></a> solve this by combining text with positional and visual features. This helps the model understand that a price in a table isn’t the same as a number in a paragraph, even if they look alike.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6802d37165ab0af8102dcce4_AD_4nXdJBZkSdFjoeQPUpRuDiFhhso-Y1406Cys_HyJVZcJFxX-37VKGgOzlaz9YI5PZbSPqXv5tVFuPTUU7Y3W1b_HDncMYu0KiH994u1mR0ybQnWuzIle4LT5JPp1GIq6sN9gKXFQKkg.jpeg"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id=""""><em id="""">In the figure above, the green and red boxes are called “bounding boxes”. Bounding boxes help the system understand not just what’s written, but also where it appears on the page—so the layout and structure are preserved</em></p><p id=""""><a href=""https://huggingface.co/docs/transformers/en/model_doc/donut"" id=""""><strong id="""">Donut</strong> </a>takes a different route. It skips OCR completely and reads document images directly using a vision transformer. Models like<a href=""https://arxiv.org/abs/2210.02849"" id=""""> <strong id="""">XDoc</strong></a> use layout graphs, treating content blocks as nodes and visual relations as edges to capture document structure.&nbsp;</p><p id="""">Without layout-aware models, the structure breaks down and the content loses meaning.</p><h3 id="""">Cross Page-Context&nbsp;</h3><p id="""">In small documents, everything you need is usually on one page. In large ones, that's rarely the case. Entities span pages, tables split across breaks, and references like “see clause 2.1” point to earlier sections.</p><p id="""">Medical documents are a good example. A patient's diagnosis might appear early on, while supporting test results are buried deep in the appendix. If a system can’t connect the two, the output loses critical context.</p><p id="""">Most basic models process one page at a time, so they miss these links. To solve this, newer approaches use long-context transformers like <a href=""https://huggingface.co/docs/transformers/en/model_doc/longformer"" id=""""><strong id="""">Longformer</strong> </a>and <a href=""https://huggingface.co/docs/transformers/en/model_doc/big_bird"" id=""""><strong id="""">BigBird</strong></a>, which can handle thousands of tokens and preserve context across pages.</p><p id="""">A strong IDP pipeline needs this continuity. Without this, large documents just don’t make sense.&nbsp;</p><h3 id="""">High Variability</h3><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1388px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1388px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6802d37283a262e9bc517916_AD_4nXeXJ3rDmeWnu-Vl1g4J4p24ize2gNF63mvjgOSmx_Iz1nNDeaHzH8X5xt42arx8_w-hD0Avnw9fl7FecWAzh5yD6qdsKvTqKhKrRMY8N3SpEDVx2VVtQb3mSMnoqSQqb3gykyQccw.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">Even when documents fall under the same type—like contracts or manuals—their formats can be completely different. One contract might list terms in a table, another in plain text. One manual uses bullet points, another uses numbered steps. The wording, layout, and structure often change across teams or versions.</p><p id="""">Fixed-template systems easily break in this setup. Layout-aware and pre-trained models, like <a href=""https://huggingface.co/docs/transformers/en/model_doc/layoutlmv3"" id=""""><strong id="""">LayoutLMv3</strong></a>, handle this better. They learn from patterns across documents rather than depending on fixed positions or templates. Instead of expecting the “price” to always appear in the same place, they learn what price <em id="""">looks like</em>—in context.</p><p id="""">This flexibility is key. The system has to generalize beyond the examples it’s seen, because in the real world, documents don't stick to a single format.</p><h3 id="""">Volume, Scalability, and Cost <strong id="""">&nbsp;</strong></h3><p id="""">Enterprises usually don’t deal with just a few documents. They process tens of thousands of pages every day, sometimes in real time.</p><p id="""">This creates three major demands:</p><ul id=""""><li id=""""><strong id="""">Speed</strong> – the system must handle documents quickly, with low latency</li><li id=""""><strong id="""">Accuracy</strong> – mistakes can be costly, especially in legal or regulated contexts</li><li id=""""><strong id="""">Cost</strong> – cloud APIs often charge per page, and processing at scale adds up fast</li></ul><p id="""">Many use services like<a href=""https://aws.amazon.com/textract/"" id=""""><strong id=""""> AWS Textract</strong></a><strong id="""">, </strong><a href=""https://azure.microsoft.com/en-us/products/ai-services/ai-document-intelligence"" id=""""><strong id="""">Azure Form Recognizer</strong></a><strong id="""">, </strong>or <a href=""https://cloud.google.com/document-ai"" id=""""><strong id="""">Google Document AI</strong></a>. These offer built-in OCR and layout parsing, so one doesn't need to build models from scratch. But every page processed adds to the bill, and retries or post-processing push costs even higher. A good IDP setup has to deliver on all three fronts, it should be fast, accurate, and affordable, even at scale.</p><h2 id="""">Core Technologies that Power IDP</h2><p id="""">We’ve already seen how IDP brings together multiple techniques to handle documents the way humans do. Now let’s take a closer look at the core technologies behind it, and how they actually work in an IDP pipeline.</p><h3 id="""">Optical Character Recognition (OCR)</h3><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:532px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""532px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6802d371e173e30b685abd84_AD_4nXeSqI6gWdWFjvsaiCphSqMbVEdSJzMDjUXFDCmK7vUii3D1EI5FCxSkEq2cDyUcGojk-QzPbZLF6KvNrcvmppCbEBhFZ8SGS2WzwAdxn1FDXBcOzM_r4mrFjDn-9Axe_-TLfRMuAQ.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">OCR is the foundation for making visual documents machine-readable. It converts scanned PDFs, images, and printed text into structured digital text that can be understood by the next steps in the IDP pipeline. OCR quality determines how well everything else performs. Misread characters, incorrect reading order, or broken layouts introduce noise that only gets amplified later. Getting this layer right is non-negotiable.</p><p id="""">But in real-world documents, plain OCR isn’t enough. There are handwritten notes, tables buried in multi-column layouts, and tightly structured forms. To handle this, modern OCR engines bundle in a few key capabilities.</p><h5 id="""">Intelligent Character Recognition (ICR)</h5><p id="""">ICR is designed for handwriting, think of handwritten forms or doctor’s notes. It deals with stylistic variations, messy handwriting, and faded text by using neural networks to learn from lots of writing styles and scripts.</p><p id="""">Here’s what that looks like in practice: this diagram shows a typical ICR use case, a doctor’s handwritten report. Specific areas on the document, like <em id="""">""NS 1-2+"" or ""CC 2+""</em>, are manually written and refer to things like Nuclear Sclerotic and Cortical cataract grades.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6802d371fbe8748dc9ab4401_AD_4nXeEoXya3MnCy_3lWslLSgDhJjPVfgPidW1KRPWS6f32nZ919685H81WTja66BoocNXS6TP_ByRL7Z8CDBoBU2iq8fmwI4YHFr1rhvEFlk0sS0SbOS-tc0NqzB3s1Fl_xLQ-Pz_ekA.jpeg"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">The ICR engine zooms in on these regions, reads the handwriting, and interprets it into structured values. For example, it maps “NS 1-2+” to ""Nuclear Sclerotic Severity: 1-2+"" and “CC 2+” to ""Cortical Severity: 2+"".</p><p id="""">Instead of just reading the text, it turns messy handwriting into clear, labeled information that can be used for things like reports, summaries, or decisions.</p><h5 id="""">Layout Detection</h5><p id="""">We’ve already seen in the <em id="""">Complex Layouts</em> section how important layout understanding is, especially for documents with columns, tables, and sidebars. OCR systems with layout detection help preserve structure and context, essential for making sense of complex documents.</p><h5 id="""">Zonal OCR</h5><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6802d3734af9b9a38a1b00a6_AD_4nXe4vyX5Im4Oy3xyvmrVGxpJBGLv5YZhRh80SFQcAOka0chY3VfVAa6e5oUvRdF14B3NtqsgEoneBwl76psF3COUBoZCLbht8whogTEwaIN3HQ4_GuLF10QVsCyHxcGvAypLxD2cJA.gif"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">Zonal OCR is built for structure. It extracts texts from predefined regions called ‘zones’ on the document. This is especially useful when the layout is fixed, like tax forms, pay slips, or boarding passes. Instead of reading the whole page, the system goes straight to the fields that matter.&nbsp;</p><p id="""">ZonalOCR does not process the&nbsp; entire document. Instead, it directly focuses on the specified zones, making it lightweight, and therefore faster and more accurate for structured documents</p><p id="""">Cloud platforms like Google Document AI, Azure Form Recognizer, and AWS Textract have in-built zonal extraction capabilities. They return structured outputs, like text blocks, positions, and even table layouts</p><h3 id="""">Natural Language Processing (NLP)</h3><p id="""">Once OCR pulls out the text, it still has no idea what that text <em id="""">means</em>. That’s where NLP and Language Models come in.</p><p id="""">NLP helps the system read documents the way humans do—understanding structure, identifying key elements, and making sense of relationships between words and phrases. This is important in unstructured documents where the same data might be expressed in different ways.</p><p id="""">Some core NLP techniques used in IDP :</p><ul id=""""><li id="""">Named Entity Recognition (NER):<br>Spots real-world entities like names, dates, companies, or amounts. If a line says “Invoice issued by Acme Corp on Jan 5,” NER tags “Acme Corp” as a company and “Jan 5” as a date.</li><li id="""">Part-of-Speech Tagging:Labels each word by its grammatical role—noun, verb, adjective, etc. That helps disambiguate meaning, like “book a flight” vs. “read a book.”</li><li id="""">Dependency Parsing:Maps how words relate. In “The manager approved the request,” it identifies “manager” as the actor and “request” as the target.</li><li id="""">Semantic Role Labeling:Figures out who did what to whom. In “John sent the contract to Sarah,” it knows John is the sender, Sarah the recipient, and the contract the object.</li></ul><p id="""">These tools allow IDP systems to extract accurate information, even when the layout changes or the phrasing varies. Without NLP, documents are just a wall of text.</p><h3 id="""">Machine Learning and Deep Learning<strong id="""">&nbsp;&nbsp;</strong></h3><p id="""">While OCR and NLP handle the basics of getting the text out and understanding what it says, ML lets the system learn from examples and get better over time</p><h5 id="""">Supervised Learning&nbsp;</h5><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:850px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""850px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6802d37228ec83296f62ee08_AD_4nXc7HMtdZVhMd5DpqVpv6_4N4022qYNxkUNOe7kFogoG_0ZSvv5i9tbvTqWx6IZcpJDC_WgIvcUJEob8-COnlZGm8uSCadVS6KUtyg3lTkaYJZi8zTpafL5Bg1GIXTJoYDYQ41Xo.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">Supervised learning is the most common setup. If you’ve got a dataset of invoices where fields like “invoice number,” “total amount,” or “due date” are already tagged, a model can learn to extract those fields from new, unseen invoices (even if the formatting changes). This is important for consistent data extraction across large, diverse datasets.</p><p id="""">‍</p><p id="""">Large language models (LLMs) can also be fine-tuned on this kind of labeled data to handle even more variation in language and layout. They generalize better across formats, making the extraction more accurate and robust—especially when handling a single type of document.</p><h5 id="""">Unsupervised Learning&nbsp;</h5><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6802d373bd8a181ceb6fd048_AD_4nXdI0EVofps78bgeG6yw-M9ss0bvUhhP1lM_I8n7B8KJ3eK60e3J75pcs5CGUo66ZUZqrqN9SWUKS4n0AFMfSiA9YFc5hmRruzgebHLWDHZty4VemXkcg01GKuT47GdHa0ybwCHKbQ.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">Unsupervised learning is particularly used in IDP, when labeled data is limited. One common use is clustering, grouping together documents that share similar structure or content. This helps organize large collections, and detect new document types. For example, it can separate purchase orders, invoices, and receipts just by observing layout and language patterns.&nbsp;</p><p id="""">It’s also useful within large, multi-page documents. Different pages like—summaries, annexures, lab reports—and unsupervised models can group these page types without any labels.&nbsp;</p><p id="""">Finally, it’s great for spotting anomalies. If a scanned invoice looks very different from the usual pattern—maybe it’s missing a stamp or uses a strange layout—unsupervised models can flag it as an outlier.&nbsp;</p><p id="""">Here, LLMs can help by bringing a deeper understanding of the content itself. Instead of just clustering documents based on how they look, LLMs consider what the text actually says. This makes the patterns they uncover more meaningful.</p><h5 id="""">Deep Learning&nbsp;</h5><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:698px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""698px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6802d372f491d1780dd39059_AD_4nXf2Kd8KTXdj_n4uxJG3adTqzk10kX_tZLgMSdCYOWZsiuP6HltxD6pT6pnu8dRAif2qx9gq8GXxddiFpGrmLDKmP4mB1hwwOAgcQbLJci1Wx9BeNdWXgfhxTXo3am0kO6hov8lNMA.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">Deep learning takes things a step further. Models like <strong id="""">LayoutLM</strong> aren’t just reading the words—they also learn how the position of text on the page affects its meaning. They can tell the difference between a date in a header (like “Report Date: March 2023”) and a date mentioned in a paragraph (“The incident occurred in March 2023”). For scanned images or visual inputs, CNNs help pick up features like handwritten notes, stamps, or signatures.</p><p id="""">‍</p><p id="""">Layout-aware LLMs like <strong id="""">DocLLM</strong> combine both language understanding and layout structure, helping the system read documents more like a human would. This combination of visual and textual context leads to more reliable extractions.</p><p id="""">‍</p><p id="""">Together, these techniques give IDP systems the flexibility to scale and adapt in real-world use.</p><h3 id=""""><strong id="""">Robotic Process Automation (RPA)</strong></h3><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1051px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1051px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6802d3717b1e5c67e040752e_AD_4nXdjNHsJY_N-GsGUtd1TJnmyGedRWoZ7fj67t_3T7tlgg3lFML1K57Bgy4OWe-0bSPvJjrydIdJlMHJDxcv7yxf8NLAH_90vAOJx1EjjPvBLIYxBBCkdb_9EJsmb2-0QV6TVK0EEig.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">Once the data has been extracted and understood, it still needs to go somewhere. That’s where RPA comes in. It moves the output from the document processing steps into business systems, like CRMs, ERPs, or case management tools—without human intervention.</p><p id="""">For example, if a document has a low-confidence field, RPA can flag it for manual review. If everything looks good, it can route the extracted data to the right database, trigger an approval workflow, or send a notification to the relevant team.</p><p id="""">The key here is flow. RPA ensures that data doesn’t just sit there. It moves, acts, and completes tasks as part of a larger process.</p><p id="""">‍</p><h2 id="""">How to build an Large-Scale IDP System</h2><p>‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6802d37277208de253c4afb8_AD_4nXdHqxR0mAXwtMv2qJh1366Sm-Y5lcEMahoG4jOrOKi3Xd6zMHqFx0d9yqNt3KEKObYd8ID8iYT1PauCD5rg-gR4x3pv3c_aAR2igzXhZ-Yr2jCU5t894qiE7Y3iZB2l5sG0lkhOhg.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">Building a scalable Intelligent Document Processing (IDP) system isn't just about plugging in some OCR and calling it a day. At scale, performance becomes non-negotiable. The architecture needs to reflect that. That's why a good IDP setup breaks the process down into clear, loosely coupled , modular stages. Each stage solves one problem and passes clean output to the next. This keeps the entire system easier to maintain, optimize, and scale.</p><p id="""">Now, Large Language Models (LLMs) are changing how these stages work. Tasks that once needed separate tools or complex rules can now be handled by a single model. It's more accurate, simpler, has fewer components and easier to build.&nbsp;</p><p id="""">In the sections that follow, we’ll look at each component of the IDP pipeline and how LLMs are transforming every step:</p><h3 id="""">Ingestion</h3><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6802d372ce26b01842e1d4b2_AD_4nXcKiFQWrkg3toXAuLHdtLEy_u9fGFFeeDpNQnn4138kVxMykj3stI6Sn0BsYsn1TBw58Z66K2bHONT1UFnacI7L4KPIB1yBgoUG8iXBYfb8vdqAky1kqRemeSJYgokBLS9YEETC8g.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">It all starts with ingestion. Documents don’t come in from just one place. Some arrive as email attachments, some get uploaded through internal tools, others are pulled in via APIs or dropped into shared folders by third-party platforms.</p><p id="""">The first thing the system does is save each file to a central document store—like Amazon S3, Azure Blob Storage, or Google Cloud Storage. This is where the raw files live. Think of it as the inbox of the system, keeping everything safe and traceable.</p><p id="""">Once stored, a message is sent to a queue—using tools like Kafka, AWS SQS, or Azure Service Bus. Every file goes in here, often tagged with metadata: details like where the document came from, what kind it is, when it arrived, or which team it belongs to.</p><p id="""">That metadata helps the system decide what to do next. A file tagged as “invoice” might go through a different processing path than one marked “contract.” The goal here is simple: capture the input cleanly and reliably, while keeping track of where it came from and what needs to happen next.</p><h5 id="""">How LLMs improve Ingestion</h5><p id="""">Ingestion hasn’t changed much—but LLMs still play a role. Once metadata is captured, an LLM can help with intelligent routing. Instead of relying on strict rules to classify documents, a model can quickly read a sample and say, “this looks like a vendor contract” or “this is an expense receipt,” even when filenames or templates vary. That reduces the need for separate classification models and makes routing easier.</p><p id="""">Integrations like Anthropic’s Claude with Google Workspace allow AI assistants to directly access and interpret documents from platforms such as Gmail and Google Docs.</p><h3 id="""">Preprocessing&nbsp;</h3><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6802d3726515a99596e1671a_AD_4nXehVEyrhQ9wnN0SSLYSwemmRYanXayPtnpiSmnGJN109UHQrq3ffZb2SBbEv6X7shItz2aeER-TM5Y10CC_TuLMggrh77LKrDrYkL9uCY2wVcCHnjUTnre86ATnlg25xtb6__Fq.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">Once documents are in, preprocessing takes over. This is the stage that gets raw input ready for intelligent extraction. If the document is a scanned image or a PDF, the first job is to clean it up , remove smudges, fix rotation, sharpen text, and improve contrast. This isn’t just for cosmetic reasons; OCR engines perform far better on clear, well-aligned input.</p><p id="""">But preprocessing isn't limited to cleaning pixels. It also prepares the document logically. For example, large PDFs might need to be split into individual pages or grouped by sections before further analysis. Language detection kicks in here too—using tools like FastText or langdetect—to route documents through the right language-specific models in the later steps.</p><p id="""">This step bridges the gap between raw input and structured understanding. It makes sure each document is in the best possible shape before any data is pulled from it. In high-volume systems, small improvements here can have a big impact on accuracy and processing speed down the line.</p><h5 id="""">How LLMs improve Preprocessing</h5><p id="""">LLMs make preprocessing smarter. They don’t just clean up documents—they can also spot missing sections, blank signatures, or corrupted pages. But the best use case here is handling multiple languages. In older setups, you’d need extra tools to detect the language and route documents to the right models.&nbsp;</p><p id="""">With LLMs, that’s built-in. The same model can read, understand, and adjust on the fly. Open models like <strong id="""">BLOOM</strong> and <strong id="""">Falcon</strong> can handle this, but <strong id="""">Claude Sonnet 3.5</strong> and <strong id="""">GPT-4o</strong> are leading the way for complex, real-world documents.</p><h3 id="""">Extraction</h3><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6802d37328ec83296f62ee86_AD_4nXcnNWdksZKde1iY3oqYI4mgCY0Q88tBuOGSmTN7c5UR8ojMAVNqmbRm4cZB78vUjDmRQ0GRJufr3ikUSBvkZW5uW8xA3ezKCIzUFwtSxBl2RBRvA5aVidn2cfU-5PIPZBaVvGjgqw.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">Extraction is the stage where the system starts pulling out the actual information you care about. The document might be a scanned invoice, a contract, or an application form—but the goal is always the same: turn that jumble of text into structured data like names, dates, totals, or clauses.</p><p id="""">To do this, the system might use simple rules (like ""look for the word ‘Total’ and grab the number next to it"") or more advanced models that have been trained to recognize patterns across many documents. If the layout matters—like in tables or forms—models like LayoutLM or Donut help by understanding both what the text says and where it appears on the page.</p><p id="""">Depending on the use case, this step might involve things like finding key-value pairs (e.g., “Name: John Doe”), spotting tables, or identifying specific sections in a legal contract.</p><p id="""">In short: this is where the system reads the document and picks out the pieces that matter.</p><h5 id="""">How LLMs improve Extraction</h5><p id="""">This is where large language models really shine. Instead of building a new model for every document type, you can just prompt the LLM with what you need:<br><em id="""">“Extract the invoice date, total amount, and vendor name.”<br></em>It figures out the rest, even when the wording changes or the fields are scattered across the page.</p><p id="""">When layout really matters—like in tables or structured forms—layout-aware techniques help the model understand not just what the text says, but where it appears. Layout-aware models like Donut or LayoutLM are still useful, especially when field position is important. But for most cases, LLMs are now good enough to handle all kinds of formats with minimal setup.</p><p id="""">‍</p><h3 id="""">Integration&nbsp;</h3><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6802d37252a066a10205b824_AD_4nXfDK2vvbVS-DMnpBcIRIKZI1PBk75UWW92UgeO0oKZYQndRgoqhidYNZxTZrwmna_HNlciu0KCv7E6J83AGyjuwdY7t7cl1Jl1Uu8WD224PIrSJAzD6rcFxxU3XDmawUNggKl9_Ww.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">Once the data is extracted and checked, it needs to go somewhere useful. That could be a database, a CRM like Salesforce, an ERP system, or even a shared storage bucket in JSON or CSV format. The goal is to plug the data into systems where it can actually be used, whether that’s updating records, kicking off approvals, or feeding into dashboards.</p><p id="""">To keep everything running smoothly, teams often use tools like message queues (Kafka, SQS) or orchestration platforms (like Airflow or Step Functions) to track what happens next and make sure nothing gets lost. This part ties the whole pipeline back to the business—turning documents into action.</p><p id="""">What really makes this kind of architecture work is Modularity. Each step takes in something clear, does its work, and passes along a well-defined output. That makes it easy to improve things over time, making it scalable and maintainable.&nbsp;</p><h5 id="""">How LLMs improve Integration</h5><p id="""">Large Language Models are starting to reshape how RPA works. Where traditional RPA relies on hand-coded scripts and rule-based flows, LLMs handle logic based on context. You don’t need to define every possible condition. You just describe what needs to happen, and the model figures it out.</p><p id="""">This blurs the old boundary between extraction and RPA. In many setups, there’s no need for a separate RPA layer. The same model that reads the document can decide where the data goes, what task to trigger, or who to notify.</p><p id="""">With LLMs, teams can define high-level prompts—“when this form comes in, validate totals, route for approval, notify finance”—and the model handles the rest. It’s simpler, more flexible, and has fewer moving parts.</p><h2 id="""">Does your Business Need IDP ?&nbsp;</h2><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6802d373ec45566821f44ec1_AD_4nXf1ntwC5om-t0AlmTiI1Oq-sXBfbM4T1rj5PrAX-ZppzHXTFZSPqiCA7b8sKOxjUcn0wEIWZJqhKNb-pV-xotiT6Y7Ly-mNTpub0jsA8TXZfDGoh5dggckhgek98cN2PvgAUv0XcQ.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">Not every business needs Intelligent Document Processing. But if you're dealing with hundreds of unstructured documents every month, it's worth exploring.</p><p id="""">The first red flag is<strong id=""""> Manual effort</strong>. Are teams copying values from PDFs into spreadsheets? Sorting files by hand? These tasks don’t scale when volume increases and often lead to mistakes.</p><p id="""">The second is <strong id="""">Variability</strong>. Invoices that look different each time. Handwritten forms. Scanned documents from mobile phones. If the process breaks whenever a layout changes, that's a sign rules-based systems aren’t enough.</p><p id="""">The third is <strong id="""">Growth</strong>. What works for 100 documents a week won’t work at 10x. Without automation, scaling means hiring more people. IDP absorbs the volume without increasing headcount, making it a more economical solution.</p><p id=""""><strong id="""">Compliance </strong>is another challenge. If documents are scattered across emails and shared drives, it's tough to trace where a value came from or prove who approved what. IDP systems solve this by making everything traceable—so you always know where the data came from, and who handled it</p><p id="""">But not every setup needs IDP. If your documents are already digital, follow a consistent format, and come in low volumes, simpler tools or manual handling might be enough.</p><p id="""">Some major industries where IDP is used prominently are :</p><h3 id="""">Healthcare</h3><p id="""">Healthcare depends on paperwork, and mistakes can delay care. Intelligent Document Processing reduces that risk by making document handling faster and more accurate.</p><p id="""">Billing documents like claim forms and EOBs arrive in different formats. IDP reads them, pulls out codes, patient info, and insurance details, and sends it straight to billing systems—reducing errors and speeding up payments.</p><p id="""">During onboarding, patients submit forms, ID scans, or handwritten insurance cards. IDP extracts the data without needing manual entry, so staff can move faster and focus on care.</p><p id="""">For EHR updates, IDP processes clinical notes, lab reports, and referrals, converting them into structured data. This keeps patient records current without extra work.</p><p id="""">Insurance approvals often depend on long forms and dense policy documents. IDP extracts diagnoses, referral info, and coverage details, cutting down on delays and back-and-forth communication.</p><p id="""">Each step moves faster, with fewer errors and less manual work. In a system where every minute counts, IDP turns scattered documents into clear, usable data.</p><h3 id="""">Legal Case Management</h3><p id="""">Law firms handle large volumes of documents where accuracy is critical and manual review can’t keep up. Intelligent Document Processing helps manage that load without slowing things down.</p><p id="""">In contract review, IDP pulls out key clauses—like payment terms, renewal dates, and liability limits—no matter the contract format. This speeds up reviews and reduces the risk of missing important details.</p><p id="""">During litigation, case files can run into thousands of pages. IDP classifies documents automatically and tags names, dates, and entities, making it easy to search and organize case materials.</p><p id="""">For compliance, IDP scans regulatory filings and internal documents to flag missing or outdated language. Legal teams can catch issues early and avoid costly oversights.</p><p id="""">Onboarding new clients also gets easier. Scanned IDs, intake forms, and engagement letters are digitized and structured into matter management systems—cutting down on admin work.&nbsp;</p><h2 id="""">Want to build your IDP Pipeline?&nbsp;</h2><p id="""">Looking to build your own IDP pipeline? We’ve helped teams across industries process complex documents at scale. With proven workflows and production-ready setups, we can help you design and deploy a solution that fits your use case.</p><p id=""""><a href=""https://www.mercity.ai/contacts"" id="""">Book a call</a>, let’s streamline your workflow.</p><p id="""">‍</p>",https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6802d57b954b54d5c7553cea_idp_blog.png,Yash Sonawane,LLMs in Document Processing,"IDP processing is at the core of many large scale companies, in this blog we will talk about what exactly an IDP system is made of and how you can build and optimize your own IDP using LLMs and new techniques.",False,"<div class=""rich-text w-richtext""><p>Extracting any value from this kind of data at scale is incredibly challenging. This is where modern IDP systems help, they turn messy documents into clean and usable data.</p><p>However, to see why IDP is such a leap forward, let's first understand the traditional approaches to document processing.</p><h2>How Document Processing worked before IDP </h2><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6802d371852569fbd7e89ae1_AD_4nXdLvW4PbfYtNGQztLZkqZWkJRVYEMkojAAFC__rf_Av3h25nm0mt0FsyIvhI97obfTdLOG1NaaErFJfMRw3eCLYt6T0v4k3BA5wBhxg95N4PL1Oaq4fatNSSK-6Xaf_epUJNVu2.jpeg""/></div></figure><p>The idea of automating document processing isn’t new. For years, businesses have long relied on software to speed up document handling and reduce manual work. One of the earliest solutions built for this was <strong>Automated Document Processing (ADP)</strong>.</p><p>‍</p><p>Traditional ADP systems rely on fixed rules, predefined templates, and consistent formats to extract data from structured or semi-structured documents. They perform reasonably well when the layout is predictable, like invoices or application forms that follow the same structure every time.</p><p>In practice, this means someone manually creates templates for each document type. Extraction rules are written to pull out specific fields like names, dates, or totals. But the moment the layout shifts, even slightly, those rules start to break. Any change means going back in to fix or rebuild the logic.</p><p>This makes ADP workable only in tightly controlled environments. It struggles with variability. It can’t deal with documents that are unstructured or inconsistent. And most critically, it doesn’t learn or adapt. Once deployed, it stays static. </p><p>But real-world documents are rarely that neat. Layouts change. Content shifts. And this level of messiness quickly overwhelms systems that rely on rigid rules.</p><p>That’s where <strong>Intelligent Document Processing (IDP)</strong> comes in, a system built to handle the complexity that ADP can’t.</p><h2>What is Intelligent Document Processing (IDP) ? </h2><figure class=""w-richtext-align-center w-richtext-figure-type-image"" style=""max-width:1600px""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6802d373ad9e71e88726cafc_AD_4nXeFf9dBbq7oD7bx5pKHvXal9q5CW2es7ZA3dfUU2WmwLUElFhmEpSpTQCNu6rGtjoV-xoE2_m96OLNgaFd9GdH2UnICMT6Soo_xNZxaWUDyteVw0oCKYyL5da_MhykK5-UPL503fg.png""/></div></figure><p>Intelligent Document Processing (IDP) is a method for extracting, classifying, and processing information from documents using techniques such as Optical Character Recognition (OCR), Natural Language Processing (NLP), Machine Learning, and in some cases, Robotic Process Automation (RPA). </p><p>Unlike traditional rule-based systems, IDP is designed to handle documents with inconsistent formats, varied structures, and unstructured content without manual intervention or rigid templates.</p><p>It starts with OCR, which converts scanned images and PDFs into machine-readable text. NLP models then interpret the language and context, identifying key fields, sections, or entities. Deep Learning/Machine learning steps in to recognize patterns across different document types and adapt as more data flows through the system. In many setups, RPA is also used to automate the actions that follow—like updating databases, triggering workflows, or sending responses.</p><p>Together, these components allow IDP to work in messy, real-world conditions where structure is the exception, not the rule. It scales better, fails less often, and removes the need for constant rule maintenance, making it far more robust than traditional approaches like ADP.</p><p>But things get trickier when documents get longer.</p><h2>Challenges in Large-Scale IDP Systems </h2><p>Large documents like contracts, policy manuals, financial reports, legal case files, bring a different level of complexity. It’s not just about pulling out text. It’s about capturing structure, preserving context, and maintaining consistency across hundreds of pages.</p><p>Here’s where things start to break.</p><h3>Complex Layouts</h3><p>Large documents come with a structure that’s hard to ignore—multi-column layouts, tables, sidebars, footnotes, headers, stamps. OCR alone treats all of this as flat text. That means merged columns, broken tables, and lost context.</p><p>Layout-aware models like <a href=""https://huggingface.co/docs/transformers/en/model_doc/layoutlm""><strong>LayoutLM</strong></a> and <a href=""https://arxiv.org/abs/2106.11539""><strong>DocFormer</strong></a> solve this by combining text with positional and visual features. This helps the model understand that a price in a table isn’t the same as a number in a paragraph, even if they look alike.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6802d37165ab0af8102dcce4_AD_4nXdJBZkSdFjoeQPUpRuDiFhhso-Y1406Cys_HyJVZcJFxX-37VKGgOzlaz9YI5PZbSPqXv5tVFuPTUU7Y3W1b_HDncMYu0KiH994u1mR0ybQnWuzIle4LT5JPp1GIq6sN9gKXFQKkg.jpeg""/></div></figure><p><em>In the figure above, the green and red boxes are called “bounding boxes”. Bounding boxes help the system understand not just what’s written, but also where it appears on the page—so the layout and structure are preserved</em></p><p><a href=""https://huggingface.co/docs/transformers/en/model_doc/donut""><strong>Donut</strong> </a>takes a different route. It skips OCR completely and reads document images directly using a vision transformer. Models like<a href=""https://arxiv.org/abs/2210.02849""> <strong>XDoc</strong></a> use layout graphs, treating content blocks as nodes and visual relations as edges to capture document structure. </p><p>Without layout-aware models, the structure breaks down and the content loses meaning.</p><h3>Cross Page-Context </h3><p>In small documents, everything you need is usually on one page. In large ones, that's rarely the case. Entities span pages, tables split across breaks, and references like “see clause 2.1” point to earlier sections.</p><p>Medical documents are a good example. A patient's diagnosis might appear early on, while supporting test results are buried deep in the appendix. If a system can’t connect the two, the output loses critical context.</p><p>Most basic models process one page at a time, so they miss these links. To solve this, newer approaches use long-context transformers like <a href=""https://huggingface.co/docs/transformers/en/model_doc/longformer""><strong>Longformer</strong> </a>and <a href=""https://huggingface.co/docs/transformers/en/model_doc/big_bird""><strong>BigBird</strong></a>, which can handle thousands of tokens and preserve context across pages.</p><p>A strong IDP pipeline needs this continuity. Without this, large documents just don’t make sense. </p><h3>High Variability</h3><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1388pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6802d37283a262e9bc517916_AD_4nXeXJ3rDmeWnu-Vl1g4J4p24ize2gNF63mvjgOSmx_Iz1nNDeaHzH8X5xt42arx8_w-hD0Avnw9fl7FecWAzh5yD6qdsKvTqKhKrRMY8N3SpEDVx2VVtQb3mSMnoqSQqb3gykyQccw.png""/></div></figure><p>Even when documents fall under the same type—like contracts or manuals—their formats can be completely different. One contract might list terms in a table, another in plain text. One manual uses bullet points, another uses numbered steps. The wording, layout, and structure often change across teams or versions.</p><p>Fixed-template systems easily break in this setup. Layout-aware and pre-trained models, like <a href=""https://huggingface.co/docs/transformers/en/model_doc/layoutlmv3""><strong>LayoutLMv3</strong></a>, handle this better. They learn from patterns across documents rather than depending on fixed positions or templates. Instead of expecting the “price” to always appear in the same place, they learn what price <em>looks like</em>—in context.</p><p>This flexibility is key. The system has to generalize beyond the examples it’s seen, because in the real world, documents don't stick to a single format.</p><h3>Volume, Scalability, and Cost <strong> </strong></h3><p>Enterprises usually don’t deal with just a few documents. They process tens of thousands of pages every day, sometimes in real time.</p><p>This creates three major demands:</p><ul role=""list""><li><strong>Speed</strong> – the system must handle documents quickly, with low latency</li><li><strong>Accuracy</strong> – mistakes can be costly, especially in legal or regulated contexts</li><li><strong>Cost</strong> – cloud APIs often charge per page, and processing at scale adds up fast</li></ul><p>Many use services like<a href=""https://aws.amazon.com/textract/""><strong> AWS Textract</strong></a><strong>, </strong><a href=""https://azure.microsoft.com/en-us/products/ai-services/ai-document-intelligence""><strong>Azure Form Recognizer</strong></a><strong>, </strong>or <a href=""https://cloud.google.com/document-ai""><strong>Google Document AI</strong></a>. These offer built-in OCR and layout parsing, so one doesn't need to build models from scratch. But every page processed adds to the bill, and retries or post-processing push costs even higher. A good IDP setup has to deliver on all three fronts, it should be fast, accurate, and affordable, even at scale.</p><h2>Core Technologies that Power IDP</h2><p>We’ve already seen how IDP brings together multiple techniques to handle documents the way humans do. Now let’s take a closer look at the core technologies behind it, and how they actually work in an IDP pipeline.</p><h3>Optical Character Recognition (OCR)</h3><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:532pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6802d371e173e30b685abd84_AD_4nXeSqI6gWdWFjvsaiCphSqMbVEdSJzMDjUXFDCmK7vUii3D1EI5FCxSkEq2cDyUcGojk-QzPbZLF6KvNrcvmppCbEBhFZ8SGS2WzwAdxn1FDXBcOzM_r4mrFjDn-9Axe_-TLfRMuAQ.png""/></div></figure><p>OCR is the foundation for making visual documents machine-readable. It converts scanned PDFs, images, and printed text into structured digital text that can be understood by the next steps in the IDP pipeline. OCR quality determines how well everything else performs. Misread characters, incorrect reading order, or broken layouts introduce noise that only gets amplified later. Getting this layer right is non-negotiable.</p><p>But in real-world documents, plain OCR isn’t enough. There are handwritten notes, tables buried in multi-column layouts, and tightly structured forms. To handle this, modern OCR engines bundle in a few key capabilities.</p><h5>Intelligent Character Recognition (ICR)</h5><p>ICR is designed for handwriting, think of handwritten forms or doctor’s notes. It deals with stylistic variations, messy handwriting, and faded text by using neural networks to learn from lots of writing styles and scripts.</p><p>Here’s what that looks like in practice: this diagram shows a typical ICR use case, a doctor’s handwritten report. Specific areas on the document, like <em>""NS 1-2+"" or ""CC 2+""</em>, are manually written and refer to things like Nuclear Sclerotic and Cortical cataract grades.</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6802d371fbe8748dc9ab4401_AD_4nXeEoXya3MnCy_3lWslLSgDhJjPVfgPidW1KRPWS6f32nZ919685H81WTja66BoocNXS6TP_ByRL7Z8CDBoBU2iq8fmwI4YHFr1rhvEFlk0sS0SbOS-tc0NqzB3s1Fl_xLQ-Pz_ekA.jpeg""/></div></figure><p>The ICR engine zooms in on these regions, reads the handwriting, and interprets it into structured values. For example, it maps “NS 1-2+” to ""Nuclear Sclerotic Severity: 1-2+"" and “CC 2+” to ""Cortical Severity: 2+"".</p><p>Instead of just reading the text, it turns messy handwriting into clear, labeled information that can be used for things like reports, summaries, or decisions.</p><h5>Layout Detection</h5><p>We’ve already seen in the <em>Complex Layouts</em> section how important layout understanding is, especially for documents with columns, tables, and sidebars. OCR systems with layout detection help preserve structure and context, essential for making sense of complex documents.</p><h5>Zonal OCR</h5><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6802d3734af9b9a38a1b00a6_AD_4nXe4vyX5Im4Oy3xyvmrVGxpJBGLv5YZhRh80SFQcAOka0chY3VfVAa6e5oUvRdF14B3NtqsgEoneBwl76psF3COUBoZCLbht8whogTEwaIN3HQ4_GuLF10QVsCyHxcGvAypLxD2cJA.gif""/></div></figure><p>Zonal OCR is built for structure. It extracts texts from predefined regions called ‘zones’ on the document. This is especially useful when the layout is fixed, like tax forms, pay slips, or boarding passes. Instead of reading the whole page, the system goes straight to the fields that matter. </p><p>ZonalOCR does not process the  entire document. Instead, it directly focuses on the specified zones, making it lightweight, and therefore faster and more accurate for structured documents</p><p>Cloud platforms like Google Document AI, Azure Form Recognizer, and AWS Textract have in-built zonal extraction capabilities. They return structured outputs, like text blocks, positions, and even table layouts</p><h3>Natural Language Processing (NLP)</h3><p>Once OCR pulls out the text, it still has no idea what that text <em>means</em>. That’s where NLP and Language Models come in.</p><p>NLP helps the system read documents the way humans do—understanding structure, identifying key elements, and making sense of relationships between words and phrases. This is important in unstructured documents where the same data might be expressed in different ways.</p><p>Some core NLP techniques used in IDP :</p><ul role=""list""><li>Named Entity Recognition (NER):<br/>Spots real-world entities like names, dates, companies, or amounts. If a line says “Invoice issued by Acme Corp on Jan 5,” NER tags “Acme Corp” as a company and “Jan 5” as a date.</li><li>Part-of-Speech Tagging:Labels each word by its grammatical role—noun, verb, adjective, etc. That helps disambiguate meaning, like “book a flight” vs. “read a book.”</li><li>Dependency Parsing:Maps how words relate. In “The manager approved the request,” it identifies “manager” as the actor and “request” as the target.</li><li>Semantic Role Labeling:Figures out who did what to whom. In “John sent the contract to Sarah,” it knows John is the sender, Sarah the recipient, and the contract the object.</li></ul><p>These tools allow IDP systems to extract accurate information, even when the layout changes or the phrasing varies. Without NLP, documents are just a wall of text.</p><h3>Machine Learning and Deep Learning<strong>  </strong></h3><p>While OCR and NLP handle the basics of getting the text out and understanding what it says, ML lets the system learn from examples and get better over time</p><h5>Supervised Learning </h5><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:850pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6802d37228ec83296f62ee08_AD_4nXc7HMtdZVhMd5DpqVpv6_4N4022qYNxkUNOe7kFogoG_0ZSvv5i9tbvTqWx6IZcpJDC_WgIvcUJEob8-COnlZGm8uSCadVS6KUtyg3lTkaYJZi8zTpafL5Bg1GIXTJoYDYQ41Xo.png""/></div></figure><p>Supervised learning is the most common setup. If you’ve got a dataset of invoices where fields like “invoice number,” “total amount,” or “due date” are already tagged, a model can learn to extract those fields from new, unseen invoices (even if the formatting changes). This is important for consistent data extraction across large, diverse datasets.</p><p>‍</p><p>Large language models (LLMs) can also be fine-tuned on this kind of labeled data to handle even more variation in language and layout. They generalize better across formats, making the extraction more accurate and robust—especially when handling a single type of document.</p><h5>Unsupervised Learning </h5><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6802d373bd8a181ceb6fd048_AD_4nXdI0EVofps78bgeG6yw-M9ss0bvUhhP1lM_I8n7B8KJ3eK60e3J75pcs5CGUo66ZUZqrqN9SWUKS4n0AFMfSiA9YFc5hmRruzgebHLWDHZty4VemXkcg01GKuT47GdHa0ybwCHKbQ.png""/></div></figure><p>Unsupervised learning is particularly used in IDP, when labeled data is limited. One common use is clustering, grouping together documents that share similar structure or content. This helps organize large collections, and detect new document types. For example, it can separate purchase orders, invoices, and receipts just by observing layout and language patterns. </p><p>It’s also useful within large, multi-page documents. Different pages like—summaries, annexures, lab reports—and unsupervised models can group these page types without any labels. </p><p>Finally, it’s great for spotting anomalies. If a scanned invoice looks very different from the usual pattern—maybe it’s missing a stamp or uses a strange layout—unsupervised models can flag it as an outlier. </p><p>Here, LLMs can help by bringing a deeper understanding of the content itself. Instead of just clustering documents based on how they look, LLMs consider what the text actually says. This makes the patterns they uncover more meaningful.</p><h5>Deep Learning </h5><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:698pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6802d372f491d1780dd39059_AD_4nXf2Kd8KTXdj_n4uxJG3adTqzk10kX_tZLgMSdCYOWZsiuP6HltxD6pT6pnu8dRAif2qx9gq8GXxddiFpGrmLDKmP4mB1hwwOAgcQbLJci1Wx9BeNdWXgfhxTXo3am0kO6hov8lNMA.png""/></div></figure><p>Deep learning takes things a step further. Models like <strong>LayoutLM</strong> aren’t just reading the words—they also learn how the position of text on the page affects its meaning. They can tell the difference between a date in a header (like “Report Date: March 2023”) and a date mentioned in a paragraph (“The incident occurred in March 2023”). For scanned images or visual inputs, CNNs help pick up features like handwritten notes, stamps, or signatures.</p><p>‍</p><p>Layout-aware LLMs like <strong>DocLLM</strong> combine both language understanding and layout structure, helping the system read documents more like a human would. This combination of visual and textual context leads to more reliable extractions.</p><p>‍</p><p>Together, these techniques give IDP systems the flexibility to scale and adapt in real-world use.</p><h3><strong>Robotic Process Automation (RPA)</strong></h3><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1051pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6802d3717b1e5c67e040752e_AD_4nXdjNHsJY_N-GsGUtd1TJnmyGedRWoZ7fj67t_3T7tlgg3lFML1K57Bgy4OWe-0bSPvJjrydIdJlMHJDxcv7yxf8NLAH_90vAOJx1EjjPvBLIYxBBCkdb_9EJsmb2-0QV6TVK0EEig.png""/></div></figure><p>Once the data has been extracted and understood, it still needs to go somewhere. That’s where RPA comes in. It moves the output from the document processing steps into business systems, like CRMs, ERPs, or case management tools—without human intervention.</p><p>For example, if a document has a low-confidence field, RPA can flag it for manual review. If everything looks good, it can route the extracted data to the right database, trigger an approval workflow, or send a notification to the relevant team.</p><p>The key here is flow. RPA ensures that data doesn’t just sit there. It moves, acts, and completes tasks as part of a larger process.</p><p>‍</p><h2>How to build an Large-Scale IDP System</h2><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6802d37277208de253c4afb8_AD_4nXdHqxR0mAXwtMv2qJh1366Sm-Y5lcEMahoG4jOrOKi3Xd6zMHqFx0d9yqNt3KEKObYd8ID8iYT1PauCD5rg-gR4x3pv3c_aAR2igzXhZ-Yr2jCU5t894qiE7Y3iZB2l5sG0lkhOhg.png""/></div></figure><p>Building a scalable Intelligent Document Processing (IDP) system isn't just about plugging in some OCR and calling it a day. At scale, performance becomes non-negotiable. The architecture needs to reflect that. That's why a good IDP setup breaks the process down into clear, loosely coupled , modular stages. Each stage solves one problem and passes clean output to the next. This keeps the entire system easier to maintain, optimize, and scale.</p><p>Now, Large Language Models (LLMs) are changing how these stages work. Tasks that once needed separate tools or complex rules can now be handled by a single model. It's more accurate, simpler, has fewer components and easier to build. </p><p>In the sections that follow, we’ll look at each component of the IDP pipeline and how LLMs are transforming every step:</p><h3>Ingestion</h3><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6802d372ce26b01842e1d4b2_AD_4nXcKiFQWrkg3toXAuLHdtLEy_u9fGFFeeDpNQnn4138kVxMykj3stI6Sn0BsYsn1TBw58Z66K2bHONT1UFnacI7L4KPIB1yBgoUG8iXBYfb8vdqAky1kqRemeSJYgokBLS9YEETC8g.png""/></div></figure><p>It all starts with ingestion. Documents don’t come in from just one place. Some arrive as email attachments, some get uploaded through internal tools, others are pulled in via APIs or dropped into shared folders by third-party platforms.</p><p>The first thing the system does is save each file to a central document store—like Amazon S3, Azure Blob Storage, or Google Cloud Storage. This is where the raw files live. Think of it as the inbox of the system, keeping everything safe and traceable.</p><p>Once stored, a message is sent to a queue—using tools like Kafka, AWS SQS, or Azure Service Bus. Every file goes in here, often tagged with metadata: details like where the document came from, what kind it is, when it arrived, or which team it belongs to.</p><p>That metadata helps the system decide what to do next. A file tagged as “invoice” might go through a different processing path than one marked “contract.” The goal here is simple: capture the input cleanly and reliably, while keeping track of where it came from and what needs to happen next.</p><h5>How LLMs improve Ingestion</h5><p>Ingestion hasn’t changed much—but LLMs still play a role. Once metadata is captured, an LLM can help with intelligent routing. Instead of relying on strict rules to classify documents, a model can quickly read a sample and say, “this looks like a vendor contract” or “this is an expense receipt,” even when filenames or templates vary. That reduces the need for separate classification models and makes routing easier.</p><p>Integrations like Anthropic’s Claude with Google Workspace allow AI assistants to directly access and interpret documents from platforms such as Gmail and Google Docs.</p><h3>Preprocessing </h3><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6802d3726515a99596e1671a_AD_4nXehVEyrhQ9wnN0SSLYSwemmRYanXayPtnpiSmnGJN109UHQrq3ffZb2SBbEv6X7shItz2aeER-TM5Y10CC_TuLMggrh77LKrDrYkL9uCY2wVcCHnjUTnre86ATnlg25xtb6__Fq.png""/></div></figure><p>Once documents are in, preprocessing takes over. This is the stage that gets raw input ready for intelligent extraction. If the document is a scanned image or a PDF, the first job is to clean it up , remove smudges, fix rotation, sharpen text, and improve contrast. This isn’t just for cosmetic reasons; OCR engines perform far better on clear, well-aligned input.</p><p>But preprocessing isn't limited to cleaning pixels. It also prepares the document logically. For example, large PDFs might need to be split into individual pages or grouped by sections before further analysis. Language detection kicks in here too—using tools like FastText or langdetect—to route documents through the right language-specific models in the later steps.</p><p>This step bridges the gap between raw input and structured understanding. It makes sure each document is in the best possible shape before any data is pulled from it. In high-volume systems, small improvements here can have a big impact on accuracy and processing speed down the line.</p><h5>How LLMs improve Preprocessing</h5><p>LLMs make preprocessing smarter. They don’t just clean up documents—they can also spot missing sections, blank signatures, or corrupted pages. But the best use case here is handling multiple languages. In older setups, you’d need extra tools to detect the language and route documents to the right models. </p><p>With LLMs, that’s built-in. The same model can read, understand, and adjust on the fly. Open models like <strong>BLOOM</strong> and <strong>Falcon</strong> can handle this, but <strong>Claude Sonnet 3.5</strong> and <strong>GPT-4o</strong> are leading the way for complex, real-world documents.</p><h3>Extraction</h3><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6802d37328ec83296f62ee86_AD_4nXcnNWdksZKde1iY3oqYI4mgCY0Q88tBuOGSmTN7c5UR8ojMAVNqmbRm4cZB78vUjDmRQ0GRJufr3ikUSBvkZW5uW8xA3ezKCIzUFwtSxBl2RBRvA5aVidn2cfU-5PIPZBaVvGjgqw.png""/></div></figure><p>Extraction is the stage where the system starts pulling out the actual information you care about. The document might be a scanned invoice, a contract, or an application form—but the goal is always the same: turn that jumble of text into structured data like names, dates, totals, or clauses.</p><p>To do this, the system might use simple rules (like ""look for the word ‘Total’ and grab the number next to it"") or more advanced models that have been trained to recognize patterns across many documents. If the layout matters—like in tables or forms—models like LayoutLM or Donut help by understanding both what the text says and where it appears on the page.</p><p>Depending on the use case, this step might involve things like finding key-value pairs (e.g., “Name: John Doe”), spotting tables, or identifying specific sections in a legal contract.</p><p>In short: this is where the system reads the document and picks out the pieces that matter.</p><h5>How LLMs improve Extraction</h5><p>This is where large language models really shine. Instead of building a new model for every document type, you can just prompt the LLM with what you need:<br/><em>“Extract the invoice date, total amount, and vendor name.”<br/></em>It figures out the rest, even when the wording changes or the fields are scattered across the page.</p><p>When layout really matters—like in tables or structured forms—layout-aware techniques help the model understand not just what the text says, but where it appears. Layout-aware models like Donut or LayoutLM are still useful, especially when field position is important. But for most cases, LLMs are now good enough to handle all kinds of formats with minimal setup.</p><p>‍</p><h3>Integration </h3><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6802d37252a066a10205b824_AD_4nXfDK2vvbVS-DMnpBcIRIKZI1PBk75UWW92UgeO0oKZYQndRgoqhidYNZxTZrwmna_HNlciu0KCv7E6J83AGyjuwdY7t7cl1Jl1Uu8WD224PIrSJAzD6rcFxxU3XDmawUNggKl9_Ww.png""/></div></figure><p>Once the data is extracted and checked, it needs to go somewhere useful. That could be a database, a CRM like Salesforce, an ERP system, or even a shared storage bucket in JSON or CSV format. The goal is to plug the data into systems where it can actually be used, whether that’s updating records, kicking off approvals, or feeding into dashboards.</p><p>To keep everything running smoothly, teams often use tools like message queues (Kafka, SQS) or orchestration platforms (like Airflow or Step Functions) to track what happens next and make sure nothing gets lost. This part ties the whole pipeline back to the business—turning documents into action.</p><p>What really makes this kind of architecture work is Modularity. Each step takes in something clear, does its work, and passes along a well-defined output. That makes it easy to improve things over time, making it scalable and maintainable. </p><h5>How LLMs improve Integration</h5><p>Large Language Models are starting to reshape how RPA works. Where traditional RPA relies on hand-coded scripts and rule-based flows, LLMs handle logic based on context. You don’t need to define every possible condition. You just describe what needs to happen, and the model figures it out.</p><p>This blurs the old boundary between extraction and RPA. In many setups, there’s no need for a separate RPA layer. The same model that reads the document can decide where the data goes, what task to trigger, or who to notify.</p><p>With LLMs, teams can define high-level prompts—“when this form comes in, validate totals, route for approval, notify finance”—and the model handles the rest. It’s simpler, more flexible, and has fewer moving parts.</p><h2>Does your Business Need IDP ? </h2><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6802d373ec45566821f44ec1_AD_4nXf1ntwC5om-t0AlmTiI1Oq-sXBfbM4T1rj5PrAX-ZppzHXTFZSPqiCA7b8sKOxjUcn0wEIWZJqhKNb-pV-xotiT6Y7Ly-mNTpub0jsA8TXZfDGoh5dggckhgek98cN2PvgAUv0XcQ.png""/></div></figure><p>Not every business needs Intelligent Document Processing. But if you're dealing with hundreds of unstructured documents every month, it's worth exploring.</p><p>The first red flag is<strong> Manual effort</strong>. Are teams copying values from PDFs into spreadsheets? Sorting files by hand? These tasks don’t scale when volume increases and often lead to mistakes.</p><p>The second is <strong>Variability</strong>. Invoices that look different each time. Handwritten forms. Scanned documents from mobile phones. If the process breaks whenever a layout changes, that's a sign rules-based systems aren’t enough.</p><p>The third is <strong>Growth</strong>. What works for 100 documents a week won’t work at 10x. Without automation, scaling means hiring more people. IDP absorbs the volume without increasing headcount, making it a more economical solution.</p><p><strong>Compliance </strong>is another challenge. If documents are scattered across emails and shared drives, it's tough to trace where a value came from or prove who approved what. IDP systems solve this by making everything traceable—so you always know where the data came from, and who handled it</p><p>But not every setup needs IDP. If your documents are already digital, follow a consistent format, and come in low volumes, simpler tools or manual handling might be enough.</p><p>Some major industries where IDP is used prominently are :</p><h3>Healthcare</h3><p>Healthcare depends on paperwork, and mistakes can delay care. Intelligent Document Processing reduces that risk by making document handling faster and more accurate.</p><p>Billing documents like claim forms and EOBs arrive in different formats. IDP reads them, pulls out codes, patient info, and insurance details, and sends it straight to billing systems—reducing errors and speeding up payments.</p><p>During onboarding, patients submit forms, ID scans, or handwritten insurance cards. IDP extracts the data without needing manual entry, so staff can move faster and focus on care.</p><p>For EHR updates, IDP processes clinical notes, lab reports, and referrals, converting them into structured data. This keeps patient records current without extra work.</p><p>Insurance approvals often depend on long forms and dense policy documents. IDP extracts diagnoses, referral info, and coverage details, cutting down on delays and back-and-forth communication.</p><p>Each step moves faster, with fewer errors and less manual work. In a system where every minute counts, IDP turns scattered documents into clear, usable data.</p><h3>Legal Case Management</h3><p>Law firms handle large volumes of documents where accuracy is critical and manual review can’t keep up. Intelligent Document Processing helps manage that load without slowing things down.</p><p>In contract review, IDP pulls out key clauses—like payment terms, renewal dates, and liability limits—no matter the contract format. This speeds up reviews and reduces the risk of missing important details.</p><p>During litigation, case files can run into thousands of pages. IDP classifies documents automatically and tags names, dates, and entities, making it easy to search and organize case materials.</p><p>For compliance, IDP scans regulatory filings and internal documents to flag missing or outdated language. Legal teams can catch issues early and avoid costly oversights.</p><p>Onboarding new clients also gets easier. Scanned IDs, intake forms, and engagement letters are digitized and structured into matter management systems—cutting down on admin work. </p><h2>Want to build your IDP Pipeline? </h2><p>Looking to build your own IDP pipeline? We’ve helped teams across industries process complex documents at scale. With proven workflows and production-ready setups, we can help you design and deploy a solution that fits your use case.</p><p><a href=""https://www.mercity.ai/contacts"">Book a call</a>, let’s streamline your workflow.</p><p>‍</p></div>"
How to build a Visual Product Search Pipeline,how-to-build-a-visual-search-pipeline,640f56f76d313b2faa631c11,64bd8e36e0131ac35ba81baa,False,False,Sun Jul 23 2023 20:31:50 GMT+0000 (Coordinated Universal Time),Thu Aug 03 2023 20:14:03 GMT+0000 (Coordinated Universal Time),Thu Aug 03 2023 20:14:03 GMT+0000 (Coordinated Universal Time),"<p id="""">Visual Product Search is changing how we shop online. This shift has been possible due to the widespread use of smartphones and significant improvements in computer vision technology. This technology allows consumers to search for and discover products using images, instead of relying on traditional keyword searches.</p><p id="""">‍</p><p id="""">A prime example of this technology in action is Google Lens. It allows users to capture an image of a product, such as a dress, and then quickly find that exact item online from a vast selection of products. This entire process takes only a few seconds.</p><p id="""">‍</p><p id="""">In the past, if you were interested in a dress or a piece of furniture, you would have to inquire about it or spend a lot of time online trying to find the same product. However, Google Lens has simplified this process, taking care of the entire search operation.</p><p id="""">‍</p><p id="""">In this blog, we will explore how you can develop your own visual search AI for products. We will walk you step by step through the <strong id="""">code</strong> to build your own visual search.</p><p id="""">‍</p><p id="""">You can find the code here: <a href=""https://colab.research.google.com/drive/1dEC6QexaZ_tWH5nhGY0fcRIOuWd1W9_w?usp=sharing"" id=""""><strong id="""">Colab Notebook</strong></a>.</p><h2 id="""">What is Visual Product Search?</h2><p id="""">Visual Product Search, also known as Image-based Search, is a cutting-edge technology that leverages the power of artificial intelligence (AI) and machine learning (ML) to revolutionize the way we search for products online. It uses computer vision, a branch of AI that enables computers to understand and interpret visual information from the real world, to identify, analyze, and interpret images.</p><p id="""">‍</p><p id="""">In a typical Visual Product Search scenario, a user uploads or captures an image of a product they are interested in. The system then analyzes the image, identifies the product's key features, and searches a database for matching or similar items. The results are then presented to the user, who can select the most suitable product.</p><p id="""">‍</p><p id="""">Visual Product Search can reduce the time and effort customers spend on finding products, making the shopping process quicker and more enjoyable. It enhances the user experience by making product searches more intuitive and efficient. This can lead to increased customer satisfaction and loyalty, key factors for business growth and success in the competitive e-commerce market.</p><h3 id="""">Attribute Extraction</h3><p id="""">Attribute extraction is another part of Visual Product Search. It is the process of identifying specific features or attributes of a product in an image. This process is facilitated by machine learning algorithms and computer vision techniques.</p><p id="""">‍</p><p id="""">When a user uploads or captures an image of a product, the system first identifies the key attributes of the product. These could include color, shape, size, pattern, brand, style, and other unique features. The system then compares these attributes with the ones in the database to find matching or similar products.</p><p id="""">‍</p><p id="""">For instance, if a user uploads an image of a blue, floral-patterned dress, the system would identify these attributes and then search the database for dresses that are blue and have a floral pattern. The system might also consider other attributes such as the dress's style, length, and brand.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1570px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1570px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64bd8ccafbdbe6e05f985f45_p-G6Br-CkYfrldHJf9OZ539Q86ZxFgShLCE_XuYAKrIP2eZEUkmpNNLknYqLZuhCUZDuRMxZFC8feLQ_emNSZeV9zIO5_e4EdDtlwGzTQ9HIodKi2agKOLVamkKLK9YdYhhQtxqNQhnhLLByQO7FEls.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">‍</p><h2 id="""">Advantages of using Visual Search</h2><p id="""">Visual search technology offers several distinct advantages that make it a valuable tool for businesses, particularly in the e-commerce and retail sectors. By leveraging the power of AI and machine learning, visual search can enhance the online shopping experience, drive customer engagement, and provide valuable insights into customer behavior.</p><p id="""">‍</p><ul id=""""><li id=""""><strong id="""">Improved User Experience:</strong> Visual search makes product discovery more intuitive and efficient, reducing the time and effort required to find a specific product. This leads to a more enjoyable shopping experience and increased customer satisfaction.</li><li id=""""><strong id="""">Increased Conversion Rates:</strong> By providing more accurate and relevant search results, visual search can drive higher conversion rates. Customers are more likely to make a purchase when they can easily find what they are looking for.</li><li id=""""><strong id="""">Insights into Customer Behavior:</strong> The images customers upload can provide valuable insights into their preferences and buying behavior. This data can be used to personalize product recommendations and marketing strategies.</li><li id=""""><strong id="""">Competitive Advantage:</strong> Implementing visual search can give businesses a competitive edge in the market. It's an innovative feature that can differentiate a business from its competitors, attracting more customers and retaining existing ones.</li></ul><p id="""">‍</p><h2 id="""">How Visual Product Search works?</h2><p id="""">A visual search pipeline has a lot of components that tie together to make everything work. Let's dive into these components deeper.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1211px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1211px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64bd8cca1b90be3b5fa696a8_6-diC_oiDihc9ouQQVqRWDV9AevqFPJ-cyRXSwqWaqEEJjhdrQHzr8HKstAKjx2F4ebCgqk4VOuG5rskDOX2lqu0Hqf8ZQvxolqLB4uOjc7yYPBe-VF5dWVaS6prnHI3YkTeg1hdUXNYjiULBa1eVp0.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h3 id="""">Embeddings</h3><p id="""">Embeddings are perhaps the most important component in the search pipeline. They are essentially numerical representations of the passed textual or image data. The key property of these embeddings is that they learn the semantic meaning of the passed data. These embedding vectors can <em id="""">carry</em> the meaning of a text. This means data points with similar meanings or contexts will have their vectors at a closer distance than data points with different meanings or contexts. This property allows us to use these embeddings for <strong id="""">semantic search</strong>.</p><p id="""">‍</p><p id="""">In the context of visual search, we will be using multi-modal embeddings. Meaning we will convert both our product images and their textual descriptions into embedding vectors and then use them to find similar products. We will use the CLIP model.</p><h4 id="""">CLIP Model</h4><p id=""""><a href=""https://openai.com/research/clip"" id="""">CLIP</a> stands for Contrastive Language–Image Pre-training. It is a model released by OpenAI. We can use the CLIP model to get our embeddings.&nbsp;</p><p id="""">‍</p><p id="""">CLIP works by pairing images and descriptions and then reducing the distance between their vector representations. This trains the network to learn the properties of the passed data across text and images. This basically means that the distance between vectors of a passed image and its description will be very less. This is because the model has learned to put them together.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1511px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1511px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64bd8ccab703f9575b4c2107_W36jFIaor5aQb9YJe_Lw3WrsJdh6-EYRDCH0_x5auxN1F3vXkdgwkBMxsr4WfIYSHSHFmyb6uHJnnfd08eYegjqRFm1Ew-wz3dpvsqrXM2dNt6rs0nbZPKrCDVRwlnTxZdQFxg5L76OFBWdP184VAGE.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h4 id="""">Fashion CLIP</h4><p id="""">Fashion CLIP is a CLIP-like model fine-tuned specifically on the product images and their descriptions. You can find the model <a href=""https://github.com/patrickjohncyh/fashion-clip"" id="""">here</a>. We will use this model to build our product search pipeline.</p><h3 id="""">Embedding Search</h3><p id="""">Once we have our embedding we can start using them to search similar products. The Embedding Search is essentially a nearest neighbor search in the high-dimensional space of the embeddings. The goal is to find the product embeddings that are closest to the input embedding, as these represent the products that are most similar to the user's input.</p><p id="""">‍</p><p id="""">The result of the Embedding Search is a list of product embeddings, ranked by their similarity to the user's input. These can then be used to retrieve the corresponding product details from the database and present them to the user.</p><p id="""">‍</p><p id="""">Here is a good visual of embedding space by OpenAI. You can see how texts with similar meanings are closer, the text with different meanings is far away. We use this property to search for similar products.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:903px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""903px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64bd8ccab6b02693f0ee639a_ustVM_XA9o-wOBahgGAGdY0h47_MwzcOkB6eAaCLjHscTPkVm4Jd15t_VZhwJO7kY940AtFU0gcJgbXNii2I8nbrfVv4S3F7XHL-Siu5evKGxjUcUejMcLFgwl47lA8-qh4wJPagSlgoKGH1_d9UL3E.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">‍</p><h3 id="""">Vector DB</h3><p id="""">Vector databases, play a crucial role in the Embedding Search phase. They are specialized databases designed to handle high-dimensional vector data, like the embeddings generated by the CLIP or the Fashion-CLIP model.</p><p id="""">‍</p><p id="""">Traditional databases are not well-suited for storing and querying high-dimensional vector data. They are not designed to perform nearest-neighbor searches efficiently, which can result in slow response times and high computational costs.</p><p id="""">‍</p><p id="""">Vector DBs are optimized for these types of operations. They use advanced indexing techniques to organize the vector data in a way that makes nearest-neighbor searches faster and more efficient. Some Vector DBs also support distributed storage and parallel query execution, which can further enhance performance and scalability. Some good examples of vector databases are <a href=""https://www.pinecone.io/"" id="""">Pinecone</a>, <a href=""https://milvus.io/"" id="""">Milvus</a>, and <a href=""https://weaviate.io/"" id="""">Weaviate</a>.</p><h2 id="""">How to Build and Deploy a Visual Product Search Pipeline?</h2><p id="""">Now let’s discuss how you can build your own product search pipeline. We will use HuggingFace to build our model. Here is the step-by-step guide to coding your own visual search engine. You can follow along in this <a href=""https://colab.research.google.com/drive/1dEC6QexaZ_tWH5nhGY0fcRIOuWd1W9_w?usp=sharing"" id="""">Colab Notebook</a>.</p><p id="""">‍</p><h3 id="""">Load the Models</h3><p id="""">We start by loading the Fashion CLIP model and the processor. The model will give us the embeddings and the processor will help us load the different inputs.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64bd8cca4f4a45e8328aa5c4_AvszUHuUPv7gkAuecyO5EMx1yCAmzeJOJXy5TVp9mRt8aHfxTgio_KQi3DTz0lxRCQ23iyzO_HHIIt4NwFNq2kRidJW5MqZpncFeJxGmAcPLP70nGz_M-kKmd33iB3X1Exl3cNVUu0yaMc2LNIGPUlA.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">‍</p><h3 id="""">Download Data</h3><p id="""">Once we have the models and the processor ready, we will create a small dataset using online images.</p><p id="""">‍</p><div data-rt-embed-type='true'><pre>
<code class=""language-py"">
image_urls = [
          'https://crazymonk.in/wp-content/uploads/2022/04/Navy-Blue-Half-Sleeve-scaled.jpg',
          'https://ttbazaar.com/cdn/shop/products/TS135ROYALBLUEFRONT.jpg',
          'https://blackberrys.com/cdn/shop/files/polo-t-shirt-in-orange-cloud-blackberrys-clothing-1.jpg', 
          'https://cdn-img.prettylittlething.com/d/a/2/b/da2b4e203b3e48578ba79e6f6a0e4692edbebd61_cmc9361_1.jpg'
          ]
folder_name = ""images""

for index, url in enumerate(image_urls):
  try:
      response = requests.get(url)
      if response.status_code == 200:
          image_name = f""image_{index + 1}.jpg""
          # image_path = os.path.join(folder_name, image_name)
          with open(f'{folder_name}/{image_name}', ""wb"") as image_file:
              image_file.write(response.content)
          print(f""Image {index + 1} downloaded successfully."")
      else:
          print(f""Failed to download image {index + 1}: {response.status_code}"")
  except Exception as e:
      print(f""Failed to download image {index + 1}: {str(e)}"")
</code>
</pre></div><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64bd8cca0b26820044a6a21a_Ira7fUx0OCI5ngQ6w1J1sCC0pvwnspaif2GVpe2sVKFmzp1OZzwgBjZhDTELw_yxNBDD-cy-eAGVuhCszpZfejb9fZhoW1lTgbGPZmwZmXv7CTzFomlUujaH7xlu_pOaAE8jP1zizXGcFceoP1tDpew.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">‍</p><p id="""">Here I am loading 4 images,&nbsp; 2 of blue t-shirts, 1 of an orange t-shirt, and one of red boots. We will use these images for testing purposes. If you want to test it on your images, just drop their URLs in the image_urls list. Or you can just save them in the images folder too.</p><p id="""">‍</p><h3 id="""">Encoding the Images</h3><p id="""">Now that we have our images ready, we will convert them into embeddings.</p><p id="""">‍</p><div data-rt-embed-type='true'><pre>
<code class=""language-py"">
image_files = [f for f in os.listdir(folder_name) if os.path.isfile(os.path.join(folder_name, f))]
image_embeddings = []

for file_name in image_files:
  try:
      image_path = os.path.join(folder_name, file_name)
      image = Image.open(image_path)
      inputs = processor(images=image, return_tensors=""pt"")
      image_features = model.get_image_features(**inputs)

      image_embeddings.append((image_features, image_path))

  except Exception as e:
      print(f""Error processing image '{file_name}': {str(e)}"")
</code>
</pre></div><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64bd8cca705ce2028138ded3_d8DMYqDFV1-a-3x9V3GhcwFDsefPT0157Pl3Yyz_t-fIM70n3bOPOeXGCuUWNcqT10fpkiKqsj1ZOlClpzelDK7Q3HaLqCqBz1zSdR3vGinHi1iP4SWpgAKtdsV7lpi8j4CgwbHA6bggJTj6icdQc3M.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">‍</p><p id="""">The image_features list will store the image embeddings generated by the model using the ‘get_image_features()’ function. Now we can start querying the embeddings!</p><h3 id="""">Querying the Embeddings</h3><p id="""">For the purposes of this tutorial, we are not using any vector database. We will simply store the embeddings in a Python list and use cosine similarity to find the most relevant image given the text.</p><p id="""">‍</p><div data-rt-embed-type='true'><pre>
<code class=""language-py"">
QUERY = ""blue t-shirt without collar""


inputs = processor([QUERY], padding=True, return_tensors=""pt"")
text_features = model.get_text_features(**inputs)

sim_scores = []

for image_features, image_path in image_embeddings:
  similarity = cosine_similarity(image_features, text_features)
  similarity = round(float(similarity),2)

  sim_scores.append((similarity, image_path))

  print(f""Similarity score is {similarity} for image {image_path}"")
</code>
</pre></div><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64bd8ccbe0131ac35ba691b9_ct890cyXCv4EXiblXiQ-r-rbtOc9EggzkaMpcxZ3cYwUlc9zrQt_b1iFyMC3ZhNvDGwQNot6cyZ_AASGOe5a1JHkWSet3uTNkTA_LV4psJ956ZDih8RSuqLdXi0sP7Ky-iUtcqv_aqfPWC0kTFPk-5k.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">‍</p><p id="""">Here in the image, I am querying for “RED BOOTS”. And I get similarity scores for all the images, and when I open the image with the highest similarity score I get:</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center"" data-rt-max-width=""""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64bd8ccba7cb1c7fd4e90679_ic4KJ_xB7P9ePXUNZC0YVoT20blYclYhtDxlWqx-E4C2tHYtOczr04i-Jusi8-Qs3H7Da2afIkqPlW7gmTeFbhiyJA6x8CvTQku3BKmbWQO-xU1TIFuk-YE9SUJowsZV6Thg-k9vHRglqoFjJV6OSTE.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">‍</p><p id="""">We can try this with other queries too.&nbsp;</p><p id="""">‍</p><p id="""">This is what I get when I search “blue t-shirt with collar”:</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center"" data-rt-max-width=""""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64bd8ccb99271195aff3e54c_j1uskA-I2ZNtIofDkuHm8osPnCX51FyYAPEp7kE7iqS3HXkY7l-IUlIUg-u4JVX3ZeACJGWITsWt69wxWIfre2OEqYFqpJ7skPxWVovXfysxEGmP2RPnZstg394G-Z3U4LuP84OMkjRfe92bbv4-aEk.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">‍</p><p id="""">And this is when I search ‘blue t-shirt <strong id="""">without</strong> a collar”:</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center"" data-rt-max-width=""""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64bd8ccbb9983cf20c6cc710_bnrf_i6FHzRz3kmg080IRoAaOfqKS0NbgDflG9e0YVlpxEKMskKTDelbMWhMlnaZrDjxjHBzXmqXGNRPoYSyKPMeHiE3yCDOw8Lh0eKTVLnVruJc1xlHnCSnGRZcRGvFU-Jp_BKwIRlYNNruys279CE.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">‍</p><p id="""">This shows that the model is able to understand and differentiate between subtle features like “with collar” and “without collar”.</p><p id="""">‍</p><h2 id="""">Want to build a Visual Search Engine for your business?</h2><p id="""">We have a team of experienced Computer Vision Engineers who have worked on multiple such projects with CLIP and other multi-modal models. We can help you implement visual search features into your product and make it much better as a result. If you want to build and deploy such a solution for your own business, <a href=""https://www.mercity.ai/contacts"" id="""">reach out to us</a>!</p><p id="""">‍</p><p id="""">‍</p>",https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64bd8caec9190e8c5783cc4c_visual_search.png,Pranav,Computer Vision,A comprehensive step-by-step guide with code to building a visual product search engine using the CLIP model. ,False,"<div class=""rich-text w-richtext""><p>Visual Product Search is changing how we shop online. This shift has been possible due to the widespread use of smartphones and significant improvements in computer vision technology. This technology allows consumers to search for and discover products using images, instead of relying on traditional keyword searches.</p><p>‍</p><p>A prime example of this technology in action is Google Lens. It allows users to capture an image of a product, such as a dress, and then quickly find that exact item online from a vast selection of products. This entire process takes only a few seconds.</p><p>‍</p><p>In the past, if you were interested in a dress or a piece of furniture, you would have to inquire about it or spend a lot of time online trying to find the same product. However, Google Lens has simplified this process, taking care of the entire search operation.</p><p>‍</p><p>In this blog, we will explore how you can develop your own visual search AI for products. We will walk you step by step through the <strong>code</strong> to build your own visual search.</p><p>‍</p><p>You can find the code here: <a href=""https://colab.research.google.com/drive/1dEC6QexaZ_tWH5nhGY0fcRIOuWd1W9_w?usp=sharing""><strong>Colab Notebook</strong></a>.</p><h2>What is Visual Product Search?</h2><p>Visual Product Search, also known as Image-based Search, is a cutting-edge technology that leverages the power of artificial intelligence (AI) and machine learning (ML) to revolutionize the way we search for products online. It uses computer vision, a branch of AI that enables computers to understand and interpret visual information from the real world, to identify, analyze, and interpret images.</p><p>‍</p><p>In a typical Visual Product Search scenario, a user uploads or captures an image of a product they are interested in. The system then analyzes the image, identifies the product's key features, and searches a database for matching or similar items. The results are then presented to the user, who can select the most suitable product.</p><p>‍</p><p>Visual Product Search can reduce the time and effort customers spend on finding products, making the shopping process quicker and more enjoyable. It enhances the user experience by making product searches more intuitive and efficient. This can lead to increased customer satisfaction and loyalty, key factors for business growth and success in the competitive e-commerce market.</p><h3>Attribute Extraction</h3><p>Attribute extraction is another part of Visual Product Search. It is the process of identifying specific features or attributes of a product in an image. This process is facilitated by machine learning algorithms and computer vision techniques.</p><p>‍</p><p>When a user uploads or captures an image of a product, the system first identifies the key attributes of the product. These could include color, shape, size, pattern, brand, style, and other unique features. The system then compares these attributes with the ones in the database to find matching or similar products.</p><p>‍</p><p>For instance, if a user uploads an image of a blue, floral-patterned dress, the system would identify these attributes and then search the database for dresses that are blue and have a floral pattern. The system might also consider other attributes such as the dress's style, length, and brand.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1570pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64bd8ccafbdbe6e05f985f45_p-G6Br-CkYfrldHJf9OZ539Q86ZxFgShLCE_XuYAKrIP2eZEUkmpNNLknYqLZuhCUZDuRMxZFC8feLQ_emNSZeV9zIO5_e4EdDtlwGzTQ9HIodKi2agKOLVamkKLK9YdYhhQtxqNQhnhLLByQO7FEls.png""/></div></figure><p>‍</p><h2>Advantages of using Visual Search</h2><p>Visual search technology offers several distinct advantages that make it a valuable tool for businesses, particularly in the e-commerce and retail sectors. By leveraging the power of AI and machine learning, visual search can enhance the online shopping experience, drive customer engagement, and provide valuable insights into customer behavior.</p><p>‍</p><ul role=""list""><li><strong>Improved User Experience:</strong> Visual search makes product discovery more intuitive and efficient, reducing the time and effort required to find a specific product. This leads to a more enjoyable shopping experience and increased customer satisfaction.</li><li><strong>Increased Conversion Rates:</strong> By providing more accurate and relevant search results, visual search can drive higher conversion rates. Customers are more likely to make a purchase when they can easily find what they are looking for.</li><li><strong>Insights into Customer Behavior:</strong> The images customers upload can provide valuable insights into their preferences and buying behavior. This data can be used to personalize product recommendations and marketing strategies.</li><li><strong>Competitive Advantage:</strong> Implementing visual search can give businesses a competitive edge in the market. It's an innovative feature that can differentiate a business from its competitors, attracting more customers and retaining existing ones.</li></ul><p>‍</p><h2>How Visual Product Search works?</h2><p>A visual search pipeline has a lot of components that tie together to make everything work. Let's dive into these components deeper.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1211pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64bd8cca1b90be3b5fa696a8_6-diC_oiDihc9ouQQVqRWDV9AevqFPJ-cyRXSwqWaqEEJjhdrQHzr8HKstAKjx2F4ebCgqk4VOuG5rskDOX2lqu0Hqf8ZQvxolqLB4uOjc7yYPBe-VF5dWVaS6prnHI3YkTeg1hdUXNYjiULBa1eVp0.png""/></div></figure><h3>Embeddings</h3><p>Embeddings are perhaps the most important component in the search pipeline. They are essentially numerical representations of the passed textual or image data. The key property of these embeddings is that they learn the semantic meaning of the passed data. These embedding vectors can <em>carry</em> the meaning of a text. This means data points with similar meanings or contexts will have their vectors at a closer distance than data points with different meanings or contexts. This property allows us to use these embeddings for <strong>semantic search</strong>.</p><p>‍</p><p>In the context of visual search, we will be using multi-modal embeddings. Meaning we will convert both our product images and their textual descriptions into embedding vectors and then use them to find similar products. We will use the CLIP model.</p><h4>CLIP Model</h4><p><a href=""https://openai.com/research/clip"">CLIP</a> stands for Contrastive Language–Image Pre-training. It is a model released by OpenAI. We can use the CLIP model to get our embeddings. </p><p>‍</p><p>CLIP works by pairing images and descriptions and then reducing the distance between their vector representations. This trains the network to learn the properties of the passed data across text and images. This basically means that the distance between vectors of a passed image and its description will be very less. This is because the model has learned to put them together.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1511pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64bd8ccab703f9575b4c2107_W36jFIaor5aQb9YJe_Lw3WrsJdh6-EYRDCH0_x5auxN1F3vXkdgwkBMxsr4WfIYSHSHFmyb6uHJnnfd08eYegjqRFm1Ew-wz3dpvsqrXM2dNt6rs0nbZPKrCDVRwlnTxZdQFxg5L76OFBWdP184VAGE.png""/></div></figure><h4>Fashion CLIP</h4><p>Fashion CLIP is a CLIP-like model fine-tuned specifically on the product images and their descriptions. You can find the model <a href=""https://github.com/patrickjohncyh/fashion-clip"">here</a>. We will use this model to build our product search pipeline.</p><h3>Embedding Search</h3><p>Once we have our embedding we can start using them to search similar products. The Embedding Search is essentially a nearest neighbor search in the high-dimensional space of the embeddings. The goal is to find the product embeddings that are closest to the input embedding, as these represent the products that are most similar to the user's input.</p><p>‍</p><p>The result of the Embedding Search is a list of product embeddings, ranked by their similarity to the user's input. These can then be used to retrieve the corresponding product details from the database and present them to the user.</p><p>‍</p><p>Here is a good visual of embedding space by OpenAI. You can see how texts with similar meanings are closer, the text with different meanings is far away. We use this property to search for similar products.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:903pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64bd8ccab6b02693f0ee639a_ustVM_XA9o-wOBahgGAGdY0h47_MwzcOkB6eAaCLjHscTPkVm4Jd15t_VZhwJO7kY940AtFU0gcJgbXNii2I8nbrfVv4S3F7XHL-Siu5evKGxjUcUejMcLFgwl47lA8-qh4wJPagSlgoKGH1_d9UL3E.png""/></div></figure><p>‍</p><h3>Vector DB</h3><p>Vector databases, play a crucial role in the Embedding Search phase. They are specialized databases designed to handle high-dimensional vector data, like the embeddings generated by the CLIP or the Fashion-CLIP model.</p><p>‍</p><p>Traditional databases are not well-suited for storing and querying high-dimensional vector data. They are not designed to perform nearest-neighbor searches efficiently, which can result in slow response times and high computational costs.</p><p>‍</p><p>Vector DBs are optimized for these types of operations. They use advanced indexing techniques to organize the vector data in a way that makes nearest-neighbor searches faster and more efficient. Some Vector DBs also support distributed storage and parallel query execution, which can further enhance performance and scalability. Some good examples of vector databases are <a href=""https://www.pinecone.io/"">Pinecone</a>, <a href=""https://milvus.io/"">Milvus</a>, and <a href=""https://weaviate.io/"">Weaviate</a>.</p><h2>How to Build and Deploy a Visual Product Search Pipeline?</h2><p>Now let’s discuss how you can build your own product search pipeline. We will use HuggingFace to build our model. Here is the step-by-step guide to coding your own visual search engine. You can follow along in this <a href=""https://colab.research.google.com/drive/1dEC6QexaZ_tWH5nhGY0fcRIOuWd1W9_w?usp=sharing"">Colab Notebook</a>.</p><p>‍</p><h3>Load the Models</h3><p>We start by loading the Fashion CLIP model and the processor. The model will give us the embeddings and the processor will help us load the different inputs.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64bd8cca4f4a45e8328aa5c4_AvszUHuUPv7gkAuecyO5EMx1yCAmzeJOJXy5TVp9mRt8aHfxTgio_KQi3DTz0lxRCQ23iyzO_HHIIt4NwFNq2kRidJW5MqZpncFeJxGmAcPLP70nGz_M-kKmd33iB3X1Exl3cNVUu0yaMc2LNIGPUlA.png""/></div></figure><p>‍</p><h3>Download Data</h3><p>Once we have the models and the processor ready, we will create a small dataset using online images.</p><p>‍</p><div class=""w-embed""><pre>
<code class=""language-py"">
image_urls = [
          'https://crazymonk.in/wp-content/uploads/2022/04/Navy-Blue-Half-Sleeve-scaled.jpg',
          'https://ttbazaar.com/cdn/shop/products/TS135ROYALBLUEFRONT.jpg',
          'https://blackberrys.com/cdn/shop/files/polo-t-shirt-in-orange-cloud-blackberrys-clothing-1.jpg', 
          'https://cdn-img.prettylittlething.com/d/a/2/b/da2b4e203b3e48578ba79e6f6a0e4692edbebd61_cmc9361_1.jpg'
          ]
folder_name = ""images""

for index, url in enumerate(image_urls):
  try:
      response = requests.get(url)
      if response.status_code == 200:
          image_name = f""image_{index + 1}.jpg""
          # image_path = os.path.join(folder_name, image_name)
          with open(f'{folder_name}/{image_name}', ""wb"") as image_file:
              image_file.write(response.content)
          print(f""Image {index + 1} downloaded successfully."")
      else:
          print(f""Failed to download image {index + 1}: {response.status_code}"")
  except Exception as e:
      print(f""Failed to download image {index + 1}: {str(e)}"")
</code>
</pre></div><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64bd8cca0b26820044a6a21a_Ira7fUx0OCI5ngQ6w1J1sCC0pvwnspaif2GVpe2sVKFmzp1OZzwgBjZhDTELw_yxNBDD-cy-eAGVuhCszpZfejb9fZhoW1lTgbGPZmwZmXv7CTzFomlUujaH7xlu_pOaAE8jP1zizXGcFceoP1tDpew.png""/></div></figure><p>‍</p><p>Here I am loading 4 images,  2 of blue t-shirts, 1 of an orange t-shirt, and one of red boots. We will use these images for testing purposes. If you want to test it on your images, just drop their URLs in the image_urls list. Or you can just save them in the images folder too.</p><p>‍</p><h3>Encoding the Images</h3><p>Now that we have our images ready, we will convert them into embeddings.</p><p>‍</p><div class=""w-embed""><pre>
<code class=""language-py"">
image_files = [f for f in os.listdir(folder_name) if os.path.isfile(os.path.join(folder_name, f))]
image_embeddings = []

for file_name in image_files:
  try:
      image_path = os.path.join(folder_name, file_name)
      image = Image.open(image_path)
      inputs = processor(images=image, return_tensors=""pt"")
      image_features = model.get_image_features(**inputs)

      image_embeddings.append((image_features, image_path))

  except Exception as e:
      print(f""Error processing image '{file_name}': {str(e)}"")
</code>
</pre></div><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64bd8cca705ce2028138ded3_d8DMYqDFV1-a-3x9V3GhcwFDsefPT0157Pl3Yyz_t-fIM70n3bOPOeXGCuUWNcqT10fpkiKqsj1ZOlClpzelDK7Q3HaLqCqBz1zSdR3vGinHi1iP4SWpgAKtdsV7lpi8j4CgwbHA6bggJTj6icdQc3M.png""/></div></figure><p>‍</p><p>The image_features list will store the image embeddings generated by the model using the ‘get_image_features()’ function. Now we can start querying the embeddings!</p><h3>Querying the Embeddings</h3><p>For the purposes of this tutorial, we are not using any vector database. We will simply store the embeddings in a Python list and use cosine similarity to find the most relevant image given the text.</p><p>‍</p><div class=""w-embed""><pre>
<code class=""language-py"">
QUERY = ""blue t-shirt without collar""


inputs = processor([QUERY], padding=True, return_tensors=""pt"")
text_features = model.get_text_features(**inputs)

sim_scores = []

for image_features, image_path in image_embeddings:
  similarity = cosine_similarity(image_features, text_features)
  similarity = round(float(similarity),2)

  sim_scores.append((similarity, image_path))

  print(f""Similarity score is {similarity} for image {image_path}"")
</code>
</pre></div><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64bd8ccbe0131ac35ba691b9_ct890cyXCv4EXiblXiQ-r-rbtOc9EggzkaMpcxZ3cYwUlc9zrQt_b1iFyMC3ZhNvDGwQNot6cyZ_AASGOe5a1JHkWSet3uTNkTA_LV4psJ956ZDih8RSuqLdXi0sP7Ky-iUtcqv_aqfPWC0kTFPk-5k.png""/></div></figure><p>‍</p><p>Here in the image, I am querying for “RED BOOTS”. And I get similarity scores for all the images, and when I open the image with the highest similarity score I get:</p><p>‍</p><figure class=""w-richtext-align-center w-richtext-figure-type-image""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64bd8ccba7cb1c7fd4e90679_ic4KJ_xB7P9ePXUNZC0YVoT20blYclYhtDxlWqx-E4C2tHYtOczr04i-Jusi8-Qs3H7Da2afIkqPlW7gmTeFbhiyJA6x8CvTQku3BKmbWQO-xU1TIFuk-YE9SUJowsZV6Thg-k9vHRglqoFjJV6OSTE.png""/></div></figure><p>‍</p><p>We can try this with other queries too. </p><p>‍</p><p>This is what I get when I search “blue t-shirt with collar”:</p><p>‍</p><figure class=""w-richtext-align-center w-richtext-figure-type-image""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64bd8ccb99271195aff3e54c_j1uskA-I2ZNtIofDkuHm8osPnCX51FyYAPEp7kE7iqS3HXkY7l-IUlIUg-u4JVX3ZeACJGWITsWt69wxWIfre2OEqYFqpJ7skPxWVovXfysxEGmP2RPnZstg394G-Z3U4LuP84OMkjRfe92bbv4-aEk.png""/></div></figure><p>‍</p><p>And this is when I search ‘blue t-shirt <strong>without</strong> a collar”:</p><p>‍</p><figure class=""w-richtext-align-center w-richtext-figure-type-image""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64bd8ccbb9983cf20c6cc710_bnrf_i6FHzRz3kmg080IRoAaOfqKS0NbgDflG9e0YVlpxEKMskKTDelbMWhMlnaZrDjxjHBzXmqXGNRPoYSyKPMeHiE3yCDOw8Lh0eKTVLnVruJc1xlHnCSnGRZcRGvFU-Jp_BKwIRlYNNruys279CE.png""/></div></figure><p>‍</p><p>This shows that the model is able to understand and differentiate between subtle features like “with collar” and “without collar”.</p><p>‍</p><h2>Want to build a Visual Search Engine for your business?</h2><p>We have a team of experienced Computer Vision Engineers who have worked on multiple such projects with CLIP and other multi-modal models. We can help you implement visual search features into your product and make it much better as a result. If you want to build and deploy such a solution for your own business, <a href=""https://www.mercity.ai/contacts"">reach out to us</a>!</p><p>‍</p><p>‍</p></div>"
How to Build and Deploy Private Custom Chatbots,how-to-build-and-deploy-private-custom-chatbots,640f56f76d313b2faa631c11,6546776f63b2e6622dd5fc81,False,False,Sat Nov 04 2023 16:55:11 GMT+0000 (Coordinated Universal Time),Sat Nov 04 2023 16:55:17 GMT+0000 (Coordinated Universal Time),Sat Nov 04 2023 16:56:09 GMT+0000 (Coordinated Universal Time),"<p id=""""><strong id=""""><em id="""">This article is focused on the private deployment of Open Source LLMs, if you are okay with using OpenAI models, check out </em></strong><a href=""https://www.mercity.ai/blog-post/custom-gpt-4-chatbot"" id=""""><strong id=""""><em id="""">this</em></strong></a><strong id=""""><em id=""""> article.</em></strong></p><p>‍</p><p id="""">With AI being on the rise, all businesses are integrating AI into their pipelines. LLMs like GPT-4, LLaMA, and Mistral have proven highly helpful for people in reducing the amount of redundant work and how much time it takes to finish simple tasks. Simply using ChatGPT can massively boost employee productivity across all tasks.</p><p>‍</p><p id="""">However, a massive problem that companies have with ChatGPT is the lack of privacy and sensitive data concerns. Even though OpenAI released ChatGPT enterprise, there are many industries that cannot share their data with any third party. Industries like the military and healthcare have a lot of sensitive data. And that’s why they need to privately deploy LLMs like GPT-4.</p><p>‍</p><p id="""">In this article, you will learn how to build and deploy private and secure LLMs in production environments. We will go over open source and private vector databases, private LLM deployment, and how you should design your API schema. We have years of experience in deploying LLMs and models to cloud servers, and everything we have learned you can find in this article.</p><p>‍</p><h2 id="""">Why build and deploy a private chatbot LLM?</h2><p id="""">Private deployment of an LLM can be messy and difficult to maintain, there must be good reasons to undertake this project. Here are some good reasons to deploy an LLM in a private cloud instead of going with OpenAI APIs:</p><h3 id="""">Data Security</h3><p id="""">This is perhaps the most important common reason why people go with private deployments. As mentioned earlier, there are industries that cannot afford to share their data at all. For example, the military cannot use ChatGPT for work purposes as their information is very sensitive and usually classified as private. The same goes for healthcare professionals, even though OpenAI claims to be compliant, most medical doctors cannot afford to share such sensitive information with other companies.</p><p>‍</p><p id="""">This is where local and private deployments come in. Deploying an LLM privately can help with a ton of tasks like summarization and automating common tasks, without having to share the data with any other company.</p><h3 id="""">Compliance</h3><p id="""">Many companies just don’t allow their data to be shared. This is usually because of compliance reasons. Industries like financial services and law enforcement are not allowed to share documents with third parties. We know that the finance industry can benefit a lot from these AI applications, for example, by <a href=""https://www.ionio.ai/blog/gpt-4-ai-for-in-depth-sec-filings-insights-go-beyond-summarization-7-actionable-prompts-inside"" id="""">gaining insights from complicated SEC filings</a>.</p><p>‍</p><p id="""">A self-hosted LLM can completely remove these issues as there is no data sharing with any third parties at all and you can also track how you are using your data and how LLM is responding to it.</p><h3 id="""">Finetuning</h3><p id="""">Another big reason other than data security and privacy is a more fine-grained control of the model itself. Even though models like GPT-4 and GPT-3.5 are highly capable, they cost a lot and can perform very poorly on very narrow and domain-specific tasks. This is where you would want to finetune a model. OpenAI now allows you to <a href=""https://openai.com/blog/gpt-3-5-turbo-fine-tuning-and-api-updates"" id="""">finetune GPT models</a>, but the costs can get very high, very fast.</p><p>‍</p><p id="""">But if you finetune a LLaMA model, you can completely decide how the model works and behaves.<a href=""https://www.mercity.ai/blog-post/fine-tuning-llms-using-peft-and-lora"" id=""""> Finetuning an open-source LLM</a> can easily boost the performance, without any cost increases as you are still hosting the same model. The only thing that changes is the model weights, not the size. If anything, when finetuning, you can use even smaller models as you are training the model for a very narrow task instead of multiple tasks.</p><h3 id="""">Costs</h3><p id="""">Lastly, but perhaps one of the most important reasons, cost. If you are going to use an LLM for your business, starting with third-party APIs makes sense as it is easy to develop and test. But as you start gaining users, the costs can get extremely high and as your prompt length increases, costs increase non-linearly.</p><p>‍</p><p id="""">One can easily finetune LLaMA and deploy them in a server or serverless setting to save a ton on deployment costs. Anyscale did a comprehensive study on the LLaMA vs GPT-4 costs and found that LLaMA can be <strong id="""">30x</strong> cheaper compared to GPT-4. You can read more <a href=""https://www.anyscale.com/blog/llama-2-is-about-as-factually-accurate-as-gpt-4-for-summaries-and-is-30x-cheaper"" id="""">here</a>. Here’s a detailed statistic from the post:</p><p>‍</p><figure class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1276px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1276px""><div><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/654676c00c7707678f961d2b_Jmw2hA3j8or8WRO39vqoy0qBfCCgRWeLoIc57wnc5YDoBP4rJqXyki9_KiSHIANqW2fy5snTln5urXre_6WuCSyx_bMdwOj_X45eTTWqAWvatXNTydxEIr9RAfipVA5i71ee5iwaHrY2WJLAgjXig_0.png"" id="""" width=""auto"" height=""auto"" alt="""" loading=""auto""></div></figure><h2 id="""">How to deploy a chatbot in private infrastructure?</h2><p id="""">Now that we know what are some good reasons to deploy chatbots and LLMs in private cloud servers, let’s talk about how exactly you can do that. We will discuss how you can deploy LLMs and use them with open-source embeddings to use with your personal data.</p><p>‍</p><figure class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/654676c0c953fdb01a278131_NEcY26BwvmACihBkLiXzrvl3LE4GoeNSw30oQnWMfswTdpuD-EbN1FeZ44yl-BkZocgxSc3QIFjJYsBm1mxyTIEy9cNBlXBE8kCMDh7sCOJ30nDTOQ8nRbzO1t2xag7vMikhRi6QOIY7EUz7KhrPPJI.png"" id="""" width=""auto"" height=""auto"" alt="""" loading=""auto""></div></figure><h3 id="""">Model</h3><p id="""">The model is the most important part of the stack. It largely depends on your needs and what kind of tasks you need it to perform. At the time of writing this, <a href=""https://huggingface.co/mistralai/Mistral-7B-v0.1"" id="""">Mistral-7B</a> is the most powerful and smallest model known. Mistral has 7 billion parameters, and most of the 13 billion parameters out there. Many models like <a href=""https://huggingface.co/teknium/OpenHermes-2-Mistral-7B"" id="""">OpenHermes2</a> are built on top of this model which are finetuned further on even more tasks and making it much better. You can go to our <a href=""https://www.mercity.ai/blog-post/comprehensive-comparison-of-llms-8-2023"" id="""">comparison of LLMs blog</a> to learn more about these models and make a decision as to which model you want to use.</p><p>‍</p><p id="""">Our general rule of thumb is to use smaller models like Mistral if you need only to prompt the model for a single one-off output, like for generating summaries and translating text. This is because there is no need for the conversation history to be provided to the LLM. But when dealing with much longer contexts, we tend to use much bigger models like LLaMA-2-13B. Bigger models can handle longer contexts and multi-turn conversations much better compared to smaller models.</p><h3 id="""">Embeddings</h3><p id="""">Embeddings are vector representations of text. These vector representations are learned by models at the time of training, or one can train models specifically to learn embedding representations of text too. These embedding vectors have a special property of encoding the input text in a way that texts with similar context or meaning will have their embedding vectors much closer to each other compared to the ones with different meanings.</p><p>‍</p><p id="""">This means if a text talks about earthquakes and another text is about underwater vibrations, those texts will be located very close to each other in the embedding space. Whereas a text about YouTube videos when turned into embeddings will be very far away from both of them.</p><p>‍</p><figure class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1306px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1306px""><div><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/654676c09be6c92dcd41fe59_fURjf1lZFHKSYdDOn82n_-ESaI-voIH6u4WXBsBogyHqMhctDcOb8UBTMnqoKpCP0fAWrhMfnJINUImJR7iLg_FxmiIFBZyRGPIZ0nzO8YUp2nciufHpTNATP8Q5HLWX2tOD8E2EdO4RI8sK6MAPRxs.png"" id="""" width=""auto"" height=""auto"" alt="""" loading=""auto""></div></figure><p>‍</p><p id="""">These embeddings will help us find relevant documents to answer user queries.</p><h4 id="""">SBERT OpenSource Embeddings</h4><p id="""">We use <a href=""https://www.sbert.net/"" id="""">SBERT models</a> to generate embedding vectors when working in a private infrastructure. Using OpenAI embeddings can be helpful in a development environment, but for private and secure settings we use SBERT embeddings.</p><p>‍</p><p id="""">Some of these models are as small as <strong id="""">90MB</strong>. This makes them very good for deployment and fast inference in production environments.</p><p>‍</p><figure class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/654676c03ac46dc4b3ad9bee_GlC9q1Q-8BkgG-T4aNeY8NIN0hWox6OqHdQqeEEs8ZafGcSIRZAVnk-hcuKZ_gdw_Yo8nkJ_jwQwYxXCeoclmTlzgjMShdAYlQpUhBBgZDlc-Ktj9MeeU1Fnn5dqh3lJGrsMS7c_GBCIgMwcrjqZEy8.png"" id="""" width=""auto"" height=""auto"" alt="""" loading=""auto""></div></figure><h4 id="""">How to embed Images?</h4><figure class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/654676c0780ec7c28f84eb71_wQjHsmtHumrdCIItubiObLyqziwbVQCiia8S_3YdOQBhATnz9Y7NnI3CLJDsy6RV0DHHzfsoH-m4K3uCnhVAmi8RD1I_lc_LRhmvzqgndZjrU4mEQj8XObl3frGlJWGVUBkPoVHHT4Tmtl5kKZ3NwkA.png"" id="""" width=""auto"" height=""auto"" alt="""" loading=""auto""></div></figure><p>‍</p><p id="""">To process and store images as vector embeddings we can store the description text of the image. But if that is missing, the second best method is to use the text around the image as the description for the image, as the image will most likely be tightly connected to the text before and after it.</p><p>‍</p><p id="""">If you have a ton of images, considering CLIP models can also be a viable option. We wrote a detailed blog on <a href=""https://www.mercity.ai/blog-post/how-to-build-a-visual-search-pipeline"" id="""">how to search images using embeddings</a>.</p><p>‍</p><h3 id="""">Private LLM hosting using vLLM</h3><p id="""">Now that embeddings are taken care of, we can move on to the most tricky part of the process, deploying the LLM. This is tricky because this can get very messy very quickly, simply because there are so many settings and little things to control. There are many options out there to optimize deployments of larger models, from Microsoft to Nvidia, and almost all of them provide options for deployment.</p><p>‍</p><p id=""""><a href=""https://github.com/vllm-project/vllm"" id="""">vLLM</a> is an open-source project that implements many models for custom deployment and handling. We use vLLM as it’s completely open source and provides the best performance known. vLLM also gives a massive boost over plain HuggingFace implementations and more than 2x boost when compared to the <a href=""https://github.com/huggingface/text-generation-inference"" id="""">Text Generation Inference</a> project.</p><p>‍</p><p id="""">Here is a performance comparison for both LLaMA-13B and 7B models:</p><p>‍</p><figure class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/654676c044e6135952c955fb_BGP_PDC99FRMwwEL8bMgekJGvsDsr0KQfNlp7sOheD7vfs5iJ1dTSEudy2wPbpOk6OmlKKfMPEPD_NglUizVL-ytg2RELe0RiO5AoMe-xGZz7MspeY09UZ6FSJeJlDyexMR_GZUoEI5xSAWCmji4Y18.png"" id="""" width=""auto"" height=""auto"" alt="""" loading=""auto""></div></figure><p>‍</p><p id="""">You can read more about vLLM and how they implement fast inference <a href=""https://vllm.ai/"" id="""">here</a>.</p><h3 id="""">Private Vector Databases</h3><p id="""">Along with private LLM hosting, we also need a vector database. Vector DB should also be hosted privately as that’s where the core and most of the data is stored. We have tried multiple databases in our stack, <a href=""https://www.trychroma.com/"" id="""">Chroma</a>, <a href=""https://weaviate.io/"" id="""">Weaviate</a>, <a href=""https://milvus.io/"" id="""">Milvus</a> and what not. And in our experience, we found that Milvus is the best choice.</p><p>‍</p><p id="""">Milvus is easily deployable using their docker containers and easily maintainable too. The microservice architecture makes sure it integrates with your stack without any issues. Milvus also provides easy vertical and horizontal scaling. It is specifically optimized for performance on large-scale applications and is blazingly fast even on large amounts of load.</p><p>‍</p><p id="""">Milvus uses a very scalable architecture underneath, that allows for very fast database queries even with <strong id="""">trillions</strong> of embedding vectors. Here’s a diagram of the Milvus’s internal architecture:</p><p>‍</p><figure class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1280px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1280px""><div><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/654676c007b3e65c62433bb6_tbLSV9O3UiYdQkmTBGDPW-gDdojsAJsN_3Puyw9mzHHK_OFcCpnBy2NPdiQZncM8ktlrZfwVfK1sssTiHa1jPfDxeZu0_jKbkiV9_35oxXb4gb_5IIOALmkm1MD3W_ELbCsNnHmKInVe59XZ67uaYx4.png"" id="""" width=""auto"" height=""auto"" alt="""" loading=""auto""></div></figure><p>‍</p><h3 id="""">Deployment options</h3><p id="""">Now that we have talked about all the components, let’s discuss what deployment options we have. There are many options like AWS, Modal, Runpod, etc. Whether you go with serverless or dedicated depends completely on how much you are okay with spending and how many users you are going to have. If you are going to have a ton of users requesting real time usage of the llm, serverless makes the most sense given that you would want auto-scaling. But if you want to run inputs in batches instead, both serverless and a dedicated GPU server make sense.</p><p>‍</p><p id="""">A dedicated GPU usually makes sense if you know how many requests you are going to serve per hour or minute and can properly accommodate that. But then you will either have to restrict the user requests or batch them together when the number of requests is more than expected.</p><p>‍</p><h3 id="""">API Design for LLM Deployment</h3><p id="""">One more thing to consider is the API design itself, it seems easy to handle and build, but when working with massive projects these things matter a lot. And even more when we are integrating the API with other products. Here are some very important factors to consider which we have learned solely through experimentation:</p><h4 id="""">Streaming with Websockets</h4><p id="""">One very important part of this which we now see pretty much everywhere is the streaming responses. This is basically the process of directly streaming the responses to the frontend as soon as they start generating from the LLM, instead of waiting for the full response.</p><p>‍</p><p id="""">This streaming paired with a WebSocket connection can massively increase the performance of your application. Simply because a REST API can work but the connection time in between requests is very inefficient and can significantly slow down the system. Streaming is supported by vLLM out of the box so this is not something that you would have to implement yourself.</p><h4 id="""">Session/State Management</h4><p id="""">State management is also a big issue. Our go-to decision is to always store as much as possible on the backend instead of the user. Simply because doing so gives us so much control over everything. Both the text files for contexts and the messages are always stored on the backend side of the application.</p><p>‍</p><p id="""">One big reason to do this is to restrict malicious control of the user on the application. If you tend to store the messages on the user end, one can edit the context of the messages and the prompts being fed into the model, and this can lead to <strong id="""">prompt injection attacks</strong>. But if the messages are stored on the backend instead, prompt injection attacks are simply not possible outside of the current user message being fed to the model.</p><h4 id="""">Schema</h4><p id="""">Schema is also a very important component of the whole stack. OpenAI’s schema is perhaps the best schema out there. You can check it out <a href=""https://platform.openai.com/docs/api-reference/chat"" id="""">here</a>. This is the schema we suggest everyone use too. vLLM also supports this API schema out of the box, so no need to implement it yourself either.</p><h4 id="""">Encryption</h4><p id="""">Encryption is the MOST important of the whole stack when it comes to building data-sensitive or private applications. We always store all the sensitive data on the backend and encrypt it properly. Not only that, all the messages between the server and the users are also fully encrypted. This might add some complexity to the stack, but this is necessary for building and deploying a privately hosted LLM.</p><p>‍</p><figure class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1400px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1400px""><div><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/654676c0216c2e42eb44c830_AkcgsXXuy9-hvQ9nVO3c_axeNcAPHOJmDX43qz2_fYJiW7GJzaX4TVjeRtv0fO9xq4Fjaz0PxpP-UdFyV3GS8EudTleX79fSb8f7_NnXZDmRvuKXjEedKC6EuebVdouASlJdh61tJ9vtOrnWmv94OJ4.png"" id="""" width=""auto"" height=""auto"" alt="""" loading=""auto""></div></figure><p>‍</p><h2 id="""">Challenges</h2><p id="""">Even with all this, deploying your LLM without much experience can be very complicated. There are multiple moving parts and almost all of them are important. This introduces many challenges to the system, let’s discuss them in more detail.</p><h4 id="""">Costs</h4><p id="""">Costs are a big issue when it comes to deployment of any sort. Especially for compute-intensive LLM deployments. Our suggestion is to go with serverless platforms like <a href=""https://modal.com/"" id="""">Modal</a> or <a href=""https://www.runpod.io/"" id="""">Runpod</a>. These can provide big savings, at the cost of complexity. Serverless can become harder to manage, but it is always worth it if implemented properly.</p><h4 id="""">GPU Server Setup</h4><p id="""">If you are using a GPU server, the project can become really complicated very quickly. GPUs are tricky to handle, and when paired with large models and the need for fast deployments, they can become a really big issue. There are way too many options to consider and settings to control, from GPU computing capability to VRAM, and all of them are important. We suggest using a prebuilt library like vLLM to reduce issues with GPUs.</p><h4 id="""">Usage Estimations</h4><p id="""">Usage estimations are something that completely depends on your application and users. Although with LLM-based applications these calculations can be really complicated. LLMs are largely dependent on the prompts and prompt lengths. The estimation where most people make mistakes is the prompt length and the number of messages in a chat.</p><p>‍</p><p id="""">Every message of 50 tokens is not a 50 tokens input to your LLM, it’s a 50 tokens + previous messages input to your LLM, hence the prompt size and compute cost increase almost non-linearly.</p><h2 id="""">Want to Build and Deploy a Private GPT-4 like LLM?</h2><p id="""">Many businesses are deploying LLMs privately within their ecosystem to preserve their data. If you are looking for similar data secure solutions, <a href=""https://www.mercity.ai/contacts"" id="""">reach out</a> to us! We have years of experience in developing AI applications and we have deployed multiple LLMs in different environments. Just reach out and we will be happy to help!</p><p>‍</p>",https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/654677323044e51137634350_private%20custom%20LLM.png,Pranav Patel,Model Deployment,Learn how you can privately deploy custom open source LLMs in your cloud for data privacy and security.,False,"<div class=""rich-text w-richtext""><p><strong><em>This article is focused on the private deployment of Open Source LLMs, if you are okay with using OpenAI models, check out </em></strong><a href=""https://www.mercity.ai/blog-post/custom-gpt-4-chatbot""><strong><em>this</em></strong></a><strong><em> article.</em></strong></p><p>‍</p><p>With AI being on the rise, all businesses are integrating AI into their pipelines. LLMs like GPT-4, LLaMA, and Mistral have proven highly helpful for people in reducing the amount of redundant work and how much time it takes to finish simple tasks. Simply using ChatGPT can massively boost employee productivity across all tasks.</p><p>‍</p><p>However, a massive problem that companies have with ChatGPT is the lack of privacy and sensitive data concerns. Even though OpenAI released ChatGPT enterprise, there are many industries that cannot share their data with any third party. Industries like the military and healthcare have a lot of sensitive data. And that’s why they need to privately deploy LLMs like GPT-4.</p><p>‍</p><p>In this article, you will learn how to build and deploy private and secure LLMs in production environments. We will go over open source and private vector databases, private LLM deployment, and how you should design your API schema. We have years of experience in deploying LLMs and models to cloud servers, and everything we have learned you can find in this article.</p><p>‍</p><h2>Why build and deploy a private chatbot LLM?</h2><p>Private deployment of an LLM can be messy and difficult to maintain, there must be good reasons to undertake this project. Here are some good reasons to deploy an LLM in a private cloud instead of going with OpenAI APIs:</p><h3>Data Security</h3><p>This is perhaps the most important common reason why people go with private deployments. As mentioned earlier, there are industries that cannot afford to share their data at all. For example, the military cannot use ChatGPT for work purposes as their information is very sensitive and usually classified as private. The same goes for healthcare professionals, even though OpenAI claims to be compliant, most medical doctors cannot afford to share such sensitive information with other companies.</p><p>‍</p><p>This is where local and private deployments come in. Deploying an LLM privately can help with a ton of tasks like summarization and automating common tasks, without having to share the data with any other company.</p><h3>Compliance</h3><p>Many companies just don’t allow their data to be shared. This is usually because of compliance reasons. Industries like financial services and law enforcement are not allowed to share documents with third parties. We know that the finance industry can benefit a lot from these AI applications, for example, by <a href=""https://www.ionio.ai/blog/gpt-4-ai-for-in-depth-sec-filings-insights-go-beyond-summarization-7-actionable-prompts-inside"">gaining insights from complicated SEC filings</a>.</p><p>‍</p><p>A self-hosted LLM can completely remove these issues as there is no data sharing with any third parties at all and you can also track how you are using your data and how LLM is responding to it.</p><h3>Finetuning</h3><p>Another big reason other than data security and privacy is a more fine-grained control of the model itself. Even though models like GPT-4 and GPT-3.5 are highly capable, they cost a lot and can perform very poorly on very narrow and domain-specific tasks. This is where you would want to finetune a model. OpenAI now allows you to <a href=""https://openai.com/blog/gpt-3-5-turbo-fine-tuning-and-api-updates"">finetune GPT models</a>, but the costs can get very high, very fast.</p><p>‍</p><p>But if you finetune a LLaMA model, you can completely decide how the model works and behaves.<a href=""https://www.mercity.ai/blog-post/fine-tuning-llms-using-peft-and-lora""> Finetuning an open-source LLM</a> can easily boost the performance, without any cost increases as you are still hosting the same model. The only thing that changes is the model weights, not the size. If anything, when finetuning, you can use even smaller models as you are training the model for a very narrow task instead of multiple tasks.</p><h3>Costs</h3><p>Lastly, but perhaps one of the most important reasons, cost. If you are going to use an LLM for your business, starting with third-party APIs makes sense as it is easy to develop and test. But as you start gaining users, the costs can get extremely high and as your prompt length increases, costs increase non-linearly.</p><p>‍</p><p>One can easily finetune LLaMA and deploy them in a server or serverless setting to save a ton on deployment costs. Anyscale did a comprehensive study on the LLaMA vs GPT-4 costs and found that LLaMA can be <strong>30x</strong> cheaper compared to GPT-4. You can read more <a href=""https://www.anyscale.com/blog/llama-2-is-about-as-factually-accurate-as-gpt-4-for-summaries-and-is-30x-cheaper"">here</a>. Here’s a detailed statistic from the post:</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1276px""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/654676c00c7707678f961d2b_Jmw2hA3j8or8WRO39vqoy0qBfCCgRWeLoIc57wnc5YDoBP4rJqXyki9_KiSHIANqW2fy5snTln5urXre_6WuCSyx_bMdwOj_X45eTTWqAWvatXNTydxEIr9RAfipVA5i71ee5iwaHrY2WJLAgjXig_0.png""/></div></figure><h2>How to deploy a chatbot in private infrastructure?</h2><p>Now that we know what are some good reasons to deploy chatbots and LLMs in private cloud servers, let’s talk about how exactly you can do that. We will discuss how you can deploy LLMs and use them with open-source embeddings to use with your personal data.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600px""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/654676c0c953fdb01a278131_NEcY26BwvmACihBkLiXzrvl3LE4GoeNSw30oQnWMfswTdpuD-EbN1FeZ44yl-BkZocgxSc3QIFjJYsBm1mxyTIEy9cNBlXBE8kCMDh7sCOJ30nDTOQ8nRbzO1t2xag7vMikhRi6QOIY7EUz7KhrPPJI.png""/></div></figure><h3>Model</h3><p>The model is the most important part of the stack. It largely depends on your needs and what kind of tasks you need it to perform. At the time of writing this, <a href=""https://huggingface.co/mistralai/Mistral-7B-v0.1"">Mistral-7B</a> is the most powerful and smallest model known. Mistral has 7 billion parameters, and most of the 13 billion parameters out there. Many models like <a href=""https://huggingface.co/teknium/OpenHermes-2-Mistral-7B"">OpenHermes2</a> are built on top of this model which are finetuned further on even more tasks and making it much better. You can go to our <a href=""https://www.mercity.ai/blog-post/comprehensive-comparison-of-llms-8-2023"">comparison of LLMs blog</a> to learn more about these models and make a decision as to which model you want to use.</p><p>‍</p><p>Our general rule of thumb is to use smaller models like Mistral if you need only to prompt the model for a single one-off output, like for generating summaries and translating text. This is because there is no need for the conversation history to be provided to the LLM. But when dealing with much longer contexts, we tend to use much bigger models like LLaMA-2-13B. Bigger models can handle longer contexts and multi-turn conversations much better compared to smaller models.</p><h3>Embeddings</h3><p>Embeddings are vector representations of text. These vector representations are learned by models at the time of training, or one can train models specifically to learn embedding representations of text too. These embedding vectors have a special property of encoding the input text in a way that texts with similar context or meaning will have their embedding vectors much closer to each other compared to the ones with different meanings.</p><p>‍</p><p>This means if a text talks about earthquakes and another text is about underwater vibrations, those texts will be located very close to each other in the embedding space. Whereas a text about YouTube videos when turned into embeddings will be very far away from both of them.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1306px""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/654676c09be6c92dcd41fe59_fURjf1lZFHKSYdDOn82n_-ESaI-voIH6u4WXBsBogyHqMhctDcOb8UBTMnqoKpCP0fAWrhMfnJINUImJR7iLg_FxmiIFBZyRGPIZ0nzO8YUp2nciufHpTNATP8Q5HLWX2tOD8E2EdO4RI8sK6MAPRxs.png""/></div></figure><p>‍</p><p>These embeddings will help us find relevant documents to answer user queries.</p><h4>SBERT OpenSource Embeddings</h4><p>We use <a href=""https://www.sbert.net/"">SBERT models</a> to generate embedding vectors when working in a private infrastructure. Using OpenAI embeddings can be helpful in a development environment, but for private and secure settings we use SBERT embeddings.</p><p>‍</p><p>Some of these models are as small as <strong>90MB</strong>. This makes them very good for deployment and fast inference in production environments.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600px""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/654676c03ac46dc4b3ad9bee_GlC9q1Q-8BkgG-T4aNeY8NIN0hWox6OqHdQqeEEs8ZafGcSIRZAVnk-hcuKZ_gdw_Yo8nkJ_jwQwYxXCeoclmTlzgjMShdAYlQpUhBBgZDlc-Ktj9MeeU1Fnn5dqh3lJGrsMS7c_GBCIgMwcrjqZEy8.png""/></div></figure><h4>How to embed Images?</h4><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600px""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/654676c0780ec7c28f84eb71_wQjHsmtHumrdCIItubiObLyqziwbVQCiia8S_3YdOQBhATnz9Y7NnI3CLJDsy6RV0DHHzfsoH-m4K3uCnhVAmi8RD1I_lc_LRhmvzqgndZjrU4mEQj8XObl3frGlJWGVUBkPoVHHT4Tmtl5kKZ3NwkA.png""/></div></figure><p>‍</p><p>To process and store images as vector embeddings we can store the description text of the image. But if that is missing, the second best method is to use the text around the image as the description for the image, as the image will most likely be tightly connected to the text before and after it.</p><p>‍</p><p>If you have a ton of images, considering CLIP models can also be a viable option. We wrote a detailed blog on <a href=""https://www.mercity.ai/blog-post/how-to-build-a-visual-search-pipeline"">how to search images using embeddings</a>.</p><p>‍</p><h3>Private LLM hosting using vLLM</h3><p>Now that embeddings are taken care of, we can move on to the most tricky part of the process, deploying the LLM. This is tricky because this can get very messy very quickly, simply because there are so many settings and little things to control. There are many options out there to optimize deployments of larger models, from Microsoft to Nvidia, and almost all of them provide options for deployment.</p><p>‍</p><p><a href=""https://github.com/vllm-project/vllm"">vLLM</a> is an open-source project that implements many models for custom deployment and handling. We use vLLM as it’s completely open source and provides the best performance known. vLLM also gives a massive boost over plain HuggingFace implementations and more than 2x boost when compared to the <a href=""https://github.com/huggingface/text-generation-inference"">Text Generation Inference</a> project.</p><p>‍</p><p>Here is a performance comparison for both LLaMA-13B and 7B models:</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600px""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/654676c044e6135952c955fb_BGP_PDC99FRMwwEL8bMgekJGvsDsr0KQfNlp7sOheD7vfs5iJ1dTSEudy2wPbpOk6OmlKKfMPEPD_NglUizVL-ytg2RELe0RiO5AoMe-xGZz7MspeY09UZ6FSJeJlDyexMR_GZUoEI5xSAWCmji4Y18.png""/></div></figure><p>‍</p><p>You can read more about vLLM and how they implement fast inference <a href=""https://vllm.ai/"">here</a>.</p><h3>Private Vector Databases</h3><p>Along with private LLM hosting, we also need a vector database. Vector DB should also be hosted privately as that’s where the core and most of the data is stored. We have tried multiple databases in our stack, <a href=""https://www.trychroma.com/"">Chroma</a>, <a href=""https://weaviate.io/"">Weaviate</a>, <a href=""https://milvus.io/"">Milvus</a> and what not. And in our experience, we found that Milvus is the best choice.</p><p>‍</p><p>Milvus is easily deployable using their docker containers and easily maintainable too. The microservice architecture makes sure it integrates with your stack without any issues. Milvus also provides easy vertical and horizontal scaling. It is specifically optimized for performance on large-scale applications and is blazingly fast even on large amounts of load.</p><p>‍</p><p>Milvus uses a very scalable architecture underneath, that allows for very fast database queries even with <strong>trillions</strong> of embedding vectors. Here’s a diagram of the Milvus’s internal architecture:</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1280px""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/654676c007b3e65c62433bb6_tbLSV9O3UiYdQkmTBGDPW-gDdojsAJsN_3Puyw9mzHHK_OFcCpnBy2NPdiQZncM8ktlrZfwVfK1sssTiHa1jPfDxeZu0_jKbkiV9_35oxXb4gb_5IIOALmkm1MD3W_ELbCsNnHmKInVe59XZ67uaYx4.png""/></div></figure><p>‍</p><h3>Deployment options</h3><p>Now that we have talked about all the components, let’s discuss what deployment options we have. There are many options like AWS, Modal, Runpod, etc. Whether you go with serverless or dedicated depends completely on how much you are okay with spending and how many users you are going to have. If you are going to have a ton of users requesting real time usage of the llm, serverless makes the most sense given that you would want auto-scaling. But if you want to run inputs in batches instead, both serverless and a dedicated GPU server make sense.</p><p>‍</p><p>A dedicated GPU usually makes sense if you know how many requests you are going to serve per hour or minute and can properly accommodate that. But then you will either have to restrict the user requests or batch them together when the number of requests is more than expected.</p><p>‍</p><h3>API Design for LLM Deployment</h3><p>One more thing to consider is the API design itself, it seems easy to handle and build, but when working with massive projects these things matter a lot. And even more when we are integrating the API with other products. Here are some very important factors to consider which we have learned solely through experimentation:</p><h4>Streaming with Websockets</h4><p>One very important part of this which we now see pretty much everywhere is the streaming responses. This is basically the process of directly streaming the responses to the frontend as soon as they start generating from the LLM, instead of waiting for the full response.</p><p>‍</p><p>This streaming paired with a WebSocket connection can massively increase the performance of your application. Simply because a REST API can work but the connection time in between requests is very inefficient and can significantly slow down the system. Streaming is supported by vLLM out of the box so this is not something that you would have to implement yourself.</p><h4>Session/State Management</h4><p>State management is also a big issue. Our go-to decision is to always store as much as possible on the backend instead of the user. Simply because doing so gives us so much control over everything. Both the text files for contexts and the messages are always stored on the backend side of the application.</p><p>‍</p><p>One big reason to do this is to restrict malicious control of the user on the application. If you tend to store the messages on the user end, one can edit the context of the messages and the prompts being fed into the model, and this can lead to <strong>prompt injection attacks</strong>. But if the messages are stored on the backend instead, prompt injection attacks are simply not possible outside of the current user message being fed to the model.</p><h4>Schema</h4><p>Schema is also a very important component of the whole stack. OpenAI’s schema is perhaps the best schema out there. You can check it out <a href=""https://platform.openai.com/docs/api-reference/chat"">here</a>. This is the schema we suggest everyone use too. vLLM also supports this API schema out of the box, so no need to implement it yourself either.</p><h4>Encryption</h4><p>Encryption is the MOST important of the whole stack when it comes to building data-sensitive or private applications. We always store all the sensitive data on the backend and encrypt it properly. Not only that, all the messages between the server and the users are also fully encrypted. This might add some complexity to the stack, but this is necessary for building and deploying a privately hosted LLM.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1400px""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/654676c0216c2e42eb44c830_AkcgsXXuy9-hvQ9nVO3c_axeNcAPHOJmDX43qz2_fYJiW7GJzaX4TVjeRtv0fO9xq4Fjaz0PxpP-UdFyV3GS8EudTleX79fSb8f7_NnXZDmRvuKXjEedKC6EuebVdouASlJdh61tJ9vtOrnWmv94OJ4.png""/></div></figure><p>‍</p><h2>Challenges</h2><p>Even with all this, deploying your LLM without much experience can be very complicated. There are multiple moving parts and almost all of them are important. This introduces many challenges to the system, let’s discuss them in more detail.</p><h4>Costs</h4><p>Costs are a big issue when it comes to deployment of any sort. Especially for compute-intensive LLM deployments. Our suggestion is to go with serverless platforms like <a href=""https://modal.com/"">Modal</a> or <a href=""https://www.runpod.io/"">Runpod</a>. These can provide big savings, at the cost of complexity. Serverless can become harder to manage, but it is always worth it if implemented properly.</p><h4>GPU Server Setup</h4><p>If you are using a GPU server, the project can become really complicated very quickly. GPUs are tricky to handle, and when paired with large models and the need for fast deployments, they can become a really big issue. There are way too many options to consider and settings to control, from GPU computing capability to VRAM, and all of them are important. We suggest using a prebuilt library like vLLM to reduce issues with GPUs.</p><h4>Usage Estimations</h4><p>Usage estimations are something that completely depends on your application and users. Although with LLM-based applications these calculations can be really complicated. LLMs are largely dependent on the prompts and prompt lengths. The estimation where most people make mistakes is the prompt length and the number of messages in a chat.</p><p>‍</p><p>Every message of 50 tokens is not a 50 tokens input to your LLM, it’s a 50 tokens + previous messages input to your LLM, hence the prompt size and compute cost increase almost non-linearly.</p><h2>Want to Build and Deploy a Private GPT-4 like LLM?</h2><p>Many businesses are deploying LLMs privately within their ecosystem to preserve their data. If you are looking for similar data secure solutions, <a href=""https://www.mercity.ai/contacts"">reach out</a> to us! We have years of experience in developing AI applications and we have deployed multiple LLMs in different environments. Just reach out and we will be happy to help!</p><p>‍</p></div>"
How to Build Custom Evals for LLMs,how-to-build-custom-ai-evals-for-llms,640f56f76d313b2faa631c11,686d93b17ab29a304829e9f1,False,False,Tue Jul 08 2025 21:54:57 GMT+0000 (Coordinated Universal Time),Sun Sep 14 2025 20:29:05 GMT+0000 (Coordinated Universal Time),Sun Sep 14 2025 20:30:51 GMT+0000 (Coordinated Universal Time),"<p id=""""><em id="""">NOTE THAT THIS IS GOING TO BE A VERY VERY COMPREHENSIVE OVERVIEW ABOUT CREATING EVALUATION PROCESSES AND EVALUATION DATASETS. WE PLAN TO RELEASE MANY TOOLS AND MANY MORE NICHE GUIDES ABOUT THIS FIELD IN THE UPCOMING FUTURE. </em><strong id=""""><em id="""">IF YOU NEED ANY ASSISTANCE IN EVALUATING YOUR MODEL, </em></strong><a href=""https://www.mercity.ai/contacts"" id=""""><strong id=""""><em id="""">PLEASE REACH OUT</em></strong></a><strong id=""""><em id="""">!</em></strong></p><p>‍</p><p>You can watch this if the blog is too long for you:</p><p>‍</p><div data-rt-embed-type='true'><div style=""position: relative; padding-bottom: 64.90384615384616%; height: 0;""><iframe src=""https://www.loom.com/embed/ab25eee1980749dea355bff3f8b50d82?sid=d77c303f-9247-483e-9730-ef6ad79e062f"" frameborder=""0"" webkitallowfullscreen mozallowfullscreen allowfullscreen style=""position: absolute; top: 0; left: 0; width: 100%; height: 100%;""></iframe></div></div><p>‍</p><p id="""">AI is on the rise. Everyone wants an AI model in their business, automating tasks, improving efficiency, and making life easier. Users want AI on every platform. There have been great models like LLaMA, Mistral, GPTs, and Claude, etc. The problem becomes what model to pick and go with; that’s where custom evaluation becomes relevant.</p><p>‍</p><p id="""">We need to be able to evaluate what model works the best for your users, and now with this mass of models, it is more necessary than ever. It is important to pick the right model and use it correctly, usually just using it 5-10 times doesn’t give you a strong understanding of where the model might be lacking. In this blog, we are going to talk about how you can create internal benchmarks or evaluation sets for your models, and test them before deploying it internally or to your users and pick the best model.</p><p>‍</p><p id=""""><em id="""">Read on</em></p><h2 id="""">What are LLM Evals?</h2><figure class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c72474df9a2299a68c1876_8b0f2ae3.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id=""""><em id="""">Evals are simply the process of evaluating the outputs of your model.</em> As simple as that. Evals help you compare models, identify weaknesses, and choose the best one for your specific application. You can evaluate various capabilities of your models like, reasoning, math, creative writing, linguistic understanding, and even <a href=""https://www.mercity.ai/blog-post/custom-gpt-4-chatbot"" id="""">RAG </a>capabilities. They are essential in evaluating any model pipeline before deploying and pushing it to other people. Using these evaluations you can catch your model early, where it is lacking, and improve it accordingly, or build systems around it to help the model.</p><p>‍</p><p id="""">Evals for LLMs are very straightforward to understand, but there are nuances that make them much harder to actually work with at times.</p><p>‍</p><h3 id="""">Difference between Evals and Benchmarks</h3><p id="""">A Benchmark is more of a standardized test that produces a quantitative score for comparison. Think of MMLU, which tests a model's knowledge across various subjects. Its purpose is to rank models against each other on a public leaderboard. It provides a single, comparable number but doesn't tell you if they're a good fit for a specific, complex job.</p><p id="""">Whereas<strong id=""""> </strong>an Evaluation is the <em id="""">process</em> of determining if an LLM is good enough for your specific business use case. <em id="""">An eval framework can include running benchmarks</em>, but it also includes qualitative measures, human feedback, prompt adherence tests, and use-case-specific testing. It is supposed to be much more specific to your usecase, and rather than measuring how good a model is in general, it mesures how good a model is in your narrow usecase.</p><p id="""">There have been many benchmarks over the years which helped in measuring progress, like <a href=""https://en.wikipedia.org/wiki/MMLU"" id="""">MMLU</a>, <a href=""https://arxiv.org/abs/2311.12022"" id="""">GPQA</a>, etc. These benchmarks are simple and measure AI performance on human knowledge tasks, and <strong id="""">they are very near being saturated </strong>as the new models are massively stronger. And as our workflows get more complex, these benchmarks are not able to properly capture the essence of the tasks we perform in the real world, hence the need for custom and personal Evaluations, especially for enterprises and companies that deploy models to millions of users.</p><h3 id="""">Good Benchmarks to Track</h3><p id="""">As mentioned before, most of the benchmarks are nearing saturation, models constantly score more then 80% on MMLU now, and Sonnet 3.7 is at around 75% on GPQA. This is leading to creation of more and more complex benchmarks, MMLU Pro, GPQA Diamond and whatnot. And it is getting more and more difficult to understand what benchmarks are really relevant in the real world and which ones are just for very specific and niche use cases.</p><p>‍</p><p id="""">Here are some lesser known benchmarks that you can still track for the general capabilities of the model.</p><p>‍</p><ul id=""""><li><a href=""https://lmarena.ai/"" id=""""><strong id="""">Chatbot Arena by LymSys</strong></a><strong id="""">:</strong> This is perhaps the best and most important benchmark you should keep an eye on. They created a “battleground” of sorts of models and paired them against each other, with actual real humans being the judges of quality.&nbsp; – Overall assessment of models, they have specific categories on the website too.</li><li><strong id="""">𝜏-Bench (Tau-Bench):</strong> Tool calling benchmark, simulates <strong id="""">real-world agent environments</strong> combining human conversations and API interactions across 12 enterprise domains.</li><li><strong id="""">Fiction Live:</strong> Long context benchmark, evaluates LLMs' ability to understand and track complex fictional narratives over extended contexts. – Good and important to track when working with large documents and contexts.</li><li><strong id="""">MT-Bench:</strong> Evaluates multi-turn conversation abilities through 80 curated questions (160 turns) across writing, reasoning, and coding domains. NECESSARY benchmark to track on especially for enterprise as chats span across multiple messages and across topics. There’s also MT-Bench++ which is even more enhanced for enterprise and real world use cases.</li><li><strong id="""">BOLD: </strong>Measures bias in open ended language tasks might not seem important, but in experience enterprises always need to make sure the model they are deploying doesn’t have any biases. Minor biases can really create major issues for big companies.</li></ul><p>‍</p><p id="""">These are some sophisticated benchmarks that you should always measure your model against. These are not very well known, and hence not very saturated yet either. And the best part is that most of them are based on real world usage, so you can always pick the best model that works for you AND your users, not just internally.</p><h2 id="""">Why do you need your own Custom Evals?</h2><p id="""">We have suggested some good and still relevant benchmarks above, but they still might not be enough. There are many issues with the public benchmarks. They are very often not the right indicator to evaluate real business value and performance. Which is why you still need some internal benchmarking and evaluation.</p><p>‍</p><p id="""">Let’s go deeper into why you need to be creating your own LLM Evaluation Sets.</p><h3 id="""">Popular ones are useless</h3><p id="""">Most of the popular benchmarks are very outdated. As mentioned before, benchmarks like MMLU and GPQA are indeed useful for some areas, but they are not at all a good indicator of what a model is capable of. <strong id="""">There is a very strong disconnect between the academic vs. business applications of LLMs</strong>. Most of the popular benchmarks lean heavily on the academic side of tasks, rather than day to day business applications. Benchmarks like MMLU tests knowledge, not application and actual performance.</p><p>‍</p><p id="""">This, along with researchers training intentionally to overfit the benchmarks that ruins the whole purpose of these rather good benchmarks too! This is a massive issue in space right now, which is often caught early, but sometimes not. A good example of this would be the Llama 4 series, which performed very well on the benchmarks, but performed poorly when people tried it out.<em id=""""> (Do note that there were some issues with the implementation of Llama4 initially, which were fixed later, but even after that, the performance has been underwhelming)</em>.</p><h3 id="""">LLMs get saturated on the Benchmarks and Public Data</h3><figure class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c72474df9a2299a68c1879_7a879c33.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p>‍</p><p id="""">When training LLMs, most of the data comes from scraping publicly available sources on the internet. Some companies do add in some private data to the mix, too. This massive amount of data is necessary for the model to learn and evolve. But very often this means training and even saturating the model on the data and tasks that are unnecessary to our use cases.</p><p>‍</p><p id="""">Doing this does deliver better performance in some areas, but often translates to needless training on tasks that are not necessary to the businesses and users, just to the benchmarks. For example, getting better performance on MMLU-like benchmarks is a big deal to researchers as it represents word knowledge, but it is mostly about STEM areas and are very sophisticated and niche questions. Something that has no place in business applications.</p><p>‍</p><p id="""">This saturation is the cause of a lot of slowdown and misunderstanding of the model's performance. People often think if a model scored 96% on a benchmark it must be better than a model that scored only 93%. But the truth is that <strong id="""">going from 93% to 96% leads to barely any gains in real world performance.</strong> <em id="""">Once you are above 90% accuracy in the most important benchmarks, small gains don’t really matter.</em>&nbsp;</p><p>‍</p><p id="""">This is why you should depend more on usage-based tests than benchmarks when past a threshold of performance, and most closed-source models are past that. You definitely need your own evals to accurately judge LLMs on your own set of tasks.</p><h3 id="""">Your use cases are too niche and specific to be reflected in other benchmarks</h3><p id="""">In our experience, very often companies present with use cases that are very specific to their industry or to their application. Sometimes generating very long data flow JSONs from predetermined sources, and sometimes role-playing as a famous person, but also subtly advertising for certain products. These can be tricky to implement sometimes. For example, a model might start a sales pitch instead of pushing products subtly. Something like that would be disastrous if it reaches production.</p><p>‍</p><figure class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c72474df9a2299a68c187c_b4100554.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">And when these use cases are tricky to implement, it becomes even trickier to evaluate these properly. <strong id="""">And there is almost no way that a public benchmark would cover your use case.</strong></p><h3 id="""">Quality is of utmost importance</h3><p id="""">Last, but perhaps most important, QUALITY. Companies always want to deliver the best solution possible. This means doing better than competitors, improving on what is already out there and often entering into novel areas. This is hard to work on and harder to properly capture. The issue being when you try to do something better than everyone else, you cannot measure yourself on the same scale as others. Methodology matters, and it is not always quantitative. This requires not only building, but even iterating and internally testing your benchmarks.</p><p>‍</p><p id="""">It is very difficult to build quality products using LLMs, it is necessary to properly test and iterate on the issues before releasing these products.</p><h2 id="""">How to pick what factors to Evaluate your model on?</h2><p id="""">When designing your own Evaluation, or when picking the right one from the massive set of benchmarks that exist out there, it is necessary to understand what exactly you <em id="""">need</em> to test your model on. If you are building a simple chatbot application, you might not need any strong math or reasoning skills. Similarly, if you are building a simple Email CTA Line generator, you might not even need to look at multi-turn chat performance. It is important to understand what are the necessary factors to test based on your use case so you can pick the right model.</p><p>‍</p><p id="""">One simple way to determine what factors are important is to break down the <em id="""">User Journey</em> of your LLM or application into smaller bits, both quantitative and qualitative. For example, if you are building a therapy bot, you’d need good performance on tasks like instruction following, empathy, multi-turn conversation skills as the chat gets long, etc.&nbsp;</p><p>‍</p><p id="""">The core task or the journey always comprises multiple smaller subtasks. In a way, you can think of what skills the model would need to do that task, and then the model should be good in all of those skills. Generating answers in a <a href=""https://www.mercity.ai/blog-post/custom-gpt-4-chatbot"" id="""">RAG</a> scenario would require <em id="""">good long context understanding, multi-turn conversation skills, low hallucination and good tool calling, etc.</em> All of these are events in how the user will interact with the model, they will send chat messages, and expect the model to call the RAG tool and ingest a lot of data, and then provide answers accurately. <em id="""">If your model is good at the sub skills, it's a safe bet to assume the model will be pretty good at combining them too.</em></p><h2 id="""">How to create your own Evals for LLMs?</h2><p id="""">Once we understand what factors are necessary and how different evaluations work, we can use that information to build custom evals. These evals will help you evaluate your models, improve, and push out a version that is properly tested and covers as many edge cases as possible.</p><p>‍</p><p id="""">Most of the testing can be put into largely two categories, Qualitative and Quantitative evals. Lets explore these and see how we can build evals to test both of these aspects of the LLMs.</p><h3 id="""">Quantitative Testing</h3><p id="""">Quantitative testing is when you can properly measure and accurately classify if the output is wrong or right. This is perhaps the easier one. There are very specific applications and niche areas where this is necessary. For example, if you are building a Math tutor bot, you ofc want it to be good at math. If you are building a History Teacher bot, you want it to be very high in factual correctness.</p><p>‍</p><p id="""">Let’s talk about a few quantitative evaluation factors in detail.</p><h4 id="""">Math</h4><p id="""">This is a rather simple one to understand. A very basic knowledge of math is essential to talk and have basic conversations. And most models have become very good at it, some can even go beyond that and can act as a math tutor to many in most situations. If you are planning to deal with normal conversations, and or up to high school math, you should be able to pick any frontier model and be able to get decent performance.</p><p>‍</p><p id="""">You can often follow a good, famous math benchmark, and it will be good enough. But if you want to build one for internal use, there are two methods:</p><p>‍</p><ul id=""""><li>Evaluating Steps: This is also known as Process Supervision. If you evaluate every step of the model and decide if it is moving in the right direction or not, if not, you kind of know that the answer is most likely going to be wrong too. OpenAI worked on this in the paper titled “<a href=""https://arxiv.org/abs/2305.20050"" id="""">Lets Verify Step by Step</a>”.</li><li>Evaluating on Answer: This is more open-ended. Sometimes there are multiple paths to the same problem, like when answering a riddle. Or maybe even in math questions. This is where you evaluate the answer and let the model figure out the intermediate steps naturally. In the <a href=""https://arxiv.org/abs/2402.03300"" id="""">DeepSeek R1 Paper</a> Deepseek team was able to not use any process reward models and still get amazing results!</li></ul><p>‍</p><p id="""">Both approaches tend to work. But evolution, it is always suggested to evaluate on steps if possible and if applicable to your task. An example would be solving math equations, the steps need to be correct as well, and the probability that the answer will be correct without the steps being correct is very low.</p><p>‍</p><figure class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c724d0e724789b67d06e07_63fd0324.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><h4 id="""">Function Calling and Tool Usage</h4><p id="""">Tool calling capabilities are very important when you are not just using LLMs, but also integrating them into your stack. Which most of the companies are now doing. It is very important to evaluate models on your internal specific tools and documentation. LLMs have improved a lot on function calling and with <a href=""https://docs.anthropic.com/en/docs/claude-code/mcp"" id="""">MCP protocols</a> they are improving faster than ever. But with very niche areas, they still struggle.</p><figure class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c724d0e724789b67d06e01_b474d5f2.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p>‍</p><p id="""">These numbers from the <a href=""https://www.anthropic.com/news/claude-4"" id="""">Claude-4 launch</a> show how tool use is still at 80% accuracy for Retail and us just at 80% for the Airline industry. These are very low numbers. 80% accuracy means it fails at one out of five-tool calls. This doesn’t seem bad until you realize that one agentic call can take multiple tool calls, sometimes more than 10. Such executions can never work at an 80% error rate, as a single failure can cause the whole thing to fail.</p><p>‍</p><p id="""">This makes it necessary to evaluate tool calls in your own industry.</p><p>‍</p><p id="""">To create your tool calling evaluation, you can simply use an LLM to create lots of functions for your industry with sophisticated documentation for each function. Then you can design queries that take multiple of those function calls to execute to answer.</p><p>‍</p><p id="""">Check out the prompt in the image:</p><p>‍</p><figure class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c724d0e724789b67d06e0d_e48abbeb.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p>‍</p><p id="""">The outputs can now be used to evaluate your model on these tool call chains of your system, or can be used to further train a model.</p><p>‍</p><p id="""">The given prompts should help you design your own tool call evaluation set. We are doing extensive work in this area internally, with more sophisticated writeups coming up soon! We would <a href=""https://www.mercity.ai/contacts"" id="""">suggest reaching out to us</a> if you are looking for help on this!</p><h4 id="""">Factuality</h4><p id="""">This is a rather simple one to test. Organizations often want to test factual correctness over their knowledge bases and after their fine-tuning. A very simple way to test for this is to look at MMLU-based benchmarks to test for knowledge retention.</p><p>‍</p><p id="""">To test your own knowledge bases, you can take your documents/texts and generate question-answer pairs over them, and then have your model generate answers and have a cheap verifier model check if the answers are correct or not. If you don’t want to use a verifier model, you can simply ask the model to output in a JSON format where you have an “answer” key which you can statically match with the generated correct answers.</p><h4 id="""">Long Context Handling</h4><p id="""">Long context capabilities are becoming increasingly important as more people integrate with LLMs. When working with agents and large knowledgebases, long context is super important, you need to be able to rely on it, if not, you need to build fancy mechanisms like context summarization or chunking to deal with it.</p><p>‍</p><p id=""""><a href=""https://fiction.live/stories/Fiction-liveBench-April-6-2025/oQdzQvKHw8JyXbN87"" id="""">Fiction.live</a> benchmark is rather the most comprehensive and the best benchmark for this. It is built on sophisticated stories. It requires the model to do long context thinking and reasoning with a lot of data points and potential answers to correctly reach an answer. As of writing this, Gemini is the best model for long context.</p><p>‍</p><figure class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1208px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1208px""><div><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c724d0e724789b67d06e0a_9f0b4d8a.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p>‍</p><p id="""">But if you want to build your own evaluation process to test for long context, a simple <a href=""https://arxiv.org/abs/2407.01437"" id="""">Needle in the Hay Stack</a> type test would work. You simply take a document or a paragraph/chunk of text and insert it in a large unrelated or related dump of text, and ask the LLM questions that could only be answered using the paragraph you inserted. A good example would be putting a specific medical record of a patient across the sea of various unnecessary medical information or medical information of other patients. This requires the model to locate the right source of information and correctly ignore the incorrect data. You can also move around the inserted text to test the model on various levels of depth.</p><p>‍</p><figure class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c724d0e724789b67d06e04_d8c8efd6.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><h3 id="""">Qualitative Testing</h3><p id="""">Qualitative testing is much harder than quantitative testing. Simply because it's harder, if not impossible, to measure. The measure of quality is very subjective too. However, it is also significantly more important than quantitative testing, simply because users care most about the quality of the responses above all else. Things like math tutoring and function calling are like the goals achieved using the LLMs, it matters just as much if not more how we achieve the goals.</p><p>‍</p><p id="""">If your AI math tutor is 100% accurate all the time, but is rude and sometimes don’t understand what exactly you are asking to explain further, you would not want to use such a bot at all.</p><p>‍</p><p id=""""><em id="""">User satisfaction is largely driven by high quality responses.</em></p><p>‍</p><p id="""">These qualitative evaluations are so much more important if you are deployig to real users and not using the LLM in an agentic or automation setting.</p><h4 id="""">Human Evaluation</h4><p id="""">The best way to perform Qualitative testing is to have actual humans test it out. This is a time-consuming and manual process, but it is the best way to evaluate an AI. You can use fancy methods like LLM-as-a-Judge or build a classifier on top of your AI’s responses to classify it as a good or bad response. But nothing will give you as good results as simply sitting down and going through at least a hundred responses from the AI across various topics. <strong id="""">Human eye and feedback are absolutely necessary to improve your AI.</strong> You cannot replace this with other models because it's humans who will be using your LLM, not other LLMs.</p><p>‍</p><p id="""">Here’s a quick overview of how we perform human evaluation internally at Mercity</p><p>‍</p><figure class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c724f4ab3f53d704f45869_3a884f10.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p>‍</p><p id="""">We have also built a much more sophisticated tool to collect human feedback on AI responses on chat settings here:</p><p>‍</p><p id=""""><a href=""https://github.com/Mercity-AI/LLM-Feedback-Collector"" id="""">Mercity AI LLM Feedback Collector</a></p><p>‍</p><figure class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c724f4ab3f53d704f4586c_bf1ad19f.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p>‍</p><p id=""""><em id="""">More on this tool in later sections!</em></p><p>‍</p><p id="""">It is very very important that you always collect as much human feedback as possible. It is good to not only know where the model is failing but also how it can be made better. A very important point is to always collect as much information as possible when collecting feedback. We collect:</p><p>‍</p><ul id=""""><li>Acceptable or Not: Required. Binary, is the response acceptable or not?</li><li>Rating /10: Required. Rate the response to the query out of 10.</li><li>Why: Optional. Why is the output acceptable or not?</li><li>How to make it better: Optional. How can the response be made better? Or where did the model mess up? Or what specifically do you like about the model response?</li><li>Next Message: What is the next message to the response of the LLM? - This can reveal important information about model behaviour too.</li></ul><p>‍</p><p id="""">Just these 4 data points can massively assist you in analyzing the model responses. This question set helps you answer crucial questions like “What length of responses do people like?” to “What tone of voice do people prefer?” and “What are common likes and dislikes about the model?”&nbsp;</p><p>‍</p><p id="""">We have always collected at least 100 datapoints when in the testing phase, when working with LLMs, before going out to release it to a wider audience.</p><h4 id="""">Creativity</h4><p id="""">This is a difficult one to evaluate. Creativity requires you to truly think outside the box and produce outputs that no one has ever done before. It can also be defined as diversity in outputs.</p><p>‍</p><p id="""">There is no direct way to measure creativity on a scale, but one good way is to measure the diversity of outputs with maintained quality. The idea comes from <a href=""https://arxiv.org/abs/2503.17126"" id="""">this</a> underrated paper. And that is something we can measure.</p><figure class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c724f4ab3f53d704f45870_5af9f3d6.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">The idea is to generate multiple responses on low and high temperature settings, where the model gets to explore and output various outputs, and we convert them into embeddings and measure the distance or standard deviation between them. These outputs are also taken and put into a larger, smarter thinking model with a proper ruleset for making sure that these are still high-quality outputs, the ones that are not ignored.</p><p>‍</p><figure class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c724f4ab3f53d704f45882_d38685db.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p>‍</p><p id="""">Once we have this dataset, we can go look at the points where a high enough quality or adherence to the ruleset is maintained, and a large enough standard deviation is maintained, and manually check those outputs and verify if they are good or not. If they are, we have the settings for the most creative outputs of the model.</p><p>‍</p><p id="""">This measure of standard deviation gives us a rough idea of diversity in the model’s outputs, and that gives us a rough idea of the creativity of the model.</p><p>‍</p><figure class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:783px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""783px""><div><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c724f4ab3f53d704f45876_cef858ec.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id=""""><em id="""">Here you can see various genres encoded and plotted as embeddings. Farther these embeddings are, more diverse or different the genres and the content in them.</em></p><h4 id="""">Communication Quality</h4><p id="""">This is a very small and subtle thing. Very often, a model makes mistakes or messes up ever so slightly that it is acceptable for an LLM to do. But completely unacceptable for an human being.</p><p>‍</p><p id="""">Here is a very simple example:</p><figure class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c724f4ab3f53d704f45873_65a00063.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p>‍</p><p id="""">Even after mentioning “Einstein” in the context already, the model asks what roleplay scenario and character we want. It should be inferred from context that we want the model to roleplay as Einstein. This is a simple example.</p><p>‍</p><p id="""">Very small things like missing subtle details/instructions in the prompt, calling a tool unnecessarily when the information is present, and making incorrect assumptions are hallmarks of a model that is not very good at communicating. These things are slowly fading away with more training and adaptation.</p><p>‍</p><p id="""">But there are still some things that you should test for:</p><ul id=""""><li>Not asking questions: Sometimes, the LLM should ask clarifying questions before proceeding with an answer</li><li>Making assumptions: LLMs make assumptions all the time, some of them are wrong, and they are called Hallucinations. It is suggested to check aggressively for these.</li><li>Under answering: Not all questions or sub-questions are answered</li><li>Overanswering: Answering more than necessary</li></ul><p>‍</p><p id="""">These issues are pretty common and happen often in a subtle manner. If your LLM is making these mistakes way too often, maybe consider changing the LLM or tuning the prompt or the model itself.</p><h4 id="""">Emotional Intelligence</h4><p id="""">Emotional Intelligence has gotten more and more important as users adapt LLMs for daily use.</p><p>‍</p><p id="""">We strongly recommend EqBench for understanding this aspect of model behaviour:</p><figure class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c724f4ab3f53d704f4587c_d5527a1e.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p>‍</p><p id="""">It is necessary to understand and tune how your model engages with users. Even if you have prompted your model to help in financial situations, you cannot have your model deny a request to console someone with a bad cough. Human interactions with models are messy, they sometimes stress test the system and break the interaction rules. It is necessary that your model handles such tricky situations with grace in production scenarios. If the model cannot do that properly, you should probably postpone any releases.</p><p>‍</p><p id="""">We suggest relying on the collected testing data and EqBench for testing these scenarios.</p><h4 id="""">Prompt Adherence</h4><p id="""">This is the subtle but most important part of any LLM deployment right now. Your PROMPT is the most important asset in your deployment. You tune it, you update it, spend hours of work, and test it out. <em id="""">But if your model doesn’t follow your prompt, does any of it matter?</em></p><p>‍</p><p id="""">It is important to pick a model that can follow complex, multifaceted, and even dynamic prompts properly. You steer the model using your prompts if it doesn’t follow them, you are in big trouble. It is better to pick the most common good prompt following LLM from the start, most big LLMs are good enough at it, but you can check <a href=""https://lmarena.ai/leaderboard/text/instruction-following"" id="""">LmArena’s instruction following benchmark</a> for a better comparison. TBH we suggest going with any model in the top ten. You can start with the cheapest one, and slowly climb the price ladder if you need to.</p><p>‍</p><p id="""">We have been writing 3000 words prompts for many of our enterprise deployments and most modern LLMs are good at it. Our rule of thumb is:</p><p>‍</p><p id=""""><em id="""">The larger the LLM, the better the prompt adherence.</em> This is proven by various papers too.</p><p>‍</p><p id="""">One very good method to test out prompt adherence is to test the model on various temperatures. We often deploy at very low temperature settings, 0.1, or 0.3 max. But when stress testing the model, we suggest testing on lower to much higher temperatures, up to 1.1 in some cases. This tells us what happens when the model is thrown into a chaotic state. What happens when the sampling is too random, and the model is still trying to provide a proper answer. If the model can still maintain good outputs in higher temperatures and recovers quickly after making a mistake, it usually means that the model will hold itself well at lower temperatures too!</p><h5 id="""">JailBreaking</h5><p id="""">Jailbreaking is a major problem with deploying LLMs right now. This is something that you should test for as much as possible. Most modern models have safety features trained into them now, but can still be broken. For enterprise clients, we always suggest using a guard LLM along with the core LLM deployment. <a href=""https://huggingface.co/google/shieldgemma-2b"" id="""">ShieldGemma</a> and <a href=""https://huggingface.co/meta-llama/Llama-Guard-3-8B"" id="""">LLaMAGuard</a> are amazing models for handling these edge cases. The only challenge is to deploy them along side the core LLM and properly funneling realtime responses from the CoreLLM to these Guard LLMs.</p><p>‍</p><figure class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c724f4ab3f53d704f45879_bb9e3192.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p>‍</p><h2 id="""">How to Collect Data for Model Evaluations</h2><p id="""">As mentioned before, good data is the foundation of good Evaluations and Benchmarks. And good data comes from good data collection practices. We will touch on how you can collect data properly internally and externally when building LLM-based applications.</p><h3 id="""">How to collect data in your Organization</h3><p id=""""><em id="""">Easier to collect, harder to clean and work with.</em></p><p>‍</p><p id="""">People building the app will understand the app best. It is often best to let your internal teams test the model before moving forward. You already know your users well and potentially the edge cases too. You know what they will use it for and how they will not. You can rigorously test the various aspects of your model deployment by simply letting your team use it.</p><p>‍</p><p id="""">But this comes with a bias <strong id="""">just because your team understands the app, doesn’t mean they understand the user too</strong>. Humans are messy. They are not going to use your model just for the things they say they are going to use it for. They are going to ask personal questions, and medical questions, and throw curveballs at your model. There is no way to model and collect such outlier data internally. That is why we need to collect data from real users too!</p><p>‍</p><p id="""">We use this software internally to collect data: <a href=""https://github.com/Mercity-AI/LLM-Feedback-Collector"" id="""">LLM Feedback Collector Tool</a></p><p>You can explore it here, we have opensourced it fully:&nbsp;Our open source <a href=""https://github.com/Mercity-AI/LLM-Feedback-Collector/"">LLM Feedback Collection tool </a></p><p>Video guide:</p><div data-rt-embed-type='true'><div style=""position: relative; padding-bottom: 64.90384615384616%; height: 0;""><iframe src=""https://www.loom.com/embed/7a1165e14d884429a4a423c9097db022?sid=c6737fe4-b100-4a33-9d6f-78e5e0753e03"" frameborder=""0"" webkitallowfullscreen mozallowfullscreen allowfullscreen style=""position: absolute; top: 0; left: 0; width: 100%; height: 100%;""></iframe></div></div><p>‍</p><h3 id="""">How to Collect Data from Users</h3><p id="""">This is so much more important. You MUST simply log everything. Every message and every conversation creates a massive dataset of these. And once you have enough, it's time to sit down and look at the data under a microscope.</p><p>‍</p><p id="""">Collect things like:</p><ul id=""""><li>How often the user interacts with the LLM? - Measure for likability</li><li>How often does the user rewrite a query? - Frustration signal or LLM misunderstanding the query</li><li>Likes/Dislikes on the LLM response - Direct strong likability signal</li><li>A/B test responses by giving the user an option to pick between two responses - Direct preference signal</li><li>What follow-up questions are asked after the LLM responds? - Tells us whether the answer on the first go was sufficient or not</li><li>How much time does the user spend reading the messages - LLM reply relevancy signal</li></ul><p>‍</p><p id="""">We have already talked about these in much more detail in the Human Evaluation heading.</p><h2 id="""">Some more advanced tips on Testing Models</h2><p id="""">These are just a few more things that we have noticed and gathered from our experience. These are also good to test once in your testing process to avoid any surprises.</p><figure class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c724f4ab3f53d704f4587f_2b8662e8.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id=""""><em id="""">Beam Search Analysis on Models</em></p><p>‍</p><ul id=""""><li><strong id="""">Time taken - unnecessary thinking:</strong> When working with reasoning LLMs, we make sure the model is not overthinking any specific aspects of the problem. Usually, errors arise out of overthinking rather than underthinking. Overthinking also wastes tokens and increases response latency.</li><li><strong id="""">Not answering until asked:</strong> Sometimes models don’t mention important details until asked specifically, which can be problematic in certain situations, like medical and legal.</li><li><strong id="""">User satisfaction:</strong> ALWAYS TRACK USER FEEDBACK</li><li><strong id="""">Reading level:</strong> Tracking reading level has been very effective for us. Some organizations prefer longer and more professional responses, whereas others prefer short, concise, and tight outputs.</li><li><strong id="""">Check out the beams or generations:</strong> When dealing with high hallucination levels, we have found it beneficial to look at the different output variations, specifically at the output beams. This gives us a good idea of whether the model is at least close to the correct answer or not. If it is, we often tune the model a little to fix the issues.</li><li><strong id="""">LLM as Judge:</strong> When using LLM as a judge, you must judge the judge itself before using it. Judge’s biases can easily overflow to your LLM if you are not careful enough.</li><li><strong id="""">RedTeaming/Jailbreaking:</strong> It is good to test your model deployment against some jailbreaking prompts and see how your deployments handle it. Very important if you are deploying to a large scale of users. Here’s a good dataset to check behavior: <a href=""https://huggingface.co/datasets/JailbreakBench/JBB-Behaviors"" id="""">JBB-Behaviour</a>.</li></ul><p>‍</p><h2 id="""">Building LLM Benchmarks or Evaluation Pipelines?</h2><p id="""">If you are building LLM Evaluation Pipelines, consider <a href=""https://www.mercity.ai/contacts"" id="""">reaching out</a>. We have been working with large organizations building custom eval pipelines extensively, and will slowly start putting out more detailed blogs and research on these topics.</p><p>‍</p><p id="""">We would love to work with you on creating your evaluations and help you deliver the best possible model for your users!<br><br>💪</p><p>‍</p>",https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/686d98251703724ec9db3479_Screenshot%202025-07-09%20at%2003.43.52.png,Pranav Patel,LLM Evaluations,"Learn how to build custom advanced LLM evaluations for enterprise AI deployment. Discover why public benchmarks fail and create internal evaluation pipelines that ensure quality, reduce costs, and improve user satisfaction.",True,"<div class=""rich-text w-richtext""><p><em>NOTE THAT THIS IS GOING TO BE A VERY VERY COMPREHENSIVE OVERVIEW ABOUT CREATING EVALUATION PROCESSES AND EVALUATION DATASETS. WE PLAN TO RELEASE MANY TOOLS AND MANY MORE NICHE GUIDES ABOUT THIS FIELD IN THE UPCOMING FUTURE. </em><strong><em>IF YOU NEED ANY ASSISTANCE IN EVALUATING YOUR MODEL, </em></strong><a href=""https://www.mercity.ai/contacts""><strong><em>PLEASE REACH OUT</em></strong></a><strong><em>!</em></strong></p><p>‍</p><p>You can watch this if the blog is too long for you:</p><p>‍</p><div class=""w-embed w-iframe""><div style=""position: relative; padding-bottom: 64.90384615384616%; height: 0;""><iframe allowfullscreen="""" frameborder=""0"" mozallowfullscreen="""" src=""https://www.loom.com/embed/ab25eee1980749dea355bff3f8b50d82?sid=d77c303f-9247-483e-9730-ef6ad79e062f"" style=""position: absolute; top: 0; left: 0; width: 100%; height: 100%;"" webkitallowfullscreen=""""></iframe></div></div><p>‍</p><p>AI is on the rise. Everyone wants an AI model in their business, automating tasks, improving efficiency, and making life easier. Users want AI on every platform. There have been great models like LLaMA, Mistral, GPTs, and Claude, etc. The problem becomes what model to pick and go with; that’s where custom evaluation becomes relevant.</p><p>‍</p><p>We need to be able to evaluate what model works the best for your users, and now with this mass of models, it is more necessary than ever. It is important to pick the right model and use it correctly, usually just using it 5-10 times doesn’t give you a strong understanding of where the model might be lacking. In this blog, we are going to talk about how you can create internal benchmarks or evaluation sets for your models, and test them before deploying it internally or to your users and pick the best model.</p><p>‍</p><p><em>Read on</em></p><h2>What are LLM Evals?</h2><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c72474df9a2299a68c1876_8b0f2ae3.png""/></div></figure><p><em>Evals are simply the process of evaluating the outputs of your model.</em> As simple as that. Evals help you compare models, identify weaknesses, and choose the best one for your specific application. You can evaluate various capabilities of your models like, reasoning, math, creative writing, linguistic understanding, and even <a href=""https://www.mercity.ai/blog-post/custom-gpt-4-chatbot"">RAG </a>capabilities. They are essential in evaluating any model pipeline before deploying and pushing it to other people. Using these evaluations you can catch your model early, where it is lacking, and improve it accordingly, or build systems around it to help the model.</p><p>‍</p><p>Evals for LLMs are very straightforward to understand, but there are nuances that make them much harder to actually work with at times.</p><p>‍</p><h3>Difference between Evals and Benchmarks</h3><p>A Benchmark is more of a standardized test that produces a quantitative score for comparison. Think of MMLU, which tests a model's knowledge across various subjects. Its purpose is to rank models against each other on a public leaderboard. It provides a single, comparable number but doesn't tell you if they're a good fit for a specific, complex job.</p><p>Whereas<strong> </strong>an Evaluation is the <em>process</em> of determining if an LLM is good enough for your specific business use case. <em>An eval framework can include running benchmarks</em>, but it also includes qualitative measures, human feedback, prompt adherence tests, and use-case-specific testing. It is supposed to be much more specific to your usecase, and rather than measuring how good a model is in general, it mesures how good a model is in your narrow usecase.</p><p>There have been many benchmarks over the years which helped in measuring progress, like <a href=""https://en.wikipedia.org/wiki/MMLU"">MMLU</a>, <a href=""https://arxiv.org/abs/2311.12022"">GPQA</a>, etc. These benchmarks are simple and measure AI performance on human knowledge tasks, and <strong>they are very near being saturated </strong>as the new models are massively stronger. And as our workflows get more complex, these benchmarks are not able to properly capture the essence of the tasks we perform in the real world, hence the need for custom and personal Evaluations, especially for enterprises and companies that deploy models to millions of users.</p><h3>Good Benchmarks to Track</h3><p>As mentioned before, most of the benchmarks are nearing saturation, models constantly score more then 80% on MMLU now, and Sonnet 3.7 is at around 75% on GPQA. This is leading to creation of more and more complex benchmarks, MMLU Pro, GPQA Diamond and whatnot. And it is getting more and more difficult to understand what benchmarks are really relevant in the real world and which ones are just for very specific and niche use cases.</p><p>‍</p><p>Here are some lesser known benchmarks that you can still track for the general capabilities of the model.</p><p>‍</p><ul role=""list""><li><a href=""https://lmarena.ai/""><strong>Chatbot Arena by LymSys</strong></a><strong>:</strong> This is perhaps the best and most important benchmark you should keep an eye on. They created a “battleground” of sorts of models and paired them against each other, with actual real humans being the judges of quality.  – Overall assessment of models, they have specific categories on the website too.</li><li><strong>𝜏-Bench (Tau-Bench):</strong> Tool calling benchmark, simulates <strong>real-world agent environments</strong> combining human conversations and API interactions across 12 enterprise domains.</li><li><strong>Fiction Live:</strong> Long context benchmark, evaluates LLMs' ability to understand and track complex fictional narratives over extended contexts. – Good and important to track when working with large documents and contexts.</li><li><strong>MT-Bench:</strong> Evaluates multi-turn conversation abilities through 80 curated questions (160 turns) across writing, reasoning, and coding domains. NECESSARY benchmark to track on especially for enterprise as chats span across multiple messages and across topics. There’s also MT-Bench++ which is even more enhanced for enterprise and real world use cases.</li><li><strong>BOLD: </strong>Measures bias in open ended language tasks might not seem important, but in experience enterprises always need to make sure the model they are deploying doesn’t have any biases. Minor biases can really create major issues for big companies.</li></ul><p>‍</p><p>These are some sophisticated benchmarks that you should always measure your model against. These are not very well known, and hence not very saturated yet either. And the best part is that most of them are based on real world usage, so you can always pick the best model that works for you AND your users, not just internally.</p><h2>Why do you need your own Custom Evals?</h2><p>We have suggested some good and still relevant benchmarks above, but they still might not be enough. There are many issues with the public benchmarks. They are very often not the right indicator to evaluate real business value and performance. Which is why you still need some internal benchmarking and evaluation.</p><p>‍</p><p>Let’s go deeper into why you need to be creating your own LLM Evaluation Sets.</p><h3>Popular ones are useless</h3><p>Most of the popular benchmarks are very outdated. As mentioned before, benchmarks like MMLU and GPQA are indeed useful for some areas, but they are not at all a good indicator of what a model is capable of. <strong>There is a very strong disconnect between the academic vs. business applications of LLMs</strong>. Most of the popular benchmarks lean heavily on the academic side of tasks, rather than day to day business applications. Benchmarks like MMLU tests knowledge, not application and actual performance.</p><p>‍</p><p>This, along with researchers training intentionally to overfit the benchmarks that ruins the whole purpose of these rather good benchmarks too! This is a massive issue in space right now, which is often caught early, but sometimes not. A good example of this would be the Llama 4 series, which performed very well on the benchmarks, but performed poorly when people tried it out.<em> (Do note that there were some issues with the implementation of Llama4 initially, which were fixed later, but even after that, the performance has been underwhelming)</em>.</p><h3>LLMs get saturated on the Benchmarks and Public Data</h3><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c72474df9a2299a68c1879_7a879c33.png""/></div></figure><p>‍</p><p>When training LLMs, most of the data comes from scraping publicly available sources on the internet. Some companies do add in some private data to the mix, too. This massive amount of data is necessary for the model to learn and evolve. But very often this means training and even saturating the model on the data and tasks that are unnecessary to our use cases.</p><p>‍</p><p>Doing this does deliver better performance in some areas, but often translates to needless training on tasks that are not necessary to the businesses and users, just to the benchmarks. For example, getting better performance on MMLU-like benchmarks is a big deal to researchers as it represents word knowledge, but it is mostly about STEM areas and are very sophisticated and niche questions. Something that has no place in business applications.</p><p>‍</p><p>This saturation is the cause of a lot of slowdown and misunderstanding of the model's performance. People often think if a model scored 96% on a benchmark it must be better than a model that scored only 93%. But the truth is that <strong>going from 93% to 96% leads to barely any gains in real world performance.</strong> <em>Once you are above 90% accuracy in the most important benchmarks, small gains don’t really matter.</em> </p><p>‍</p><p>This is why you should depend more on usage-based tests than benchmarks when past a threshold of performance, and most closed-source models are past that. You definitely need your own evals to accurately judge LLMs on your own set of tasks.</p><h3>Your use cases are too niche and specific to be reflected in other benchmarks</h3><p>In our experience, very often companies present with use cases that are very specific to their industry or to their application. Sometimes generating very long data flow JSONs from predetermined sources, and sometimes role-playing as a famous person, but also subtly advertising for certain products. These can be tricky to implement sometimes. For example, a model might start a sales pitch instead of pushing products subtly. Something like that would be disastrous if it reaches production.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c72474df9a2299a68c187c_b4100554.png""/></div></figure><p>And when these use cases are tricky to implement, it becomes even trickier to evaluate these properly. <strong>And there is almost no way that a public benchmark would cover your use case.</strong></p><h3>Quality is of utmost importance</h3><p>Last, but perhaps most important, QUALITY. Companies always want to deliver the best solution possible. This means doing better than competitors, improving on what is already out there and often entering into novel areas. This is hard to work on and harder to properly capture. The issue being when you try to do something better than everyone else, you cannot measure yourself on the same scale as others. Methodology matters, and it is not always quantitative. This requires not only building, but even iterating and internally testing your benchmarks.</p><p>‍</p><p>It is very difficult to build quality products using LLMs, it is necessary to properly test and iterate on the issues before releasing these products.</p><h2>How to pick what factors to Evaluate your model on?</h2><p>When designing your own Evaluation, or when picking the right one from the massive set of benchmarks that exist out there, it is necessary to understand what exactly you <em>need</em> to test your model on. If you are building a simple chatbot application, you might not need any strong math or reasoning skills. Similarly, if you are building a simple Email CTA Line generator, you might not even need to look at multi-turn chat performance. It is important to understand what are the necessary factors to test based on your use case so you can pick the right model.</p><p>‍</p><p>One simple way to determine what factors are important is to break down the <em>User Journey</em> of your LLM or application into smaller bits, both quantitative and qualitative. For example, if you are building a therapy bot, you’d need good performance on tasks like instruction following, empathy, multi-turn conversation skills as the chat gets long, etc. </p><p>‍</p><p>The core task or the journey always comprises multiple smaller subtasks. In a way, you can think of what skills the model would need to do that task, and then the model should be good in all of those skills. Generating answers in a <a href=""https://www.mercity.ai/blog-post/custom-gpt-4-chatbot"">RAG</a> scenario would require <em>good long context understanding, multi-turn conversation skills, low hallucination and good tool calling, etc.</em> All of these are events in how the user will interact with the model, they will send chat messages, and expect the model to call the RAG tool and ingest a lot of data, and then provide answers accurately. <em>If your model is good at the sub skills, it's a safe bet to assume the model will be pretty good at combining them too.</em></p><h2>How to create your own Evals for LLMs?</h2><p>Once we understand what factors are necessary and how different evaluations work, we can use that information to build custom evals. These evals will help you evaluate your models, improve, and push out a version that is properly tested and covers as many edge cases as possible.</p><p>‍</p><p>Most of the testing can be put into largely two categories, Qualitative and Quantitative evals. Lets explore these and see how we can build evals to test both of these aspects of the LLMs.</p><h3>Quantitative Testing</h3><p>Quantitative testing is when you can properly measure and accurately classify if the output is wrong or right. This is perhaps the easier one. There are very specific applications and niche areas where this is necessary. For example, if you are building a Math tutor bot, you ofc want it to be good at math. If you are building a History Teacher bot, you want it to be very high in factual correctness.</p><p>‍</p><p>Let’s talk about a few quantitative evaluation factors in detail.</p><h4>Math</h4><p>This is a rather simple one to understand. A very basic knowledge of math is essential to talk and have basic conversations. And most models have become very good at it, some can even go beyond that and can act as a math tutor to many in most situations. If you are planning to deal with normal conversations, and or up to high school math, you should be able to pick any frontier model and be able to get decent performance.</p><p>‍</p><p>You can often follow a good, famous math benchmark, and it will be good enough. But if you want to build one for internal use, there are two methods:</p><p>‍</p><ul role=""list""><li>Evaluating Steps: This is also known as Process Supervision. If you evaluate every step of the model and decide if it is moving in the right direction or not, if not, you kind of know that the answer is most likely going to be wrong too. OpenAI worked on this in the paper titled “<a href=""https://arxiv.org/abs/2305.20050"">Lets Verify Step by Step</a>”.</li><li>Evaluating on Answer: This is more open-ended. Sometimes there are multiple paths to the same problem, like when answering a riddle. Or maybe even in math questions. This is where you evaluate the answer and let the model figure out the intermediate steps naturally. In the <a href=""https://arxiv.org/abs/2402.03300"">DeepSeek R1 Paper</a> Deepseek team was able to not use any process reward models and still get amazing results!</li></ul><p>‍</p><p>Both approaches tend to work. But evolution, it is always suggested to evaluate on steps if possible and if applicable to your task. An example would be solving math equations, the steps need to be correct as well, and the probability that the answer will be correct without the steps being correct is very low.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c724d0e724789b67d06e07_63fd0324.png""/></div></figure><h4>Function Calling and Tool Usage</h4><p>Tool calling capabilities are very important when you are not just using LLMs, but also integrating them into your stack. Which most of the companies are now doing. It is very important to evaluate models on your internal specific tools and documentation. LLMs have improved a lot on function calling and with <a href=""https://docs.anthropic.com/en/docs/claude-code/mcp"">MCP protocols</a> they are improving faster than ever. But with very niche areas, they still struggle.</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c724d0e724789b67d06e01_b474d5f2.png""/></div></figure><p>‍</p><p>These numbers from the <a href=""https://www.anthropic.com/news/claude-4"">Claude-4 launch</a> show how tool use is still at 80% accuracy for Retail and us just at 80% for the Airline industry. These are very low numbers. 80% accuracy means it fails at one out of five-tool calls. This doesn’t seem bad until you realize that one agentic call can take multiple tool calls, sometimes more than 10. Such executions can never work at an 80% error rate, as a single failure can cause the whole thing to fail.</p><p>‍</p><p>This makes it necessary to evaluate tool calls in your own industry.</p><p>‍</p><p>To create your tool calling evaluation, you can simply use an LLM to create lots of functions for your industry with sophisticated documentation for each function. Then you can design queries that take multiple of those function calls to execute to answer.</p><p>‍</p><p>Check out the prompt in the image:</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c724d0e724789b67d06e0d_e48abbeb.png""/></div></figure><p>‍</p><p>The outputs can now be used to evaluate your model on these tool call chains of your system, or can be used to further train a model.</p><p>‍</p><p>The given prompts should help you design your own tool call evaluation set. We are doing extensive work in this area internally, with more sophisticated writeups coming up soon! We would <a href=""https://www.mercity.ai/contacts"">suggest reaching out to us</a> if you are looking for help on this!</p><h4>Factuality</h4><p>This is a rather simple one to test. Organizations often want to test factual correctness over their knowledge bases and after their fine-tuning. A very simple way to test for this is to look at MMLU-based benchmarks to test for knowledge retention.</p><p>‍</p><p>To test your own knowledge bases, you can take your documents/texts and generate question-answer pairs over them, and then have your model generate answers and have a cheap verifier model check if the answers are correct or not. If you don’t want to use a verifier model, you can simply ask the model to output in a JSON format where you have an “answer” key which you can statically match with the generated correct answers.</p><h4>Long Context Handling</h4><p>Long context capabilities are becoming increasingly important as more people integrate with LLMs. When working with agents and large knowledgebases, long context is super important, you need to be able to rely on it, if not, you need to build fancy mechanisms like context summarization or chunking to deal with it.</p><p>‍</p><p><a href=""https://fiction.live/stories/Fiction-liveBench-April-6-2025/oQdzQvKHw8JyXbN87"">Fiction.live</a> benchmark is rather the most comprehensive and the best benchmark for this. It is built on sophisticated stories. It requires the model to do long context thinking and reasoning with a lot of data points and potential answers to correctly reach an answer. As of writing this, Gemini is the best model for long context.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1208pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c724d0e724789b67d06e0a_9f0b4d8a.png""/></div></figure><p>‍</p><p>But if you want to build your own evaluation process to test for long context, a simple <a href=""https://arxiv.org/abs/2407.01437"">Needle in the Hay Stack</a> type test would work. You simply take a document or a paragraph/chunk of text and insert it in a large unrelated or related dump of text, and ask the LLM questions that could only be answered using the paragraph you inserted. A good example would be putting a specific medical record of a patient across the sea of various unnecessary medical information or medical information of other patients. This requires the model to locate the right source of information and correctly ignore the incorrect data. You can also move around the inserted text to test the model on various levels of depth.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c724d0e724789b67d06e04_d8c8efd6.png""/></div></figure><h3>Qualitative Testing</h3><p>Qualitative testing is much harder than quantitative testing. Simply because it's harder, if not impossible, to measure. The measure of quality is very subjective too. However, it is also significantly more important than quantitative testing, simply because users care most about the quality of the responses above all else. Things like math tutoring and function calling are like the goals achieved using the LLMs, it matters just as much if not more how we achieve the goals.</p><p>‍</p><p>If your AI math tutor is 100% accurate all the time, but is rude and sometimes don’t understand what exactly you are asking to explain further, you would not want to use such a bot at all.</p><p>‍</p><p><em>User satisfaction is largely driven by high quality responses.</em></p><p>‍</p><p>These qualitative evaluations are so much more important if you are deployig to real users and not using the LLM in an agentic or automation setting.</p><h4>Human Evaluation</h4><p>The best way to perform Qualitative testing is to have actual humans test it out. This is a time-consuming and manual process, but it is the best way to evaluate an AI. You can use fancy methods like LLM-as-a-Judge or build a classifier on top of your AI’s responses to classify it as a good or bad response. But nothing will give you as good results as simply sitting down and going through at least a hundred responses from the AI across various topics. <strong>Human eye and feedback are absolutely necessary to improve your AI.</strong> You cannot replace this with other models because it's humans who will be using your LLM, not other LLMs.</p><p>‍</p><p>Here’s a quick overview of how we perform human evaluation internally at Mercity</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600px""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c724f4ab3f53d704f45869_3a884f10.png""/></div></figure><p>‍</p><p>We have also built a much more sophisticated tool to collect human feedback on AI responses on chat settings here:</p><p>‍</p><p><a href=""https://github.com/Mercity-AI/LLM-Feedback-Collector"">Mercity AI LLM Feedback Collector</a></p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600px""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c724f4ab3f53d704f4586c_bf1ad19f.png""/></div></figure><p>‍</p><p><em>More on this tool in later sections!</em></p><p>‍</p><p>It is very very important that you always collect as much human feedback as possible. It is good to not only know where the model is failing but also how it can be made better. A very important point is to always collect as much information as possible when collecting feedback. We collect:</p><p>‍</p><ul role=""list""><li>Acceptable or Not: Required. Binary, is the response acceptable or not?</li><li>Rating /10: Required. Rate the response to the query out of 10.</li><li>Why: Optional. Why is the output acceptable or not?</li><li>How to make it better: Optional. How can the response be made better? Or where did the model mess up? Or what specifically do you like about the model response?</li><li>Next Message: What is the next message to the response of the LLM? - This can reveal important information about model behaviour too.</li></ul><p>‍</p><p>Just these 4 data points can massively assist you in analyzing the model responses. This question set helps you answer crucial questions like “What length of responses do people like?” to “What tone of voice do people prefer?” and “What are common likes and dislikes about the model?” </p><p>‍</p><p>We have always collected at least 100 datapoints when in the testing phase, when working with LLMs, before going out to release it to a wider audience.</p><h4>Creativity</h4><p>This is a difficult one to evaluate. Creativity requires you to truly think outside the box and produce outputs that no one has ever done before. It can also be defined as diversity in outputs.</p><p>‍</p><p>There is no direct way to measure creativity on a scale, but one good way is to measure the diversity of outputs with maintained quality. The idea comes from <a href=""https://arxiv.org/abs/2503.17126"">this</a> underrated paper. And that is something we can measure.</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600px""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c724f4ab3f53d704f45870_5af9f3d6.png""/></div></figure><p>The idea is to generate multiple responses on low and high temperature settings, where the model gets to explore and output various outputs, and we convert them into embeddings and measure the distance or standard deviation between them. These outputs are also taken and put into a larger, smarter thinking model with a proper ruleset for making sure that these are still high-quality outputs, the ones that are not ignored.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600px""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c724f4ab3f53d704f45882_d38685db.png""/></div></figure><p>‍</p><p>Once we have this dataset, we can go look at the points where a high enough quality or adherence to the ruleset is maintained, and a large enough standard deviation is maintained, and manually check those outputs and verify if they are good or not. If they are, we have the settings for the most creative outputs of the model.</p><p>‍</p><p>This measure of standard deviation gives us a rough idea of diversity in the model’s outputs, and that gives us a rough idea of the creativity of the model.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:783px""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c724f4ab3f53d704f45876_cef858ec.png""/></div></figure><p><em>Here you can see various genres encoded and plotted as embeddings. Farther these embeddings are, more diverse or different the genres and the content in them.</em></p><h4>Communication Quality</h4><p>This is a very small and subtle thing. Very often, a model makes mistakes or messes up ever so slightly that it is acceptable for an LLM to do. But completely unacceptable for an human being.</p><p>‍</p><p>Here is a very simple example:</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600px""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c724f4ab3f53d704f45873_65a00063.png""/></div></figure><p>‍</p><p>Even after mentioning “Einstein” in the context already, the model asks what roleplay scenario and character we want. It should be inferred from context that we want the model to roleplay as Einstein. This is a simple example.</p><p>‍</p><p>Very small things like missing subtle details/instructions in the prompt, calling a tool unnecessarily when the information is present, and making incorrect assumptions are hallmarks of a model that is not very good at communicating. These things are slowly fading away with more training and adaptation.</p><p>‍</p><p>But there are still some things that you should test for:</p><ul role=""list""><li>Not asking questions: Sometimes, the LLM should ask clarifying questions before proceeding with an answer</li><li>Making assumptions: LLMs make assumptions all the time, some of them are wrong, and they are called Hallucinations. It is suggested to check aggressively for these.</li><li>Under answering: Not all questions or sub-questions are answered</li><li>Overanswering: Answering more than necessary</li></ul><p>‍</p><p>These issues are pretty common and happen often in a subtle manner. If your LLM is making these mistakes way too often, maybe consider changing the LLM or tuning the prompt or the model itself.</p><h4>Emotional Intelligence</h4><p>Emotional Intelligence has gotten more and more important as users adapt LLMs for daily use.</p><p>‍</p><p>We strongly recommend EqBench for understanding this aspect of model behaviour:</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600px""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c724f4ab3f53d704f4587c_d5527a1e.png""/></div></figure><p>‍</p><p>It is necessary to understand and tune how your model engages with users. Even if you have prompted your model to help in financial situations, you cannot have your model deny a request to console someone with a bad cough. Human interactions with models are messy, they sometimes stress test the system and break the interaction rules. It is necessary that your model handles such tricky situations with grace in production scenarios. If the model cannot do that properly, you should probably postpone any releases.</p><p>‍</p><p>We suggest relying on the collected testing data and EqBench for testing these scenarios.</p><h4>Prompt Adherence</h4><p>This is the subtle but most important part of any LLM deployment right now. Your PROMPT is the most important asset in your deployment. You tune it, you update it, spend hours of work, and test it out. <em>But if your model doesn’t follow your prompt, does any of it matter?</em></p><p>‍</p><p>It is important to pick a model that can follow complex, multifaceted, and even dynamic prompts properly. You steer the model using your prompts if it doesn’t follow them, you are in big trouble. It is better to pick the most common good prompt following LLM from the start, most big LLMs are good enough at it, but you can check <a href=""https://lmarena.ai/leaderboard/text/instruction-following"">LmArena’s instruction following benchmark</a> for a better comparison. TBH we suggest going with any model in the top ten. You can start with the cheapest one, and slowly climb the price ladder if you need to.</p><p>‍</p><p>We have been writing 3000 words prompts for many of our enterprise deployments and most modern LLMs are good at it. Our rule of thumb is:</p><p>‍</p><p><em>The larger the LLM, the better the prompt adherence.</em> This is proven by various papers too.</p><p>‍</p><p>One very good method to test out prompt adherence is to test the model on various temperatures. We often deploy at very low temperature settings, 0.1, or 0.3 max. But when stress testing the model, we suggest testing on lower to much higher temperatures, up to 1.1 in some cases. This tells us what happens when the model is thrown into a chaotic state. What happens when the sampling is too random, and the model is still trying to provide a proper answer. If the model can still maintain good outputs in higher temperatures and recovers quickly after making a mistake, it usually means that the model will hold itself well at lower temperatures too!</p><h5>JailBreaking</h5><p>Jailbreaking is a major problem with deploying LLMs right now. This is something that you should test for as much as possible. Most modern models have safety features trained into them now, but can still be broken. For enterprise clients, we always suggest using a guard LLM along with the core LLM deployment. <a href=""https://huggingface.co/google/shieldgemma-2b"">ShieldGemma</a> and <a href=""https://huggingface.co/meta-llama/Llama-Guard-3-8B"">LLaMAGuard</a> are amazing models for handling these edge cases. The only challenge is to deploy them along side the core LLM and properly funneling realtime responses from the CoreLLM to these Guard LLMs.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600px""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c724f4ab3f53d704f45879_bb9e3192.png""/></div></figure><p>‍</p><h2>How to Collect Data for Model Evaluations</h2><p>As mentioned before, good data is the foundation of good Evaluations and Benchmarks. And good data comes from good data collection practices. We will touch on how you can collect data properly internally and externally when building LLM-based applications.</p><h3>How to collect data in your Organization</h3><p><em>Easier to collect, harder to clean and work with.</em></p><p>‍</p><p>People building the app will understand the app best. It is often best to let your internal teams test the model before moving forward. You already know your users well and potentially the edge cases too. You know what they will use it for and how they will not. You can rigorously test the various aspects of your model deployment by simply letting your team use it.</p><p>‍</p><p>But this comes with a bias <strong>just because your team understands the app, doesn’t mean they understand the user too</strong>. Humans are messy. They are not going to use your model just for the things they say they are going to use it for. They are going to ask personal questions, and medical questions, and throw curveballs at your model. There is no way to model and collect such outlier data internally. That is why we need to collect data from real users too!</p><p>‍</p><p>We use this software internally to collect data: <a href=""https://github.com/Mercity-AI/LLM-Feedback-Collector"">LLM Feedback Collector Tool</a></p><p>You can explore it here, we have opensourced it fully: Our open source <a href=""https://github.com/Mercity-AI/LLM-Feedback-Collector/"">LLM Feedback Collection tool </a></p><p>Video guide:</p><div class=""w-embed w-iframe""><div style=""position: relative; padding-bottom: 64.90384615384616%; height: 0;""><iframe allowfullscreen="""" frameborder=""0"" mozallowfullscreen="""" src=""https://www.loom.com/embed/7a1165e14d884429a4a423c9097db022?sid=c6737fe4-b100-4a33-9d6f-78e5e0753e03"" style=""position: absolute; top: 0; left: 0; width: 100%; height: 100%;"" webkitallowfullscreen=""""></iframe></div></div><p>‍</p><h3>How to Collect Data from Users</h3><p>This is so much more important. You MUST simply log everything. Every message and every conversation creates a massive dataset of these. And once you have enough, it's time to sit down and look at the data under a microscope.</p><p>‍</p><p>Collect things like:</p><ul role=""list""><li>How often the user interacts with the LLM? - Measure for likability</li><li>How often does the user rewrite a query? - Frustration signal or LLM misunderstanding the query</li><li>Likes/Dislikes on the LLM response - Direct strong likability signal</li><li>A/B test responses by giving the user an option to pick between two responses - Direct preference signal</li><li>What follow-up questions are asked after the LLM responds? - Tells us whether the answer on the first go was sufficient or not</li><li>How much time does the user spend reading the messages - LLM reply relevancy signal</li></ul><p>‍</p><p>We have already talked about these in much more detail in the Human Evaluation heading.</p><h2>Some more advanced tips on Testing Models</h2><p>These are just a few more things that we have noticed and gathered from our experience. These are also good to test once in your testing process to avoid any surprises.</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600px""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c724f4ab3f53d704f4587f_2b8662e8.png""/></div></figure><p><em>Beam Search Analysis on Models</em></p><p>‍</p><ul role=""list""><li><strong>Time taken - unnecessary thinking:</strong> When working with reasoning LLMs, we make sure the model is not overthinking any specific aspects of the problem. Usually, errors arise out of overthinking rather than underthinking. Overthinking also wastes tokens and increases response latency.</li><li><strong>Not answering until asked:</strong> Sometimes models don’t mention important details until asked specifically, which can be problematic in certain situations, like medical and legal.</li><li><strong>User satisfaction:</strong> ALWAYS TRACK USER FEEDBACK</li><li><strong>Reading level:</strong> Tracking reading level has been very effective for us. Some organizations prefer longer and more professional responses, whereas others prefer short, concise, and tight outputs.</li><li><strong>Check out the beams or generations:</strong> When dealing with high hallucination levels, we have found it beneficial to look at the different output variations, specifically at the output beams. This gives us a good idea of whether the model is at least close to the correct answer or not. If it is, we often tune the model a little to fix the issues.</li><li><strong>LLM as Judge:</strong> When using LLM as a judge, you must judge the judge itself before using it. Judge’s biases can easily overflow to your LLM if you are not careful enough.</li><li><strong>RedTeaming/Jailbreaking:</strong> It is good to test your model deployment against some jailbreaking prompts and see how your deployments handle it. Very important if you are deploying to a large scale of users. Here’s a good dataset to check behavior: <a href=""https://huggingface.co/datasets/JailbreakBench/JBB-Behaviors"">JBB-Behaviour</a>.</li></ul><p>‍</p><h2>Building LLM Benchmarks or Evaluation Pipelines?</h2><p>If you are building LLM Evaluation Pipelines, consider <a href=""https://www.mercity.ai/contacts"">reaching out</a>. We have been working with large organizations building custom eval pipelines extensively, and will slowly start putting out more detailed blogs and research on these topics.</p><p>‍</p><p>We would love to work with you on creating your evaluations and help you deliver the best possible model for your users!<br/><br/>💪</p><p>‍</p></div>"
How to Build Real Time Voice Cloning Pipelines,how-to-build-real-time-voice-cloning-pipelines,640f56f76d313b2faa631c11,664face4ce888d1a2b3eae17,False,False,Thu May 23 2024 20:53:56 GMT+0000 (Coordinated Universal Time),Mon Jul 22 2024 10:19:29 GMT+0000 (Coordinated Universal Time),Mon Jul 22 2024 10:19:50 GMT+0000 (Coordinated Universal Time),"<blockquote id="""">⭐ All the code shown in this blog can be found in our <a href=""https://github.com/Mercity-AI/Voice-Cloning-Demo"" id="""">here</a> with all the necessary documentation and instructions to run.</blockquote><p id="""">‍</p><p id="""">In the evolving landscape of artificial intelligence and machine learning, real-time voice cloning has emerged as a groundbreaking technology. By leveraging advanced generative models and neural networks, it is now possible to create digital replicas of human voices with impressive accuracy. This technology not only captures the spoken words but also mimics the unique vocal characteristics, intonations, and emotions of the speaker. This groundbreaking technology is also being explored by top AI companies like <a href=""https://openai.com/index/hello-gpt-4o/"" id="""">Open AI</a>,<a href=""https://www.lindy.ai"" id="""">Lindy AI</a>,<a href=""https://www.microsoft.com/en-us/cortana"" id="""">Microsoft Cortana</a> and much more.</p><p id="""">‍</p><h2 id="""">What is Real Time Voice Cloning?</h2><p id="""">‍</p><p id="""">Real time voice cloning is the process of cloning the human voice using Generative model and Neural Network to create a digital copy of the True human voice. It gives a statistical representation of human voice using the process of spectrogram analysis. The neural network is the Machine learning algorithm which is responsible for the training of the generative model to produce the AI mimic Voice.</p><p id="""">‍</p><p id="""">A spectrogram is a visual representation of the spectrum of frequencies in a sound signal as they vary with time. It is created by applying a Fast Fourier Transform (FFT) to short, overlapping segments of an audio signal, which provides information about the amplitude (or power) of different frequency components over time.</p><p id="""">‍</p><h2 id="""">Why&nbsp; Real Time Audio Generation?</h2><p id="""">‍</p><p id="""">Real time audio cloning technology instantly captures the human voice with such high fidelity not just the words spoken by them but also the unique vocal characteristics,intonation and emotions of the speaker.</p><p id="""">Where this technology has great potential in sectors which have human- system communication streams, new forms of content creation and also act as accessible solutions to the society of blind and deaf people. Let’s see about the real time applicable sectors and the existing top technologies in them.</p><h3 id="""">Voice Assistants</h3><p id="""">‍</p><p id="""">A natural and engaging experience can be introduced to the existing voice assistants like siri,alexa and google assistant and also the drawback of multilingual support can also be overcomed with the real time voice cloning architecture. By replicating tonal nuances and emotions in voice, Voice Assistants can respond more empathetic and appropriately to users, enhancing user satisfaction.</p><p id="""">‍</p><p id="""">Moreover, the <a href=""https://openai.com/index/hello-gpt-4o/"" id="""">GPT-4o</a> new release by OpenAI introduces advanced features in natural language understanding and generation. By integrating real-time voice cloning with GPT-4o, voice assistants can achieve even higher levels of sophistication in dialogue management, providing seamless and intuitive user experiences. This combination enables the creation of highly interactive and human-like voice assistants that can cater to a diverse range of needs and preferences.</p><p id="""">‍</p><h3 id="""">Assistive Technology For Blind and Visually Impaired People</h3><p id="""">‍</p><p id="""">Real-time audio cloning can create more natural-sounding screen readers and navigation aids. These tools can read out text, describe surroundings, or provide directions in a voice that users find comforting and familiar. The user experience is ultimately improved due to the personalisation feature where it helps to choose the voice they wish or like.</p><p id="""">‍</p><p id="""">Moreover, existing assistive technologies such as <a href=""https://aira.io"" id="""">Aira</a>, <a href=""https://www.bemyeyes.com"" id="""">Be My Eyes</a>, <a href=""https://play.google.com/store/apps/details?id=com.google.android.apps.accessibility.reveal&hl=en_US&pli=1"" id="""">Lookout</a>, and <a href=""https://www.seeingai.com"" id="""">Seeing AI</a> are revolutionizing support for the visually impaired. <a href=""https://aira.io"" id="""">Aira</a> connects users with trained agents who provide real-time assistance through live video calls, helping with navigation, reading, and other tasks. <a href=""https://www.bemyeyes.com"" id="""">Be My Eyes</a> also uses live video calls to link visually impaired individuals with sighted volunteers for assistance with various daily activities. <a href=""https://play.google.com/store/apps/details?id=com.google.android.apps.accessibility.reveal&hl=en_US&pli=1"" id="""">Lookout</a>, an Android app, provides spoken feedback about the user’s surroundings by utilizing the device's camera to recognize text, people, and objects. <a href=""https://www.seeingai.com"" id="""">Seeing AI</a>, available on iOS, narrates the world around the user by reading text, identifying products, recognizing faces, and describing scenes. Though the existing technology is useful, When we integrate real-time audio generation with advanced computer vision technology we don’t have to rely on human agents and the user experience would be smooth and consistent with their personal closed one voice navigating and exploring the world.</p><p id="""">‍</p><p id="""">The new GPT-4o release by OpenAI offers advanced features in natural language understanding and generation, which can further enhance these assistive technologies. Integrating real-time audio cloning with GPT-4o allows for more sophisticated and intuitive interactions, providing a higher level of support and independence for visually impaired users. Feel free to watch the <a href=""https://www.youtube.com/watch?v=KwNUJ69RbwY"" id="""">demo</a> by OpenAI X Be My Eyes. This combination ensures that assistive tools not only convey information effectively but also do so in a voice that resonates personally with the user, making the technology more accessible and user-friendly.</p><h3 id="""">Audiobooks and Stories On-Demand</h3><p id="""">Real-time audio cloning allows for the swift creation of audiobooks, enabling authors and publishers to meet the growing demand for audio content without the lengthy process of traditional recording.</p><p id="""">Users can also listen to their audio books with the desired characteristics of voice either it can be their loved or inspirational persons.This personalization can make the listening experience more enjoyable and intimate.The ability to replicate the tonality and emotion in a voice helps convey the intended meaning more effectively. This is crucial in applications like virtual therapy, remote education, and customer service, where understanding subtleties in communication is essential.</p><p id="""">‍</p><p id="""">A new tool in the market called <a href=""https://www.anytopic.io"" id="""">AnyTopic</a> which enables users to create their own audio book using the technique of <a href=""https://github.com/assafelovic/gpt-researcher"" id="""">GPT researcher concep</a>t where you will provide the keen topic you're interested in and then the agent will prepare a sole audiobook for the user's intended timing. The integration of real audio generation concepts over here will enhance the user personalisation concept drastically.</p><p id="""">‍</p><h3 id="""">Scalable and&nbsp; Reliable Customer Service</h3><p id="""">‍</p><p id="""">Customer Service Agent markets are huge and wide till date which can be replaced by the Real voice cloning architecture to answer the customer queries instead of agents which significantly make huge cost cutting and also not a compromisable solution as the tonality and characteristic of the voice is mimicked real. Cloned voices can be used to provide support in multiple languages, each with native-like pronunciation and tone. This can make non-native speakers feel more comfortable and understood.</p><p id="""">‍</p><p id="""">Additionally, existing customer service platforms such as <a href=""https://www.bland.ai"" id="""">Bland.AI</a> and <a href=""https://aws.amazon.com/pm/lex/?trk=436e9c39-382a-42a6-a49f-4cdbdfe8cadc&sc_channel=ps&ef_id=Cj0KCQjwxqayBhDFARIsAANWRnQUWV4Bfm6w3TPw-jSYaST1fciJyqaLnUSGoKHYvYZFZV8SvwajaCMaAsn6EALw_wcB:G:s&s_kwcid=AL!4422!3!652868433334!e!!g!!amazon%20lex!19910624536!147207932349&gclid=Cj0KCQjwxqayBhDFARIsAANWRnQUWV4Bfm6w3TPw-jSYaST1fciJyqaLnUSGoKHYvYZFZV8SvwajaCMaAsn6EALw_wcB"" id="""">Amazon Lex</a> stand to benefit significantly from real-time voice cloning. Bland.AI can leverage this technology to enhance its user interactions, making conversations more fluid and natural across different languages and dialects. Amazon Lex, which powers Alexa's voice capabilities, can utilize real-time voice cloning to offer more personalized and context-aware interactions, improving user engagement and satisfaction.</p><p id="""">‍</p><p id="""">Apart from the customer-agent perspective it is also well suitable for any line of&nbsp; communication between humans. For example A healthcare provider can use voice cloning to offer compassionate support to patients, recognizing when a patient is stressed or anxious and responding with appropriate empathy and care.</p><p id="""">‍</p><h2 id="""">How does Real Time Audio Cloning work?</h2><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1347px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1347px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/664fa970ae2d25063be7bb7a_OyZ8ZK_DcIja7G0_U4AX6T3BM1FXQR24S6IkOlz_BeK8O1SPMyltFm1yWvPoI--CRhl2u2j4SC2fhyD_ebSIcKUXtvvJA9EbQzucmnrTE_Do3xEYS3KIuf1uXNCgakhfunBsYHW_reAGJREkMt5sfqQ.jpeg"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><p id="""">Real-time audio cloning, also known as voice cloning, is a process that replicates a person's voice using artificial intelligence (AI) and machine learning (ML) techniques. This technology can generate synthetic speech that mimics the tone, pitch, and inflection of a target voice in real time. Here's a detailed explanation of how this process works:</p><h3 id="""">Speaker Encoder</h3><p id="""">The feature extractor processes the input audio (speaker reference waveform) to extract essential features that capture the unique characteristics of the speaker's voice. These features are crucial for maintaining the speaker's identity in the synthesized speech.</p><p id="""">‍</p><p id="""">The speaker encoder takes the features extracted by the feature extractor and encodes them into a latent representation. This encoded representation is a compact form that retains all the necessary speaker-specific information required for voice cloning.</p><h3 id="""">Acoustic Model</h3><p id="""">‍</p><p id="""">The acoustic model receives input from two sources: the encoded speaker representation and the text (grapheme or phoneme sequence) to be spoken. It combines these inputs to generate intermediate acoustic representations that reflect both the content of the speech and the speaker's unique voice characteristics. The model leverages advanced neural network architectures, such as recurrent neural networks (RNNs) or transformer networks, to process the temporal and contextual aspects of the speech. By doing so, it ensures that the generated acoustic features accurately capture the nuances of the speaker's voice, including intonation, rhythm, and emotional tone. This allows for the production of synthetic speech that sounds natural, closely mimicking the reference speaker's vocal attributes while conveying the intended message clearly and effectively.</p><p id="""">‍</p><h3 id="""">Vocoder</h3><p id="""">‍</p><p id="""">The vocoder is responsible for converting the intermediate acoustic representations generated by the acoustic model into a waveform. This waveform is the final synthesized speech output, designed to closely mimic the reference speaker's voice, including their unique tonal qualities, pitch, and inflections. By effectively translating the detailed acoustic features into a smooth, continuous audio signal, the vocoder ensures that the synthesized voice maintains a high degree of fidelity and naturalness, making it indistinguishable from the original speaker in both clarity and expressiveness.</p><h3 id="""">Synthesizer</h3><p id="""">‍</p><p id="""">The synthesizer is a critical component that includes the encoder, concatenation, attention mechanism, and decoder. The Encoder converts the input text (grapheme or phoneme sequence) into a high-dimensional representation. The concat concatenates the encoded text representation with the speaker's encoded voice features. Attention Mechanism ensures that the synthesizer focuses on the relevant parts of the text and speaker features at each step of the speech synthesis process.The Decoder converts the concatenated representation into a sequence of acoustic features that the vocoder can process.</p><p id="""">‍</p><h3 id="""">Workflow</h3><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/664fa970d44e7c729d84d400_vcrZyGmP01HSgxtlTzzreKVSNcarA7HHUdpI5gQN8JRZru7FKr_j3F5ZbO9Svge7iWsO2U79d5jhQtuJcXIY12i3kluu_JgS6jdZHpEx1gLWSELmWpJO2dW1JicV6NO6GU5XKRNsm6NmS-KWGFAI4Zs.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><p id="""">The process begins with the Speaker Reference Waveform, where an audio sample of the speaker's voice is provided as input. The Speaker Encoder processes mel spectrograms from this audio sample to extract essential features, which are then encoded into speaker embeddings. These embeddings are optimized using a gradient-based approach and a GE2E (Generalized End-to-End) loss function.</p><p id="""">‍</p><p id="""">Next, the system utilizes Dataset 2, which includes both text and mel spectrograms. The text input is processed by the Speaker Encoder to generate embeddings, and these are combined with the speaker embeddings in the Synthesizer. The synthesizer generates predicted mel spectrograms, which are refined using a spectral loss function to ensure they closely match the target spectrograms.</p><p id="""">‍</p><p id="""">Finally, the Vocoder converts the predicted mel spectrograms into audio waveforms. These predicted audio waveforms are compared with the target audio using a waveform loss function, and the gradients from this comparison are used to further optimize the synthesizer and vocoder. This comprehensive process ensures that the final synthesized speech output closely mimics the reference speaker's voice, capturing their unique tonal qualities, pitch, and inflections. This innovative architecture allows for real-time audio cloning by effectively processing and integrating both textual and speaker-specific data, making it applicable for use in voice assistants, customer support services, interactive user interfaces, and more. To know more about the architecture of TTS refer to this <a href=""https://www.jetir.org/papers/JETIR2006189.pdf"" id="""">paper</a>.</p><p id="""">‍</p><h2 id="""">How to build a Voice Cloning Pipeline</h2><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1095px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1095px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/664fa97066672efd86ccad18_DTrKIyYIK-nW1S60DDXD6iy6POaKRJQhQqy2xGpb-U3XLC-UHLtaY0Bv7YDVSXC3Gzh4MMAuQf8vP5VsofB7jYr76Dur-3iKoUMXivqL5WH-9pchii1Q1_VZ5krVVSpVAVp_E0-CwaZdztjN4b2xObU.jpeg"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><p id="""">Building a voice cloning pipeline involves setting up a system that can take an audio input of a speaker's voice and generate new speech that mimics the same voice by matching with the text given by the user.&nbsp;</p><p id="""">‍</p><p id="""">Below is an iPython script that uses the TTS library to perform voice cloning. It initializes a text-to-speech (TTS) model, loads a pre-trained model checkpoint, and synthesizes speech using an input text and a specific speaker's voice characteristics extracted from audio files. Here's a detailed explanation of each part of the code and the purpose of the libraries used:</p><p id="""">‍</p><h3 id="""">Importing Libraries</h3><div data-rt-embed-type='true'><pre>
<code class=""language-py"">
from TTS.tts.configs.bark_config import BarkConfig
from TTS.tts.models.bark import Bark

from scipy.io.wavfile import write as write_wav

import os
</code>
</pre></div><p id="""">‍</p><p id="""">The TTS library is a powerful tool for text-to-speech conversion. It supports multiple TTS models, including Bark. These imports specifically bring in the configuration and model components required to set up and use the Bark TTS model.</p><p id="""">‍</p><p id="""">SciPy is a scientific computing library in Python. Here, it is used to save the generated speech waveform to an audio file.</p><p id="""">write_wav: This function writes a NumPy array to a WAV file, which is a common format for storing audio data.</p><p id="""">‍</p><p id="""">The OS library provides a way to interact with the operating system. It is used for handling directory paths and file management.</p><p id="""">‍</p><h3 id="""">Setting Up Configuration:</h3><div data-rt-embed-type='true'><pre>
<code class=""language-py"">
config = BarkConfig()
model = Bark.init_from_config(config)
model.load_checkpoint(config, checkpoint_dir=""bark/"", eval=True)
</code>
</pre></div><p id="""">‍</p><p id="""">Initializes the configuration for the Bark model. This configuration includes various parameters that control the model's behavior during speech synthesis.</p><p id="""">‍</p><p id="""">Initializes the Bark TTS model using the specified configuration. This sets up the model architecture and prepares it for loading pre-trained weights.Loads the pre-trained weights for the Bark model from the specified checkpoint directory. This is crucial for ensuring the model has learned to generate high-quality speech based on extensive training data.</p><p id="""">‍</p><h3 id="""">Speech Synthesis</h3><div data-rt-embed-type='true'><pre>
<code class=""language-py"">
text = ""Mercity ai is a leading AI innovator in India, with OpenAI planning collaboration.""
voice_dirs = ""/Users/username/Desktop/projects/AI voice Cloning/Speaker voice/""
</code>
</pre></div><p id="""">‍</p><p id="""">Defines the text that will be converted into speech. This is the input that the TTS model will process to generate the corresponding audio output.</p><p id="""">‍</p><p id="""">Specifies the directory containing the speaker's audio files. These files are used to extract speaker-specific characteristics (embeddings) for voice cloning.</p><p id="""">‍</p><h3 id="""">Synthesizing Speech</h3><div data-rt-embed-type='true'><pre>
<code class=""language-py"">
output_dict = model.synthesize(text, config, speaker_id='speaker', voice_dirs=""bark_voices"", temperature=0.95)
</code>
</pre></div><p id="""">‍</p><p id="""">Uses the Bark model to synthesize speech from the input text. The method combines the text with the speaker-specific embeddings extracted from the audio files in the voice_dirs directory.</p><p id="""">‍</p><p id="""">Parameters:</p><p id="""">text: The text to be converted to speech.</p><p id="""">config: The model configuration.</p><p id="""">speaker_id: An identifier for the speaker (not deeply detailed here but typically used to select the appropriate speaker embedding).</p><p id="""">voice_dirs: Directory containing the speaker's audio files.</p><p id="""">temperature: A parameter that controls the randomness of the output. Lower values make the output more deterministic, while higher values introduce more variation.</p><p id="""">‍</p><h3 id="""">Saving the Generated Speech:</h3><div data-rt-embed-type='true'><pre>
<code class=""language-py"">
write_wav(""SamAltman.wav"", 24000, output_dict[""wav""])
</code>
</pre></div><p id="""">‍</p><p id="""">Saves the synthesized speech to a WAV file. The sample rate is set to 24,000 Hz.&nbsp;</p><p id="""">‍</p><p id="""">This guide should help you understand how to build a real-time voice cloning pipeline using the Bark TTS model.</p><p id="""">‍</p><h2 id="""">What is Open Voice</h2><p id=""""><a href=""https://github.com/myshell-ai/OpenVoice"" id="""">Open voice</a> is the innovative open-source project recently released by <a href=""https://research.myshell.ai/open-voice"" id="""">Myshell.ai</a> that provides instant voice cloning capabilities.It enables accurate voice capture functionalities like accurate tone color cloning, flexible voice style control, and zero-shot cross-lingual voice cloning.OpenVoice V1 supports multiple languages and accents, while <a href=""https://huggingface.co/myshell-ai/OpenVoiceV2"" id="""">OpenVoice V2</a>, released in April 2024, offers improved audio quality and native multi-lingual support.</p><p id="""">‍</p><h2 id="""">Open Voice vs Bark</h2><p id="""">The <a href=""https://github.com/suno-ai/bark"" id="""">Bark library</a> by <a href=""https://suno.com"" id="""">Suno AI</a> is a transformer-based text-to-audio model that offers a unique approach to generating audio content. Bark is not a conventional text-to-speech model but a fully generative text-to-audio model, capable of producing various types of audio, including highly realistic multilingual speech, music, background noise, and simple sound effects.</p><p id="""">‍</p><p id="""">OpenVoice is chosen over Bark for real voice cloning tasks due to its superior capabilities in replicating the tone color of the reference speaker and achieving granular control over voice styles including accent, rhythm, intonation, pauses, and even emotions.</p><p id="""">‍</p><p id="""">OpenVoice can mimic a speaker's voice using only a short audio clip, typically requiring less than 30 seconds to clone a voice. It generates a second of speech in just 85 milliseconds by decoupling tone color extraction from other voice attributes..&nbsp;</p><p id="""">‍</p><p id="""">For zero-shot multi-language voice cloning, OpenVoice supports cross-lingual synthesis without needing the specific languages in the training dataset, ensuring high-quality voice cloning in various languages.</p><p id="""">‍</p><p id="""">In contrast, Bark, although a powerful text-to-audio model, lacks the flexibility and control over voice styles that OpenVoice offers. Bark's probabilistic nature can lead to inconsistent generation results, which may not be suitable for real voice cloning tasks where high-quality and consistent voice replication is essential. Furthermore, Bark's requirements for massive-speaker multilingual datasets for cross-lingual voice cloning can be a significant limitation in certain applications.</p><p id="""">‍</p><h2 id="""">How Open Voice Works?</h2><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/664fa970363a458930ea991a_FryncW8_yyeX-nMfh9PIlP8FO_EPAZfQ8ZxhBwBC6nyYQVOH0B9q4v9dUSEuBmFW6KY7AOwI32twZc_8nvaVBytX2gsEdsbKzY2M3K647wDVQ88tz3jCQ4euxJ8JZQp58tH2n6v3i9fzWUhXNE8C_rQ.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><p id="""">The OpenVoice framework for instant voice cloning works by combining text content with style parameters (such as accent, emotion, and intonation) and processing them through a base speaker TTS (Text-to-Speech) model to control the overall speech styles and languages.This model processes the text and style parameters to generate an initial speech output, controlling the desired styles and languages. Simultaneously, the tone color of the reference speaker's voice is extracted to capture its unique characteristics. These elements are then encoded, passed through a flow-based model to remove the tone color while preserving other styles, and decoded to produce speech that integrates the reference speaker's tone color with controlled styles and languages. This process allows for high-quality, versatile speech synthesis.</p><p id="""">‍</p><p id="""">This architecture ensures the produced speech closely mimics the reference speaker’s unique vocal characteristics while allowing for versatile style control, making it highly suitable for various applications like media content creation and personalized virtual assistants. Refer this <a href=""https://arxiv.org/abs/2312.01479"" id="""">paper</a> to read more about Open voice.</p><p id="""">‍</p><h2 id="""">How is OpenVoice different from others?</h2><p id="""">‍</p><p id="""">OpenVoice stands out from other text-to-speech (TTS) architectures due to its unique <a href=""https://arxiv.org/html/2312.01479v5#:~:text=The%20contribution%20of%20OpenVoice%20is,or%20generalize%20to%20new%20languages."" id="""">decoupled framework</a>, which separates tone color cloning from other voice style and language controls. Unlike traditional TTS systems that often require extensive datasets to manage voice styles, accents, and emotions, OpenVoice leverages a two-step process. First, it uses a Base Speaker TTS model to generate initial speech with specific style parameters such as emotion, rhythm, and speed. Then, a Tone Color Converter applies the tone color of a reference speaker to this base speech. This decoupled approach allows for fine-grained control over voice attributes and enables high-quality voice cloning with minimal training data, making it more efficient and versatile compared to other architectures that do not separate these processes.</p><p id="""">‍</p><p id="""">Additionally, OpenVoice supports zero-shot cross-lingual voice cloning, meaning it can clone voices in multiple languages even if the specific language was not included in the training dataset. This capability is powered by its innovative use of flow-based models for tone color conversion and advanced speaker embedding extraction techniques. Other TTS systems, like <a href=""https://arxiv.org/abs/1703.10135"" id="""">Google's Tacotron</a> or <a href=""https://deepmind.google/discover/blog/wavenet-a-generative-model-for-raw-audio/"" id="""">DeepMind's WaveNet</a>, typically require large multilingual datasets and extensive retraining to achieve similar results. OpenVoice’s architecture not only reduces the computational burden but also provides more flexibility in adjusting voice attributes on-the-fly, making it a powerful tool for applications requiring dynamic and contextually appropriate speech synthesis.</p><p id="""">‍</p><h2 id="""">Why Open Voice</h2><h3 id="""">High-Fidelity Tone Color Cloning</h3><p id="""">‍</p><p id="""">OpenVoice excels at accurately replicating the tone color of a reference speaker, ensuring that the cloned voice sounds natural and true to the original. Tone color, also known as timbre, refers to the unique quality or character of a voice that distinguishes it from other voices, even when the pitch and loudness are the same. It encompasses the various nuances, overtones, and subtleties in a person's voice, such as warmth, brightness, or breathiness. This high-fidelity tone color cloning capability is achieved through the system's ability to maintain high audio quality even when generating speech in multiple languages.</p><h3 id="""">Flexible Voice Style Control</h3><p id="""">‍</p><p id="""">OpenVoice allows users to finely control voice attributes such as emotion, accent, rhythm, pauses, and intonation. This flexibility enables the creation of diverse and contextually appropriate speech outputs, making it suitable for a wide range of applications.To add pauses, emotion, accent, and rhythm in OpenVoice, users can set specific parameters in the BaseSpeakerTTS and ToneColorConverter methods. For example, to synthesize speech with these attributes, you can define the speaker parameter for emotion (e.g., 'cheerful' or 'whispering'), the language parameter for accent (e.g., 'English' or 'Chinese'), and the speed parameter for rhythm (e.g., 0.9 for slower speech).</p><h3 id="""">Zero-Shot Cross-Lingual Voice Cloning</h3><p id="""">‍</p><p id="""">OpenVoice can clone voices and generate speech in languages not included in the training data, eliminating the need for extensive multilingual datasets. This capability makes OpenVoice particularly valuable for applications requiring multilingual support without additional data collection and training.</p><h3 id="""">User-Friendly and Accessible</h3><p id="""">‍</p><p id="""">OpenVoice is available as an open-source technology, facilitating easy adoption and integration into various projects. The community support and collaborative environment fostered by the open-source nature of OpenVoice encourage further research and development.</p><h3 id="""">Wide Range of Applications</h3><p id="""">‍</p><p id="""">OpenVoice is suitable for a variety of applications, including content creation, customer support, and accessibility. It can generate voiceovers for videos, animations, and other multimedia content, enhance virtual assistants and chatbots with personalized and natural-sounding voices, and support assistive technologies for individuals with disabilities.</p><p id="""">‍</p><h2 id="""">Ready to Transform Your Voice Solutions?</h2><p id="""">‍</p><p id="""">If you are seeking to enhance your projects with high-fidelity, real-time voice cloning, OpenVoice is your answer. Whether you need personalized voice assistants, assistive technology for the visually impaired, or scalable customer support solutions, OpenVoice offers unparalleled accuracy and flexibility. With Mercity.ai, you can build innovative solutions and optimize your business processes using cutting-edge voice technology.</p><p id="""">‍</p><p id=""""><a href=""https://www.mercity.ai/contacts"" id="""">Contact us</a> today to elevate your voice cloning applications and see immediate results. Let's create the future of voice technology together!</p><p id="""">‍</p>",https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/664fac8511b91ff72a4d5b25_voice%20cloning.png,Mathavan,Audio Cloning and TTS,"In this blog we show how you can build a pipeline to clone anybody's voice using AI tools like Bark, Open Voice and Coqui and use it for your custom TTS pipelines.",False,"<div class=""rich-text w-richtext""><blockquote>⭐ All the code shown in this blog can be found in our <a href=""https://github.com/Mercity-AI/Voice-Cloning-Demo"">here</a> with all the necessary documentation and instructions to run.</blockquote><p>‍</p><p>In the evolving landscape of artificial intelligence and machine learning, real-time voice cloning has emerged as a groundbreaking technology. By leveraging advanced generative models and neural networks, it is now possible to create digital replicas of human voices with impressive accuracy. This technology not only captures the spoken words but also mimics the unique vocal characteristics, intonations, and emotions of the speaker. This groundbreaking technology is also being explored by top AI companies like <a href=""https://openai.com/index/hello-gpt-4o/"">Open AI</a>,<a href=""https://www.lindy.ai"">Lindy AI</a>,<a href=""https://www.microsoft.com/en-us/cortana"">Microsoft Cortana</a> and much more.</p><p>‍</p><h2>What is Real Time Voice Cloning?</h2><p>‍</p><p>Real time voice cloning is the process of cloning the human voice using Generative model and Neural Network to create a digital copy of the True human voice. It gives a statistical representation of human voice using the process of spectrogram analysis. The neural network is the Machine learning algorithm which is responsible for the training of the generative model to produce the AI mimic Voice.</p><p>‍</p><p>A spectrogram is a visual representation of the spectrum of frequencies in a sound signal as they vary with time. It is created by applying a Fast Fourier Transform (FFT) to short, overlapping segments of an audio signal, which provides information about the amplitude (or power) of different frequency components over time.</p><p>‍</p><h2>Why  Real Time Audio Generation?</h2><p>‍</p><p>Real time audio cloning technology instantly captures the human voice with such high fidelity not just the words spoken by them but also the unique vocal characteristics,intonation and emotions of the speaker.</p><p>Where this technology has great potential in sectors which have human- system communication streams, new forms of content creation and also act as accessible solutions to the society of blind and deaf people. Let’s see about the real time applicable sectors and the existing top technologies in them.</p><h3>Voice Assistants</h3><p>‍</p><p>A natural and engaging experience can be introduced to the existing voice assistants like siri,alexa and google assistant and also the drawback of multilingual support can also be overcomed with the real time voice cloning architecture. By replicating tonal nuances and emotions in voice, Voice Assistants can respond more empathetic and appropriately to users, enhancing user satisfaction.</p><p>‍</p><p>Moreover, the <a href=""https://openai.com/index/hello-gpt-4o/"">GPT-4o</a> new release by OpenAI introduces advanced features in natural language understanding and generation. By integrating real-time voice cloning with GPT-4o, voice assistants can achieve even higher levels of sophistication in dialogue management, providing seamless and intuitive user experiences. This combination enables the creation of highly interactive and human-like voice assistants that can cater to a diverse range of needs and preferences.</p><p>‍</p><h3>Assistive Technology For Blind and Visually Impaired People</h3><p>‍</p><p>Real-time audio cloning can create more natural-sounding screen readers and navigation aids. These tools can read out text, describe surroundings, or provide directions in a voice that users find comforting and familiar. The user experience is ultimately improved due to the personalisation feature where it helps to choose the voice they wish or like.</p><p>‍</p><p>Moreover, existing assistive technologies such as <a href=""https://aira.io"">Aira</a>, <a href=""https://www.bemyeyes.com"">Be My Eyes</a>, <a href=""https://play.google.com/store/apps/details?id=com.google.android.apps.accessibility.reveal&amp;hl=en_US&amp;pli=1"">Lookout</a>, and <a href=""https://www.seeingai.com"">Seeing AI</a> are revolutionizing support for the visually impaired. <a href=""https://aira.io"">Aira</a> connects users with trained agents who provide real-time assistance through live video calls, helping with navigation, reading, and other tasks. <a href=""https://www.bemyeyes.com"">Be My Eyes</a> also uses live video calls to link visually impaired individuals with sighted volunteers for assistance with various daily activities. <a href=""https://play.google.com/store/apps/details?id=com.google.android.apps.accessibility.reveal&amp;hl=en_US&amp;pli=1"">Lookout</a>, an Android app, provides spoken feedback about the user’s surroundings by utilizing the device's camera to recognize text, people, and objects. <a href=""https://www.seeingai.com"">Seeing AI</a>, available on iOS, narrates the world around the user by reading text, identifying products, recognizing faces, and describing scenes. Though the existing technology is useful, When we integrate real-time audio generation with advanced computer vision technology we don’t have to rely on human agents and the user experience would be smooth and consistent with their personal closed one voice navigating and exploring the world.</p><p>‍</p><p>The new GPT-4o release by OpenAI offers advanced features in natural language understanding and generation, which can further enhance these assistive technologies. Integrating real-time audio cloning with GPT-4o allows for more sophisticated and intuitive interactions, providing a higher level of support and independence for visually impaired users. Feel free to watch the <a href=""https://www.youtube.com/watch?v=KwNUJ69RbwY"">demo</a> by OpenAI X Be My Eyes. This combination ensures that assistive tools not only convey information effectively but also do so in a voice that resonates personally with the user, making the technology more accessible and user-friendly.</p><h3>Audiobooks and Stories On-Demand</h3><p>Real-time audio cloning allows for the swift creation of audiobooks, enabling authors and publishers to meet the growing demand for audio content without the lengthy process of traditional recording.</p><p>Users can also listen to their audio books with the desired characteristics of voice either it can be their loved or inspirational persons.This personalization can make the listening experience more enjoyable and intimate.The ability to replicate the tonality and emotion in a voice helps convey the intended meaning more effectively. This is crucial in applications like virtual therapy, remote education, and customer service, where understanding subtleties in communication is essential.</p><p>‍</p><p>A new tool in the market called <a href=""https://www.anytopic.io"">AnyTopic</a> which enables users to create their own audio book using the technique of <a href=""https://github.com/assafelovic/gpt-researcher"">GPT researcher concep</a>t where you will provide the keen topic you're interested in and then the agent will prepare a sole audiobook for the user's intended timing. The integration of real audio generation concepts over here will enhance the user personalisation concept drastically.</p><p>‍</p><h3>Scalable and  Reliable Customer Service</h3><p>‍</p><p>Customer Service Agent markets are huge and wide till date which can be replaced by the Real voice cloning architecture to answer the customer queries instead of agents which significantly make huge cost cutting and also not a compromisable solution as the tonality and characteristic of the voice is mimicked real. Cloned voices can be used to provide support in multiple languages, each with native-like pronunciation and tone. This can make non-native speakers feel more comfortable and understood.</p><p>‍</p><p>Additionally, existing customer service platforms such as <a href=""https://www.bland.ai"">Bland.AI</a> and <a href=""https://aws.amazon.com/pm/lex/?trk=436e9c39-382a-42a6-a49f-4cdbdfe8cadc&amp;sc_channel=ps&amp;ef_id=Cj0KCQjwxqayBhDFARIsAANWRnQUWV4Bfm6w3TPw-jSYaST1fciJyqaLnUSGoKHYvYZFZV8SvwajaCMaAsn6EALw_wcB:G:s&amp;s_kwcid=AL!4422!3!652868433334!e!!g!!amazon%20lex!19910624536!147207932349&amp;gclid=Cj0KCQjwxqayBhDFARIsAANWRnQUWV4Bfm6w3TPw-jSYaST1fciJyqaLnUSGoKHYvYZFZV8SvwajaCMaAsn6EALw_wcB"">Amazon Lex</a> stand to benefit significantly from real-time voice cloning. Bland.AI can leverage this technology to enhance its user interactions, making conversations more fluid and natural across different languages and dialects. Amazon Lex, which powers Alexa's voice capabilities, can utilize real-time voice cloning to offer more personalized and context-aware interactions, improving user engagement and satisfaction.</p><p>‍</p><p>Apart from the customer-agent perspective it is also well suitable for any line of  communication between humans. For example A healthcare provider can use voice cloning to offer compassionate support to patients, recognizing when a patient is stressed or anxious and responding with appropriate empathy and care.</p><p>‍</p><h2>How does Real Time Audio Cloning work?</h2><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1347pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/664fa970ae2d25063be7bb7a_OyZ8ZK_DcIja7G0_U4AX6T3BM1FXQR24S6IkOlz_BeK8O1SPMyltFm1yWvPoI--CRhl2u2j4SC2fhyD_ebSIcKUXtvvJA9EbQzucmnrTE_Do3xEYS3KIuf1uXNCgakhfunBsYHW_reAGJREkMt5sfqQ.jpeg""/></div></figure><p>‍</p><p>Real-time audio cloning, also known as voice cloning, is a process that replicates a person's voice using artificial intelligence (AI) and machine learning (ML) techniques. This technology can generate synthetic speech that mimics the tone, pitch, and inflection of a target voice in real time. Here's a detailed explanation of how this process works:</p><h3>Speaker Encoder</h3><p>The feature extractor processes the input audio (speaker reference waveform) to extract essential features that capture the unique characteristics of the speaker's voice. These features are crucial for maintaining the speaker's identity in the synthesized speech.</p><p>‍</p><p>The speaker encoder takes the features extracted by the feature extractor and encodes them into a latent representation. This encoded representation is a compact form that retains all the necessary speaker-specific information required for voice cloning.</p><h3>Acoustic Model</h3><p>‍</p><p>The acoustic model receives input from two sources: the encoded speaker representation and the text (grapheme or phoneme sequence) to be spoken. It combines these inputs to generate intermediate acoustic representations that reflect both the content of the speech and the speaker's unique voice characteristics. The model leverages advanced neural network architectures, such as recurrent neural networks (RNNs) or transformer networks, to process the temporal and contextual aspects of the speech. By doing so, it ensures that the generated acoustic features accurately capture the nuances of the speaker's voice, including intonation, rhythm, and emotional tone. This allows for the production of synthetic speech that sounds natural, closely mimicking the reference speaker's vocal attributes while conveying the intended message clearly and effectively.</p><p>‍</p><h3>Vocoder</h3><p>‍</p><p>The vocoder is responsible for converting the intermediate acoustic representations generated by the acoustic model into a waveform. This waveform is the final synthesized speech output, designed to closely mimic the reference speaker's voice, including their unique tonal qualities, pitch, and inflections. By effectively translating the detailed acoustic features into a smooth, continuous audio signal, the vocoder ensures that the synthesized voice maintains a high degree of fidelity and naturalness, making it indistinguishable from the original speaker in both clarity and expressiveness.</p><h3>Synthesizer</h3><p>‍</p><p>The synthesizer is a critical component that includes the encoder, concatenation, attention mechanism, and decoder. The Encoder converts the input text (grapheme or phoneme sequence) into a high-dimensional representation. The concat concatenates the encoded text representation with the speaker's encoded voice features. Attention Mechanism ensures that the synthesizer focuses on the relevant parts of the text and speaker features at each step of the speech synthesis process.The Decoder converts the concatenated representation into a sequence of acoustic features that the vocoder can process.</p><p>‍</p><h3>Workflow</h3><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/664fa970d44e7c729d84d400_vcrZyGmP01HSgxtlTzzreKVSNcarA7HHUdpI5gQN8JRZru7FKr_j3F5ZbO9Svge7iWsO2U79d5jhQtuJcXIY12i3kluu_JgS6jdZHpEx1gLWSELmWpJO2dW1JicV6NO6GU5XKRNsm6NmS-KWGFAI4Zs.png""/></div></figure><p>‍</p><p>The process begins with the Speaker Reference Waveform, where an audio sample of the speaker's voice is provided as input. The Speaker Encoder processes mel spectrograms from this audio sample to extract essential features, which are then encoded into speaker embeddings. These embeddings are optimized using a gradient-based approach and a GE2E (Generalized End-to-End) loss function.</p><p>‍</p><p>Next, the system utilizes Dataset 2, which includes both text and mel spectrograms. The text input is processed by the Speaker Encoder to generate embeddings, and these are combined with the speaker embeddings in the Synthesizer. The synthesizer generates predicted mel spectrograms, which are refined using a spectral loss function to ensure they closely match the target spectrograms.</p><p>‍</p><p>Finally, the Vocoder converts the predicted mel spectrograms into audio waveforms. These predicted audio waveforms are compared with the target audio using a waveform loss function, and the gradients from this comparison are used to further optimize the synthesizer and vocoder. This comprehensive process ensures that the final synthesized speech output closely mimics the reference speaker's voice, capturing their unique tonal qualities, pitch, and inflections. This innovative architecture allows for real-time audio cloning by effectively processing and integrating both textual and speaker-specific data, making it applicable for use in voice assistants, customer support services, interactive user interfaces, and more. To know more about the architecture of TTS refer to this <a href=""https://www.jetir.org/papers/JETIR2006189.pdf"">paper</a>.</p><p>‍</p><h2>How to build a Voice Cloning Pipeline</h2><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1095pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/664fa97066672efd86ccad18_DTrKIyYIK-nW1S60DDXD6iy6POaKRJQhQqy2xGpb-U3XLC-UHLtaY0Bv7YDVSXC3Gzh4MMAuQf8vP5VsofB7jYr76Dur-3iKoUMXivqL5WH-9pchii1Q1_VZ5krVVSpVAVp_E0-CwaZdztjN4b2xObU.jpeg""/></div></figure><p>‍</p><p>Building a voice cloning pipeline involves setting up a system that can take an audio input of a speaker's voice and generate new speech that mimics the same voice by matching with the text given by the user. </p><p>‍</p><p>Below is an iPython script that uses the TTS library to perform voice cloning. It initializes a text-to-speech (TTS) model, loads a pre-trained model checkpoint, and synthesizes speech using an input text and a specific speaker's voice characteristics extracted from audio files. Here's a detailed explanation of each part of the code and the purpose of the libraries used:</p><p>‍</p><h3>Importing Libraries</h3><div class=""w-embed""><pre>
<code class=""language-py"">
from TTS.tts.configs.bark_config import BarkConfig
from TTS.tts.models.bark import Bark

from scipy.io.wavfile import write as write_wav

import os
</code>
</pre></div><p>‍</p><p>The TTS library is a powerful tool for text-to-speech conversion. It supports multiple TTS models, including Bark. These imports specifically bring in the configuration and model components required to set up and use the Bark TTS model.</p><p>‍</p><p>SciPy is a scientific computing library in Python. Here, it is used to save the generated speech waveform to an audio file.</p><p>write_wav: This function writes a NumPy array to a WAV file, which is a common format for storing audio data.</p><p>‍</p><p>The OS library provides a way to interact with the operating system. It is used for handling directory paths and file management.</p><p>‍</p><h3>Setting Up Configuration:</h3><div class=""w-embed""><pre>
<code class=""language-py"">
config = BarkConfig()
model = Bark.init_from_config(config)
model.load_checkpoint(config, checkpoint_dir=""bark/"", eval=True)
</code>
</pre></div><p>‍</p><p>Initializes the configuration for the Bark model. This configuration includes various parameters that control the model's behavior during speech synthesis.</p><p>‍</p><p>Initializes the Bark TTS model using the specified configuration. This sets up the model architecture and prepares it for loading pre-trained weights.Loads the pre-trained weights for the Bark model from the specified checkpoint directory. This is crucial for ensuring the model has learned to generate high-quality speech based on extensive training data.</p><p>‍</p><h3>Speech Synthesis</h3><div class=""w-embed""><pre>
<code class=""language-py"">
text = ""Mercity ai is a leading AI innovator in India, with OpenAI planning collaboration.""
voice_dirs = ""/Users/username/Desktop/projects/AI voice Cloning/Speaker voice/""
</code>
</pre></div><p>‍</p><p>Defines the text that will be converted into speech. This is the input that the TTS model will process to generate the corresponding audio output.</p><p>‍</p><p>Specifies the directory containing the speaker's audio files. These files are used to extract speaker-specific characteristics (embeddings) for voice cloning.</p><p>‍</p><h3>Synthesizing Speech</h3><div class=""w-embed""><pre>
<code class=""language-py"">
output_dict = model.synthesize(text, config, speaker_id='speaker', voice_dirs=""bark_voices"", temperature=0.95)
</code>
</pre></div><p>‍</p><p>Uses the Bark model to synthesize speech from the input text. The method combines the text with the speaker-specific embeddings extracted from the audio files in the voice_dirs directory.</p><p>‍</p><p>Parameters:</p><p>text: The text to be converted to speech.</p><p>config: The model configuration.</p><p>speaker_id: An identifier for the speaker (not deeply detailed here but typically used to select the appropriate speaker embedding).</p><p>voice_dirs: Directory containing the speaker's audio files.</p><p>temperature: A parameter that controls the randomness of the output. Lower values make the output more deterministic, while higher values introduce more variation.</p><p>‍</p><h3>Saving the Generated Speech:</h3><div class=""w-embed""><pre>
<code class=""language-py"">
write_wav(""SamAltman.wav"", 24000, output_dict[""wav""])
</code>
</pre></div><p>‍</p><p>Saves the synthesized speech to a WAV file. The sample rate is set to 24,000 Hz. </p><p>‍</p><p>This guide should help you understand how to build a real-time voice cloning pipeline using the Bark TTS model.</p><p>‍</p><h2>What is Open Voice</h2><p><a href=""https://github.com/myshell-ai/OpenVoice"">Open voice</a> is the innovative open-source project recently released by <a href=""https://research.myshell.ai/open-voice"">Myshell.ai</a> that provides instant voice cloning capabilities.It enables accurate voice capture functionalities like accurate tone color cloning, flexible voice style control, and zero-shot cross-lingual voice cloning.OpenVoice V1 supports multiple languages and accents, while <a href=""https://huggingface.co/myshell-ai/OpenVoiceV2"">OpenVoice V2</a>, released in April 2024, offers improved audio quality and native multi-lingual support.</p><p>‍</p><h2>Open Voice vs Bark</h2><p>The <a href=""https://github.com/suno-ai/bark"">Bark library</a> by <a href=""https://suno.com"">Suno AI</a> is a transformer-based text-to-audio model that offers a unique approach to generating audio content. Bark is not a conventional text-to-speech model but a fully generative text-to-audio model, capable of producing various types of audio, including highly realistic multilingual speech, music, background noise, and simple sound effects.</p><p>‍</p><p>OpenVoice is chosen over Bark for real voice cloning tasks due to its superior capabilities in replicating the tone color of the reference speaker and achieving granular control over voice styles including accent, rhythm, intonation, pauses, and even emotions.</p><p>‍</p><p>OpenVoice can mimic a speaker's voice using only a short audio clip, typically requiring less than 30 seconds to clone a voice. It generates a second of speech in just 85 milliseconds by decoupling tone color extraction from other voice attributes.. </p><p>‍</p><p>For zero-shot multi-language voice cloning, OpenVoice supports cross-lingual synthesis without needing the specific languages in the training dataset, ensuring high-quality voice cloning in various languages.</p><p>‍</p><p>In contrast, Bark, although a powerful text-to-audio model, lacks the flexibility and control over voice styles that OpenVoice offers. Bark's probabilistic nature can lead to inconsistent generation results, which may not be suitable for real voice cloning tasks where high-quality and consistent voice replication is essential. Furthermore, Bark's requirements for massive-speaker multilingual datasets for cross-lingual voice cloning can be a significant limitation in certain applications.</p><p>‍</p><h2>How Open Voice Works?</h2><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/664fa970363a458930ea991a_FryncW8_yyeX-nMfh9PIlP8FO_EPAZfQ8ZxhBwBC6nyYQVOH0B9q4v9dUSEuBmFW6KY7AOwI32twZc_8nvaVBytX2gsEdsbKzY2M3K647wDVQ88tz3jCQ4euxJ8JZQp58tH2n6v3i9fzWUhXNE8C_rQ.png""/></div></figure><p>‍</p><p>The OpenVoice framework for instant voice cloning works by combining text content with style parameters (such as accent, emotion, and intonation) and processing them through a base speaker TTS (Text-to-Speech) model to control the overall speech styles and languages.This model processes the text and style parameters to generate an initial speech output, controlling the desired styles and languages. Simultaneously, the tone color of the reference speaker's voice is extracted to capture its unique characteristics. These elements are then encoded, passed through a flow-based model to remove the tone color while preserving other styles, and decoded to produce speech that integrates the reference speaker's tone color with controlled styles and languages. This process allows for high-quality, versatile speech synthesis.</p><p>‍</p><p>This architecture ensures the produced speech closely mimics the reference speaker’s unique vocal characteristics while allowing for versatile style control, making it highly suitable for various applications like media content creation and personalized virtual assistants. Refer this <a href=""https://arxiv.org/abs/2312.01479"">paper</a> to read more about Open voice.</p><p>‍</p><h2>How is OpenVoice different from others?</h2><p>‍</p><p>OpenVoice stands out from other text-to-speech (TTS) architectures due to its unique <a href=""https://arxiv.org/html/2312.01479v5#:~:text=The%20contribution%20of%20OpenVoice%20is,or%20generalize%20to%20new%20languages."">decoupled framework</a>, which separates tone color cloning from other voice style and language controls. Unlike traditional TTS systems that often require extensive datasets to manage voice styles, accents, and emotions, OpenVoice leverages a two-step process. First, it uses a Base Speaker TTS model to generate initial speech with specific style parameters such as emotion, rhythm, and speed. Then, a Tone Color Converter applies the tone color of a reference speaker to this base speech. This decoupled approach allows for fine-grained control over voice attributes and enables high-quality voice cloning with minimal training data, making it more efficient and versatile compared to other architectures that do not separate these processes.</p><p>‍</p><p>Additionally, OpenVoice supports zero-shot cross-lingual voice cloning, meaning it can clone voices in multiple languages even if the specific language was not included in the training dataset. This capability is powered by its innovative use of flow-based models for tone color conversion and advanced speaker embedding extraction techniques. Other TTS systems, like <a href=""https://arxiv.org/abs/1703.10135"">Google's Tacotron</a> or <a href=""https://deepmind.google/discover/blog/wavenet-a-generative-model-for-raw-audio/"">DeepMind's WaveNet</a>, typically require large multilingual datasets and extensive retraining to achieve similar results. OpenVoice’s architecture not only reduces the computational burden but also provides more flexibility in adjusting voice attributes on-the-fly, making it a powerful tool for applications requiring dynamic and contextually appropriate speech synthesis.</p><p>‍</p><h2>Why Open Voice</h2><h3>High-Fidelity Tone Color Cloning</h3><p>‍</p><p>OpenVoice excels at accurately replicating the tone color of a reference speaker, ensuring that the cloned voice sounds natural and true to the original. Tone color, also known as timbre, refers to the unique quality or character of a voice that distinguishes it from other voices, even when the pitch and loudness are the same. It encompasses the various nuances, overtones, and subtleties in a person's voice, such as warmth, brightness, or breathiness. This high-fidelity tone color cloning capability is achieved through the system's ability to maintain high audio quality even when generating speech in multiple languages.</p><h3>Flexible Voice Style Control</h3><p>‍</p><p>OpenVoice allows users to finely control voice attributes such as emotion, accent, rhythm, pauses, and intonation. This flexibility enables the creation of diverse and contextually appropriate speech outputs, making it suitable for a wide range of applications.To add pauses, emotion, accent, and rhythm in OpenVoice, users can set specific parameters in the BaseSpeakerTTS and ToneColorConverter methods. For example, to synthesize speech with these attributes, you can define the speaker parameter for emotion (e.g., 'cheerful' or 'whispering'), the language parameter for accent (e.g., 'English' or 'Chinese'), and the speed parameter for rhythm (e.g., 0.9 for slower speech).</p><h3>Zero-Shot Cross-Lingual Voice Cloning</h3><p>‍</p><p>OpenVoice can clone voices and generate speech in languages not included in the training data, eliminating the need for extensive multilingual datasets. This capability makes OpenVoice particularly valuable for applications requiring multilingual support without additional data collection and training.</p><h3>User-Friendly and Accessible</h3><p>‍</p><p>OpenVoice is available as an open-source technology, facilitating easy adoption and integration into various projects. The community support and collaborative environment fostered by the open-source nature of OpenVoice encourage further research and development.</p><h3>Wide Range of Applications</h3><p>‍</p><p>OpenVoice is suitable for a variety of applications, including content creation, customer support, and accessibility. It can generate voiceovers for videos, animations, and other multimedia content, enhance virtual assistants and chatbots with personalized and natural-sounding voices, and support assistive technologies for individuals with disabilities.</p><p>‍</p><h2>Ready to Transform Your Voice Solutions?</h2><p>‍</p><p>If you are seeking to enhance your projects with high-fidelity, real-time voice cloning, OpenVoice is your answer. Whether you need personalized voice assistants, assistive technology for the visually impaired, or scalable customer support solutions, OpenVoice offers unparalleled accuracy and flexibility. With Mercity.ai, you can build innovative solutions and optimize your business processes using cutting-edge voice technology.</p><p>‍</p><p><a href=""https://www.mercity.ai/contacts"">Contact us</a> today to elevate your voice cloning applications and see immediate results. Let's create the future of voice technology together!</p><p>‍</p></div>"
NLP and LLM Applications in Accounting,nlp-and-llm-in-accounting,640f56f76d313b2faa631c11,6732835bfa0c138ea942d2b4,False,False,Mon Nov 11 2024 22:21:15 GMT+0000 (Coordinated Universal Time),Mon Nov 11 2024 22:22:08 GMT+0000 (Coordinated Universal Time),Mon Nov 11 2024 22:24:20 GMT+0000 (Coordinated Universal Time),"<p id="""">AI is spreading like wildfire in all the industries. It has automated a ton of grunt work and probably saved millions of hours of users collectively. One field that can use some automation is accounting, in this blog we will understand how AI and more specifically LLMs can be used in Accounting and Finance to analyze and process a lot of data. Automation, if done properly can save workers in accounting a ton of time and money, this blog is a deep dive into what is possible with LLMs in Accounting.</p><p id="""">‍</p><h2 id="""">What is NLP?</h2><p id="""">‍</p><p id="""">Natural Language Processing (NLP) is a field of artificial intelligence that enables computers to understand, interpret, and generate human language. It’s revolutionizing accounting and finance processes by automating tasks that traditionally required significant manual effort, thanks to the latest advancements in <a href=""https://ar5iv.labs.arxiv.org/html/2307.06435"" id="""">large language models (LLMs)</a>. These models, with their ability to understand and generate human-like text, can analyze vast amounts of financial data, identify trends, and provide insights that were previously difficult to obtain.</p><p id="""">‍</p><p id="""">&nbsp;For instance, LLMs can automate the extraction and categorization of financial information from documents, such as invoices, contracts, and financial statements, reducing errors and increasing efficiency. Additionally, <a href=""https://www.mckinsey.com/industries/financial-services/our-insights/capturing-the-full-value-of-generative-ai-in-banking"" id="""">McKinsey reports</a> that banking leaders are leveraging GPT to upskill employees and integrate AI into business processes, reflecting a broader trend of adopting generative AI to improve operational effectiveness. Finance teams are also using GPT to generate financial reports, analyze market trends, and automate routine tasks, demonstrating its practicality and transformative impact in the finance sector.</p><h2 id="""">Implementing NLP and AI in Financial Services</h2><p id="""">‍</p><p id="""">Natural Language Processing (NLP) and Artificial Intelligence (AI) are transforming the financial services industry by enabling automated analysis of vast amounts of unstructured data. Here are some key ways NLP and AI are being implemented:</p><p id="""">‍</p><h3 id="""">Automated Financial Report Analysis</h3><p id="""">‍</p><p id="""">Natural Language Processing (NLP) is increasingly being used to enhance the efficiency and accuracy of financial reporting. NLP can process large volumes of unstructured data from financial reports, contracts, and market analysis, transforming them into structured, actionable insights. This automation reduces the time required for manual data entry and analysis, enabling quicker and more accurate financial decision-making. Tools like <a href=""https://phrazor.ai"" id="""">Phrazor</a> utilize NLP to generate personalized financial reports, pinpointing errors and standardizing data, which significantly saves time and costs for businesses. For example let’s create a simple visualization of the dashboard using GPT for a random sample financial report.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/673280a7359af8121d73e520_AD_4nXeDbHaPW-hneFfnyLMJUSrdt8ZVSEu_6RZqaHpyXWmTt2U-VeqjgPyrWq0GbXUi4fXPajPovY5kx8dk48ZbvnDbRB9vlyYBk3YYJm-lM4JRCY0c2yqRBksyKLLiEHzh4I5PriIyxOcE5dX3MmBndPtSNkM.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><p id="""">The use of NLP in creating financial dashboards is a game-changer for businesses, providing quick, efficient, and insightful visualizations without the need to rely on human analysts. By leveraging GPT and other advanced NLP technologies, organizations can automatically generate dashboards like the one shown above, where various financial metrics such as assets distribution, liabilities, and stockholders' equity are visually represented in real-time.</p><p id="""">‍</p><p id="""">This automation allows for the immediate presentation of financial data, enabling decision-makers to swiftly interpret key financial indicators and make informed decisions without delay. A <a href=""https://bfi.uchicago.edu/wp-content/uploads/2024/05/BFI_WP_2024-65.pdf"" id="""">recent study</a> also showed that GPT-4 achieved 60.31% accuracy in correctly analyzing financial statements, compared to 56.7% for human analysts. The study used a ""Chain-of-Thought"" prompting technique to mimic the step-by-step reasoning of financial analysts.</p><p id="""">‍</p><h3 id="""">NLP-powered Fraud Detection Systems</h3><p id="""">‍</p><p id="""">NLP-powered fraud detection systems leverage advanced natural language processing techniques to enhance the identification and prevention of fraudulent activities across various sectors, particularly in finance and insurance. These systems analyze vast amounts of unstructured text data, such as transaction descriptions, customer communications, and claims narratives, to uncover patterns and anomalies indicative of fraud. For instance, NLP techniques like named entity recognition (NER) can extract key information from claims, while sentiment analysis can gauge the emotional tone of the language used, helping to identify potential deception.</p><p id="""">‍</p><p id=""""><a href=""https://link.springer.com/article/10.1007/s10994-023-06354-5"" id="""">Recent studies</a> have highlighted the effectiveness of NLP in this domain. A notable research paper introduced FraudNLP, the first publicly available dataset for online fraud detection, demonstrating how NLP methods can significantly improve fraud detection performance by modeling online actions similarly to natural language. The study emphasized the importance of using privacy-safe features and showed that NLP-based approaches could outperform traditional machine learning methods in detecting fraudulent transactions. Another <a href=""https://ieeexplore.ieee.org/document/10430658"" id="""">study</a> focused on insurance fraud detection, illustrating how NLP can process incoming claims to assess their risk levels, flagging suspicious claims for further investigation.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1200px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1200px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/673280a6d92ebad7d7691133_AD_4nXcS81uWjj9-kIn560J-JUD4wE-KAo18cTb5dBVejOjvbXfitQG445J_e8xGdKIdkQFL76PAIOr14E0wI5a9wb8Z8u9GodsnnR3f33dFHaxYfe2S-O1vuwHxkh2ICaebFA4YERWWagHEJKGm8T7djbNq9ef3.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><p id="""">Traditional fraud detection methods, such as Logistic Regression, Decision Trees, and Support Vector Machines (SVMs), have been effective in identifying fraudulent activities by analyzing historical data and predefined patterns. However, these models often fall short in handling the complexity and evolving nature of fraud. Large Language Models (LLMs) like GPT have significantly advanced fraud detection by analyzing both structured and unstructured data, such as transaction records and communications, to detect sophisticated fraud schemes. Companies like PayPal have successfully implemented LSTM networks to improve fraud detection accuracy by analyzing event-based user behavior, leading to a 7-10% improvement in performance reported by the <a href=""https://www.avenga.com/magazine/fraud-detection-machine-learning/"" id="""">article</a>. These modern approaches enable real-time detection and adaptation to new fraud patterns, making them more effective than traditional methods</p><p id="""">‍</p><h2 id="""">Document Processing in Accounting</h2><p id="""">‍</p><p id="""">Document processing in accounting involves the automated handling and analysis of financial documents, such as invoices, purchase orders, receipts, and financial statements. This process utilizes advanced technologies like Optical Character Recognition (OCR), Natural Language Processing (NLP), and Machine Learning (ML) to extract, classify, and interpret data from these documents. Extracting key information from financial statements involves pulling out critical financial metrics such as revenue, income, expenses, assets, liabilities, and cash flow, which are essential for analysis and informed decision-making. The automation of document processing significantly reduces manual data entry, enhances accuracy, and accelerates the processing time, thereby increasing the efficiency of accounting operations.</p><h3 id="""">How to Build an Intelligent Document Processing Pipeline</h3><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:778px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""778px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/673280a60c4c98b3837640aa_AD_4nXcUd4VNzQWBO1MTYoI100_oxqKS041A2OxUo3EeTnqGtyQ50c0MvJunl0VatdL7KRFeRZJntjm6pnSJv6zGCVjPgZZ5U3VoFrezSP06M55psYGwWPQ_pzn8ZgByXR9MmVAUqwH3cRDxpFEgj6E5i5upZma3.jpeg"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><p id="""">The Document Processing Pipeline that automates the extraction and analysis of financial data from documents like invoices. The process begins with Data Ingestion, where raw documents, such as scanned images, PDFs, or digital files, are collected from various sources and fed into the pipeline. The next critical step is Optical Character Recognition (OCR), which converts the text within these documents into machine-readable format. This step is crucial for transforming non-digital documents into structured data that can be further processed. Common OCR models used at this stage include <a href=""https://github.com/tesseract-ocr/tesseract"" id="""">Tesseract</a>, a widely used open-source OCR engine, and <a href=""https://www.mindee.com/product/doctr"" id="""">DocTR</a> (Document Text Recognition), a deep learning-based OCR model known for its high accuracy in recognizing complex document structures.</p><p id="""">‍</p><p id="""">Following OCR, the pipeline moves to Data Extraction, where the text is parsed to identify and extract key financial information, such as invoice numbers, dates, and amounts. This data is then stored in a Data Warehouse, a centralized repository that enables the aggregation, storage, and easy retrieval of large datasets for further analysis or reporting. Finally, the process includes Human Evaluation, where the extracted data is reviewed by human experts to ensure accuracy and identify any errors or discrepancies that may not have been caught by the automated system. This step is essential for maintaining high data quality and allows for continuous improvement of the pipeline. By leveraging advanced OCR models like Tesseract and DocTR, this pipeline offers a scalable and robust solution for efficiently managing and processing large volumes of financial documents, reducing the time and effort required for manual data entry, and enhancing the reliability of financial decision-making.</p><p id="""">‍</p><h3 id="""">Why use AI for Document Processing in Finance?</h3><p id="""">‍</p><p id="""">AI is transforming document processing in finance by significantly enhancing efficiency, accuracy, and scalability. As depicted in the provided diagram, AI-powered systems can swiftly process large volumes of financial documents, such as invoices, by utilizing Optical Character Recognition (OCR) technologies like <a href=""https://github.com/tesseract-ocr/tesseract"" id="""">Tesseract</a> and <a href=""https://mindee.com/product/doctr"" id="""">DocTR</a>. This automation reduces the time required to extract critical financial metrics, minimizes human error, and ensures that financial reports are more reliable and compliant with regulatory standards. Additionally, AI systems are highly scalable, making them ideal for large enterprises that need to manage extensive document flows.&nbsp;</p><p id="""">‍</p><p id="""">However, the role of a Human Evaluator remains crucial in this pipeline. While AI excels at processing and analyzing data, human oversight is essential to review the extracted information for any discrepancies or errors that may not be captured by the automated system. This ensures the accuracy and integrity of the processed data, which is vital for maintaining high-quality financial records. Furthermore, human evaluators can provide the nuanced judgment necessary for handling complex cases, making AI and human collaboration a robust approach to document processing in finance. Several companies, including <a href=""https://www.leewayhertz.com/ai-for-financial-document-processing/"" id="""">LeewayHertz</a>, <a href=""https://www.emagia.com/top-financial-use-cases-for-intelligent-document-processing/"" id="""">Emagia</a>, <a href=""https://www.docvu.ai/industry/finance-accounting/"" id="""">DocVu.AI</a>, <a href=""https://www.abbyy.com/hub/poi/most-rewarding-intelligent-document-processing/?page=1"" id="""">ABBYY</a>, and <a href=""https://rossum.ai"" id="""">Rossum</a>, are the&nbsp; AI-powered document processing services for finance, enhancing efficiency, accuracy, and scalability in managing financial documents.</p><p id="""">‍</p><h2 id="""">Financial Text Summarization</h2><p id="""">Financial Text Summarization involves using AI-powered techniques to condense complex financial documents into concise, easily understandable summaries. This process is crucial for analysts, investors, and decision-makers who need to quickly grasp the key points of lengthy reports and disclosures without wading through every detail.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:801px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""801px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/673280a6a15ac9e5c523b938_AD_4nXexICKioLsYHquBxTvrHeJamz8QagDKUPX8v-2rVahII5UkO1E2CPmLW50saim_QTw3MwMVRFXlu_XS-B5XahlbR2MafQcgGSuhXk2XN6sdnF1zEMDI35V-ytG-EIycFvQ6b_8iOcZNx9n9B7p4bl5SfT06.jpeg"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><h3 id="""">Summarizing lengthy financial reports and disclosures</h3><p id="""">‍</p><p id="""">Financial reports, such as annual reports and regulatory filings, are often dense with data and legal jargon. AI-driven summarization tools can extract the most important information, such as revenue figures, net income, and significant changes in financial position, and present it in a clear and concise manner. This allows stakeholders to quickly assess a company's financial health and make informed decisions.</p><p id="""">‍</p><h3 id="""">Condensing earnings call transcripts</h3><p id="""">‍</p><p id="""">Earnings call transcripts provide insights into a company's performance and future outlook, but they can be lengthy and time-consuming to analyze. Summarization tools can condense these transcripts by highlighting key points discussed during the call, such as financial results, management's commentary on performance, and guidance for future quarters. This enables investors and analysts to quickly understand the most critical information without reading through the entire transcript.</p><p id="""">‍</p><h2 id="""">Financial News and Market Sentiment Analysis</h2><p id="""">‍</p><p id="""">Financial News and Market Sentiment Analysis is an advanced application of Natural Language Processing (NLP) and machine learning techniques that allows investors and analysts to derive actionable insights from financial news and predict market trends based on the sentiment conveyed in the news.</p><h3 id="""">Extracting Market Insights from News Articles</h3><p id="""">‍</p><p id="""">Market insights can be extracted from financial news articles by using NLP to analyze the content and identify key information related to company performance, industry trends, and economic indicators. This process involves parsing the text to find mentions of specific companies, financial metrics, or economic events, and categorizing the sentiment or tone of the news (positive, negative, or neutral). For example, if multiple articles report strong quarterly results for a company, this could indicate positive sentiment and a potential increase in stock prices.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1250px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1250px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/673280a6cc35873ba2e64ee8_AD_4nXex5lTdG1MWmtHk15Fxlga4C0D4l9PcjcPWiZwEH5KhgF3CyBJhXnYroTU7Dc-3vMfI6go2ba0ka1dmH0BkPjH1qiqCDbGQIBHdbbup1hobuKD17hE7q7ORWHUL1hC7UOoPuNLiLQ.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><p id="""">The transition from rule-based approaches to NLP, and now to GPT models, marks a significant advancement in financial text analysis. Initially, rule-based systems used predefined patterns and keywords to extract information, but they were limited by their static nature and inability to adapt to language nuances. With the advent of NLP, techniques like Named Entity Recognition (NER) and sentiment analysis improved the accuracy of text processing by enabling machines to understand context, relationships, and sentiment within financial documents. Algorithms such as Support Vector Machines (SVM) and Decision Trees were commonly employed to classify text and extract insights. The introduction of GPT models, which leverage deep learning architectures like transformers, further revolutionized this process by allowing for context-aware, highly adaptive analysis. GPT models not only understand and generate human-like text but also predict market impacts by recognizing complex patterns across large datasets, offering a more sophisticated and dynamic approach to financial analysis.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/673280a7ffc388df73b59bf2_AD_4nXdhI955ziuAZXFwv620X4bfv4W2RWpxKFtRZIRHqW3seFR1mMoOVTTg1XqsijMErf69Jg6TikUK8a9bq8etQ16lWAdQ3fojsK5zJOq4pJ-VEAF2gXJdGhQmBxS-AbyM7GGU34PhoLoEoxYE2jvhiOnXyige.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><h3 id="""">Predicting Market Trends Using Sentiment Analysis</h3><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/673280a6adab5393875d96cd_AD_4nXfTXgu9TPrN-RZ4J3AGP4NI_hw5atg4gfYg7rWFcpFLBsKLnT34WlE5K4c3UXeIf_O1Zyj6BI7M8ZuBmR7SrKhfhvAQFZe_FWUWM3aWD-nRn1lvdhtk_CeK4gM3QF1EtvfFmy9BBUi3JZHh1umfII1cQF3j.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">Sentiment analysis is a technique that involves assessing the emotional tone of news articles, social media posts, and other textual data to gauge public sentiment towards the market or a particular stock. By analyzing the frequency and sentiment of keywords or phrases, such as ""growth,"" ""profit,"" or ""decline,"" sentiment analysis can help predict market trends. For instance, an increase in positive sentiment around a specific industry could signal a potential upward trend in that sector, while a surge in negative sentiment may indicate potential risks or downturns. Recent advancements in this field, such as the methodologies discussed in the paper on sentiment analysis for market forecasting from <a href=""https://arxiv.org/pdf/2402.18563"" id="""">Arxiv</a> and the LLM-based forecasting techniques available on <a href=""https://github.com/dannyallover/llm_forecasting"" id="""">GitHub</a>, highlight the growing importance of integrating large language models for more accurate and dynamic predictions in financial markets. These resources provide valuable insights into how sentiment analysis can be applied in financial forecasting, leveraging the power of AI and machine learning to better understand and anticipate market movements similar project links for your better understanding refer this <a href=""https://github.com/AI4Finance-Foundation/FinGPT/tree/master/fingpt/FinGPT_Forecaster"" id="""">link</a>.</p><h3 id="""">Real-time Financial Risk Assessment</h3><p id="""">‍</p><p id="""">Real-time financial risk assessment uses sentiment analysis in conjunction with other financial indicators to assess the risk level of investments or market conditions as they unfold. By continuously monitoring news streams and social media feeds, AI-powered systems can alert investors to emerging risks, such as geopolitical events or economic downturns, enabling them to take proactive measures. This capability is particularly valuable for risk managers who need to react quickly to volatile market conditions.&nbsp;</p><p id="""">‍</p><p id="""">Two notable real-world examples of such AI-powered systems are the <a href=""https://github.com/colfeng/CALM?tab=readme-ov-file#credit-and-risk-assessment-benchmark"" id="""">colfeng/CALM</a> and FinGPT projects on GitHub, both of which leverage large language models (LLMs) for financial applications. The <a href=""https://github.com/colfeng/CALM?tab=readme-ov-file#credit-and-risk-assessment-benchmark"" id="""">CALM</a> project is focused on real-time credit and risk assessment. It uses a fine-tuned version of the <a href=""https://ollama.com/library/llama2:chat"" id="""">Llama2-chat</a> model, trained on a comprehensive benchmark and instruction datasets specifically created for tasks like credit scoring, fraud detection, and financial distress identification. The <a href=""https://github.com/colfeng/CALM?tab=readme-ov-file#credit-and-risk-assessment-benchmark"" id="""">CALM</a> model was evaluated against other popular LLMs such as GPT-4 and Llama2, demonstrating improved accuracy in financial risk evaluation, while also addressing potential biases that are critical in ensuring fairness in credit and risk assessments.</p><p id="""">‍</p><p id="""">On the other hand, <a href=""https://github.com/AI4Finance-Foundation/FinGPT"" id="""">FinGPT</a> is a general-purpose financial LLM designed for broader financial tasks including forecasting, sentiment analysis, and other financial NLP applications. <a href=""https://github.com/AI4Finance-Foundation/FinGPT"" id="""">FinGPT</a> is trained on diverse financial data sources, allowing it to understand and predict market trends more accurately. By incorporating these models into financial risk assessments, the projects have shown significant improvements in predictive accuracy and the ability to handle large-scale financial data in real-time, thereby enhancing the decision-making capabilities of financial institutions.</p><p id="""">‍</p><p id="""">Both models contribute to advancing the field of financial AI by providing tools that not only improve the accuracy and efficiency of risk assessments but also help mitigate the biases that can arise from the data and models used in these critical applications.</p><h2 id="""">AI-Assisted Financial Forecasting</h2><h3 id="""">Traditional Forecasting Methods</h3><p id="""">‍</p><p id="""">Traditional financial forecasting methods rely heavily on historical data and a variety of statistical algorithms to predict future trends. Some of the key algorithms used in these traditional approaches include ARIMA (AutoRegressive Integrated Moving Average), SARIMA (Seasonal ARIMA), and other related models.</p><p id="""">‍</p><h4 id="""">Arima</h4><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:399px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""399px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/673280a6017327522bef18c3_AD_4nXe67ElD8VQoraBeVDqq15Syo5aib4J_G34BiGf5kH3QUbejksocVTbXj_27I1A_b4tix7_e6nG-Ut7vPZsIx3TXZ2St_BIRml-MmkuMpqqf_O4965Nkk5bBVcr9J1-USuyyna6pudjFfsQ-4SEMiyrm0e8T.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">ARIMA models predict future data points by combining three key components: the autoregressive (AR) part, the integrated (I) part, and the moving average (MA) part.&nbsp;</p><p id="""">‍</p><h5 id="""">Autoregressive (AR)</h5><p id="""">This component captures the relationship between the current observation and its previous values. Essentially, it predicts future values by regressing the series on its own past values. For instance, if you're forecasting next month's sales, the AR part would use sales figures from previous months, based on the idea that past values directly influence future values.</p><p id="""">‍</p><h5 id="""">Integrated (I)</h5><p id="""">‍</p><p id="""">Many time series data exhibit trends or seasonality, which can make the series non-stationary (its statistical properties change over time). The integrated part addresses this by differencing the data, which involves subtracting the previous observation from the current one. This process removes trends or seasonality, making the series stationary, which is a key requirement for many time series models.</p><p id="""">‍</p><h5 id="""">Moving Average (MA)&nbsp;</h5><p id="""">‍</p><p id="""">This component models the relationship between an observation and the residual errors from previous forecasts. It corrects the forecast by considering the errors made in prior predictions. If past errors followed a certain pattern, the MA part learns from these errors and adjusts future forecasts accordingly.</p><p id="""">‍</p><h4 id="""">SARIMA</h4><p id="""">‍</p><p id="""">SARIMA (Seasonal ARIMA) is an extension of the ARIMA model that explicitly incorporates a seasonal component, making it well-suited for data that exhibits regular, repeating patterns over time, such as monthly or quarterly sales figures. While ARIMA models are designed to handle non-seasonal time series data by capturing trends and patterns through autoregressive (AR), differencing (I), and moving average (MA) components, SARIMA goes a step further by applying these same principles to the seasonal elements of the data.</p><p id="""">‍</p><p id="""">In SARIMA, the model accounts for seasonality by introducing additional terms that specifically capture the periodic patterns in the data. For example, if you have monthly sales data that peaks every December, SARIMA can model this repeating pattern. It essentially applies the ARIMA components (AR, I, MA) twice—once to the non-seasonal aspects of the data and once to the seasonal aspects—allowing it to effectively separate and model both the overall trend and the seasonal fluctuations. This dual approach enables SARIMA to provide more accurate forecasts for time series data that exhibits both trend and seasonality.</p><p id="""">‍</p><p id="""">In addition to ARIMA and SARIMA, traditional methods also include Exponential Smoothing (ETS), which applies exponentially decreasing weights to past observations for short-term forecasting, and Linear Regression, which models the linear relationship between a dependent variable and one or more independent variables. While these methods have been widely used and are effective under certain conditions, they often struggle with capturing complex, non-linear patterns and sudden market changes, which limits their adaptability and accuracy in more dynamic environments. This is where modern AI techniques, such as machine learning and deep learning models, offer significant advantages by providing more flexible and data-driven approaches to forecasting.</p><p id="""">‍</p><h3 id="""">How can AI improve traditional forecasting methods?</h3><p id="""">‍</p><p id="""">AI-assisted financial forecasting significantly enhances traditional methods by leveraging advanced data processing capabilities, real-time analysis, and machine learning algorithms. Unlike traditional forecasting, which often relies on historical data and limited variables, AI can handle vast amounts of data from diverse sources, including real-time market updates, news articles, and social media. This enables AI to provide up-to-date forecasts that adapt quickly to new information, ensuring more timely and accurate predictions. Additionally, AI models excel at recognizing complex patterns in data that traditional methods might overlook, leading to better forecasts, especially in volatile markets. AI also helps reduce human bias in forecasting by offering more objective analysis based on data rather than subjective judgment. Furthermore, AI's ability to generate multiple scenarios and continuously improve its accuracy over time makes it an invaluable tool for financial forecasting, providing deeper insights and more robust risk assessments than traditional approaches.</p><p id="""">‍</p><h2 id="""">Regulatory Compliance and Risk Management</h2><p id="""">‍</p><p id="""">Regulatory Compliance and Risk Management are critical aspects of the financial industry, where ensuring adherence to laws and regulations is essential for maintaining trust and avoiding penalties. AI, particularly Natural Language Processing (NLP), plays a significant role in automating these processes and identifying potential risks in financial documents.</p><p id="""">‍</p><h3 id="""">Automating Compliance Checks with NLP</h3><p id="""">‍</p><p id="""">NLP can automate the labor-intensive process of compliance checks by analyzing large volumes of legal and financial documents for specific regulatory requirements. Traditionally, compliance officers would manually review documents to ensure they meet regulatory standards, a process prone to human error and inefficiency. With NLP, AI systems can scan documents for specific keywords, phrases, or patterns that indicate compliance or non-compliance with relevant regulations. For example, NLP models can be trained to recognize references to anti-money laundering (AML) laws, data privacy requirements, or financial reporting standards within contracts and transaction records. By automating these checks, NLP not only reduces the time and effort required but also improves accuracy and consistency, helping financial institutions stay compliant with ever-evolving regulations.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/673280a7034567a6d762d244_AD_4nXc90cz0vQnx74hBCUpB9csaLh7EHS97j8bSJ8Sxw7g7VLvK6GQt-p7AIjR9nHx4Jfp8bH4HVz0UmllSkQ4VEN6p8Boj4qeTCu4_X3kdL-3ty08b2eg98r9ea6Xl4oUGlgChIGlNm8zeZHx6x9cCAyc0tkb1.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><p id="""">A pertinent example of this approach is demonstrated in the paper <a href=""https://arxiv.org/pdf/2209.09722"" id="""">""NLP-based Automated Compliance Checking of Data Processing Agreements against GDPR"" by Amaral et al.</a>, which highlights how NLP can be used to automate the compliance verification of Data Processing Agreements (DPAs) with GDPR. The system developed in the paper uses an embedding-based technique to compare the textual content of DPAs with predefined GDPR requirements. This approach not only checks for compliance but also provides recommendations on missing information, achieving a high degree of accuracy in identifying violations and satisfying requirements. The method shows a significant improvement in accuracy compared to baseline models, demonstrating the effectiveness of NLP in automating compliance checks.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:962px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""962px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/673280a6703d174c2544d698_AD_4nXfafZavIIChQFBAN6eArhinPzQ4zQMIRv2v6lAQV9QXrvJ6m7KuS5rz3zDv9fYEF3jV5LfDyRd_w0SAVZ_hTFs-UXWLyTjD8WYGiivsX8XaPZ3r96KhEUH4MXzNWndSPQSM3zufTx8nPf97Oo3xiblr8pMn.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><p id="""">Building on this, the paper <a href=""https://arxiv.org/html/2404.14356v1"" id="""">""Rethinking Legal Compliance Automation: Opportunities with Large Language Models"" by Hassani et al</a>. explores a different approach by leveraging Large Language Models (LLMs) such as GPT-4. Unlike the embedding-based method, this approach uses LLMs to analyze broader contexts within legal texts, allowing for a more comprehensive and nuanced understanding of compliance requirements. The authors introduce a systematic method that involves content chunking and prompt construction tailored to specific compliance rules, enabling LLMs to provide explanations for compliance decisions. This method demonstrated a significant improvement in accuracy—up to 40% better than traditional methods—showcasing the potential of LLMs to revolutionize legal compliance automation by considering larger textual contexts and providing more reliable justifications for compliance assessments.</p><p id="""">‍</p><h3 id="""">Identifying Potential Risks in Financial Documents</h3><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1172px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1172px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/673280a7017327522bef18d2_AD_4nXe2kz-hu_QdCoHl7sjyB8AH8t6uLGB9KibkB3bWigm0HFDnROWyPZjQjwdP-mL7t6Z6Ur3z3qfIb4CmFfpGOpx3g_yQbmLKuUyLSbrbSAmg1BXGWo18QWP16Xl_n9p_t9YQvZdGiAKYPA9dwEPcUsTEax0U.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><p id="""">NLP is also instrumental in identifying potential risks hidden within financial documents. Financial institutions deal with a massive volume of documents, including contracts, loan agreements, and financial statements, which may contain clauses or terms that expose the institution to financial, legal, or operational risks. NLP algorithms can parse these documents to identify high-risk language, such as clauses that might lead to default or unfavorable conditions, or terms that violate regulatory requirements. For instance, in an automated compliance check, a specific clause (e.g., S8) may be flagged with a red question mark, suggesting potential non-compliance or ambiguity in satisfying certain regulatory requirements (e.g., R7 and R9 of GDPR). Additionally, NLP can flag unusual patterns in financial data that might indicate fraudulent activity or financial instability, enabling institutions to take preemptive actions to mitigate these risks.</p><p id="""">‍</p><h2 id="""">Personalized Financial Advice with Large Language Models</h2><p id="""">‍</p><p id="""">Large Language Models (LLMs) are increasingly being utilized in the financial sector to provide personalized investment advice by leveraging their ability to analyze vast amounts of data and generate tailored recommendations. These models, such as GPT-3 and GPT-4, are transforming how financial advice is delivered, offering a blend of AI-driven insights and traditional advisory services.</p><h3 id="""">Tailoring Investment Recommendations Using AI</h3><p id="""">‍</p><p id="""">LLMs like GPT are revolutionizing personalized financial advice by analyzing a wide array of data sources, including market trends, user financial histories, and real-time economic indicators, to generate customized investment recommendations. For instance, platforms like <a href=""https://magnifi.com/#"" id="""">Magnifi</a> utilize LLMs to provide real-time, personalized advice based on user queries and preferences, effectively replicating the insights typically offered by human financial advisors. Similarly, <a href=""https://www.wealthfront.com"" id="""">Wealthfront</a> employs AI to continuously refine its investment strategies by monitoring user behavior and market conditions, ensuring that the recommendations remain relevant and optimized for individual financial goals.</p><p id="""">‍</p><h3 id="""">Real-World Surveys and Market Adoption</h3><p id="""">‍</p><p id="""">Despite the growing adoption of AI in financial services, real-world surveys reveal a mix of curiosity and skepticism among users. <a href=""https://www.cnbc.com/2023/09/12/3-in-10-adults-would-use-ai-for-financial-advice-cnbc-survey-finds.html"" id="""">According to a CNBC survey</a>, while 37% of U.S. adults are interested in using AI tools like ChatGPT for managing their finances, only 4% currently do so. This disparity indicates a latent interest that could drive future adoption as trust in AI tools grows. However, the same survey highlighted that 51% of adults have little or no trust in AI-generated financial advice, underscoring the necessity of human oversight to validate AI-driven recommendations.</p><p id="""">‍</p><p id="""">Moreover, reports by <a href=""https://kms-solutions.asia/blogs/large-language-models-in-financial-services"" id="""">KMS Solutions</a> suggest that the financial industry is increasingly adopting LLMs for various applications, including personalized investment advice. The enhanced operational efficiency and improved decision-making facilitated by LLMs are key drivers behind this trend, indicating that as the technology matures, its adoption will likely accelerate.</p><p id="""">‍</p><h2 id="""">Enhancing Audit Processes with NLP</h2><p id="""">‍</p><p id="""">The integration of Natural Language Processing (NLP) into financial auditing processes has significantly transformed the way audits are conducted, offering enhanced accuracy, efficiency, and reliability. By leveraging NLP, financial institutions can automate the traditionally manual and labor-intensive tasks associated with audits, enabling quicker identification of anomalies and ensuring compliance with regulatory standards.</p><p id="""">‍</p><h3 id="""">Automated Anomaly Detection in Financial Data</h3><p id="""">‍</p><p id="""">Automated anomaly detection using NLP has become a cornerstone in modern financial audits. Traditional audit methods relied heavily on manual reviews and sampling techniques, which are both time-consuming and prone to human error. With the advent of NLP, tools/approach like&nbsp; <a href=""https://arxiv.org/pdf/2308.06111"" id="""">ZeroShotALI</a>&nbsp; and <a href=""https://arxiv.org/pdf/2402.09334v2"" id="""">AuditLLM</a> have emerged, providing robust solutions for anomaly detection in financial data.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:980px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""980px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/673280a7ed2032aa5aa6bd37_AD_4nXfr-qW3jrfyac3lUXsNwidUeHPVKAvXS7eV5UqthbD65-uN-6qml67Zz4_lx7xu0m79WtKASmPsTkp_OGJqxpTXknZe07K9afYftfvgTfR5txfwfKMIX_YO7ugBtmkYexhqUwnuvjAzjn9xd5OQVAJPDc0.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><p id=""""><a href=""https://arxiv.org/pdf/2308.06111"" id="""">ZeroShotALI</a> focuses on improving the efficiency of financial audits by using a domain-specific <a href=""https://arxiv.org/abs/1908.10084"" id="""">SentenceBERT</a> model in combination with GPT-4 to match text segments from financial documents to legal requirements outlined in standards like IFRS. The system identifies relevant sections of financial reports, compares them to compliance requirements, and highlights potential issues with high precision. This two-step approach—first narrowing down relevant sections using <a href=""https://arxiv.org/abs/1908.10084"" id="""">SentenceBERT</a> and then refining these using GPT-4—results in significant improvements in sensitivity, mean average precision (MAP), and F1 score over traditional methods. This automated system not only reduces the workload for auditors but also enhances the reliability of the audit process by accurately identifying anomalies and non-compliance issues.</p><p id="""">‍</p><h3 id="""">Improving Audit Efficiency and Accuracy</h3><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/673280a61966d87e5280c52e_AD_4nXdZ-XICa8589SloXpJtaSQoa50PYHRjVpX1kn3Ysghm8kwvlL--LCk-y6eQldoKr9SYkUzMh4iHSB5cC_M61K7XjEFDmGQPZpfTtzyACsHv3Wf9OofdFmiEWBUmAYcFOMkMTGMAWMgPEJrHBlfv2UAghi8G.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><p id="""">The accuracy and efficiency of audits are further enhanced by tools like <a href=""https://arxiv.org/pdf/2402.09334v2"" id="""">AuditLLM</a>, which introduces a multiprobe approach to audit the consistency of Large Language Models (LLMs) in their responses to financial audit queries. AuditLLM generates multiple ""probes,"" or variably phrased versions of the same audit question, to test the consistency and reliability of LLM outputs. By comparing the semantic similarity of the LLM's responses to these probes, AuditLLM detects inconsistencies that could indicate underlying issues such as bias or hallucinations. The tool demonstrated significant improvements in accuracy, up to 40% better than traditional methods, when applied to real-world datasets like TruthfulQA.</p><p id="""">‍</p><p id="""">This multiprobe approach offers a novel way to ensure the reliability of LLMs used in financial audits, providing a deeper level of scrutiny than single-query methods. It highlights how LLMs can be both powerful tools and potential risks if not adequately monitored and validated.&nbsp;</p><p id="""">‍</p><h2 id="""">Natural Language Interfaces for Financial Systems</h2><p id="""">‍</p><p id="""">Natural Language Processing (NLP) is revolutionizing the financial services industry by enabling the creation of conversational AI systems that can interact with users in a human-like manner. These systems, including chatbots and virtual assistants, are increasingly being adopted by financial institutions to enhance customer service, streamline operations, and provide personalized financial advice. Here, we explore some of the significant advancements in building conversational AI for banking services and the emergence of voice-activated financial assistants.</p><h3 id="""">Building Conversational AI for Banking Services</h3><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/673280a6622d156faa398439_AD_4nXeMLoXFaY6IYmworvpMV8dmr9hUaWEGXdcB_1OWgsbfBUbD17BbziiS4xUKOywPe1eCbERpEymJ8p8vFMZLhxweILKRgiXeFqOwnU8FhPRZxSFMd82WgcQSxyfqg4T0ElntEDYJ1zAhm_zmCiOGoNw87DJK.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><p id="""">Conversational AI systems are transforming the banking sector by providing customers with efficient, round-the-clock service. Notable examples include Bank of America's Erica, JPMorgan Chase's COIN, and Wells Fargo's Intelligent Virtual Agent (IVA), each of which utilizes NLP to deliver personalized customer interactions.</p><p id="""">‍</p><h4 id="""">Bank of America's Erica</h4><p id="""">This <a href=""https://promotions.bankofamerica.com/digitalbanking/mobilebanking/erica"" id="""">virtual assistant</a> leverages NLP and machine learning to provide customers with personalized financial guidance. <a href=""https://promotions.bankofamerica.com/digitalbanking/mobilebanking/erica"" id="""">Erica</a> can assist with a variety of tasks, including checking account balances, making payments, and offering insights into spending habits. The system continuously learns from user interactions, enhancing its ability to understand and respond to queries over time .</p><h4 id="""">JPMorgan Chase's COIN</h4><p id="""">&nbsp;<a href=""https://www.jpmorgan.com/onyx/coin-system"" id="""">COIN (Contract Intelligence)</a> is an NLP-based system designed to analyze legal documents and extract key data points. This tool significantly reduces the time required for document review—from 360,000 hours to just a few seconds—by processing 12,000 new contracts per second. This application of NLP not only streamlines operations but also minimizes human error in critical financial processes .</p><h4 id="""">Wells Fargo's Intelligent Virtual Agent</h4><p id="""">This <a href=""https://sites.wf.com/fargo/"" id="""">AI-driven system</a> helps customers with routine banking tasks, such as finding nearby ATMs or making payments. By understanding natural language queries, the IVA reduces the burden on human customer service representatives, enabling them to focus on more complex issues. This system has improved customer satisfaction by providing quick and accurate responses to common inquiries .</p><p id="""">‍</p><h3 id="""">Voice-Activated Financial Assistants</h3><p id="""">The advent of voice-activated assistants in banking represents a significant shift towards more intuitive user interfaces. These assistants allow customers to interact with financial services using simple voice commands, making banking more accessible and convenient. According to a <a href=""https://www.pwc.com/us/en/services/consulting/library/consumer-intelligence-series/voice-assistants.html"" id="""">survey by PwC</a>, 72% of respondents familiar with voice-enabled products have used a voice assistant, with 50% considering making purchases through these devices. While trust and privacy concerns remain barriers to wider adoption, the high satisfaction rates among users suggest a growing potential for voice-activated financial assistants in the future .</p><h2 id="""">Transform Your Accounting Processes with AI and NLP</h2><p id="""">‍</p><p id="""">Unlock the future of accounting with cutting-edge AI and NLP technologies. From automating financial report analysis to enhancing fraud detection, our AI solutions are designed to streamline your operations and boost accuracy. Don’t miss out on the opportunity to revolutionize your financial management. <a href=""https://www.mercity.ai/contacts"" id="""">Contact us</a> today to discover how AI can elevate your accounting practices to new heights.</p><p id="""">‍</p>",https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6732830c4997d6c840348408_AI%20in%20Accounting.png,Mathavan,NLP in Accounting,Deep dive into how LLMs and NLP in accounting can save a lot of time and resources,False,"<div class=""rich-text w-richtext""><p>AI is spreading like wildfire in all the industries. It has automated a ton of grunt work and probably saved millions of hours of users collectively. One field that can use some automation is accounting, in this blog we will understand how AI and more specifically LLMs can be used in Accounting and Finance to analyze and process a lot of data. Automation, if done properly can save workers in accounting a ton of time and money, this blog is a deep dive into what is possible with LLMs in Accounting.</p><p>‍</p><h2>What is NLP?</h2><p>‍</p><p>Natural Language Processing (NLP) is a field of artificial intelligence that enables computers to understand, interpret, and generate human language. It’s revolutionizing accounting and finance processes by automating tasks that traditionally required significant manual effort, thanks to the latest advancements in <a href=""https://ar5iv.labs.arxiv.org/html/2307.06435"">large language models (LLMs)</a>. These models, with their ability to understand and generate human-like text, can analyze vast amounts of financial data, identify trends, and provide insights that were previously difficult to obtain.</p><p>‍</p><p> For instance, LLMs can automate the extraction and categorization of financial information from documents, such as invoices, contracts, and financial statements, reducing errors and increasing efficiency. Additionally, <a href=""https://www.mckinsey.com/industries/financial-services/our-insights/capturing-the-full-value-of-generative-ai-in-banking"">McKinsey reports</a> that banking leaders are leveraging GPT to upskill employees and integrate AI into business processes, reflecting a broader trend of adopting generative AI to improve operational effectiveness. Finance teams are also using GPT to generate financial reports, analyze market trends, and automate routine tasks, demonstrating its practicality and transformative impact in the finance sector.</p><h2>Implementing NLP and AI in Financial Services</h2><p>‍</p><p>Natural Language Processing (NLP) and Artificial Intelligence (AI) are transforming the financial services industry by enabling automated analysis of vast amounts of unstructured data. Here are some key ways NLP and AI are being implemented:</p><p>‍</p><h3>Automated Financial Report Analysis</h3><p>‍</p><p>Natural Language Processing (NLP) is increasingly being used to enhance the efficiency and accuracy of financial reporting. NLP can process large volumes of unstructured data from financial reports, contracts, and market analysis, transforming them into structured, actionable insights. This automation reduces the time required for manual data entry and analysis, enabling quicker and more accurate financial decision-making. Tools like <a href=""https://phrazor.ai"">Phrazor</a> utilize NLP to generate personalized financial reports, pinpointing errors and standardizing data, which significantly saves time and costs for businesses. For example let’s create a simple visualization of the dashboard using GPT for a random sample financial report.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600px""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/673280a7359af8121d73e520_AD_4nXeDbHaPW-hneFfnyLMJUSrdt8ZVSEu_6RZqaHpyXWmTt2U-VeqjgPyrWq0GbXUi4fXPajPovY5kx8dk48ZbvnDbRB9vlyYBk3YYJm-lM4JRCY0c2yqRBksyKLLiEHzh4I5PriIyxOcE5dX3MmBndPtSNkM.png""/></div></figure><p>‍</p><p>The use of NLP in creating financial dashboards is a game-changer for businesses, providing quick, efficient, and insightful visualizations without the need to rely on human analysts. By leveraging GPT and other advanced NLP technologies, organizations can automatically generate dashboards like the one shown above, where various financial metrics such as assets distribution, liabilities, and stockholders' equity are visually represented in real-time.</p><p>‍</p><p>This automation allows for the immediate presentation of financial data, enabling decision-makers to swiftly interpret key financial indicators and make informed decisions without delay. A <a href=""https://bfi.uchicago.edu/wp-content/uploads/2024/05/BFI_WP_2024-65.pdf"">recent study</a> also showed that GPT-4 achieved 60.31% accuracy in correctly analyzing financial statements, compared to 56.7% for human analysts. The study used a ""Chain-of-Thought"" prompting technique to mimic the step-by-step reasoning of financial analysts.</p><p>‍</p><h3>NLP-powered Fraud Detection Systems</h3><p>‍</p><p>NLP-powered fraud detection systems leverage advanced natural language processing techniques to enhance the identification and prevention of fraudulent activities across various sectors, particularly in finance and insurance. These systems analyze vast amounts of unstructured text data, such as transaction descriptions, customer communications, and claims narratives, to uncover patterns and anomalies indicative of fraud. For instance, NLP techniques like named entity recognition (NER) can extract key information from claims, while sentiment analysis can gauge the emotional tone of the language used, helping to identify potential deception.</p><p>‍</p><p><a href=""https://link.springer.com/article/10.1007/s10994-023-06354-5"">Recent studies</a> have highlighted the effectiveness of NLP in this domain. A notable research paper introduced FraudNLP, the first publicly available dataset for online fraud detection, demonstrating how NLP methods can significantly improve fraud detection performance by modeling online actions similarly to natural language. The study emphasized the importance of using privacy-safe features and showed that NLP-based approaches could outperform traditional machine learning methods in detecting fraudulent transactions. Another <a href=""https://ieeexplore.ieee.org/document/10430658"">study</a> focused on insurance fraud detection, illustrating how NLP can process incoming claims to assess their risk levels, flagging suspicious claims for further investigation.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1200px""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/673280a6d92ebad7d7691133_AD_4nXcS81uWjj9-kIn560J-JUD4wE-KAo18cTb5dBVejOjvbXfitQG445J_e8xGdKIdkQFL76PAIOr14E0wI5a9wb8Z8u9GodsnnR3f33dFHaxYfe2S-O1vuwHxkh2ICaebFA4YERWWagHEJKGm8T7djbNq9ef3.png""/></div></figure><p>‍</p><p>Traditional fraud detection methods, such as Logistic Regression, Decision Trees, and Support Vector Machines (SVMs), have been effective in identifying fraudulent activities by analyzing historical data and predefined patterns. However, these models often fall short in handling the complexity and evolving nature of fraud. Large Language Models (LLMs) like GPT have significantly advanced fraud detection by analyzing both structured and unstructured data, such as transaction records and communications, to detect sophisticated fraud schemes. Companies like PayPal have successfully implemented LSTM networks to improve fraud detection accuracy by analyzing event-based user behavior, leading to a 7-10% improvement in performance reported by the <a href=""https://www.avenga.com/magazine/fraud-detection-machine-learning/"">article</a>. These modern approaches enable real-time detection and adaptation to new fraud patterns, making them more effective than traditional methods</p><p>‍</p><h2>Document Processing in Accounting</h2><p>‍</p><p>Document processing in accounting involves the automated handling and analysis of financial documents, such as invoices, purchase orders, receipts, and financial statements. This process utilizes advanced technologies like Optical Character Recognition (OCR), Natural Language Processing (NLP), and Machine Learning (ML) to extract, classify, and interpret data from these documents. Extracting key information from financial statements involves pulling out critical financial metrics such as revenue, income, expenses, assets, liabilities, and cash flow, which are essential for analysis and informed decision-making. The automation of document processing significantly reduces manual data entry, enhances accuracy, and accelerates the processing time, thereby increasing the efficiency of accounting operations.</p><h3>How to Build an Intelligent Document Processing Pipeline</h3><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:778px""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/673280a60c4c98b3837640aa_AD_4nXcUd4VNzQWBO1MTYoI100_oxqKS041A2OxUo3EeTnqGtyQ50c0MvJunl0VatdL7KRFeRZJntjm6pnSJv6zGCVjPgZZ5U3VoFrezSP06M55psYGwWPQ_pzn8ZgByXR9MmVAUqwH3cRDxpFEgj6E5i5upZma3.jpeg""/></div></figure><p>‍</p><p>The Document Processing Pipeline that automates the extraction and analysis of financial data from documents like invoices. The process begins with Data Ingestion, where raw documents, such as scanned images, PDFs, or digital files, are collected from various sources and fed into the pipeline. The next critical step is Optical Character Recognition (OCR), which converts the text within these documents into machine-readable format. This step is crucial for transforming non-digital documents into structured data that can be further processed. Common OCR models used at this stage include <a href=""https://github.com/tesseract-ocr/tesseract"">Tesseract</a>, a widely used open-source OCR engine, and <a href=""https://www.mindee.com/product/doctr"">DocTR</a> (Document Text Recognition), a deep learning-based OCR model known for its high accuracy in recognizing complex document structures.</p><p>‍</p><p>Following OCR, the pipeline moves to Data Extraction, where the text is parsed to identify and extract key financial information, such as invoice numbers, dates, and amounts. This data is then stored in a Data Warehouse, a centralized repository that enables the aggregation, storage, and easy retrieval of large datasets for further analysis or reporting. Finally, the process includes Human Evaluation, where the extracted data is reviewed by human experts to ensure accuracy and identify any errors or discrepancies that may not have been caught by the automated system. This step is essential for maintaining high data quality and allows for continuous improvement of the pipeline. By leveraging advanced OCR models like Tesseract and DocTR, this pipeline offers a scalable and robust solution for efficiently managing and processing large volumes of financial documents, reducing the time and effort required for manual data entry, and enhancing the reliability of financial decision-making.</p><p>‍</p><h3>Why use AI for Document Processing in Finance?</h3><p>‍</p><p>AI is transforming document processing in finance by significantly enhancing efficiency, accuracy, and scalability. As depicted in the provided diagram, AI-powered systems can swiftly process large volumes of financial documents, such as invoices, by utilizing Optical Character Recognition (OCR) technologies like <a href=""https://github.com/tesseract-ocr/tesseract"">Tesseract</a> and <a href=""https://mindee.com/product/doctr"">DocTR</a>. This automation reduces the time required to extract critical financial metrics, minimizes human error, and ensures that financial reports are more reliable and compliant with regulatory standards. Additionally, AI systems are highly scalable, making them ideal for large enterprises that need to manage extensive document flows. </p><p>‍</p><p>However, the role of a Human Evaluator remains crucial in this pipeline. While AI excels at processing and analyzing data, human oversight is essential to review the extracted information for any discrepancies or errors that may not be captured by the automated system. This ensures the accuracy and integrity of the processed data, which is vital for maintaining high-quality financial records. Furthermore, human evaluators can provide the nuanced judgment necessary for handling complex cases, making AI and human collaboration a robust approach to document processing in finance. Several companies, including <a href=""https://www.leewayhertz.com/ai-for-financial-document-processing/"">LeewayHertz</a>, <a href=""https://www.emagia.com/top-financial-use-cases-for-intelligent-document-processing/"">Emagia</a>, <a href=""https://www.docvu.ai/industry/finance-accounting/"">DocVu.AI</a>, <a href=""https://www.abbyy.com/hub/poi/most-rewarding-intelligent-document-processing/?page=1"">ABBYY</a>, and <a href=""https://rossum.ai"">Rossum</a>, are the  AI-powered document processing services for finance, enhancing efficiency, accuracy, and scalability in managing financial documents.</p><p>‍</p><h2>Financial Text Summarization</h2><p>Financial Text Summarization involves using AI-powered techniques to condense complex financial documents into concise, easily understandable summaries. This process is crucial for analysts, investors, and decision-makers who need to quickly grasp the key points of lengthy reports and disclosures without wading through every detail.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:801px""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/673280a6a15ac9e5c523b938_AD_4nXexICKioLsYHquBxTvrHeJamz8QagDKUPX8v-2rVahII5UkO1E2CPmLW50saim_QTw3MwMVRFXlu_XS-B5XahlbR2MafQcgGSuhXk2XN6sdnF1zEMDI35V-ytG-EIycFvQ6b_8iOcZNx9n9B7p4bl5SfT06.jpeg""/></div></figure><h3>Summarizing lengthy financial reports and disclosures</h3><p>‍</p><p>Financial reports, such as annual reports and regulatory filings, are often dense with data and legal jargon. AI-driven summarization tools can extract the most important information, such as revenue figures, net income, and significant changes in financial position, and present it in a clear and concise manner. This allows stakeholders to quickly assess a company's financial health and make informed decisions.</p><p>‍</p><h3>Condensing earnings call transcripts</h3><p>‍</p><p>Earnings call transcripts provide insights into a company's performance and future outlook, but they can be lengthy and time-consuming to analyze. Summarization tools can condense these transcripts by highlighting key points discussed during the call, such as financial results, management's commentary on performance, and guidance for future quarters. This enables investors and analysts to quickly understand the most critical information without reading through the entire transcript.</p><p>‍</p><h2>Financial News and Market Sentiment Analysis</h2><p>‍</p><p>Financial News and Market Sentiment Analysis is an advanced application of Natural Language Processing (NLP) and machine learning techniques that allows investors and analysts to derive actionable insights from financial news and predict market trends based on the sentiment conveyed in the news.</p><h3>Extracting Market Insights from News Articles</h3><p>‍</p><p>Market insights can be extracted from financial news articles by using NLP to analyze the content and identify key information related to company performance, industry trends, and economic indicators. This process involves parsing the text to find mentions of specific companies, financial metrics, or economic events, and categorizing the sentiment or tone of the news (positive, negative, or neutral). For example, if multiple articles report strong quarterly results for a company, this could indicate positive sentiment and a potential increase in stock prices.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1250px""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/673280a6cc35873ba2e64ee8_AD_4nXex5lTdG1MWmtHk15Fxlga4C0D4l9PcjcPWiZwEH5KhgF3CyBJhXnYroTU7Dc-3vMfI6go2ba0ka1dmH0BkPjH1qiqCDbGQIBHdbbup1hobuKD17hE7q7ORWHUL1hC7UOoPuNLiLQ.png""/></div></figure><p>‍</p><p>The transition from rule-based approaches to NLP, and now to GPT models, marks a significant advancement in financial text analysis. Initially, rule-based systems used predefined patterns and keywords to extract information, but they were limited by their static nature and inability to adapt to language nuances. With the advent of NLP, techniques like Named Entity Recognition (NER) and sentiment analysis improved the accuracy of text processing by enabling machines to understand context, relationships, and sentiment within financial documents. Algorithms such as Support Vector Machines (SVM) and Decision Trees were commonly employed to classify text and extract insights. The introduction of GPT models, which leverage deep learning architectures like transformers, further revolutionized this process by allowing for context-aware, highly adaptive analysis. GPT models not only understand and generate human-like text but also predict market impacts by recognizing complex patterns across large datasets, offering a more sophisticated and dynamic approach to financial analysis.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600px""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/673280a7ffc388df73b59bf2_AD_4nXdhI955ziuAZXFwv620X4bfv4W2RWpxKFtRZIRHqW3seFR1mMoOVTTg1XqsijMErf69Jg6TikUK8a9bq8etQ16lWAdQ3fojsK5zJOq4pJ-VEAF2gXJdGhQmBxS-AbyM7GGU34PhoLoEoxYE2jvhiOnXyige.png""/></div></figure><h3>Predicting Market Trends Using Sentiment Analysis</h3><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600px""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/673280a6adab5393875d96cd_AD_4nXfTXgu9TPrN-RZ4J3AGP4NI_hw5atg4gfYg7rWFcpFLBsKLnT34WlE5K4c3UXeIf_O1Zyj6BI7M8ZuBmR7SrKhfhvAQFZe_FWUWM3aWD-nRn1lvdhtk_CeK4gM3QF1EtvfFmy9BBUi3JZHh1umfII1cQF3j.png""/></div></figure><p>Sentiment analysis is a technique that involves assessing the emotional tone of news articles, social media posts, and other textual data to gauge public sentiment towards the market or a particular stock. By analyzing the frequency and sentiment of keywords or phrases, such as ""growth,"" ""profit,"" or ""decline,"" sentiment analysis can help predict market trends. For instance, an increase in positive sentiment around a specific industry could signal a potential upward trend in that sector, while a surge in negative sentiment may indicate potential risks or downturns. Recent advancements in this field, such as the methodologies discussed in the paper on sentiment analysis for market forecasting from <a href=""https://arxiv.org/pdf/2402.18563"">Arxiv</a> and the LLM-based forecasting techniques available on <a href=""https://github.com/dannyallover/llm_forecasting"">GitHub</a>, highlight the growing importance of integrating large language models for more accurate and dynamic predictions in financial markets. These resources provide valuable insights into how sentiment analysis can be applied in financial forecasting, leveraging the power of AI and machine learning to better understand and anticipate market movements similar project links for your better understanding refer this <a href=""https://github.com/AI4Finance-Foundation/FinGPT/tree/master/fingpt/FinGPT_Forecaster"">link</a>.</p><h3>Real-time Financial Risk Assessment</h3><p>‍</p><p>Real-time financial risk assessment uses sentiment analysis in conjunction with other financial indicators to assess the risk level of investments or market conditions as they unfold. By continuously monitoring news streams and social media feeds, AI-powered systems can alert investors to emerging risks, such as geopolitical events or economic downturns, enabling them to take proactive measures. This capability is particularly valuable for risk managers who need to react quickly to volatile market conditions. </p><p>‍</p><p>Two notable real-world examples of such AI-powered systems are the <a href=""https://github.com/colfeng/CALM?tab=readme-ov-file#credit-and-risk-assessment-benchmark"">colfeng/CALM</a> and FinGPT projects on GitHub, both of which leverage large language models (LLMs) for financial applications. The <a href=""https://github.com/colfeng/CALM?tab=readme-ov-file#credit-and-risk-assessment-benchmark"">CALM</a> project is focused on real-time credit and risk assessment. It uses a fine-tuned version of the <a href=""https://ollama.com/library/llama2:chat"">Llama2-chat</a> model, trained on a comprehensive benchmark and instruction datasets specifically created for tasks like credit scoring, fraud detection, and financial distress identification. The <a href=""https://github.com/colfeng/CALM?tab=readme-ov-file#credit-and-risk-assessment-benchmark"">CALM</a> model was evaluated against other popular LLMs such as GPT-4 and Llama2, demonstrating improved accuracy in financial risk evaluation, while also addressing potential biases that are critical in ensuring fairness in credit and risk assessments.</p><p>‍</p><p>On the other hand, <a href=""https://github.com/AI4Finance-Foundation/FinGPT"">FinGPT</a> is a general-purpose financial LLM designed for broader financial tasks including forecasting, sentiment analysis, and other financial NLP applications. <a href=""https://github.com/AI4Finance-Foundation/FinGPT"">FinGPT</a> is trained on diverse financial data sources, allowing it to understand and predict market trends more accurately. By incorporating these models into financial risk assessments, the projects have shown significant improvements in predictive accuracy and the ability to handle large-scale financial data in real-time, thereby enhancing the decision-making capabilities of financial institutions.</p><p>‍</p><p>Both models contribute to advancing the field of financial AI by providing tools that not only improve the accuracy and efficiency of risk assessments but also help mitigate the biases that can arise from the data and models used in these critical applications.</p><h2>AI-Assisted Financial Forecasting</h2><h3>Traditional Forecasting Methods</h3><p>‍</p><p>Traditional financial forecasting methods rely heavily on historical data and a variety of statistical algorithms to predict future trends. Some of the key algorithms used in these traditional approaches include ARIMA (AutoRegressive Integrated Moving Average), SARIMA (Seasonal ARIMA), and other related models.</p><p>‍</p><h4>Arima</h4><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:399px""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/673280a6017327522bef18c3_AD_4nXe67ElD8VQoraBeVDqq15Syo5aib4J_G34BiGf5kH3QUbejksocVTbXj_27I1A_b4tix7_e6nG-Ut7vPZsIx3TXZ2St_BIRml-MmkuMpqqf_O4965Nkk5bBVcr9J1-USuyyna6pudjFfsQ-4SEMiyrm0e8T.png""/></div></figure><p>ARIMA models predict future data points by combining three key components: the autoregressive (AR) part, the integrated (I) part, and the moving average (MA) part. </p><p>‍</p><h5>Autoregressive (AR)</h5><p>This component captures the relationship between the current observation and its previous values. Essentially, it predicts future values by regressing the series on its own past values. For instance, if you're forecasting next month's sales, the AR part would use sales figures from previous months, based on the idea that past values directly influence future values.</p><p>‍</p><h5>Integrated (I)</h5><p>‍</p><p>Many time series data exhibit trends or seasonality, which can make the series non-stationary (its statistical properties change over time). The integrated part addresses this by differencing the data, which involves subtracting the previous observation from the current one. This process removes trends or seasonality, making the series stationary, which is a key requirement for many time series models.</p><p>‍</p><h5>Moving Average (MA) </h5><p>‍</p><p>This component models the relationship between an observation and the residual errors from previous forecasts. It corrects the forecast by considering the errors made in prior predictions. If past errors followed a certain pattern, the MA part learns from these errors and adjusts future forecasts accordingly.</p><p>‍</p><h4>SARIMA</h4><p>‍</p><p>SARIMA (Seasonal ARIMA) is an extension of the ARIMA model that explicitly incorporates a seasonal component, making it well-suited for data that exhibits regular, repeating patterns over time, such as monthly or quarterly sales figures. While ARIMA models are designed to handle non-seasonal time series data by capturing trends and patterns through autoregressive (AR), differencing (I), and moving average (MA) components, SARIMA goes a step further by applying these same principles to the seasonal elements of the data.</p><p>‍</p><p>In SARIMA, the model accounts for seasonality by introducing additional terms that specifically capture the periodic patterns in the data. For example, if you have monthly sales data that peaks every December, SARIMA can model this repeating pattern. It essentially applies the ARIMA components (AR, I, MA) twice—once to the non-seasonal aspects of the data and once to the seasonal aspects—allowing it to effectively separate and model both the overall trend and the seasonal fluctuations. This dual approach enables SARIMA to provide more accurate forecasts for time series data that exhibits both trend and seasonality.</p><p>‍</p><p>In addition to ARIMA and SARIMA, traditional methods also include Exponential Smoothing (ETS), which applies exponentially decreasing weights to past observations for short-term forecasting, and Linear Regression, which models the linear relationship between a dependent variable and one or more independent variables. While these methods have been widely used and are effective under certain conditions, they often struggle with capturing complex, non-linear patterns and sudden market changes, which limits their adaptability and accuracy in more dynamic environments. This is where modern AI techniques, such as machine learning and deep learning models, offer significant advantages by providing more flexible and data-driven approaches to forecasting.</p><p>‍</p><h3>How can AI improve traditional forecasting methods?</h3><p>‍</p><p>AI-assisted financial forecasting significantly enhances traditional methods by leveraging advanced data processing capabilities, real-time analysis, and machine learning algorithms. Unlike traditional forecasting, which often relies on historical data and limited variables, AI can handle vast amounts of data from diverse sources, including real-time market updates, news articles, and social media. This enables AI to provide up-to-date forecasts that adapt quickly to new information, ensuring more timely and accurate predictions. Additionally, AI models excel at recognizing complex patterns in data that traditional methods might overlook, leading to better forecasts, especially in volatile markets. AI also helps reduce human bias in forecasting by offering more objective analysis based on data rather than subjective judgment. Furthermore, AI's ability to generate multiple scenarios and continuously improve its accuracy over time makes it an invaluable tool for financial forecasting, providing deeper insights and more robust risk assessments than traditional approaches.</p><p>‍</p><h2>Regulatory Compliance and Risk Management</h2><p>‍</p><p>Regulatory Compliance and Risk Management are critical aspects of the financial industry, where ensuring adherence to laws and regulations is essential for maintaining trust and avoiding penalties. AI, particularly Natural Language Processing (NLP), plays a significant role in automating these processes and identifying potential risks in financial documents.</p><p>‍</p><h3>Automating Compliance Checks with NLP</h3><p>‍</p><p>NLP can automate the labor-intensive process of compliance checks by analyzing large volumes of legal and financial documents for specific regulatory requirements. Traditionally, compliance officers would manually review documents to ensure they meet regulatory standards, a process prone to human error and inefficiency. With NLP, AI systems can scan documents for specific keywords, phrases, or patterns that indicate compliance or non-compliance with relevant regulations. For example, NLP models can be trained to recognize references to anti-money laundering (AML) laws, data privacy requirements, or financial reporting standards within contracts and transaction records. By automating these checks, NLP not only reduces the time and effort required but also improves accuracy and consistency, helping financial institutions stay compliant with ever-evolving regulations.</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600px""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/673280a7034567a6d762d244_AD_4nXc90cz0vQnx74hBCUpB9csaLh7EHS97j8bSJ8Sxw7g7VLvK6GQt-p7AIjR9nHx4Jfp8bH4HVz0UmllSkQ4VEN6p8Boj4qeTCu4_X3kdL-3ty08b2eg98r9ea6Xl4oUGlgChIGlNm8zeZHx6x9cCAyc0tkb1.png""/></div></figure><p>‍</p><p>A pertinent example of this approach is demonstrated in the paper <a href=""https://arxiv.org/pdf/2209.09722"">""NLP-based Automated Compliance Checking of Data Processing Agreements against GDPR"" by Amaral et al.</a>, which highlights how NLP can be used to automate the compliance verification of Data Processing Agreements (DPAs) with GDPR. The system developed in the paper uses an embedding-based technique to compare the textual content of DPAs with predefined GDPR requirements. This approach not only checks for compliance but also provides recommendations on missing information, achieving a high degree of accuracy in identifying violations and satisfying requirements. The method shows a significant improvement in accuracy compared to baseline models, demonstrating the effectiveness of NLP in automating compliance checks.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:962px""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/673280a6703d174c2544d698_AD_4nXfafZavIIChQFBAN6eArhinPzQ4zQMIRv2v6lAQV9QXrvJ6m7KuS5rz3zDv9fYEF3jV5LfDyRd_w0SAVZ_hTFs-UXWLyTjD8WYGiivsX8XaPZ3r96KhEUH4MXzNWndSPQSM3zufTx8nPf97Oo3xiblr8pMn.png""/></div></figure><p>‍</p><p>Building on this, the paper <a href=""https://arxiv.org/html/2404.14356v1"">""Rethinking Legal Compliance Automation: Opportunities with Large Language Models"" by Hassani et al</a>. explores a different approach by leveraging Large Language Models (LLMs) such as GPT-4. Unlike the embedding-based method, this approach uses LLMs to analyze broader contexts within legal texts, allowing for a more comprehensive and nuanced understanding of compliance requirements. The authors introduce a systematic method that involves content chunking and prompt construction tailored to specific compliance rules, enabling LLMs to provide explanations for compliance decisions. This method demonstrated a significant improvement in accuracy—up to 40% better than traditional methods—showcasing the potential of LLMs to revolutionize legal compliance automation by considering larger textual contexts and providing more reliable justifications for compliance assessments.</p><p>‍</p><h3>Identifying Potential Risks in Financial Documents</h3><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1172px""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/673280a7017327522bef18d2_AD_4nXe2kz-hu_QdCoHl7sjyB8AH8t6uLGB9KibkB3bWigm0HFDnROWyPZjQjwdP-mL7t6Z6Ur3z3qfIb4CmFfpGOpx3g_yQbmLKuUyLSbrbSAmg1BXGWo18QWP16Xl_n9p_t9YQvZdGiAKYPA9dwEPcUsTEax0U.png""/></div></figure><p>‍</p><p>NLP is also instrumental in identifying potential risks hidden within financial documents. Financial institutions deal with a massive volume of documents, including contracts, loan agreements, and financial statements, which may contain clauses or terms that expose the institution to financial, legal, or operational risks. NLP algorithms can parse these documents to identify high-risk language, such as clauses that might lead to default or unfavorable conditions, or terms that violate regulatory requirements. For instance, in an automated compliance check, a specific clause (e.g., S8) may be flagged with a red question mark, suggesting potential non-compliance or ambiguity in satisfying certain regulatory requirements (e.g., R7 and R9 of GDPR). Additionally, NLP can flag unusual patterns in financial data that might indicate fraudulent activity or financial instability, enabling institutions to take preemptive actions to mitigate these risks.</p><p>‍</p><h2>Personalized Financial Advice with Large Language Models</h2><p>‍</p><p>Large Language Models (LLMs) are increasingly being utilized in the financial sector to provide personalized investment advice by leveraging their ability to analyze vast amounts of data and generate tailored recommendations. These models, such as GPT-3 and GPT-4, are transforming how financial advice is delivered, offering a blend of AI-driven insights and traditional advisory services.</p><h3>Tailoring Investment Recommendations Using AI</h3><p>‍</p><p>LLMs like GPT are revolutionizing personalized financial advice by analyzing a wide array of data sources, including market trends, user financial histories, and real-time economic indicators, to generate customized investment recommendations. For instance, platforms like <a href=""https://magnifi.com/#"">Magnifi</a> utilize LLMs to provide real-time, personalized advice based on user queries and preferences, effectively replicating the insights typically offered by human financial advisors. Similarly, <a href=""https://www.wealthfront.com"">Wealthfront</a> employs AI to continuously refine its investment strategies by monitoring user behavior and market conditions, ensuring that the recommendations remain relevant and optimized for individual financial goals.</p><p>‍</p><h3>Real-World Surveys and Market Adoption</h3><p>‍</p><p>Despite the growing adoption of AI in financial services, real-world surveys reveal a mix of curiosity and skepticism among users. <a href=""https://www.cnbc.com/2023/09/12/3-in-10-adults-would-use-ai-for-financial-advice-cnbc-survey-finds.html"">According to a CNBC survey</a>, while 37% of U.S. adults are interested in using AI tools like ChatGPT for managing their finances, only 4% currently do so. This disparity indicates a latent interest that could drive future adoption as trust in AI tools grows. However, the same survey highlighted that 51% of adults have little or no trust in AI-generated financial advice, underscoring the necessity of human oversight to validate AI-driven recommendations.</p><p>‍</p><p>Moreover, reports by <a href=""https://kms-solutions.asia/blogs/large-language-models-in-financial-services"">KMS Solutions</a> suggest that the financial industry is increasingly adopting LLMs for various applications, including personalized investment advice. The enhanced operational efficiency and improved decision-making facilitated by LLMs are key drivers behind this trend, indicating that as the technology matures, its adoption will likely accelerate.</p><p>‍</p><h2>Enhancing Audit Processes with NLP</h2><p>‍</p><p>The integration of Natural Language Processing (NLP) into financial auditing processes has significantly transformed the way audits are conducted, offering enhanced accuracy, efficiency, and reliability. By leveraging NLP, financial institutions can automate the traditionally manual and labor-intensive tasks associated with audits, enabling quicker identification of anomalies and ensuring compliance with regulatory standards.</p><p>‍</p><h3>Automated Anomaly Detection in Financial Data</h3><p>‍</p><p>Automated anomaly detection using NLP has become a cornerstone in modern financial audits. Traditional audit methods relied heavily on manual reviews and sampling techniques, which are both time-consuming and prone to human error. With the advent of NLP, tools/approach like  <a href=""https://arxiv.org/pdf/2308.06111"">ZeroShotALI</a>  and <a href=""https://arxiv.org/pdf/2402.09334v2"">AuditLLM</a> have emerged, providing robust solutions for anomaly detection in financial data.</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:980px""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/673280a7ed2032aa5aa6bd37_AD_4nXfr-qW3jrfyac3lUXsNwidUeHPVKAvXS7eV5UqthbD65-uN-6qml67Zz4_lx7xu0m79WtKASmPsTkp_OGJqxpTXknZe07K9afYftfvgTfR5txfwfKMIX_YO7ugBtmkYexhqUwnuvjAzjn9xd5OQVAJPDc0.png""/></div></figure><p>‍</p><p><a href=""https://arxiv.org/pdf/2308.06111"">ZeroShotALI</a> focuses on improving the efficiency of financial audits by using a domain-specific <a href=""https://arxiv.org/abs/1908.10084"">SentenceBERT</a> model in combination with GPT-4 to match text segments from financial documents to legal requirements outlined in standards like IFRS. The system identifies relevant sections of financial reports, compares them to compliance requirements, and highlights potential issues with high precision. This two-step approach—first narrowing down relevant sections using <a href=""https://arxiv.org/abs/1908.10084"">SentenceBERT</a> and then refining these using GPT-4—results in significant improvements in sensitivity, mean average precision (MAP), and F1 score over traditional methods. This automated system not only reduces the workload for auditors but also enhances the reliability of the audit process by accurately identifying anomalies and non-compliance issues.</p><p>‍</p><h3>Improving Audit Efficiency and Accuracy</h3><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600px""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/673280a61966d87e5280c52e_AD_4nXdZ-XICa8589SloXpJtaSQoa50PYHRjVpX1kn3Ysghm8kwvlL--LCk-y6eQldoKr9SYkUzMh4iHSB5cC_M61K7XjEFDmGQPZpfTtzyACsHv3Wf9OofdFmiEWBUmAYcFOMkMTGMAWMgPEJrHBlfv2UAghi8G.png""/></div></figure><p>‍</p><p>The accuracy and efficiency of audits are further enhanced by tools like <a href=""https://arxiv.org/pdf/2402.09334v2"">AuditLLM</a>, which introduces a multiprobe approach to audit the consistency of Large Language Models (LLMs) in their responses to financial audit queries. AuditLLM generates multiple ""probes,"" or variably phrased versions of the same audit question, to test the consistency and reliability of LLM outputs. By comparing the semantic similarity of the LLM's responses to these probes, AuditLLM detects inconsistencies that could indicate underlying issues such as bias or hallucinations. The tool demonstrated significant improvements in accuracy, up to 40% better than traditional methods, when applied to real-world datasets like TruthfulQA.</p><p>‍</p><p>This multiprobe approach offers a novel way to ensure the reliability of LLMs used in financial audits, providing a deeper level of scrutiny than single-query methods. It highlights how LLMs can be both powerful tools and potential risks if not adequately monitored and validated. </p><p>‍</p><h2>Natural Language Interfaces for Financial Systems</h2><p>‍</p><p>Natural Language Processing (NLP) is revolutionizing the financial services industry by enabling the creation of conversational AI systems that can interact with users in a human-like manner. These systems, including chatbots and virtual assistants, are increasingly being adopted by financial institutions to enhance customer service, streamline operations, and provide personalized financial advice. Here, we explore some of the significant advancements in building conversational AI for banking services and the emergence of voice-activated financial assistants.</p><h3>Building Conversational AI for Banking Services</h3><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600px""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/673280a6622d156faa398439_AD_4nXeMLoXFaY6IYmworvpMV8dmr9hUaWEGXdcB_1OWgsbfBUbD17BbziiS4xUKOywPe1eCbERpEymJ8p8vFMZLhxweILKRgiXeFqOwnU8FhPRZxSFMd82WgcQSxyfqg4T0ElntEDYJ1zAhm_zmCiOGoNw87DJK.png""/></div></figure><p>‍</p><p>Conversational AI systems are transforming the banking sector by providing customers with efficient, round-the-clock service. Notable examples include Bank of America's Erica, JPMorgan Chase's COIN, and Wells Fargo's Intelligent Virtual Agent (IVA), each of which utilizes NLP to deliver personalized customer interactions.</p><p>‍</p><h4>Bank of America's Erica</h4><p>This <a href=""https://promotions.bankofamerica.com/digitalbanking/mobilebanking/erica"">virtual assistant</a> leverages NLP and machine learning to provide customers with personalized financial guidance. <a href=""https://promotions.bankofamerica.com/digitalbanking/mobilebanking/erica"">Erica</a> can assist with a variety of tasks, including checking account balances, making payments, and offering insights into spending habits. The system continuously learns from user interactions, enhancing its ability to understand and respond to queries over time .</p><h4>JPMorgan Chase's COIN</h4><p> <a href=""https://www.jpmorgan.com/onyx/coin-system"">COIN (Contract Intelligence)</a> is an NLP-based system designed to analyze legal documents and extract key data points. This tool significantly reduces the time required for document review—from 360,000 hours to just a few seconds—by processing 12,000 new contracts per second. This application of NLP not only streamlines operations but also minimizes human error in critical financial processes .</p><h4>Wells Fargo's Intelligent Virtual Agent</h4><p>This <a href=""https://sites.wf.com/fargo/"">AI-driven system</a> helps customers with routine banking tasks, such as finding nearby ATMs or making payments. By understanding natural language queries, the IVA reduces the burden on human customer service representatives, enabling them to focus on more complex issues. This system has improved customer satisfaction by providing quick and accurate responses to common inquiries .</p><p>‍</p><h3>Voice-Activated Financial Assistants</h3><p>The advent of voice-activated assistants in banking represents a significant shift towards more intuitive user interfaces. These assistants allow customers to interact with financial services using simple voice commands, making banking more accessible and convenient. According to a <a href=""https://www.pwc.com/us/en/services/consulting/library/consumer-intelligence-series/voice-assistants.html"">survey by PwC</a>, 72% of respondents familiar with voice-enabled products have used a voice assistant, with 50% considering making purchases through these devices. While trust and privacy concerns remain barriers to wider adoption, the high satisfaction rates among users suggest a growing potential for voice-activated financial assistants in the future .</p><h2>Transform Your Accounting Processes with AI and NLP</h2><p>‍</p><p>Unlock the future of accounting with cutting-edge AI and NLP technologies. From automating financial report analysis to enhancing fraud detection, our AI solutions are designed to streamline your operations and boost accuracy. Don’t miss out on the opportunity to revolutionize your financial management. <a href=""https://www.mercity.ai/contacts"">Contact us</a> today to discover how AI can elevate your accounting practices to new heights.</p><p>‍</p></div>"
Comprehensive Guide to ReAct Prompting and ReAct based Agentic Systems,react-prompting-and-react-based-agentic-systems,640f56f76d313b2faa631c11,6803664d17c442704a1d0715,False,False,Sat Apr 19 2025 09:01:01 GMT+0000 (Coordinated Universal Time),Sat Apr 19 2025 09:08:59 GMT+0000 (Coordinated Universal Time),Sat Apr 19 2025 09:08:59 GMT+0000 (Coordinated Universal Time),"<h2 id="""">What is ReAct prompting?</h2><p id="""">ReAct prompting technique combines the “reasoning” and “acting” capabilities of an LLM to help with tasks like action planning, verbal reasoning, decision-making, and knowledge integration. It does so by forcing the model to <strong id="""">reason</strong> and <strong id="""">observe</strong> before <strong id="""">acting</strong>. This helps the model to analyze the context and situation so far, and then take the actions necessary to move forward. This technique has shown great improvements over past prompting techniques like <a href=""https://www.mercity.ai/blog-post/guide-to-chain-of-thought-prompting"" id="""">CoT (Chain or Thought)</a> and Zero-Shot Prompting.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68036129ff8442fde23d57db_AD_4nXc4NlGUm5S-w-BXcigzcEJrJlRy6HjsRoTlCDQa63C9egX-47Nl7AhcAaNm58K8UPC6F-KAh3gHKAeqgHtytpG0f4dMVyMbi3PNHAVaPEfYN28iQN4DHs3jwBlIYx9hrf2zv-yLrUDYOYO-WnXs3jpcX2n6.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><p id="""">ReAct prompting has also proved to be very effective in tasks like function calling or tool integration, planning ahead, agentic behavior, etc. And it is highly customizable to the specific task at hand. This makes ReAct one of the most used and prominent prompting techniques out there.</p><h2 id="""">How to Implement React Prompting</h2><p id="""">Implementing React prompting is actually a very straightforward and easy process. Most of the time a simple prompt with clean instructions suffice. The same goes with customizing the prompt too, minor changes and some examples can easily make the prompt aligned with your specific task.</p><h3 id="""">Zero-Shot</h3><p id="""">Zero-shot prompting is when the prompt does not contain examples of how to do the task at hand. Zero shot provides <strong id="""">instructions</strong> on how to do a task instead of examples, this is done when the task at hand is very complicated and there is no specific pattern to follow or it is not possible to provide enough examples to explain the task properly to the LLM model.</p><p id="""">‍</p><p id="""">Zero Shot prompting works because language models like ChatGPT are “instruction tuned”, meaning they don’t just follow patterns in prompts, but can follow abstract instructions and provide outputs based on that. This removes the need for examples, as the instructions can provide the model with directions on what to do and how to perform the task at hand.</p><p id="""">‍</p><p id="""">To implement React with Zero-Shot prompting, you can simply provide a set of instructions on how to follow a React based output scheme in detail, and the LLM will follow. Look at the image below on how we implement the React based prompting simply with instructions and no examples at all.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68036129440ff51b064d1166_AD_4nXdfHNl4LkHwSeKJNTTqFn-kb4tXqP0p5u4BXBJo_ityS6ejyP5_mr8M5UgLxkReTuRB0zx22tJLs7w8hCxjhKd7EViCqsolLc2WEr6_5c9wkXORoffSV-yZy0K9LHESm4wweNe6-Q.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><p id="""">Here is the prompt in the above example, you can use this with any model you like:</p><div data-rt-embed-type='true'><div style=""background-color: #2A2A2B;padding: 2.5%;"">

<code>
<i>

I want you to solve problems using the ReACT (Reasoning and Acting) approach.For each step, follow the format:‍
<br>
<br>
Thought: Reason step-by-step about the current situation and what to do next.
<br>
Action: [The specific action to take]
<br>
Observation: [The result of the action]‍
<br>
<br>
Continue this Thought/Action/Observation cycle until you solve the problem.Then provide your Final Answer.‍
<br>
<br>
—
<br>
Always output in the given format.

</i>
</code>

</div></div><p id="""">‍</p><h3 id="""">Few Shot</h3><p id="""">Few shot prompting on the other hand provides multiple examples to the model for the task at hand. This technique works really well for models that have not been finetuned to follow instructions. Models like GPT-3 are deemed to be “few shot learners” as shown in their <a href=""https://arxiv.org/abs/2005.14165"" id="""">paper</a>. The provided pattern allows the models to understand the problem and learn from the input-output pairs provided in the context. This high-level pattern matching allows models to solve many sophisticated problems without any finetuning.</p><p id="""">‍</p><p id="""">Now, few shot prompting is rather rare because of the instruction-tuned models, but we often use few shot along with zero shot. Meaning, that we often provide detailed instructions for the task the LLM is supposed to perform, but along with that we also provide many examples in the prompt to give a demonstration of how to do the task properly. This eliminates the guesswork and covers the edge cases where instructions might not be enough. Few Shot can also be helpful in scenarios where the model can learn more from the examples, and providing a proper set of instructions might be more complicated and introduce more edge cases.</p><p id="""">‍</p><p id="""">To implement React with Few-Shot prompting, you just need to provide some additional examples of your use cases and walk the LLM through them if necessary, along with the instructions we provided in the Zero Shot setting. Here’s how we implement ReAct with few shot:</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1480px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1480px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6803612ac381ab9567f51cd2_AD_4nXejr8-P_OyX-J1YE4R_bN9SZAqwyaFLWZCAD8sqtvI47R1-I7XJHSOaJeQH_zIkWRgfcfLzaSMlnkC1qsF4bQdOH3yuSW-m6pq9Igg8SuToaI7DP1NMJg7DgmBxw_JuQvevq-UEpw.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><p id="""">And here’s the prompt used in the above image:</p><div data-rt-embed-type='true'><div style=""background-color: #2A2A2B;padding: 2.5%;"">

<code>
<i>

I want you to solve problems using the ReACT (Reasoning and Acting) approach.
<br>

For each step, follow the format:
<br>
<br>
Thought: Reason step-by-step about the current situation and what to do next.
<br>

Action: [The specific action to take]
<br>

Observation: [The result of the action]
<br>
<br>
Continue this Thought/Action/Observation cycle until you solve the problem.
<br>
Then provide your Final Answer.
<br>
<br>
Example 1:
<br>
User: What's the population difference between New York City and Los Angeles?
<br>
<br>
Thought: I need to find the populations of both New York City and Los Angeles, then calculate the difference.
<br>
Action: Search for population of New York City
<br>
Observation: New York City has a population of approximately 8.8 million people.
<br>
<br>
Thought: Now I need the population of Los Angeles.
<br>
Action: Search for population of Los Angeles
<br>
Observation: Los Angeles has a population of approximately 3.9 million people.
<br>
<br>
Thought: Now I can calculate the difference between the two populations.
<br>
Action: Calculate 8.8 million - 3.9 million
<br>
Observation: The difference is 4.9 million.
<br>
<br>
Final Answer: The population difference between New York City and Los Angeles is approximately 4.9 million people, with New York City having the larger population.
<br>
<br>
Example 2:
<br>
User: What's the capital of France and what's its population?
<br>
<br>
Thought: I need to find the capital of France and then its population.
<br>
Action: Identify the capital of France
<br>
Observation: The capital of France is Paris.
<br>
<br>
Thought: Now I need to find the population of Paris.
<br>
Action: Search for population of Paris
<br>
Observation: Paris has a population of approximately 2.1 million people.
<br>
<br>
Final Answer: The capital of France is Paris, and it has a population of approximately 2.1 million people.
<br>
<br>
—
<br>
Always output in this format

</i>
</code>

</div></div><p id="""">‍</p><h3 id="""">Parameters and Points to keep in mind</h3><p id="""">When working with React, there are many variations and parameters to keep in mind. The classical LLM parameters like temperature and top-p are not very relevant here. As in the paper authors have not mentioned temperature or any specific LLM configuration anywhere, we can assume that they worked mostly with temperature set to 0.&nbsp;</p><p id="""">‍</p><p id="""">The authors do mention that they worked with Chain of Thought prompting with Self-Consistency where they generated 21 chains of thoughts using the LLM with temperature set to 0.7 and used the majority as the answer. They mentioned that this technique boosted performance over classical Chain of Thought prompting, but was still less than ReAct’s accuracy. However, Chain of Thought and Chain of Thought with Self Consistency performed much better than React on the <a href=""https://hotpotqa.github.io/"" id="""">HotpotQA benchmark</a>.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6803612a17c442704a19bc45_AD_4nXdjkolIreCe1WWfSA660cbM1flEjLS8HovGbG9Ghrpr8kQZWs_pq7xeyIMhLi4ot08teN5BDeoUwTkfVQutNmH4JBTmAaNSZXQgdiDnK5kQKme9jInNM7rDah0GO9XnKCsCeljH7YsdKxvIIiRdGfhFcSVC.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">Keeping this in mind, React performs much better with temperature 0 on most tasks, except those similar to HotpotQA, which is mainly fact-based question-answering. But, React combined with CoT-SC gives SOTA results, so it’s a very difficult decision on what to use exactly. It is suggested to test out different variations and then pick one that works for your task.</p><p id="""">‍</p><p id="""">Another variation to test is something the authors call “Act Only” prompting. This is where you take away the REasoning part of the prompt and the model, leaving only Action and Observation. Similar approaches have been used before to make LLMs interact with tools and external information but ReAct improves upon them almost all the time. Still, Act-only prompting is worth testing if your task is basic enough and doesn’t require too much reasoning, this can help you save on token costs and reduce latency a little.</p><p id="""">‍</p><h2 id="""">Function Calling With React</h2><p id="""">Because React prompting has the idea of “act” built into it, it is very easy to build function calling or tool calling flows with it. You simply have to create various “actions” for your model and give it permission to call them, and it will do so in the action section of its outputs.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6803612ac381ab9567f51cf7_AD_4nXdiYF8dedxvvdGGABN8SzbSOkyd06SbYgxGed466OkeOWbZ-gqMzn1iDUEY8-oGekF7mkTQ3UADhccsjhbkE24EaE5r5qVYq2YD44DImwsdaAwC3QJMdXjoiCvlT8lijxExjRmd1g.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><p id="""">Then you would need to register triggers based on certain actions of the model, to do that you can use a simple <em id="""">.contains</em> or a regex to detect presence of actions. Once detected, pause the flow of the React model, and execute the action the model wanted, and then provide back the response. You can easily provide back the response by pasting it in the Action section after the function call, like this:&nbsp;</p><p id="""">‍</p><p id=""""><strong id="""">Act: GetCurrentWeather() -&gt; {“temp”: “72F”, “wind”: “8mph”, “cond.”: “Partly Cloudy”}</strong></p><p id="""">‍</p><p id="""">You can simply append the output of the function after an <strong id="""">-&gt;</strong> and resume execution from there. The model will then output an “Observation”, unless some other step is specified.</p><p id="""">‍</p><p id="""">This is a very basic and straightforward flow, although massively extendable and you can change the set of actions and how the model reasons as the model moves ahead. Maybe the model has already deleted a file once, you can just remove the <strong id="""">DELETE</strong> action from the model’s action set, or can add new actions to the model hence improving the performance of the model as the flow progresses.</p><h3 id="""">Function calling with Arguments</h3><p id="""">Another very important aspect of calling functions using LLMs is calling them with arguments. Not all functions are stateless. Some require specific information to act upon, or simply to work with to provide an output to the user. A classic example is if the user asks for weather at a specific place like Seattle or New York, then the function needs to also accept a location as an argument or a parameter to execute properly.</p><p id="""">‍</p><p id="""">This is also very simple to execute with React as it is mostly about changing how the functions are defined and how they are parsed by the parser afterwards. You can simply tell the model to output actions in a specific format when calling the function with arguments. For example:</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6803612a1dd54367186f4596_AD_4nXeGAUYEEkCEl3JsZbHeQv2MNFxPvoXjUuCrLY3kc26LZqaRA4O-niM69nx1_YQ6O6xC_S9VvUzDJdX8S1litpzJYfyEbtvWpR8eHmf9FugQyrynDya-igc9VJKan45DVcIwElS7qg.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><p id="""">Once the format is established, you can update the parser to parse the function calls accordingly and pass on to the execution pipeline.</p><p id="""">‍</p><p id="""">One very important point to note here is that as you introduce more and more functions and tools for the model, it starts making more mistakes and hallucinations might become more common. This is very rare and happens when you have a lot of functions with lots of parameters, but still, can and does happen specifically in enterprise applications, it is important to maintain these issues properly. One common way is to use forced grammar to make sure the model doesn’t make syntactical errors. The <a href=""https://github.com/dottxt-ai/outlines"" id="""">outlines</a> library implements this pretty seamlessly, HuggingFace has a good guide for <a href=""https://huggingface.co/docs/text-generation-inference/en/basic_tutorials/using_guidance"" id="""">TGI Structured Outputs</a>, and VLLM also supports <a href=""https://docs.vllm.ai/en/latest/features/structured_outputs.html"" id="""">multiple libraries</a> for the same task.</p><p id="""">‍</p><h3 id="""">Integrate with External Knowledge Bases</h3><p id="""">Because of the function calling capabilities, it makes it much easier to integrate with external sources using React prompting. Simply need to add the action set to call APIs, perform RAG operations, etc and the model will take care of it.</p><p id="""">‍</p><p id="""">Here’s our guide on how you can integrate tools and functions with LLMs.</p><p id=""""><a href=""https://www.mercity.ai/blog-post/guide-to-integrating-tools-and-apis-with-language-models"" id="""">Comprehensive Guide to Integrating Tools and APIs with Language Models</a></p><p id="""">‍</p><p id="""">We do not use ReACT in this guide, but React can simply be extended to use the same principles as we outlined in the blog.</p><h2 id="""">ReAct vs Chain of Thought Prompting</h2><p id="""">Now both ReAct and <a href=""https://www.mercity.ai/blog-post/guide-to-chain-of-thought-prompting"" id="""">Chain of Thought (COT)</a> are very similar prompting styles given that they force the model to <strong id="""">reason before answering</strong>, and that’s where most of the gains in performance comes from.</p><p id="""">‍</p><p id="""">Although, Chain of thought is much better when it comes to extensive reasoning, and multi step thoughts are required. For example when solving a math problem, the problem needs to be simplified and then solved. COT is much better at linear reasoning and then reaching a result.</p><p id="""">‍</p><p id="""">Example:</p><p id="""">‍</p><p id=""""><em id="""">Problem: What is 17 × 24?</em></p><p id=""""><em id="""">CoT: I'll multiply step by step.</em></p><p id=""""><em id="""">&nbsp;	17 × 20 = 340</em></p><p id=""""><em id="""">&nbsp;	17 × 4 = 68</em></p><p id=""""><em id="""">&nbsp;	340 + 68 = 408</em></p><p id=""""><em id="""">Answer: 408</em></p><p id="""">‍</p><p id="""">React is more dynamic than Chain of thought. Along with reasoning it lets you introduce a cycle of observations and actions, which let you use react for dynamic conditions and rather for “actions” than just for reasoning and solving problems. This makes it a little worse at solving linear problems, but makes it much better and easier to work with when tools, dynamic conditions and other challenges are involved.</p><p id="""">‍</p><p id=""""><em id="""">Task: Find population data for New York and calculate its growth rate</em></p><p id="""">‍</p><p id=""""><em id="""">Thought: I need population data for New York for different years.</em></p><p id=""""><em id="""">Action: Search for ""New York population data 2010 and 2020""</em></p><p id=""""><em id="""">Observation: 2010 population: 19,378,102; 2020 population: 20,201,249</em></p><p id="""">‍</p><p id=""""><em id="""">Thought: Now I can calculate the growth rate.</em></p><p id=""""><em id="""">Action: Calculate (20,201,249 - 19,378,102) / 19,378,102 × 100</em></p><p id=""""><em id="""">Observation: 4.25% growth</em></p><p id="""">‍</p><p id=""""><em id="""">Thought: The population growth rate of New York from 2010 to 2020 was 4.25%.</em></p><h2 id="""">Advantages of ReAct Prompting over normal prompting?</h2><p id="""">There are a few reasons why you would pick the ReAct prompting structure over normal prompting structures. It usually provides you with more control, more structured outputs, and lets you direct the model exactly how to proceed in most scenarios. Let’s discuss these points in more detail.</p><h3 id="""">Gives a window into the LLM’s thinking process</h3><p id="""">Using ReAct gives you a better understanding of how the LLM is working to solve the problem as it deconstructs the problem and solution steps into a very well-structured format. This means you as the developer or prompter, can sit down and nitpick exactly what you don’t like in the model’s thinking process.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6803612968e3355dea8605df_AD_4nXej5dBBVCePyAJMMkCKzXgOQSxlpFc0GG-kZziohHdM0qbO7jJi65pktKd6yYbuB9LVVE5u1F6hjs2kf7Vycx6LkLYPXuoxG82WUuLkzxBMK5AaAOf6r5oUB9HVxTgQlpeaoRqG.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><p id="""">You can tell the model to reason in a specific way, and define exactly how to act and what to observe in certain scenarios. Along with this control, you can group or cluster LLM’s responses and figure out where the model makes the most mistakes, maybe most of the mistakes are happening when a certain ACTION is executed. This gives you a very deep insight into how the model is behaving, and how you can go about steering it to provide better responses.</p><p id="""">‍</p><h3 id="""">Gives stricter control over the LLM’s outputs</h3><p id="""">Better observability leads to better control. Once you see all the thoughts of the LLM, you know exactly where to interject. For example, if the model is working on mathematical problems, you can prompt the model to <em id="""">“Spend some time breaking the equation down into the most basic parts and then call the Calculator too”</em>, or when the model is in self-correction mode, you can tell it to analyze all of its past reasoning steps and actions in massive detail and ask it to find out errors.</p><p id="""">‍</p><p id="""">This is just from an prompting perspective, you can do much more complex things like asking the model to “Reason Twice” – one time from two different perspectives. And then combine that and in the third reasoning step and then act. Such things are much more complex but are needed to solve some more tricker and complex problems. For example, if you are building a bot which talks about philosophy with the users, you really need it to think and evaluate its arguments from multiple perspectives before providing an answer. Using React prompting in such scenarios is a very straightforward way to build your solution.</p><h3 id="""">ReAct can help design Programmatic Flows and Multistep Solutions</h3><p id="""">This is an application of ReACT prompting that has not been explored much, but it is possible and leads to amazing results. Because you can control React’s outputs and reasoning traces very strictly, you can build flows on top of them. Often multiple steps are needed to solve a problem, rather than just one single step. For example, generating SEO articles, if you do it in one step the results can be good, but much better flow is to generate the article and THEN optimize it with a SEO optimization specific prompt.</p><p id="""">‍</p><p id="""">Here’s a visual example of how an agentic or multistep flow would work with ReACT</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6803612a440ff51b064d118a_AD_4nXdn12coPoznKiYlD1V6JkPGfFrHLx5IHeu6NaQwwhTlKtcIG9QNo_IZiQ5hDbPsV3-NLxQRkYSFIY8-I_bo06PFnUtKL6eTDcYg-LLcE2mLNWqK1Q6DIpLTSulUye6GRoys9QmBdg.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><p id="""">You can write sophisticated prompts to do all this in one-shot, yes, but then it makes it much harder to control and change parts of the model and the output. Using ReACT and a multistep flow gives you the freedom to tune every single step individually.</p><p id="""">‍</p><p id="""">Straightforward way to do this is to hook up <strong id="""">triggers</strong> to certain actions or observations of the model, example, if the model’s action is to edit a specific part of the article, you use another prompt and model set that is more tuned to editing articles than writing it, and if the model observes a factual inconsistency, you can use another model to fix it, than fixing it in the same react flow. This method lets you branch off from the core flow of the model output and do multiple tasks parallelly. In such a flow the ReACT flow sits as the main stream of the model’s actions, and outputs triggers, and various other actions are done based on the triggers from the main stream.</p><h2 id="""">React Prompting with Langchain</h2><p id="""">‍</p><p id="""">Now <a href=""https://www.langchain.com/"" id="""">Langchain</a> is one of the most popular methods to build AI agents and simple workflows, here’s a small snippet on how to build a basic react agent using langchain. You simply need to import “create_react_agent” and configure it with your tools and query and you are good to go!</p><p id="""">‍</p><p id="""">One small thing is that Langchain is pretty good, but sometimes it abstracts away too much, don’t use it if you want more customization in your flows!</p><p id="""">‍</p><div data-rt-embed-type='true'><pre>
<code class=""language-py"">
from langgraph.prebuilt import create_react_agent

langgraph_agent_executor = create_react_agent(model, tools)


messages = langgraph_agent_executor.invoke({""messages"": [(""human"", query)]})
{
	""input"": query,
	""output"": messages[""messages""][-1].content,
}

</code>
</pre></div><h2 id="""">Some tips when working with ReACT Prompting</h2><p id="""">We at Mercity have been working with LLMs for years now, here are some points that we have learned from a lot of experience in ReACT prompting that you can use too.</p><h3 id="""">ReACT is very Extensible</h3><p id="""">We mentioned in the starting that the React works by breaking the outputs down into <strong id="""">reasoning</strong>, <strong id="""">action </strong>and then<strong id=""""> observation</strong>. This is actually not a strict rule, just a general pattern that is suggested to be followed. You can completely remove the <strong id="""">action</strong> step if you do not need it. And you can add multiple reasoning steps if you want, or maybe multiple action steps.</p><p id="""">‍</p><p id="""">This comes in handy when you are working with more tricker and sophisticated applications, maybe it requires you think from multiple perspectives before answering an question, that;s where you can have multiple reasonings, maybe you want to have only one very long and extensive reasoning and then multiple observations and actions based off of that single very long reasoning step – that is possible too. And it will save you a lot of time, as reasoning is where most of the time is spent by the model.</p><p id="""">‍</p><p id="""">This extensibility allows you to reshape the technique completely for your own use case, without hampering, and often increasing performance over baseline react prompting method.</p><h3 id="""">ReACT prompting format does not matter</h3><p id="""">Along with extensibility, the format is very flexible too. You can output in JSON if you want, XML or even YAML, none of which affect performance in any major way.</p><p id="""">‍</p><p id="""">We like to use XML internally, simply because it is easier to manage and work with when things get complex and there are many nested clauses. But, most of the people like to use JSON as it is the easiest way to get the model working as soon as possible and parsing it is very easy too.</p><p id="""">‍</p><p id="""">The idea is, that you are not bound to any specific format, as long as your are following the general ReACT output pattern, you should be able to get similar performance!</p><h3 id="""">Utilize direct Function Calling</h3><p id="""">Many providers like OpenAI and Anthropic now provide their own function calling features and have internal systems set up to make sure you get high accuracy when dealing with functions. Here’s <a href=""https://platform.openai.com/docs/guides/function-calling?api-mode=responses"" id="""">openAI’s guide on function calling</a>.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1190px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1190px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6803612a17c442704a19bc33_AD_4nXc5K3T8RGsGHJE-rWtXfEogx8vbCNffBDusGBoBbfbIjw4ZdSUtScvhpOYgxh_kPaWEGiRPML46Ll0npPi0O8vN00gZmCVNK-BsyHIlgjDdtAQX_XKKHM73cvWyktz8vbrAxwgt.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><p id="""">React is indeed a good method to induce function calling from LLMs, but when working with providers it's usually better to use the features they provide for it. They set up a lot of things underneath, make sure you use all that.&nbsp;</p><p id="""">‍</p><p id="""">Another very important thing to note is that even Open Source models like LLaMA and Mistral now have special tokens to manage function list and function calling, here is a <a href=""https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/prompt_format.md"" id="""">guide on how llama does it.</a></p><h2 id="""">Want help with your React Prompting workflows?</h2><p id="""">We at mercity have been working with LLMs for years now and have written all sorts of prompts, and especially many variants of React.</p><p id="""">‍</p><p id=""""><a href=""https://www.mercity.ai/contacts"" id="""">Contact us</a>, if you need any help with your prompting workflows!</p><p id="""">‍</p>",https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/680365971dd5436718724d9d_react_promptinh.png,Pranav,Prompt Engineering,In-depth Guide to prompting LLMs via ReACT prompting and how to build production grade agentic systems using ReACT.,False,"<div class=""rich-text w-richtext""><h2>What is ReAct prompting?</h2><p>ReAct prompting technique combines the “reasoning” and “acting” capabilities of an LLM to help with tasks like action planning, verbal reasoning, decision-making, and knowledge integration. It does so by forcing the model to <strong>reason</strong> and <strong>observe</strong> before <strong>acting</strong>. This helps the model to analyze the context and situation so far, and then take the actions necessary to move forward. This technique has shown great improvements over past prompting techniques like <a href=""https://www.mercity.ai/blog-post/guide-to-chain-of-thought-prompting"">CoT (Chain or Thought)</a> and Zero-Shot Prompting.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68036129ff8442fde23d57db_AD_4nXc4NlGUm5S-w-BXcigzcEJrJlRy6HjsRoTlCDQa63C9egX-47Nl7AhcAaNm58K8UPC6F-KAh3gHKAeqgHtytpG0f4dMVyMbi3PNHAVaPEfYN28iQN4DHs3jwBlIYx9hrf2zv-yLrUDYOYO-WnXs3jpcX2n6.png""/></div></figure><p>‍</p><p>ReAct prompting has also proved to be very effective in tasks like function calling or tool integration, planning ahead, agentic behavior, etc. And it is highly customizable to the specific task at hand. This makes ReAct one of the most used and prominent prompting techniques out there.</p><h2>How to Implement React Prompting</h2><p>Implementing React prompting is actually a very straightforward and easy process. Most of the time a simple prompt with clean instructions suffice. The same goes with customizing the prompt too, minor changes and some examples can easily make the prompt aligned with your specific task.</p><h3>Zero-Shot</h3><p>Zero-shot prompting is when the prompt does not contain examples of how to do the task at hand. Zero shot provides <strong>instructions</strong> on how to do a task instead of examples, this is done when the task at hand is very complicated and there is no specific pattern to follow or it is not possible to provide enough examples to explain the task properly to the LLM model.</p><p>‍</p><p>Zero Shot prompting works because language models like ChatGPT are “instruction tuned”, meaning they don’t just follow patterns in prompts, but can follow abstract instructions and provide outputs based on that. This removes the need for examples, as the instructions can provide the model with directions on what to do and how to perform the task at hand.</p><p>‍</p><p>To implement React with Zero-Shot prompting, you can simply provide a set of instructions on how to follow a React based output scheme in detail, and the LLM will follow. Look at the image below on how we implement the React based prompting simply with instructions and no examples at all.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68036129440ff51b064d1166_AD_4nXdfHNl4LkHwSeKJNTTqFn-kb4tXqP0p5u4BXBJo_ityS6ejyP5_mr8M5UgLxkReTuRB0zx22tJLs7w8hCxjhKd7EViCqsolLc2WEr6_5c9wkXORoffSV-yZy0K9LHESm4wweNe6-Q.png""/></div></figure><p>‍</p><p>Here is the prompt in the above example, you can use this with any model you like:</p><div class=""w-embed""><div style=""background-color: #2A2A2B;padding: 2.5%;"">
<code>
<i>

I want you to solve problems using the ReACT (Reasoning and Acting) approach.For each step, follow the format:‍
<br/>
<br/>
Thought: Reason step-by-step about the current situation and what to do next.
<br/>
Action: [The specific action to take]
<br/>
Observation: [The result of the action]‍
<br/>
<br/>
Continue this Thought/Action/Observation cycle until you solve the problem.Then provide your Final Answer.‍
<br/>
<br/>
—
<br/>
Always output in the given format.

</i>
</code>
</div></div><p>‍</p><h3>Few Shot</h3><p>Few shot prompting on the other hand provides multiple examples to the model for the task at hand. This technique works really well for models that have not been finetuned to follow instructions. Models like GPT-3 are deemed to be “few shot learners” as shown in their <a href=""https://arxiv.org/abs/2005.14165"">paper</a>. The provided pattern allows the models to understand the problem and learn from the input-output pairs provided in the context. This high-level pattern matching allows models to solve many sophisticated problems without any finetuning.</p><p>‍</p><p>Now, few shot prompting is rather rare because of the instruction-tuned models, but we often use few shot along with zero shot. Meaning, that we often provide detailed instructions for the task the LLM is supposed to perform, but along with that we also provide many examples in the prompt to give a demonstration of how to do the task properly. This eliminates the guesswork and covers the edge cases where instructions might not be enough. Few Shot can also be helpful in scenarios where the model can learn more from the examples, and providing a proper set of instructions might be more complicated and introduce more edge cases.</p><p>‍</p><p>To implement React with Few-Shot prompting, you just need to provide some additional examples of your use cases and walk the LLM through them if necessary, along with the instructions we provided in the Zero Shot setting. Here’s how we implement ReAct with few shot:</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1480pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6803612ac381ab9567f51cd2_AD_4nXejr8-P_OyX-J1YE4R_bN9SZAqwyaFLWZCAD8sqtvI47R1-I7XJHSOaJeQH_zIkWRgfcfLzaSMlnkC1qsF4bQdOH3yuSW-m6pq9Igg8SuToaI7DP1NMJg7DgmBxw_JuQvevq-UEpw.png""/></div></figure><p>‍</p><p>And here’s the prompt used in the above image:</p><div class=""w-embed""><div style=""background-color: #2A2A2B;padding: 2.5%;"">
<code>
<i>

I want you to solve problems using the ReACT (Reasoning and Acting) approach.
<br/>

For each step, follow the format:
<br/>
<br/>
Thought: Reason step-by-step about the current situation and what to do next.
<br/>

Action: [The specific action to take]
<br/>

Observation: [The result of the action]
<br/>
<br/>
Continue this Thought/Action/Observation cycle until you solve the problem.
<br/>
Then provide your Final Answer.
<br/>
<br/>
Example 1:
<br/>
User: What's the population difference between New York City and Los Angeles?
<br/>
<br/>
Thought: I need to find the populations of both New York City and Los Angeles, then calculate the difference.
<br/>
Action: Search for population of New York City
<br/>
Observation: New York City has a population of approximately 8.8 million people.
<br/>
<br/>
Thought: Now I need the population of Los Angeles.
<br/>
Action: Search for population of Los Angeles
<br/>
Observation: Los Angeles has a population of approximately 3.9 million people.
<br/>
<br/>
Thought: Now I can calculate the difference between the two populations.
<br/>
Action: Calculate 8.8 million - 3.9 million
<br/>
Observation: The difference is 4.9 million.
<br/>
<br/>
Final Answer: The population difference between New York City and Los Angeles is approximately 4.9 million people, with New York City having the larger population.
<br/>
<br/>
Example 2:
<br/>
User: What's the capital of France and what's its population?
<br/>
<br/>
Thought: I need to find the capital of France and then its population.
<br/>
Action: Identify the capital of France
<br/>
Observation: The capital of France is Paris.
<br/>
<br/>
Thought: Now I need to find the population of Paris.
<br/>
Action: Search for population of Paris
<br/>
Observation: Paris has a population of approximately 2.1 million people.
<br/>
<br/>
Final Answer: The capital of France is Paris, and it has a population of approximately 2.1 million people.
<br/>
<br/>
—
<br/>
Always output in this format

</i>
</code>
</div></div><p>‍</p><h3>Parameters and Points to keep in mind</h3><p>When working with React, there are many variations and parameters to keep in mind. The classical LLM parameters like temperature and top-p are not very relevant here. As in the paper authors have not mentioned temperature or any specific LLM configuration anywhere, we can assume that they worked mostly with temperature set to 0. </p><p>‍</p><p>The authors do mention that they worked with Chain of Thought prompting with Self-Consistency where they generated 21 chains of thoughts using the LLM with temperature set to 0.7 and used the majority as the answer. They mentioned that this technique boosted performance over classical Chain of Thought prompting, but was still less than ReAct’s accuracy. However, Chain of Thought and Chain of Thought with Self Consistency performed much better than React on the <a href=""https://hotpotqa.github.io/"">HotpotQA benchmark</a>.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6803612a17c442704a19bc45_AD_4nXdjkolIreCe1WWfSA660cbM1flEjLS8HovGbG9Ghrpr8kQZWs_pq7xeyIMhLi4ot08teN5BDeoUwTkfVQutNmH4JBTmAaNSZXQgdiDnK5kQKme9jInNM7rDah0GO9XnKCsCeljH7YsdKxvIIiRdGfhFcSVC.png""/></div></figure><p>Keeping this in mind, React performs much better with temperature 0 on most tasks, except those similar to HotpotQA, which is mainly fact-based question-answering. But, React combined with CoT-SC gives SOTA results, so it’s a very difficult decision on what to use exactly. It is suggested to test out different variations and then pick one that works for your task.</p><p>‍</p><p>Another variation to test is something the authors call “Act Only” prompting. This is where you take away the REasoning part of the prompt and the model, leaving only Action and Observation. Similar approaches have been used before to make LLMs interact with tools and external information but ReAct improves upon them almost all the time. Still, Act-only prompting is worth testing if your task is basic enough and doesn’t require too much reasoning, this can help you save on token costs and reduce latency a little.</p><p>‍</p><h2>Function Calling With React</h2><p>Because React prompting has the idea of “act” built into it, it is very easy to build function calling or tool calling flows with it. You simply have to create various “actions” for your model and give it permission to call them, and it will do so in the action section of its outputs.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6803612ac381ab9567f51cf7_AD_4nXdiYF8dedxvvdGGABN8SzbSOkyd06SbYgxGed466OkeOWbZ-gqMzn1iDUEY8-oGekF7mkTQ3UADhccsjhbkE24EaE5r5qVYq2YD44DImwsdaAwC3QJMdXjoiCvlT8lijxExjRmd1g.png""/></div></figure><p>‍</p><p>Then you would need to register triggers based on certain actions of the model, to do that you can use a simple <em>.contains</em> or a regex to detect presence of actions. Once detected, pause the flow of the React model, and execute the action the model wanted, and then provide back the response. You can easily provide back the response by pasting it in the Action section after the function call, like this: </p><p>‍</p><p><strong>Act: GetCurrentWeather() -&gt; {“temp”: “72F”, “wind”: “8mph”, “cond.”: “Partly Cloudy”}</strong></p><p>‍</p><p>You can simply append the output of the function after an <strong>-&gt;</strong> and resume execution from there. The model will then output an “Observation”, unless some other step is specified.</p><p>‍</p><p>This is a very basic and straightforward flow, although massively extendable and you can change the set of actions and how the model reasons as the model moves ahead. Maybe the model has already deleted a file once, you can just remove the <strong>DELETE</strong> action from the model’s action set, or can add new actions to the model hence improving the performance of the model as the flow progresses.</p><h3>Function calling with Arguments</h3><p>Another very important aspect of calling functions using LLMs is calling them with arguments. Not all functions are stateless. Some require specific information to act upon, or simply to work with to provide an output to the user. A classic example is if the user asks for weather at a specific place like Seattle or New York, then the function needs to also accept a location as an argument or a parameter to execute properly.</p><p>‍</p><p>This is also very simple to execute with React as it is mostly about changing how the functions are defined and how they are parsed by the parser afterwards. You can simply tell the model to output actions in a specific format when calling the function with arguments. For example:</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6803612a1dd54367186f4596_AD_4nXeGAUYEEkCEl3JsZbHeQv2MNFxPvoXjUuCrLY3kc26LZqaRA4O-niM69nx1_YQ6O6xC_S9VvUzDJdX8S1litpzJYfyEbtvWpR8eHmf9FugQyrynDya-igc9VJKan45DVcIwElS7qg.png""/></div></figure><p>‍</p><p>Once the format is established, you can update the parser to parse the function calls accordingly and pass on to the execution pipeline.</p><p>‍</p><p>One very important point to note here is that as you introduce more and more functions and tools for the model, it starts making more mistakes and hallucinations might become more common. This is very rare and happens when you have a lot of functions with lots of parameters, but still, can and does happen specifically in enterprise applications, it is important to maintain these issues properly. One common way is to use forced grammar to make sure the model doesn’t make syntactical errors. The <a href=""https://github.com/dottxt-ai/outlines"">outlines</a> library implements this pretty seamlessly, HuggingFace has a good guide for <a href=""https://huggingface.co/docs/text-generation-inference/en/basic_tutorials/using_guidance"">TGI Structured Outputs</a>, and VLLM also supports <a href=""https://docs.vllm.ai/en/latest/features/structured_outputs.html"">multiple libraries</a> for the same task.</p><p>‍</p><h3>Integrate with External Knowledge Bases</h3><p>Because of the function calling capabilities, it makes it much easier to integrate with external sources using React prompting. Simply need to add the action set to call APIs, perform RAG operations, etc and the model will take care of it.</p><p>‍</p><p>Here’s our guide on how you can integrate tools and functions with LLMs.</p><p><a href=""https://www.mercity.ai/blog-post/guide-to-integrating-tools-and-apis-with-language-models"">Comprehensive Guide to Integrating Tools and APIs with Language Models</a></p><p>‍</p><p>We do not use ReACT in this guide, but React can simply be extended to use the same principles as we outlined in the blog.</p><h2>ReAct vs Chain of Thought Prompting</h2><p>Now both ReAct and <a href=""https://www.mercity.ai/blog-post/guide-to-chain-of-thought-prompting"">Chain of Thought (COT)</a> are very similar prompting styles given that they force the model to <strong>reason before answering</strong>, and that’s where most of the gains in performance comes from.</p><p>‍</p><p>Although, Chain of thought is much better when it comes to extensive reasoning, and multi step thoughts are required. For example when solving a math problem, the problem needs to be simplified and then solved. COT is much better at linear reasoning and then reaching a result.</p><p>‍</p><p>Example:</p><p>‍</p><p><em>Problem: What is 17 × 24?</em></p><p><em>CoT: I'll multiply step by step.</em></p><p><em> 	17 × 20 = 340</em></p><p><em> 	17 × 4 = 68</em></p><p><em> 	340 + 68 = 408</em></p><p><em>Answer: 408</em></p><p>‍</p><p>React is more dynamic than Chain of thought. Along with reasoning it lets you introduce a cycle of observations and actions, which let you use react for dynamic conditions and rather for “actions” than just for reasoning and solving problems. This makes it a little worse at solving linear problems, but makes it much better and easier to work with when tools, dynamic conditions and other challenges are involved.</p><p>‍</p><p><em>Task: Find population data for New York and calculate its growth rate</em></p><p>‍</p><p><em>Thought: I need population data for New York for different years.</em></p><p><em>Action: Search for ""New York population data 2010 and 2020""</em></p><p><em>Observation: 2010 population: 19,378,102; 2020 population: 20,201,249</em></p><p>‍</p><p><em>Thought: Now I can calculate the growth rate.</em></p><p><em>Action: Calculate (20,201,249 - 19,378,102) / 19,378,102 × 100</em></p><p><em>Observation: 4.25% growth</em></p><p>‍</p><p><em>Thought: The population growth rate of New York from 2010 to 2020 was 4.25%.</em></p><h2>Advantages of ReAct Prompting over normal prompting?</h2><p>There are a few reasons why you would pick the ReAct prompting structure over normal prompting structures. It usually provides you with more control, more structured outputs, and lets you direct the model exactly how to proceed in most scenarios. Let’s discuss these points in more detail.</p><h3>Gives a window into the LLM’s thinking process</h3><p>Using ReAct gives you a better understanding of how the LLM is working to solve the problem as it deconstructs the problem and solution steps into a very well-structured format. This means you as the developer or prompter, can sit down and nitpick exactly what you don’t like in the model’s thinking process.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6803612968e3355dea8605df_AD_4nXej5dBBVCePyAJMMkCKzXgOQSxlpFc0GG-kZziohHdM0qbO7jJi65pktKd6yYbuB9LVVE5u1F6hjs2kf7Vycx6LkLYPXuoxG82WUuLkzxBMK5AaAOf6r5oUB9HVxTgQlpeaoRqG.png""/></div></figure><p>‍</p><p>You can tell the model to reason in a specific way, and define exactly how to act and what to observe in certain scenarios. Along with this control, you can group or cluster LLM’s responses and figure out where the model makes the most mistakes, maybe most of the mistakes are happening when a certain ACTION is executed. This gives you a very deep insight into how the model is behaving, and how you can go about steering it to provide better responses.</p><p>‍</p><h3>Gives stricter control over the LLM’s outputs</h3><p>Better observability leads to better control. Once you see all the thoughts of the LLM, you know exactly where to interject. For example, if the model is working on mathematical problems, you can prompt the model to <em>“Spend some time breaking the equation down into the most basic parts and then call the Calculator too”</em>, or when the model is in self-correction mode, you can tell it to analyze all of its past reasoning steps and actions in massive detail and ask it to find out errors.</p><p>‍</p><p>This is just from an prompting perspective, you can do much more complex things like asking the model to “Reason Twice” – one time from two different perspectives. And then combine that and in the third reasoning step and then act. Such things are much more complex but are needed to solve some more tricker and complex problems. For example, if you are building a bot which talks about philosophy with the users, you really need it to think and evaluate its arguments from multiple perspectives before providing an answer. Using React prompting in such scenarios is a very straightforward way to build your solution.</p><h3>ReAct can help design Programmatic Flows and Multistep Solutions</h3><p>This is an application of ReACT prompting that has not been explored much, but it is possible and leads to amazing results. Because you can control React’s outputs and reasoning traces very strictly, you can build flows on top of them. Often multiple steps are needed to solve a problem, rather than just one single step. For example, generating SEO articles, if you do it in one step the results can be good, but much better flow is to generate the article and THEN optimize it with a SEO optimization specific prompt.</p><p>‍</p><p>Here’s a visual example of how an agentic or multistep flow would work with ReACT</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6803612a440ff51b064d118a_AD_4nXdn12coPoznKiYlD1V6JkPGfFrHLx5IHeu6NaQwwhTlKtcIG9QNo_IZiQ5hDbPsV3-NLxQRkYSFIY8-I_bo06PFnUtKL6eTDcYg-LLcE2mLNWqK1Q6DIpLTSulUye6GRoys9QmBdg.png""/></div></figure><p>‍</p><p>You can write sophisticated prompts to do all this in one-shot, yes, but then it makes it much harder to control and change parts of the model and the output. Using ReACT and a multistep flow gives you the freedom to tune every single step individually.</p><p>‍</p><p>Straightforward way to do this is to hook up <strong>triggers</strong> to certain actions or observations of the model, example, if the model’s action is to edit a specific part of the article, you use another prompt and model set that is more tuned to editing articles than writing it, and if the model observes a factual inconsistency, you can use another model to fix it, than fixing it in the same react flow. This method lets you branch off from the core flow of the model output and do multiple tasks parallelly. In such a flow the ReACT flow sits as the main stream of the model’s actions, and outputs triggers, and various other actions are done based on the triggers from the main stream.</p><h2>React Prompting with Langchain</h2><p>‍</p><p>Now <a href=""https://www.langchain.com/"">Langchain</a> is one of the most popular methods to build AI agents and simple workflows, here’s a small snippet on how to build a basic react agent using langchain. You simply need to import “create_react_agent” and configure it with your tools and query and you are good to go!</p><p>‍</p><p>One small thing is that Langchain is pretty good, but sometimes it abstracts away too much, don’t use it if you want more customization in your flows!</p><p>‍</p><div class=""w-embed""><pre>
<code class=""language-py"">
from langgraph.prebuilt import create_react_agent

langgraph_agent_executor = create_react_agent(model, tools)


messages = langgraph_agent_executor.invoke({""messages"": [(""human"", query)]})
{
	""input"": query,
	""output"": messages[""messages""][-1].content,
}

</code>
</pre></div><h2>Some tips when working with ReACT Prompting</h2><p>We at Mercity have been working with LLMs for years now, here are some points that we have learned from a lot of experience in ReACT prompting that you can use too.</p><h3>ReACT is very Extensible</h3><p>We mentioned in the starting that the React works by breaking the outputs down into <strong>reasoning</strong>, <strong>action </strong>and then<strong> observation</strong>. This is actually not a strict rule, just a general pattern that is suggested to be followed. You can completely remove the <strong>action</strong> step if you do not need it. And you can add multiple reasoning steps if you want, or maybe multiple action steps.</p><p>‍</p><p>This comes in handy when you are working with more tricker and sophisticated applications, maybe it requires you think from multiple perspectives before answering an question, that;s where you can have multiple reasonings, maybe you want to have only one very long and extensive reasoning and then multiple observations and actions based off of that single very long reasoning step – that is possible too. And it will save you a lot of time, as reasoning is where most of the time is spent by the model.</p><p>‍</p><p>This extensibility allows you to reshape the technique completely for your own use case, without hampering, and often increasing performance over baseline react prompting method.</p><h3>ReACT prompting format does not matter</h3><p>Along with extensibility, the format is very flexible too. You can output in JSON if you want, XML or even YAML, none of which affect performance in any major way.</p><p>‍</p><p>We like to use XML internally, simply because it is easier to manage and work with when things get complex and there are many nested clauses. But, most of the people like to use JSON as it is the easiest way to get the model working as soon as possible and parsing it is very easy too.</p><p>‍</p><p>The idea is, that you are not bound to any specific format, as long as your are following the general ReACT output pattern, you should be able to get similar performance!</p><h3>Utilize direct Function Calling</h3><p>Many providers like OpenAI and Anthropic now provide their own function calling features and have internal systems set up to make sure you get high accuracy when dealing with functions. Here’s <a href=""https://platform.openai.com/docs/guides/function-calling?api-mode=responses"">openAI’s guide on function calling</a>.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1190pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/6803612a17c442704a19bc33_AD_4nXc5K3T8RGsGHJE-rWtXfEogx8vbCNffBDusGBoBbfbIjw4ZdSUtScvhpOYgxh_kPaWEGiRPML46Ll0npPi0O8vN00gZmCVNK-BsyHIlgjDdtAQX_XKKHM73cvWyktz8vbrAxwgt.png""/></div></figure><p>‍</p><p>React is indeed a good method to induce function calling from LLMs, but when working with providers it's usually better to use the features they provide for it. They set up a lot of things underneath, make sure you use all that. </p><p>‍</p><p>Another very important thing to note is that even Open Source models like LLaMA and Mistral now have special tokens to manage function list and function calling, here is a <a href=""https://github.com/meta-llama/llama-models/blob/main/models/llama3_3/prompt_format.md"">guide on how llama does it.</a></p><h2>Want help with your React Prompting workflows?</h2><p>We at mercity have been working with LLMs for years now and have written all sorts of prompts, and especially many variants of React.</p><p>‍</p><p><a href=""https://www.mercity.ai/contacts"">Contact us</a>, if you need any help with your prompting workflows!</p><p>‍</p></div>"
Step by Step Guide to Generate Podcasts using TTS and LLMs,step-by-step-guide-to-generate-podcasts-using-tts-and-llms-ai-elevenlabs,640f56f76d313b2faa631c11,669e2c4e244be2210cc020b4,False,True,Mon Jul 22 2024 09:54:22 GMT+0000 (Coordinated Universal Time),Sat Jul 26 2025 20:49:05 GMT+0000 (Coordinated Universal Time),Mon Jul 22 2024 10:19:50 GMT+0000 (Coordinated Universal Time),"<h2 id="""">What is AI-Generated Podcast</h2><p id="""">‍</p><p id="""">AI-generated podcasts represent a fusion of artificial intelligence technologies with traditional audio content creation, fundamentally transforming how podcasts are produced, distributed, and consumed. These podcasts use machine learning models, especially large language models (LLMs), to synthesize audio content that can range from entirely scripted to dynamically generated conversations. AI can simulate voices, personalities, and discussion styles of specific individuals, whether they are real-life figures, historical personalities, or fictional characters.</p><p id="""">‍</p><p id="""">Advanced text-to-speech (TTS) technologies and voice cloning tools are used to generate lifelike audio from text scripts. These tools can mimic specific voice characteristics, making it possible to produce episodes that sound like they are hosted by any chosen personality or the topic of the episode which are chosen by the user. This customisation helps to engage listeners in real-time, encouraging a deeper sense of connection and community around the podcast.</p><p id="""">‍</p><h3 id="""">AI's Impact on Podcast Content Creation</h3><p id="""">‍</p><p id="""">One of the most labor-intensive aspects of podcast production is the research and scriptwriting phase, which involves gathering information, fact-checking, and crafting a narrative that is both informative and engaging. AI can handle these tasks efficiently by quickly processing vast amounts of data and generating content that adheres to a given outline or set of topics.</p><p id="""">‍</p><p id="""">AI can scour multiple data sources, including recent news articles, academic papers, and other relevant materials, to compile detailed background information on chosen topics. This capability is particularly useful for podcasts that cover complex subjects requiring extensive background research, such as scientific developments, historical events, or current affairs. By automating the initial research and draft creation, AI allows podcast creators to focus more on refining the content, adding personal insights, and engaging with their audience.</p><p id="""">‍</p><h2 id="""">Why AI-Podcast Generation</h2><p id="""">‍</p><h3 id="""">User-Defined Guests</h3><p id="""">‍</p><p id="""">AI allows for the creation of virtual episodes where listeners can choose their preferred personalities to feature, be they renowned historical figures or modern celebrities. This personalization enables fans to hear from figures who are no longer accessible, like past leaders, or from current figures who are otherwise unreachable due to their busy schedules. This flexibility enhances listener engagement by making the experience more tailored and interactive.</p><p id="""">‍</p><h3 id="""">Exploring Unanswered Questions</h3><p id="""">‍</p><p id="""">AI facilitates conversations on topics that are either too niche for mainstream podcasts or require expertise that would be difficult to assemble in a traditional setting. For example, an AI could simulate a discussion between Nikola Tesla and Elon Musk on energy futures, a conversation impossible in reality but rich with educational and entertainment value. The generation of voice totally depends on the quality of training audio data available for the particular person so we must take into account it while choosing the speaker and guest.</p><p id="""">‍</p><h3 id="""">Simulating Controversial Conversations</h3><p id="""">‍</p><p id="""">Many topics are too sensitive or divisive for real people to tackle in a public format due to potential backlash or political correctness. AI can navigate these issues by simulating dialogues that explore these areas without personal risk to the speakers. For example, an AI-generated podcast could simulate a conversation where U.S. President Joe Biden discusses his regrets and reflections on the consequences of military actions in Rafah, particularly addressing the loss of civilian lives during recent conflicts in Palestine. Such a dialogue, while entirely hypothetical, would allow for a nuanced exploration of the moral and strategic complexities involved, which might be too controversial for a sitting president to address openly. This not only broadens the scope of discussion but also allows for a deeper exploration of complex themes without real-world repercussions, providing listeners with insights into critical global issues that are rarely discussed in such an open and personal manner.</p><p id="""">‍</p><h3 id="""">Bringing Fiction to Life</h3><p id="""">‍</p><p id="""">AI can create podcasts where fictional characters from beloved books or films interact in new scenarios, extending the universe of a story beyond its original media. For example, imagine a podcast episode where Naruto Uzumaki from ""Naruto"" and Izuku Midoriya from ""My Hero Academia'' discuss the nature of heroism and the responsibilities that come with power. Such interactions allow fans to enjoy fresh content with familiar characters, enhancing their connection to the narrative and exploring 'what if' scenarios that fuel fan theories and discussions. This unique content generation leverages AI to delve into creative discussions that would not be possible in the original works, offering new insights and entertainment to the audience.</p><p id="""">‍</p><h3 id="""">Legacy Voices Preservation</h3><p id="""">‍</p><p id="""">Through voice cloning technology, AI can preserve and replicate the voices of deceased or aging personalities, making it possible for them to continue 'participating' in new dialogues. This not only keeps their legacies alive but also makes historical education more engaging by allowing students to 'hear' history from the very individuals who made it. For example, giving life to Martin Luther King Jr a prominent civil rights leader, King is remembered for his powerful speeches.</p><p id="""">‍</p><h2 id="""">How to Generate AI Podcasts</h2><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:811px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""811px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/669e0e756fc7a8bf4d1f9739_AD_4nXftAxGAJuFSrXrn_sz1FCOGjqUWKqDCObXuTYofNUz1J58wCpN8L3p3Q2CD7nLAqzThmHX3wHnxJ-7rzj8dAsyeTjr6A8fLsez4EtjyJK22NKIigyhAGJ52kV93pSzRRdmA6AJRAh29vIaALIiF6KEdj7mh.jpeg"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><h3 id="""">Knowledge Base Creation</h3><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:780px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""780px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/669e0e750a23d5657340a9de_AD_4nXfdf5R8LQfXkShIaNiHCfb4X7h1-huUsZ_16hVviYWT6mirrjW_t43pAaPxTt0xjLPa6SNHiNtaxLk0jkYpp6BXysdq_3oX5UXZjRU6cni1TAXdZiwX2C3M6DiuIeOgOwFTvanqMAfM25z3egsb5AS6G4hv.jpeg"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><p id="""">The process begins by identifying and downloading the audio of YouTube videos. This is achieved using the <a href=""https://github.com/ytdl-org/youtube-dl"" id="""">youtube_dl</a> library, which is a command-line program to download videos from YouTube and a few more sites, get_channel_id_by_username and get_youtube_videos-these functions use the <a href=""https://rapidapi.com/ytdlfree/api/youtube-v31"" id="""">YouTube Data API v3</a>, accessed via <a href=""https://rapidapi.com/hub"" id="""">RapidAPI's</a> interface. Users input the name of a YouTube channel, and the API returns videos based on specified criteria such as minimum duration and video count.The download_audio function utilizes <a href=""https://github.com/ytdl-org/youtube-dl"" id="""">youtube_dl</a> to download the best audio quality available and saves it as an MP3 file in a specified output directory. This is crucial for obtaining clear audio for transcription.</p><p id="""">‍</p><p id="""">Code-</p><div data-rt-embed-type='true'><pre>
<code class=""language-py"">
import os
from transcription_utils import get_channel_id_by_username, download_audio, get_video_details, get_video_urls, get_youtube_videos, process_audio_files

def main():
    # Get user inputs interactively
    channel_name = input(""Enter the name of the channel: "")
    min_minutes = int(input(""Enter the minimum minutes of the videos: ""))
    count = int(input(""Enter the number of videos: ""))
    output_folder = input(""Enter the output folder name where audio files will be saved: "")

    channel_id = get_channel_id_by_username(channel_name)
    print(""The channel id:"", channel_id)

    videos = get_youtube_videos(channel_id, min_minutes, count)
    print(""The videos are: "", videos)
    urls = get_video_urls(videos)

    # Ensure the output folder is created at the project root level
    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    output_path = os.path.join(project_root, output_folder)
    os.makedirs(output_path, exist_ok=True)

    for url in urls:
        download_audio(url, output_folder=output_path)

    # Create a transcriptions directory and ensure each transcription is saved in its own folder
    transcriptions_root = os.path.join(project_root, 'transcriptions')
    os.makedirs(transcriptions_root, exist_ok=True)

    transcription_folder = f'transcriptions_of_{output_folder}'
    transcription_path = os.path.join(transcriptions_root, transcription_folder)
    os.makedirs(transcription_path, exist_ok=True)
    
    process_audio_files(output_path, transcription_path)
    print(""Transcription generated successfully"")

if __name__ == ""__main__"":
    main()
</code>
</pre></div><p id="""">‍</p><p id="""">Some of the new listing youtube videos or the channels&nbsp; are not being found via the youtube v3 api calls therefore the below script&nbsp; will be able to allow the user download the particular video in MP3 format via the URL provided by the user.</p><div data-rt-embed-type='true'><pre>
<code class=""language-py"">
import argparse
import os
from transcription_utils import download_audio, process_audio_files

def main(urls, output_folder):
    # Ensure the output folder is created at the project root level
    project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    output_path = os.path.join(project_root, output_folder)
    os.makedirs(output_path, exist_ok=True)

    for url in urls:
        download_audio(url.strip(), output_folder=output_path)  # Ensure each URL is stripped of whitespace

    # Create a transcriptions directory and ensure each transcription is saved in its own folder
    transcriptions_root = os.path.join(project_root, 'Transcriptions')
    os.makedirs(transcriptions_root, exist_ok=True)

    transcription_folder = f'transcriptions_of_{output_folder}'
    transcription_path = os.path.join(transcriptions_root, transcription_folder)
    os.makedirs(transcription_path, exist_ok=True)
    
    process_audio_files(output_path, transcription_path)

if __name__ == ""__main__"":
    parser = argparse.ArgumentParser(description='Download and process audio files from YouTube URLs.')
    parser.add_argument('-o', '--output', required=True, help='The output folder name where audio files will be saved')
    parser.add_argument('--urls', required=True, help='A comma-separated list of YouTube URLs to download and process')
    args = parser.parse_args()

    # Split the URLs by comma and strip any extra whitespace
    urls = [url.strip() for url in args.urls.split(',')]

    main(urls, args.output)
</code>
</pre></div><p id="""">‍</p><h3 id="""">Dialogue Generation</h3><p id="""">‍</p><p id="""">To produce dialogues that are both natural-sounding and engaging, leveraging advanced LLMs (Large Language Models) involves a specific set of steps and technologies. Below, I'll outline how the provided code achieves dialogue generation, specifically detailing the role of these models and their customization to reflect distinct speech patterns and personality traits.</p><p id="""">‍</p><h4 id="""">Text Splitting and Data Preparation</h4><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1136px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1136px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/669e0e75b746f0602a0b4cf7_AD_4nXeXbni9_RbUEcIDHTW1MKSwop5FIKmkdE5tXHkkCJAWcVGtf0cM07b6cjyzZGx3ke0C96P0VKmwBKtN7n8IIhzRFRrYwngtPwEepvE1PSdpVGyUnWBGSIPBPefW6IkKBDZWF2Oij4eGE_LG55O1pLjav1k.jpeg"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><p id="""">The audio transcripts received from assembly ai are loaded in this stage where the texts are retrieved for creating a vector store for helping the LLM to inherit the personality of the speaker from the knowledge base.</p><p id="""">‍</p><p id="""">Before generating the vector store, the text data (such as transcripts) must be properly formatted. The get_text_chunks function is designed to read text files from the transcript directory, combine their contents for a particular speaker file into a single string, and then split this string into smaller chunks for further processing. This step is crucial for processing large datasets without overwhelming the model.</p><p id="""">‍</p><p id="""">Code-</p><h4 id="""">Embedding and Index Creation</h4><p id="""">‍</p><p id="""">Using the <a href=""https://sbert.net/docs/package_reference/sentence_transformer/SentenceTransformer.html"" id="""">SentenceTransformer model</a>, text chunks are converted into vector embeddings. These embeddings represent the textual data in a high-dimensional space, capturing semantic meanings that are used for generating dialogue. <a href=""https://www.trychroma.com"" id="""">Chroma DB</a> is utilized to efficiently store and retrieve these embeddings during the dialogue generation process, allowing the model to access relevant context quickly.</p><p id=""""> </p><div data-rt-embed-type='true'><pre>
<code class=""language-py"">
def get_vectorstore(text_chunks, collection_name):
    model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
    embeddings = model.encode(text_chunks, convert_to_tensor=False, show_progress_bar=True)
    
    # Initialize Chroma client with PersistentClient
    chroma_client = chromadb.PersistentClient(path=""./chroma"")

    # Create a collection in Chroma
    collection = chroma_client.create_collection(collection_name)

    # Add the text chunks and their embeddings to the Chroma collection
    for i, chunk in enumerate(text_chunks):
        collection.add(
            documents=[chunk],
            metadatas=[{""source"": f""chunk_{i}""}],
            ids=[str(i)],
            embeddings=[embeddings[i].tolist()]
        )
    
    return collection,model

import os
from sentence_transformers import SentenceTransformer
import chromadb
from podcast_utils import get_text_chunks, get_vectorstore
from dotenv import load_dotenv

load_dotenv()

def setup_chroma_directory(chroma_directory, transcriptions_folder):
    # Read guest and speaker names from environment variables
    guest_name = os.getenv('GUEST_NAME')
    speaker_name = os.getenv('SPEAKER_NAME')

    if not guest_name or not speaker_name:
        raise ValueError(""Guest and Speaker names must be set in the environment variables."")

    # Define the paths for the guest and speaker transcriptions
    transcripts_location_guest = os.path.join(transcriptions_folder, f""transcriptions_of_{guest_name}"")
    transcripts_location_speaker = os.path.join(transcriptions_folder, f""transcriptions_of_{speaker_name}"")

    print(f""Guest transcription path: {transcripts_location_guest}"")
    print(f""Speaker transcription path: {transcripts_location_speaker}"")

    if not os.path.exists(transcripts_location_guest) or not os.path.exists(transcripts_location_speaker):
        print(""Contents of the Transcriptions folder:"")
        for item in os.listdir(transcriptions_folder):
            print(item)
        raise ValueError(f""Transcription directories for {guest_name} and {speaker_name} must exist."")

    if not os.path.exists(chroma_directory):
        # Get text chunks from transcripts for both LLMs
        text_chunks_guest = get_text_chunks(transcripts_location_guest)
        text_chunks_speaker = get_text_chunks(transcripts_location_speaker)

        # Create vector stores for both LLMs with unique collection names
        print(""\n \n Creating vector stores for GUEST"")
        collection_guest, model_guest = get_vectorstore(text_chunks_guest, ""podcast_conversations_guest"")
        print(""\n \n Creating vector stores for SPEAKER"")
        collection_speaker, model_speaker = get_vectorstore(text_chunks_speaker, ""podcast_conversations_speaker"")

        # Model assignment
        model = model_guest  # Assuming both models are the same
    else:
        # Initialize Chroma client with PersistentClient
        chroma_client = chromadb.PersistentClient(path=chroma_directory)
        
        collection_guest = chroma_client.get_collection(""podcast_conversations_guest"")
        collection_speaker = chroma_client.get_collection(""podcast_conversations_speaker"")
        
        model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')

    return collection_guest, collection_speaker, model

if __name__ == ""__main__"":
    # Get the script's directory
    script_dir = os.path.dirname(os.path.abspath(__file__))
    transcriptions_folder = os.path.join(script_dir, 'Transcriptions')
    chroma_directory = os.path.join(script_dir, 'chroma')

    collection_guest, collection_speaker, model = setup_chroma_directory(
        chroma_directory=chroma_directory,
        transcriptions_folder=transcriptions_folder
    )

    print(""Vector store built successfully."")
</code>
</pre></div><p id="""">‍</p><h4 id="""">Enhancing&nbsp; Conversations with Personalized Prompts</h4><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:662px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""662px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/669e0e752675b730fa30f507_AD_4nXeWpSUhhlpGEift6P5pX8DhniDNZMGKhQ8BXz4fldL5qYCU9rreuaiYX25lDA-WBHHhULMRdXAs9u3xlO4-ZqzAdPotTvVpZ-GpPOcjwl1luWCfsl7qQj_EOrTI6HpfrTxprHY1UPirYREK8hX4NHli_84J.jpeg"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><p id="""">The use of customized prompts is pivotal in guiding a language model (LLM) to mimic specific personalities effectively, enhancing the quality and relevance of AI-generated responses. As depicted in the diagram, the Speaker_prompt, Guest_prompt, Personality_guide_speaker, and Personality_guide_guest are tailored to fine-tune the model conversational style and characteristics for different personas. These prompts provide structured guidance, allowing the model to generate responses that are contextually appropriate and align with the predefined traits of the speaker and guest.&nbsp;</p><p id="""">‍</p><p id="""">By customizing these prompts, users can ensure that the AI maintains a consistent and engaging dialogue, reflecting the nuances of the specified personalities. This approach not only improves the relatability and authenticity of the interaction but also significantly enhances the overall user experience by delivering responses that feel more natural and human-like. Whereas the master and judge prompts are the basic outline on how these models should generate the personalities therefore customisation is strictly not necessary.</p><p id="""">‍</p><h4 id="""">Generating Dialogue</h4><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:681px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""681px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/669e0e755b3f4d9bba8afa9b_AD_4nXex-hCBsBdcCDt1qCNHZs6EbjWYe-8HCKJGJJV5p9etK59gIybdV50YkJvx9fZNBdMt8D_j8kMmYjoIr_B9XYBtHc6odgGII-ralh1DwmJKk6GN4RTGUqelBrhJRzK9ba8Ojt9NT1azjK8tlUTLEX34iI6l.jpeg"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><p id="""">In the process of generating dialogue for AI-driven podcasts, several steps and components work together to create a coherent and engaging conversation. The script begins by loading necessary configurations and personality guides, which are crucial for shaping the responses to match the defined personas of the speaker and guest. These guides are read from text files and environment variables, ensuring that each interaction is personalized and contextually appropriate.</p><p id="""">‍</p><p id="""">The dialogue generation starts with the creation of master prompts for both the speaker and guest using the `get_speaker_master_prompt` and `get_guest_master_prompt` functions, which incorporate the discussion topic provided by the user. The total number of messages for the conversation is set, and initial message arrays for both the speaker and guest are established.</p><p id="""">‍</p><p id="""">During each iteration of the conversation loop, the script first checks and redacts older messages to maintain a manageable history size. It then generates responses using the `get_llm_response` function, which interacts with the LLM through the OpenAI API. The responses are processed to extract ""think"" and ""out"" texts using the `extract_out_think_texts` and `extract_out_text` functions, respectively.</p><p id="""">‍</p><p id="""">These responses are then embedded using a sentence transformer model, and relevant personality information is retrieved from pre-built vector stores (`collection_guest` and `collection_speaker`). This information is used to adjust the prompts dynamically, ensuring the dialogue remains contextually relevant and true to the defined personas. The `judge_response` function further refines the responses by evaluating them against a set of predefined criteria and adjusting them for coherence and personality consistency. The cleaned and judged responses are appended to the conversation log, which captures raw responses, think texts, and the final output for each message. Finally, the complete conversation log is saved to a JSON file with a timestamped filename.</p><p id="""">‍</p><h3 id="""">Eleven Labs Voice Cloning for Podcast Production</h3><p id="""">‍</p><p id="""">The process of using <a href=""https://elevenlabs.io"" id="""">Eleven Labs</a> for voice cloning in podcast production involves several steps, from setting up the API to integrating custom voice settings for generating dynamic audio content. This technology allows podcast creators to use realistic synthesized voices that can be tailored to specific characters or personalities, enhancing the auditory experience of podcasts.</p><p id="""">‍</p><h4 id="""">Initialization and API Setup</h4><p id="""">‍</p><p id="""">First, the <a href=""https://github.com/elevenlabs/elevenlabs-python"" id="""">Eleven Labs API</a> client is initialized using an API key. This client facilitates all interactions with the Eleven Labs services, including retrieving available voices and generating audio.The API allows you to retrieve a list of available voices. This can be used to select a voice that matches the character or personality you wish to clone.</p><p id="""">‍</p><div data-rt-embed-type='true'><pre>
<code class=""language-py"">
from elevenlabs.client import ElevenLabs

client = ElevenLabs(
    api_key="""",
)

response = client.voices.get_all()
print(response.voices)  # Displays all available voices
</code>
</pre></div><p id="""">‍</p><h4 id="""">Build Custom Voice</h4><p id="""">‍</p><p id="""">Eleven Labs provides a cloning functionality&nbsp; from the starter pack of its subscription where the user can clone the targeted voice using a few clear cut audio samples. You can access this feature and run a demo by accessing directly on their <a href=""https://elevenlabs.io/app/voice-lab"" id="""">website voice lab section</a> or via code.</p><p id="""">‍</p><div data-rt-embed-type='true'><pre>
<code class=""language-py"">
voice = client.clone(
    name=""Alex"",
    description=""An old American male voice with a slight hoarseness in his throat. Perfect for news"", # Optional
    files=[""./sample_0.mp3"", ""./sample_1.mp3"", ""./sample_2.mp3""],
)

audio = client.generate(text=""Hi! I'm a cloned voice!"", voice=voice)

play(audio)
</code>
</pre></div><p id="""">‍</p><h4 id="""">Find the Best Fit Parameters</h4><p id="""">When utilizing voice cloning technology for podcast production, particularly with Eleven Labs, it's essential to adjust the voice settings to closely match the desired vocal characteristics of the characters being emulated. This ensures that the generated audio maintains a high degree of realism and fidelity to the original voice. Configuring the correct parameters in VoiceSettings is crucial for achieving the best quality and most authentic-sounding cloned voice. Each parameter plays a specific role in how the voice will sound.</p><p id="""">‍</p><p id="""">stability: This parameter controls the consistency of the voice output. A higher stability value ensures that the voice does not fluctuate too much, maintaining a consistent tone throughout the dialogue. For example, setting stability to 0.60 provides a balance between natural variation and consistency.</p><p id="""">‍</p><p id="""">similarity_boost: This setting adjusts how closely the cloned voice matches the target voice. A higher similarity_boost increases the likelihood that the voice sounds like the intended person. For voice 1, a similarity boost of 0.95 suggests a strong resemblance to the original voice, whereas voice 2 setting at 0.80 allows for slightly more deviation, which might be useful for character voices where exact replication isn't critical.</p><p id="""">‍</p><p id="""">style: This parameter influences the dynamic range and expressiveness of the voice. A lower style value might make the voice sound more monotone, while a higher value increases expressiveness. At 0.20, the style is moderately expressive, suitable for general podcast dialogues where too much expressiveness could distract from the content.</p><p id="""">‍</p><p id="""">use_speaker_boost: When set to True, this enhances the clarity and presence of the voice, making it stand out more clearly in the audio mix. This is particularly useful in podcast production where voice clarity is paramount for listener engagement but note it also excessively reduces the credits in your account.</p><p id="""">‍</p><div data-rt-embed-type='true'><pre>
<code class=""language-py"">
from elevenlabs.client import Voice, VoiceSettings

Voice_1_voice_settings = VoiceSettings(
    stability=0.60,
    similarity_boost=0.95,
    style=0.20,
    use_speaker_boost=True
)

Voice_2_voice_settings = VoiceSettings(
    stability=0.60,
    similarity_boost=0.80,
    style=0.20,
    use_speaker_boost=True
)
</code>
</pre></div><p id="""">‍</p><h4 id="""">Generating and Combining Audio Segments for a Podcast</h4><p id="""">‍</p><p id="""">The process of creating a podcast episode using AI-generated voices involves two main steps: generating individual audio segments for each part of the conversation and then combining these segments into a single coherent audio file. First, the audio for each segment of the podcast is generated using the Eleven Labs API. The&nbsp; code facilitates this by loading a conversation log, applying the appropriate voice settings for the speaker and guest, and generating audio segments accordingly. The get_voice function handles the generation, where it initializes a combined audio segment and iteratively adds audio for each dialogue entry. Voice settings such as stability, similarity boost, and style are tailored for the speaker and guest to ensure distinct and realistic character voices. Each generated audio segment is combined sequentially, with random delays added to simulate natural conversation pauses. Finally, the combined audio file is saved with a timestamped filename in a structured directory, enhancing organization and ease of access for podcast production.</p><p id="""">‍</p><div data-rt-embed-type='true'><pre>
<code class=""language-py"">
from elevenlabs import play
from elevenlabs.client import ElevenLabs, VoiceSettings, Voice
from pydub import AudioSegment
import io
import random
import os
from dotenv import load_dotenv
from datetime import datetime

# Load environment variables
load_dotenv()

client = ElevenLabs(api_key=os.getenv(""ELEVENLABS_API_KEY""))

# Initialize an empty audio segment
combined_audio = AudioSegment.empty()

# Define voice settings 
speaker_voice_settings = VoiceSettings(
    stability=0.45,
    similarity_boost=0.75,
    style=0.2,
    # use_speaker_boost=True
)

# Define voice settings 
guest_voice_settings = VoiceSettings(
    stability=0.5,
    similarity_boost=0.60,
    style=0.35,
    # use_speaker_boost=True
)

def get_voice(conversations, speaker_voice_id, guest_voice_id, speaker_name, guest_name):
    combined_audio = AudioSegment.empty()
    for entry in conversations:
        if 'speaker' in entry:
            voice_id = speaker_voice_id
            settings = speaker_voice_settings
            text = entry['speaker']
            speaker = speaker_name
        elif 'guest' in entry:
            voice_id = guest_voice_id
            settings = guest_voice_settings
            text = entry['guest']
            speaker = guest_name
        else:
            continue  # Skip if speaker is not recognized

        # Generate audio with custom voice settings
        audio_generator = client.generate(
            text=text,
            voice=Voice(voice_id=voice_id, settings=settings),
            model=""eleven_multilingual_v2""
        )

        # Initialize a bytes buffer
        buffer = io.BytesIO()
        # Collect all data from the generator
        for chunk in audio_generator:
            buffer.write(chunk)
        # Move back to the beginning of the BytesIO buffer
        buffer.seek(0)
        # Load the audio segment from the buffer
        audio_segment = AudioSegment.from_file(buffer, format=""mp3"")
        combined_audio += audio_segment  # Combine sequentially as they appear
        print(f""Audio segment added for {speaker}."")

        # Add a random delay between 0.2 to 0.5 seconds
        delay_duration = random.uniform(0.2, 0.5) * 1000  # Convert to milliseconds
        silent_segment = AudioSegment.silent(duration=delay_duration)
        combined_audio += silent_segment

    # Generate filename with timestamp and speaker/guest names
    timestamp = datetime.now().strftime(""%Y%m%d_%H%M%S"")
    filename = f""{timestamp}_{speaker_name}_conversation_with_{guest_name}.mp3""
    
    # Ensure the output folders exist
    podcast_out_folder = os.path.join(""generated_podcast"", ""podcast_out"")
    os.makedirs(podcast_out_folder, exist_ok=True)
    
    # Output the combined audio to the file
    audio_file_path = os.path.join(podcast_out_folder, filename)
    combined_audio.export(audio_file_path, format=""mp3"")
    print(f""Audio file saved to {audio_file_path}"")

    return filename, audio_file_path
</code>
</pre></div><p id="""">‍</p><h2 id="""">Elevate Your Podcasting Experience with AI-Powered Solutions</h2><p id="""">‍</p><p id="""">If you're ready to harness the innovative power of AI for your podcast production, our team at <a href=""http://mercity.ai"" id="""">Mercity.ai</a> is here to help. From crafting engaging, AI-generated dialogues to simulating conversations with historical figures or fictional characters, we offer custom AI podcasting solutions tailored to your creative needs. Whether you aim to explore untouched topics, bring to life the voices of the past, or captivate your audience with unique, personalized content, <a href=""http://mercity.ai"" id="""">Mercity.ai</a> has the expertise to transform your podcasting aspirations into reality. Don't miss out on the opportunity to revolutionize your podcast production. Contact <a href=""http://mercity.ai"" id="""">Mercity.ai</a> today to discover how our advanced AI technologies can enhance your storytelling and engage your audience like never before. Reach out now!</p><p id="""">‍</p>",https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/669e2ab88ee6f2223ffefa52_ai%20podcast%20geb.png,Mathavan,AI Generated Podcasts,Learn how to generate podcasts using voice cloning via Eleven Labs and LLM agents architecture,False,
How to do Topic Modeling on Podcasts and Videos ,topic-modeling-on-podcasts-and-videos-using-bertopic,640f56f76d313b2faa631c11,664e35a48283d9ed06f9cf9d,False,False,Wed May 22 2024 18:12:52 GMT+0000 (Coordinated Universal Time),Wed May 22 2024 18:42:06 GMT+0000 (Coordinated Universal Time),Wed May 22 2024 19:35:12 GMT+0000 (Coordinated Universal Time),"<h2 id="""">What is Topic Modeling?</h2><p id="""">Topic Modeling is like finding the main themes or subjects in a large collection of written documents without knowing what those themes are ahead of time. Imagine you have a big pile of books or articles, and you want to understand what they're generally about without reading each one in detail. Topic Modeling uses smart computer algorithms to do this.</p><p id="""">‍</p><p id="""">It is a type of statistical modeling that leverages unsupervised machine learning to analyze and identify clusters or groups of similar words within a body of text, thereby discovering hidden patterns and automatically identifying topics that exist within a text corpus.</p><p id="""">‍</p><p id="""">Some of the common algorithms to identify and extract topics from a collection of documents include Latent Dirichlet Allocation(LDA), Latent Semantic Analysis (LSA), Non-Negative Matrix Factorization(NMF), Correlated Topic Model (CTM), Top2Vec, BERTopic, etc.</p><p id="""">‍</p><p id="""">Each topic is represented as a distribution over words, and each document is represented as a distribution over topics. This allows for the extraction of the main themes from large sets of text data, making it easier to organize, search, and summarize the information.</p><p id="""">‍</p><p id="""">Topic modeling has numerous applications, including document clustering, organizing large document collections, improving information retrieval, and enhancing recommendations in various domains like academia, business, and social media. It provides insights that are not apparent through simple keyword searches, enabling a more nuanced understanding of the content.</p><p id="""">‍</p><h3 id="""">Topic Modeling vs Topic Classification</h3><p id="""">Topic Modeling utilizes unsupervised machine learning techniques. It does not require a pre-labeled dataset and can autonomously identify patterns within your text data. By analyzing word co-occurrences and distributions, Topic Modeling can reveal hidden themes and topics without any prior knowledge of the content. This process is particularly effective with high-quality, large datasets; the more data you feed into the model, the better it becomes at detecting and delineating the underlying topics. Larger datasets provide more context and variation, allowing the model to capture more subtle patterns and nuances in the text.</p><p id="""">‍</p><p id="""">Conversely, Topic Classification employs supervised machine learning. It relies on datasets that have been manually labeled to train the model. This method can work effectively with smaller datasets, provided that they are well-labeled and representative of the topics of interest. The manual labeling process creates a structured dataset that the algorithm can use to learn to classify new texts into predefined categories accurately.</p><p id="""">‍</p><p id="""">In terms of long-term effectiveness, teaching a machine to identify high-value words through text analysis—such as in Topic Classification—can be more strategic compared to the unsupervised approach of Topic Modeling. If you have a predefined list of topics and wish to label sets of texts like reviews or surveys quickly, a Topic Classification algorithm is more applicable. This approach allows for the automatic extraction of valuable insights from texts based on predefined categories, making it a practical solution for tasks that require quick and accurate topic identification.</p><p id="""">‍</p><h2 id="""">Why should we do Topic Modeling?</h2><p id="""">‍</p><p id="""">Topic Modeling allows you to examine multiple topics and organize, understand, and summarize them on a large scale. It enables you to swiftly uncover hidden patterns within the data, providing insights that can inform data-driven decisions.</p><h3 id="""">Document Classification</h3><p id="""">Topic modeling helps classify documents into predefined categories based on the topics they contain. For instance, a collection of news articles can be categorized into topics like politics, sports, technology, and health. The algorithm identifies the predominant themes in each document and assigns it to the most relevant category. This process automates and accelerates the classification of large volumes of text, making it easier to organize and retrieve information.</p><p id="""">‍</p><p id="""">Additionally, topic modeling enhances the accuracy and efficiency of document classification by reducing human error and bias. It can handle multilingual text and adapt to different domains without needing extensive manual adjustments. By continuously learning from new data, topic modeling algorithms can evolve and improve over time, ensuring that the classification remains relevant and up-to-date.</p><h3 id="""">Effortlessly Tag Customer Support Requests</h3><p id="""">‍</p><p id="""">Customer support teams receive numerous queries daily. Topic modeling can analyze these requests and automatically tag them with relevant topics such as billing issues, technical support, product inquiries, or service feedback. By categorizing requests, support teams can prioritize and route them to the appropriate departments or specialists, improving response times and customer satisfaction.</p><p id="""">‍</p><p id="""">Topic modeling enhances the accuracy of tagging by minimizing manual errors and ensuring consistency in categorization. This automated tagging system can also identify emerging trends or recurring issues, enabling support teams to proactively address common problems.</p><h3 id="""">Scaling Customer Feedback Analysis</h3><p id="""">‍</p><p id="""">Companies often collect vast amounts of feedback from customers through surveys, reviews, social media, and other channels. Topic modeling can process this feedback to identify recurring themes and sentiments, such as common complaints, product suggestions, or praise. This analysis helps businesses understand customer needs and preferences at scale, allowing them to make data-driven decisions to enhance products and services.</p><p id="""">‍</p><p id="""">Topic modeling allows businesses to detect shifts in customer sentiment over time, providing early warnings about potential issues or emerging trends. It can segment feedback by demographic or geographic factors, offering more nuanced insights into customer behavior. By automating the analysis of large-scale feedback, topic modeling frees up resources, enabling teams to focus on strategic initiatives rather than manual data processing.</p><h3 id="""">Crafting Content That Resonates</h3><p id="""">‍</p><p id="""">Content creators, marketers, and writers aim to produce content that engages their audience. Topic modeling can analyze existing content and audience interactions to identify trending topics and themes. By understanding what resonates with their audience, creators can tailor their content to match these interests, increasing engagement, and relevance. This approach ensures that the content is aligned with the audience’s preferences and needs.</p><p id="""">‍</p><p id="""">Topic modeling allows content creators to discover gaps in current content offerings, revealing opportunities for new and unique topics that have not yet been explored. It can also track changes in audience interests over time, helping creators to adapt their strategies accordingly. By analyzing feedback and interactions, such as comments, likes, and shares, topic modeling provides insights into the types of content that generate the most engagement.</p><h3 id="""">Understanding Employee Sentiments</h3><p id="""">‍</p><p id="""">Organizations often conduct employee surveys and collect feedback through various channels to gauge employee sentiment and workplace satisfaction. Topic modeling can analyze this data to uncover underlying themes and sentiments, such as concerns about work-life balance, management practices, or workplace culture. By understanding these sentiments, organizations can address issues, improve employee morale, and create a more positive work environment.</p><p id="""">‍</p><p id="""">Topic modeling enables organizations to track changes in employee sentiment over time, allowing them to measure the impact of implemented policies and initiatives. This analysis can segment feedback by department, tenure, or other relevant factors, providing a detailed understanding of specific groups' experiences within the organization.</p><h2 id="""">How Does Topic Modeling Work</h2><p id="""">‍</p><p id="""">Topic modeling is a method used in natural language processing (NLP) and text mining to uncover hidden patterns within a large collection of texts. It is an unsupervised machine-learning technique, meaning it does not require labeled data. Here’s a detailed explanation of how topic modeling works, particularly focusing on the most commonly used algorithm, Latent Dirichlet Allocation (LDA). The steps involved in Topic Modeling are:</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:589px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""589px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/664e3542d24e75199ca9f8f8_SCdVxmFhEvhFs0nKaPHVbExM7ygLaMlURW_XJrjGNYrg5c6Tlz296GOPof6keIqGQo-hhwbFyte7DvEiI-GVDbDbYXfSB5Dmgty_AbKK7cfnzvPTJlyUcJidmTa4FnsYcwTCoTdjnq4UfqXps1RYXMs.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><h3 id="""">Text Preprocessing</h3><p id="""">‍</p><p id="""">Text Preprocessing is a vital step in preparing text data for modeling, particularly for algorithms like Latent Dirichlet Allocation (LDA), which is used for topic modeling. This process involves several steps aimed at cleaning and standardizing the text data to enhance the performance of the model.&nbsp;</p><p id="""">‍</p><p id="""">Tokenization is the process of breaking down text into individual units, usually words or phrases, called tokens. This step simplifies the text and makes it easier to analyze. For the sentence ""The cat sat on the mat,"" tokenization would result in [""The"", ""cat"", ""sat"", ""on"", ""the"", ""mat""]. Stop words are common words that carry little semantic meaning and are often removed to focus on the more significant words. Removing these words helps reduce the noise in the data. Words like ""the,"" ""is,"" and ""and"" are typically removed from the token list. For the tokenized example above, removing stop words might result in [""cat"", ""sat"", ""mat""].</p><p id="""">‍</p><p id="""">Lemmatization and stemming are both techniques used to reduce words to their base forms. Lemmatization reduces words to their base or dictionary form by considering the word's meaning and context, resulting in more accurate reductions such as ""better"" becoming ""good"" and ""ran"" becoming ""run."" On the other hand, stemming removes suffixes to reduce words to their root form, without considering context, which can lead to less accurate reductions. For example, ""running,"" ""runner,"" and ""ran"" might all be reduced to ""run."" While lemmatization ensures the base form is a proper word, stemming focuses on simplifying text for analysis.</p><h3 id="""">Model Initialization</h3><p id="""">‍</p><p id="""">The model initialization step in Latent Dirichlet Allocation (LDA) is crucial for setting the stage for the next (iterative) process. Initially, each word in each document is randomly assigned to one of the predefined numbers of topics. This random assignment provides a starting point for the iterative refinement process. Since LDA is a probabilistic model, starting with a random distribution allows the algorithm to explore the topic space effectively. Consider a document with the words [""cat"", ""sat"", ""mat""]. If we have three topics, the initial assignment might randomly assign ""cat"" to Topic 1, ""sat"" to Topic 2, and ""mat"" to Topic 3.</p><p id="""">‍</p><p id="""">This random assignment is purely an initial guess. The LDA algorithm will iteratively refine these assignments to discover the true underlying topic structure in the data. The random initialization helps ensure that the algorithm does not start with any biases and can explore a wide range of possible topic distributions.</p><p id="""">‍</p><p id="""">LDA not only adjusts which topic a word belongs to, but it also updates its understanding of what each topic is about. This dual updating process—adjusting word assignments and refining topic descriptions—helps LDA to accurately capture the main themes or topics within a collection of documents.</p><h3 id="""">Word-Topic Assignment</h3><p id="""">‍</p><p id="""">The word-topic assignment step in Latent Dirichlet Allocation (LDA) is a key component of the iterative process that refines the model to discover the underlying topics in a corpus (collection of documents). For each word in each document, the LDA algorithm reassigns the word to a topic based on the probability distribution that considers two factors:</p><p id="""">‍</p><h4 id="""">Topic-Word Distribution</h4><p id="""">The probability of the word given the topic. It indicates how strongly a word is associated with a particular topic. This is computed as the number of times a word is assigned to a topic across all documents, divided by the total number of words assigned to that topic. It is denoted by a symbol Φ.</p><p id="""">‍</p><h4 id="""">Document-Topic Distribution</h4><p id="""">The probability of the topic given the document. It reflects how strongly a topic is associated with a particular document. This is computed as the number of words in a document assigned to a topic, divided by the total number of words in a document. It is denoted by a symbol θ.</p><p id="""">After several iterations, the algorithm converges, and the final topic and document distributions are calculated</p><h2 id="""">Topic Modeling for YouTube Videos using BERTopic</h2><h3 id=""""><a href=""https://maartengr.github.io/BERTopic/index.html"" id="""">What is BERTopic?</a></h3><p id="""">‍</p><p id="""">BERTopic is a sophisticated topic modeling technique that leverages the power of transformers and a variant of Term Frequency-Inverse Document Frequency (TF-IDF) called Class-based TF-IDF (c-TF-IDF). This combination allows BERTopic to create dense clusters of topics that are easily interpretable while maintaining the relevance of important words in the topic descriptions. Here is a detailed explanation of BERTopic and its key components:</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1800px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1800px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/664e370dd07f80a0d65329ea_Screenshot%202024-05-22%20234614.png"" loading=""lazy"" alt=""__wf_reserved_inherit"" width=""auto"" height=""auto"" id=""""></div></figure><p id="""">‍</p><h4 id="""">A Transformer Embedding Model</h4><p id="""">‍</p><p id="""">A transformer embedding model is a type of neural network architecture designed to generate high-quality, contextual representations of text. BERTopic supports several libraries for encoding our text to dense vector embeddings to capture contextual relationships between words in a document. We can use a suitable embedding model from one of the supported libraries, which includes Sentence Transformers, Flair, Spacy, Gensim, etc.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1400px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1400px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/664e35425a358e67d89bf5fc_sriJTkv6N1pJvznLaAE7-jzPYpkSyInz1h6ajtP56uxR4D1AHMvd3d-uacTsNnaK4nDiZeX4r3KnZfRwktLBsr0Fb0zW8qb9LG_aALii9sMIa25JI7WsdNi6i9xEW64d6XsGoXwr2cPrS2hJoHFWzmQ.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><h4 id="""">Dimensionality Reduction</h4><p id="""">‍</p><p id="""">Dimensionality reduction is a process in data analysis and machine learning that reduces the number of random variables under consideration. It involves transforming data from a high-dimensional space into a lower-dimensional space while preserving as much relevant information as possible. This technique is crucial for simplifying models, improving computational efficiency, and overcoming issues associated with high-dimensional data, such as the ""curse of dimensionality.""</p><p id="""">‍</p><p id="""">After building our embeddings, BERTopic compresses them into a lower-dimensional space to perform the clustering step effectively and visualize our data. BERTopic employs UMAP to perform the dimensionality reduction step.</p><p id="""">‍</p><p id="""">UMAP(Uniform Manifold Approximation and Projection) is a non-linear dimensionality reduction technique that preserves the local and global structure of the data, making it suitable for clustering.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:756px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""756px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/664e354205e8ba85fef7eae9_LbOJRXfyv6hyGsoDHVimlT4qB8R_HWff5iS4Azt-nmR-SRVBFgGOTc8Ezwwq5yKtS2jUky-h-tvZm1PAuo7lS0TVRTczhOGzro9cdBfccxsYcxY1fSMvZg2C83PpUheeLFRAACYslxyLA4RfOYVan2Y.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><h4 id="""">Clustering</h4><p id="""">‍</p><p id="""">Clustering is a technique used in data analysis and machine learning to group a set of objects in such a way that objects in the same group (or cluster) are more similar to each other than to those in other groups. It is an unsupervised learning method, meaning that it does not require labeled data to perform the grouping.</p><p id="""">‍</p><p id="""">In BERTopic, transformer models like BERT generate high-quality, contextual embeddings of text. These embeddings capture the semantic meaning of words and sentences. HDBSCAN is then applied to these embeddings to cluster documents into meaningful topics. By leveraging HDBSCAN’s strengths in handling varying densities, identifying noise, and not requiring a predefined number of clusters, BERTopic can produce robust and interpretable topics.</p><p id="""">‍</p><p id="""">For instance, when analyzing a large corpus of text such as customer reviews, BERTopic with HDBSCAN can effectively group similar reviews into topics like ""product quality,"" ""customer service,"" and ""delivery experience,"" while filtering out irrelevant or noisy data. This results in a more nuanced and actionable understanding of the data.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1017px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1017px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/664e3542acc41fa74d2128a9_Ye74yaDjRRfM1ivJ9of2_XahnUBaVun17T8FFQn-Vrbhkjegm0VTa3N_GwjrjXrz2YA5mDh0MvwjvIMjOYNd9VY3rgUzUi1YcMJjZBQgxvuMgoq-d7fDOKHr4QiRAu1svwd41lhSPXHFWRjCWPHuRto.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><h4 id="""">Topic Representation</h4><p id="""">‍</p><p id="""">Once the clusters (topics) are formed, BERTopic uses TF-IDF to extract the most representative words for each topic. TF-IDF scores words based on their frequency in a document relative to their frequency in the entire corpus, highlighting unique terms for each topic.</p><p id="""">‍</p><h3 id="""">Why should we use BERTopic over the traditional Topic Modeling algorithm?</h3><p id="""">‍</p><p id="""">BERTopic offers several advantages over traditional topic modeling algorithms like Latent Dirichlet Allocation (LDA) and Non-negative Matrix Factorization (NMF).</p><p id="""">‍</p><h4 id="""">Contextual Understanding</h4><p id="""">‍</p><p id="""">BERTopic captures the context of words using transformer-based models, unlike traditional methods such as LDA and NMF, which do not account for context. Transformers, like BERT, provide deep contextual embeddings by considering the position and relationship of words within a sentence. This leads to more accurate and meaningful topic extraction, as the model understands the nuanced meanings and relationships between words, resulting in higher-quality topics.</p><p id="""">‍</p><h4 id="""">High-Quality Embeddings</h4><p id="""">‍</p><p id="""">BERTopic uses dense, high-quality embeddings from BERT, offering better semantic representation than traditional sparse representations like TF-IDF. BERT embeddings capture the nuanced meaning and relationships between words, resulting in more coherent and semantically rich topics. This enhances the clustering of topics, making them more meaningful and easier to interpret.</p><p id="""">‍</p><h4 id="""">Flexibility and Customization</h4><p id="""">‍</p><p id="""">BERTopic allows integration with different embedding models and customization of dimensionality reduction and clustering techniques. This flexibility enables users to fine-tune the topic modeling process based on specific datasets and use cases. For example, one can choose between various transformers and dimensionality reduction methods, adapting the model to best fit the data's characteristics.</p><p id="""">‍</p><h4 id="""">Dynamic Topic Modeling</h4><p id="""">‍</p><p id="""">BERTopic can dynamically update topics as new data arrives, eliminating the need to retrain the entire model from scratch. This capability is especially beneficial for applications with continuously evolving data, such as real-time social media analysis, where topics need to remain current and reflect the latest trends without extensive computational overhead.</p><p id="""">‍</p><h4 id="""">Handling Short Texts</h4><p id="""">‍</p><p id="""">BERTopic excels in handling short texts, effectively capturing their meaning, while traditional methods like LDA and NMF often struggle with limited text. BERT’s contextual embeddings allow BERTopic to understand and cluster short texts, such as tweets or reviews, more accurately, ensuring that even brief documents are meaningfully categorized.</p><p id="""">‍</p><h4 id="""">Visualization and Insights</h4><p id="""">‍</p><p id="""">BERTopic offers integrated and powerful visualization tools, such as intertopic distance maps and bar charts, which enhance the interpretability and analysis of generated topics. These visualizations help users explore and understand the relationships between topics, making it easier to derive actionable insights and communicate findings effectively. Traditional methods often lack these advanced visualization capabilities.</p><p id="""">‍</p><h3 id="""">Using Whisper AI and BERTopic to model Youtube Videos</h3><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:746px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""746px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/664e3542ab8926534743a245_PB6xMwNa_C-gsLA9NLCVEdhAtZV28WtkUdWnoCnkp_L8fRyMKk32FpZLbSOvTWXNoAl7bwoUyWeYpIgOz_QbGIotpZrlSqLGFrR3t1CxzXwyQUUQ_-ZfkrUDY1ZUDUwK3MYWc5e_ZJ-ajLyLqS7Xycs.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><h4 id="""">Installation</h4><p id="""">‍</p><p id="""">Before diving into the actual code, it is important to install a few essential packages, namely Whisper, BERTopic, and Pytube. These packages provide crucial functionalities for our project and ensure the smooth implementation of various tasks.</p><p id="""">‍</p><p id="""">Whisper AI is a state-of-the-art speech recognition system developed by OpenAI. This advanced tool is designed to accurately convert spoken language into written text. It excels in providing high accuracy and robustness across different audio qualities, accents, and languages. Whisper AI’s capabilities make it an invaluable tool for tasks requiring precise transcription of audio content into text, accommodating diverse speech patterns and environments.</p><p id="""">‍</p><p id="""">The Pytube library is a Python package designed to facilitate the downloading of videos from YouTube. It provides a simple and intuitive interface for accessing YouTube content, extracting metadata from videos, and downloading video or audio streams in various formats. Pytube makes it easy to handle YouTube videos programmatically, offering functionalities like video search, resolution selection, and format conversion, which are essential for managing and utilizing online video content efficiently.</p><p id="""">‍</p><div data-rt-embed-type='true'><pre>
<code class=""language-py"">
!pip install bertopic
!pip install pytube
!pip install --upgrade git+https://github.com/openai/whisper.git
</code>
</pre></div><p id="""">‍</p><h4 id="""">Data Ingestion</h4><p id="""">‍</p><p id="""">We are performing topic modeling on YouTube videos from the 'Take U Forward' channel, specifically focusing on the Binary Search Playlist. To proceed with this task, we need to gather the video URLs from this playlist. This can be achieved using the YouTube Data API with an API key, which allows programmatic access to YouTube content, or by manually inputting some video URLs.</p><p id="""">‍</p><div data-rt-embed-type='true'><pre>
<code class=""language-py"">
video_urls = [ 
  'https://www.youtube.com/watch?v=MHf6awe89xw', 
  'https://www.youtube.com/watch?v=6zhGS79oQ4k', 
  'https://www.youtube.com/watch?v=hjR1IYVx9lY', 
  'https://www.youtube.com/watch?v=5qGrJbHhqFs', 
  'https://www.youtube.com/watch?v=w2G2W8l__pc'
  ]
</code>
</pre></div><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/664e354369f990fbb68a0bf4_TOwX35c4fSppG2KH1U5vSAMP3B9MNrTyopP75PHcc96c_5XMYNKf6o_LW1lPJIzKw8NWrmDnU1L7pg_rpKauiSMC3cfYXJoUhd7NX0J7OfhRZyy0xPOwgbVt3jUtVuXxM3ZHxVebyeIgKm-LXPtUIeA.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><p id="""">When we have our URLs, we can start downloading the videos and extracting the transcripts. To create those transcripts, we make use of the recently released Whisper.</p><p id="""">‍</p><p id="""">Below, we will import our Whisper model</p><p id="""">‍</p><div data-rt-embed-type='true'><pre>
<code class=""language-py"">
import whisper
whisper_model = whisper.load_model(""tiny"")‍
</code>
</pre></div><p id="""">‍</p><p id="""">Then, we iterate over our YouTube URLs, download the audio, and finally pass them through our Whisper model in order to generate the transcriptions.</p><p id="""">‍</p><p id="""">The YouTube(url) creates an instance of the YouTube class from the pytube library with the specified URL, granting access to the video's streams and metadata. The streams.filter(only_audio=True) method filters to only audio streams, and [0] selects the first audio stream from the filtered list. The download(filename=""audio.mp4"") method downloads this selected audio stream as ""audio.mp4"", with the path to the downloaded file stored in the variable path.</p><p id="""">‍</p><div data-rt-embed-type='true'><pre>
<code class=""language-py"">
from pytube import YouTube
docs = []

# Loop through the video URLs, transcribe them, and store in docs list
for url in video_urls:
	path = YouTube(url).streams.filter(only_audio=True)[0].download(filename=""audio.mp4"")  
	transcription = whisper_model.transcribe(path)  
  docs.append(transcription[""text""])

</code>
</pre></div><p id="""">‍</p><h4 id="""">Text Preprocessing</h4><p id="""">‍</p><p id="""">We have split our text based on full stops and question marks to increase our dataset, thereby improving the clustering for topic modeling</p><p id="""">‍</p><div data-rt-embed-type='true'><pre>
<code class=""language-py"">
def split_text_into_sentences(text):
    # Split text into sentences based on full stop and question mark
    import re
    chunks = re.split(r'(?<=[.?\n])\s+', text.strip())
    return chunks


texts = []


for doc in docs:
  chunks = split_text_into_sentences(doc)
  texts += chunks


print(len(texts))
# Output - 2162


</code>
</pre></div><p id="""">‍</p><h4 id="""">BERTopic Pipeline</h4><p id="""">These components form a comprehensive topic modeling pipeline. First, text documents are converted into dense embeddings and then reduced to a lower-dimensional space for easier clustering. Next, the reduced embeddings are clustered into topics, and the text is tokenized to create a matrix of token counts. Finally, the token counts are transformed into a class-based TF-IDF matrix to clearly represent topics.</p><p id="""">‍</p><div data-rt-embed-type='true'><pre>
<code class=""language-py"">

from umap import UMAP
from hdbscan import HDBSCAN
from sentence_transformers import SentenceTransformer
from sklearn.feature_extraction.text import CountVectorizer

from bertopic import BERTopic
from bertopic.representation import KeyBERTInspired
from bertopic.vectorizers import ClassTfidfTransformer

</code>
</pre></div><p id="""">‍</p><p id="""">First, we initialize a sentence embedding model using the <a href=""https://huggingface.co/sentence-transformers"" id="""">SentenceTransformer</a> library with the all-MiniLM-L6-v2 model. This model is used to convert sentences into numerical vectors. These embeddings capture the semantic meanings of the sentences, which can be used for comparison and clustering in later steps.</p><p id="""">‍</p><p id="""">Here, a UMAP (Uniform Manifold Approximation and Projection) model is set up to reduce the dimensionality of the high-dimensional embeddings from the previous step. This reduction makes the data easier to handle and visualize. The parameters define how the UMAP model behaves:</p><p id="""">‍</p><ul id=""""><li id="""">n_neighbors=5: The number of neighboring points used in manifold approximation.</li><li id="""">n_components=3: The number of dimensions to reduce the data to.</li><li id="""">min_dist=0.0: The minimum distance between points in low-dimensional space.</li><li id="""">metric='cosine': The metric used to measure distance in high-dimensional space, focusing on angles instead of Euclidean distance.</li></ul><p id="""">‍</p><p id="""">The clustering step involves clustering the dimensionally reduced embeddings using HDBSCAN, a density-based clustering algorithm. It helps to group the data into clusters based on density, with the following parameters:</p><p id="""">‍</p><ul id=""""><li id="""">min_cluster_size=5: The smallest size a cluster can be.</li><li id="""">metric='euclidean': The distance metric for clustering (in this case, standard Euclidean distance).</li><li id="""">cluster_selection_method='eom': The method for selecting clusters from the cluster hierarchy.</li><li id="""">prediction_data=True: Allows the model to predict which cluster new data points belong to.</li></ul><p id="""">‍</p><p id="""">A CountVectorizer is initialized to convert text data into a token count matrix, effectively tokenizing the text. It removes common English stop words to focus on more meaningful words. ClassTfidfTransformer is a modification of the TF-IDF approach to be more suited for topic modeling, emphasizing words that are more unique to each topic.</p><p id="""">‍</p><div data-rt-embed-type='true'><pre>
<code class=""language-py"">

# Step 1 - Extract embeddings
embedding_model = SentenceTransformer(""all-MiniLM-L6-v2"")

# Step 2 - Reduce dimensionality
umap_model = UMAP(n_neighbors=5, n_components=3, min_dist=0.0, metric='cosine')

# Step 3 - Cluster reduced embeddings
hdbscan_model = HDBSCAN(min_cluster_size=5, metric='euclidean', cluster_selection_method='eom', prediction_data=True)

# Step 4 - Tokenize topics
vectorizer_model = CountVectorizer(stop_words=""english"")

# Step 5 - Create topic representation
ctfidf_model = ClassTfidfTransformer()

# Step 6 - (Optional) Fine-tune topic representations with
# a `bertopic.representation` model
representation_model = KeyBERTInspired()

</code>
</pre></div><p id="""">‍</p><p id="""">Initializes a BERTopic model with specified embedding, dimensionality reduction, clustering, tokenization, and representation models. It then fits this model to a set of texts, producing topics and their associated probabilities.</p><p id="""">‍</p><div data-rt-embed-type='true'><pre>
<code class=""language-py"">

topic_model = BERTopic(
  embedding_model=embedding_model,          
  umap_model=umap_model,                    
  hdbscan_model=hdbscan_model,              
  vectorizer_model=vectorizer_model,        
  ctfidf_model=ctfidf_model,                
  representation_model=representation_model
)


topics, probs = topic_model.fit_transform(texts)

topic_model.get_topic_info()

</code>
</pre></div><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1448px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1448px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/664e3543ad1aa42b0d5725dd_2bnTKe14Co1zZaNjxhLI58IEi7j8SQaf9hwA6qE4clwdLZ8tZc3KBH39306L-Y9dv1PPeHfjpCTN87Tx1VTkdWfyRJ52wKMMuvyCtylmGcvy4Qc7VpYgpdOGewAkX1XWmySP63yuxMfek8e6jdI7Ntk.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><p id="""">topic_model.get_topic(0)</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:432px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""432px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/664e354227fb34f1f9ba641d_fe6ON2s0bB52ckmGqdh4eHFuUGiXQZTHmwa6F6dfyzPWzP2y3yavilcuZRNPLz5c1sIjynfJoZu-5t0VxSGT-evdow22m2gsY-CdbQozszmwRytiNq1t3XhslrmE_-cKwne3U5R7gpZ651kNuzbwREo.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><div data-rt-embed-type='true'><pre>
<code class=""language-py"">

import pandas as pd
df = pd.DataFrame({""topic"": topics, ""document"": texts})
df


</code>
</pre></div><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/664e35437ed8244753e96de0_9VeJ6uqJN2Egl5yIVj7KGpuSQqlzHfLVtWTjJ3K6Kb0TUI6BnRKsH5-7HwaX6cGrcPaUt42UOAeF0fMrem3mSgTkzls3JCWe3INd3emMLeBdVIHIdsKvRJQahsXssF0Z1jMVGv_1zEXtEl6pXQneSQ0.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><h4 id="""">Visualization</h4><p id="""">‍</p><p id="""">There are many visualization techniques, but some of the important are:</p><p id="""">‍</p><p id="""">Intertopic Distance Map:</p><p id="""">‍</p><p id="""">An Intertopic Distance Map is a visualization tool used in topic modeling to represent the relationships and distances between different topics in a two-dimensional space. This map helps to understand how similar or different the topics are to each other, providing valuable insights into the structure of the data.</p><p id="""">‍</p><div data-rt-embed-type='true'><pre>
<code class=""language-py"">

topic_model.visualize_topics()

</code>
</pre></div><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:650px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""650px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/664e3543e10ac48445216476_iDx9NHE5-KdvQSNUbNYbJAGRfxm_SJqJfGBuH7Qiy-06M84ujdNHypMnI4qGQEXh4LqawXVwUidFATN2qFrnKDx8BlxoUP6gil2Bx5AMF7yt9JNPX8r_wtwvcX6_iBs-O7os_q7t9TeftF53WYAbkYw.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><p id="""">Bar Chart:</p><p id="""">‍</p><p id="""">The bar chart tool in BERTopic is used to visualize the most frequent words within each topic, providing a clear representation of the key terms that define each topic. This visualization helps in understanding the essence of each topic by highlighting the top words associated with it.</p><div data-rt-embed-type='true'><pre>
<code class=""language-py"">

topic_model.visualize_barchart()

</code>
</pre></div><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1528px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1528px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/664e3542771831388faf597c_9fPopZntpHf6FFwxQ_Q3TmpjEhNSXaYp3vQ1UYNUKlDcqUJz1UzoE2rxa_1-6h21gwv7yKOjJfcSd97K-mIY6rD_5RCbz_tTmmv7zhLp_AekkO0p-g9dshdsiPXmYnS8taYu-rhoV27WqQxkGwSUfzQ.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">By analyzing the topic modeling results and identifying the most frequent words for Topic 0, we can confidently predict that the topic is centered around binary search algorithms in coding.</p><p id="""">‍</p><h2 id="""">Do you want to add Topic Modeling to your Application?</h2><p id="""">‍</p><p id="""">If you are looking to integrate Topic Modeling to your application, Mercity.ai can help. We specialize in developing highly tailored NLP solutions for various industries and business domains. <a href=""https://www.mercity.ai/contacts"" id="""">Contact us</a> today and let us create a Topic Modeling solution for your application.</p><p id="""">‍</p>",https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/664e3a20593febe70525d4e5_topicmodeling.png,Yash Vardhan,NLP and LLMs,Topic modeling is a strong tool for extracting insights from texts and any sort of content. In this blog we will see how to use BertTopic for podcast and video topic analysis.,False,"<div class=""rich-text w-richtext""><h2>What is Topic Modeling?</h2><p>Topic Modeling is like finding the main themes or subjects in a large collection of written documents without knowing what those themes are ahead of time. Imagine you have a big pile of books or articles, and you want to understand what they're generally about without reading each one in detail. Topic Modeling uses smart computer algorithms to do this.</p><p>‍</p><p>It is a type of statistical modeling that leverages unsupervised machine learning to analyze and identify clusters or groups of similar words within a body of text, thereby discovering hidden patterns and automatically identifying topics that exist within a text corpus.</p><p>‍</p><p>Some of the common algorithms to identify and extract topics from a collection of documents include Latent Dirichlet Allocation(LDA), Latent Semantic Analysis (LSA), Non-Negative Matrix Factorization(NMF), Correlated Topic Model (CTM), Top2Vec, BERTopic, etc.</p><p>‍</p><p>Each topic is represented as a distribution over words, and each document is represented as a distribution over topics. This allows for the extraction of the main themes from large sets of text data, making it easier to organize, search, and summarize the information.</p><p>‍</p><p>Topic modeling has numerous applications, including document clustering, organizing large document collections, improving information retrieval, and enhancing recommendations in various domains like academia, business, and social media. It provides insights that are not apparent through simple keyword searches, enabling a more nuanced understanding of the content.</p><p>‍</p><h3>Topic Modeling vs Topic Classification</h3><p>Topic Modeling utilizes unsupervised machine learning techniques. It does not require a pre-labeled dataset and can autonomously identify patterns within your text data. By analyzing word co-occurrences and distributions, Topic Modeling can reveal hidden themes and topics without any prior knowledge of the content. This process is particularly effective with high-quality, large datasets; the more data you feed into the model, the better it becomes at detecting and delineating the underlying topics. Larger datasets provide more context and variation, allowing the model to capture more subtle patterns and nuances in the text.</p><p>‍</p><p>Conversely, Topic Classification employs supervised machine learning. It relies on datasets that have been manually labeled to train the model. This method can work effectively with smaller datasets, provided that they are well-labeled and representative of the topics of interest. The manual labeling process creates a structured dataset that the algorithm can use to learn to classify new texts into predefined categories accurately.</p><p>‍</p><p>In terms of long-term effectiveness, teaching a machine to identify high-value words through text analysis—such as in Topic Classification—can be more strategic compared to the unsupervised approach of Topic Modeling. If you have a predefined list of topics and wish to label sets of texts like reviews or surveys quickly, a Topic Classification algorithm is more applicable. This approach allows for the automatic extraction of valuable insights from texts based on predefined categories, making it a practical solution for tasks that require quick and accurate topic identification.</p><p>‍</p><h2>Why should we do Topic Modeling?</h2><p>‍</p><p>Topic Modeling allows you to examine multiple topics and organize, understand, and summarize them on a large scale. It enables you to swiftly uncover hidden patterns within the data, providing insights that can inform data-driven decisions.</p><h3>Document Classification</h3><p>Topic modeling helps classify documents into predefined categories based on the topics they contain. For instance, a collection of news articles can be categorized into topics like politics, sports, technology, and health. The algorithm identifies the predominant themes in each document and assigns it to the most relevant category. This process automates and accelerates the classification of large volumes of text, making it easier to organize and retrieve information.</p><p>‍</p><p>Additionally, topic modeling enhances the accuracy and efficiency of document classification by reducing human error and bias. It can handle multilingual text and adapt to different domains without needing extensive manual adjustments. By continuously learning from new data, topic modeling algorithms can evolve and improve over time, ensuring that the classification remains relevant and up-to-date.</p><h3>Effortlessly Tag Customer Support Requests</h3><p>‍</p><p>Customer support teams receive numerous queries daily. Topic modeling can analyze these requests and automatically tag them with relevant topics such as billing issues, technical support, product inquiries, or service feedback. By categorizing requests, support teams can prioritize and route them to the appropriate departments or specialists, improving response times and customer satisfaction.</p><p>‍</p><p>Topic modeling enhances the accuracy of tagging by minimizing manual errors and ensuring consistency in categorization. This automated tagging system can also identify emerging trends or recurring issues, enabling support teams to proactively address common problems.</p><h3>Scaling Customer Feedback Analysis</h3><p>‍</p><p>Companies often collect vast amounts of feedback from customers through surveys, reviews, social media, and other channels. Topic modeling can process this feedback to identify recurring themes and sentiments, such as common complaints, product suggestions, or praise. This analysis helps businesses understand customer needs and preferences at scale, allowing them to make data-driven decisions to enhance products and services.</p><p>‍</p><p>Topic modeling allows businesses to detect shifts in customer sentiment over time, providing early warnings about potential issues or emerging trends. It can segment feedback by demographic or geographic factors, offering more nuanced insights into customer behavior. By automating the analysis of large-scale feedback, topic modeling frees up resources, enabling teams to focus on strategic initiatives rather than manual data processing.</p><h3>Crafting Content That Resonates</h3><p>‍</p><p>Content creators, marketers, and writers aim to produce content that engages their audience. Topic modeling can analyze existing content and audience interactions to identify trending topics and themes. By understanding what resonates with their audience, creators can tailor their content to match these interests, increasing engagement, and relevance. This approach ensures that the content is aligned with the audience’s preferences and needs.</p><p>‍</p><p>Topic modeling allows content creators to discover gaps in current content offerings, revealing opportunities for new and unique topics that have not yet been explored. It can also track changes in audience interests over time, helping creators to adapt their strategies accordingly. By analyzing feedback and interactions, such as comments, likes, and shares, topic modeling provides insights into the types of content that generate the most engagement.</p><h3>Understanding Employee Sentiments</h3><p>‍</p><p>Organizations often conduct employee surveys and collect feedback through various channels to gauge employee sentiment and workplace satisfaction. Topic modeling can analyze this data to uncover underlying themes and sentiments, such as concerns about work-life balance, management practices, or workplace culture. By understanding these sentiments, organizations can address issues, improve employee morale, and create a more positive work environment.</p><p>‍</p><p>Topic modeling enables organizations to track changes in employee sentiment over time, allowing them to measure the impact of implemented policies and initiatives. This analysis can segment feedback by department, tenure, or other relevant factors, providing a detailed understanding of specific groups' experiences within the organization.</p><h2>How Does Topic Modeling Work</h2><p>‍</p><p>Topic modeling is a method used in natural language processing (NLP) and text mining to uncover hidden patterns within a large collection of texts. It is an unsupervised machine-learning technique, meaning it does not require labeled data. Here’s a detailed explanation of how topic modeling works, particularly focusing on the most commonly used algorithm, Latent Dirichlet Allocation (LDA). The steps involved in Topic Modeling are:</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:589pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/664e3542d24e75199ca9f8f8_SCdVxmFhEvhFs0nKaPHVbExM7ygLaMlURW_XJrjGNYrg5c6Tlz296GOPof6keIqGQo-hhwbFyte7DvEiI-GVDbDbYXfSB5Dmgty_AbKK7cfnzvPTJlyUcJidmTa4FnsYcwTCoTdjnq4UfqXps1RYXMs.png""/></div></figure><h3>Text Preprocessing</h3><p>‍</p><p>Text Preprocessing is a vital step in preparing text data for modeling, particularly for algorithms like Latent Dirichlet Allocation (LDA), which is used for topic modeling. This process involves several steps aimed at cleaning and standardizing the text data to enhance the performance of the model. </p><p>‍</p><p>Tokenization is the process of breaking down text into individual units, usually words or phrases, called tokens. This step simplifies the text and makes it easier to analyze. For the sentence ""The cat sat on the mat,"" tokenization would result in [""The"", ""cat"", ""sat"", ""on"", ""the"", ""mat""]. Stop words are common words that carry little semantic meaning and are often removed to focus on the more significant words. Removing these words helps reduce the noise in the data. Words like ""the,"" ""is,"" and ""and"" are typically removed from the token list. For the tokenized example above, removing stop words might result in [""cat"", ""sat"", ""mat""].</p><p>‍</p><p>Lemmatization and stemming are both techniques used to reduce words to their base forms. Lemmatization reduces words to their base or dictionary form by considering the word's meaning and context, resulting in more accurate reductions such as ""better"" becoming ""good"" and ""ran"" becoming ""run."" On the other hand, stemming removes suffixes to reduce words to their root form, without considering context, which can lead to less accurate reductions. For example, ""running,"" ""runner,"" and ""ran"" might all be reduced to ""run."" While lemmatization ensures the base form is a proper word, stemming focuses on simplifying text for analysis.</p><h3>Model Initialization</h3><p>‍</p><p>The model initialization step in Latent Dirichlet Allocation (LDA) is crucial for setting the stage for the next (iterative) process. Initially, each word in each document is randomly assigned to one of the predefined numbers of topics. This random assignment provides a starting point for the iterative refinement process. Since LDA is a probabilistic model, starting with a random distribution allows the algorithm to explore the topic space effectively. Consider a document with the words [""cat"", ""sat"", ""mat""]. If we have three topics, the initial assignment might randomly assign ""cat"" to Topic 1, ""sat"" to Topic 2, and ""mat"" to Topic 3.</p><p>‍</p><p>This random assignment is purely an initial guess. The LDA algorithm will iteratively refine these assignments to discover the true underlying topic structure in the data. The random initialization helps ensure that the algorithm does not start with any biases and can explore a wide range of possible topic distributions.</p><p>‍</p><p>LDA not only adjusts which topic a word belongs to, but it also updates its understanding of what each topic is about. This dual updating process—adjusting word assignments and refining topic descriptions—helps LDA to accurately capture the main themes or topics within a collection of documents.</p><h3>Word-Topic Assignment</h3><p>‍</p><p>The word-topic assignment step in Latent Dirichlet Allocation (LDA) is a key component of the iterative process that refines the model to discover the underlying topics in a corpus (collection of documents). For each word in each document, the LDA algorithm reassigns the word to a topic based on the probability distribution that considers two factors:</p><p>‍</p><h4>Topic-Word Distribution</h4><p>The probability of the word given the topic. It indicates how strongly a word is associated with a particular topic. This is computed as the number of times a word is assigned to a topic across all documents, divided by the total number of words assigned to that topic. It is denoted by a symbol Φ.</p><p>‍</p><h4>Document-Topic Distribution</h4><p>The probability of the topic given the document. It reflects how strongly a topic is associated with a particular document. This is computed as the number of words in a document assigned to a topic, divided by the total number of words in a document. It is denoted by a symbol θ.</p><p>After several iterations, the algorithm converges, and the final topic and document distributions are calculated</p><h2>Topic Modeling for YouTube Videos using BERTopic</h2><h3><a href=""https://maartengr.github.io/BERTopic/index.html"">What is BERTopic?</a></h3><p>‍</p><p>BERTopic is a sophisticated topic modeling technique that leverages the power of transformers and a variant of Term Frequency-Inverse Document Frequency (TF-IDF) called Class-based TF-IDF (c-TF-IDF). This combination allows BERTopic to create dense clusters of topics that are easily interpretable while maintaining the relevance of important words in the topic descriptions. Here is a detailed explanation of BERTopic and its key components:</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1800pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/664e370dd07f80a0d65329ea_Screenshot%202024-05-22%20234614.png""/></div></figure><p>‍</p><h4>A Transformer Embedding Model</h4><p>‍</p><p>A transformer embedding model is a type of neural network architecture designed to generate high-quality, contextual representations of text. BERTopic supports several libraries for encoding our text to dense vector embeddings to capture contextual relationships between words in a document. We can use a suitable embedding model from one of the supported libraries, which includes Sentence Transformers, Flair, Spacy, Gensim, etc.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1400pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/664e35425a358e67d89bf5fc_sriJTkv6N1pJvznLaAE7-jzPYpkSyInz1h6ajtP56uxR4D1AHMvd3d-uacTsNnaK4nDiZeX4r3KnZfRwktLBsr0Fb0zW8qb9LG_aALii9sMIa25JI7WsdNi6i9xEW64d6XsGoXwr2cPrS2hJoHFWzmQ.png""/></div></figure><h4>Dimensionality Reduction</h4><p>‍</p><p>Dimensionality reduction is a process in data analysis and machine learning that reduces the number of random variables under consideration. It involves transforming data from a high-dimensional space into a lower-dimensional space while preserving as much relevant information as possible. This technique is crucial for simplifying models, improving computational efficiency, and overcoming issues associated with high-dimensional data, such as the ""curse of dimensionality.""</p><p>‍</p><p>After building our embeddings, BERTopic compresses them into a lower-dimensional space to perform the clustering step effectively and visualize our data. BERTopic employs UMAP to perform the dimensionality reduction step.</p><p>‍</p><p>UMAP(Uniform Manifold Approximation and Projection) is a non-linear dimensionality reduction technique that preserves the local and global structure of the data, making it suitable for clustering.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:756pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/664e354205e8ba85fef7eae9_LbOJRXfyv6hyGsoDHVimlT4qB8R_HWff5iS4Azt-nmR-SRVBFgGOTc8Ezwwq5yKtS2jUky-h-tvZm1PAuo7lS0TVRTczhOGzro9cdBfccxsYcxY1fSMvZg2C83PpUheeLFRAACYslxyLA4RfOYVan2Y.png""/></div></figure><p>‍</p><h4>Clustering</h4><p>‍</p><p>Clustering is a technique used in data analysis and machine learning to group a set of objects in such a way that objects in the same group (or cluster) are more similar to each other than to those in other groups. It is an unsupervised learning method, meaning that it does not require labeled data to perform the grouping.</p><p>‍</p><p>In BERTopic, transformer models like BERT generate high-quality, contextual embeddings of text. These embeddings capture the semantic meaning of words and sentences. HDBSCAN is then applied to these embeddings to cluster documents into meaningful topics. By leveraging HDBSCAN’s strengths in handling varying densities, identifying noise, and not requiring a predefined number of clusters, BERTopic can produce robust and interpretable topics.</p><p>‍</p><p>For instance, when analyzing a large corpus of text such as customer reviews, BERTopic with HDBSCAN can effectively group similar reviews into topics like ""product quality,"" ""customer service,"" and ""delivery experience,"" while filtering out irrelevant or noisy data. This results in a more nuanced and actionable understanding of the data.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1017pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/664e3542acc41fa74d2128a9_Ye74yaDjRRfM1ivJ9of2_XahnUBaVun17T8FFQn-Vrbhkjegm0VTa3N_GwjrjXrz2YA5mDh0MvwjvIMjOYNd9VY3rgUzUi1YcMJjZBQgxvuMgoq-d7fDOKHr4QiRAu1svwd41lhSPXHFWRjCWPHuRto.png""/></div></figure><h4>Topic Representation</h4><p>‍</p><p>Once the clusters (topics) are formed, BERTopic uses TF-IDF to extract the most representative words for each topic. TF-IDF scores words based on their frequency in a document relative to their frequency in the entire corpus, highlighting unique terms for each topic.</p><p>‍</p><h3>Why should we use BERTopic over the traditional Topic Modeling algorithm?</h3><p>‍</p><p>BERTopic offers several advantages over traditional topic modeling algorithms like Latent Dirichlet Allocation (LDA) and Non-negative Matrix Factorization (NMF).</p><p>‍</p><h4>Contextual Understanding</h4><p>‍</p><p>BERTopic captures the context of words using transformer-based models, unlike traditional methods such as LDA and NMF, which do not account for context. Transformers, like BERT, provide deep contextual embeddings by considering the position and relationship of words within a sentence. This leads to more accurate and meaningful topic extraction, as the model understands the nuanced meanings and relationships between words, resulting in higher-quality topics.</p><p>‍</p><h4>High-Quality Embeddings</h4><p>‍</p><p>BERTopic uses dense, high-quality embeddings from BERT, offering better semantic representation than traditional sparse representations like TF-IDF. BERT embeddings capture the nuanced meaning and relationships between words, resulting in more coherent and semantically rich topics. This enhances the clustering of topics, making them more meaningful and easier to interpret.</p><p>‍</p><h4>Flexibility and Customization</h4><p>‍</p><p>BERTopic allows integration with different embedding models and customization of dimensionality reduction and clustering techniques. This flexibility enables users to fine-tune the topic modeling process based on specific datasets and use cases. For example, one can choose between various transformers and dimensionality reduction methods, adapting the model to best fit the data's characteristics.</p><p>‍</p><h4>Dynamic Topic Modeling</h4><p>‍</p><p>BERTopic can dynamically update topics as new data arrives, eliminating the need to retrain the entire model from scratch. This capability is especially beneficial for applications with continuously evolving data, such as real-time social media analysis, where topics need to remain current and reflect the latest trends without extensive computational overhead.</p><p>‍</p><h4>Handling Short Texts</h4><p>‍</p><p>BERTopic excels in handling short texts, effectively capturing their meaning, while traditional methods like LDA and NMF often struggle with limited text. BERT’s contextual embeddings allow BERTopic to understand and cluster short texts, such as tweets or reviews, more accurately, ensuring that even brief documents are meaningfully categorized.</p><p>‍</p><h4>Visualization and Insights</h4><p>‍</p><p>BERTopic offers integrated and powerful visualization tools, such as intertopic distance maps and bar charts, which enhance the interpretability and analysis of generated topics. These visualizations help users explore and understand the relationships between topics, making it easier to derive actionable insights and communicate findings effectively. Traditional methods often lack these advanced visualization capabilities.</p><p>‍</p><h3>Using Whisper AI and BERTopic to model Youtube Videos</h3><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:746pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/664e3542ab8926534743a245_PB6xMwNa_C-gsLA9NLCVEdhAtZV28WtkUdWnoCnkp_L8fRyMKk32FpZLbSOvTWXNoAl7bwoUyWeYpIgOz_QbGIotpZrlSqLGFrR3t1CxzXwyQUUQ_-ZfkrUDY1ZUDUwK3MYWc5e_ZJ-ajLyLqS7Xycs.png""/></div></figure><p>‍</p><h4>Installation</h4><p>‍</p><p>Before diving into the actual code, it is important to install a few essential packages, namely Whisper, BERTopic, and Pytube. These packages provide crucial functionalities for our project and ensure the smooth implementation of various tasks.</p><p>‍</p><p>Whisper AI is a state-of-the-art speech recognition system developed by OpenAI. This advanced tool is designed to accurately convert spoken language into written text. It excels in providing high accuracy and robustness across different audio qualities, accents, and languages. Whisper AI’s capabilities make it an invaluable tool for tasks requiring precise transcription of audio content into text, accommodating diverse speech patterns and environments.</p><p>‍</p><p>The Pytube library is a Python package designed to facilitate the downloading of videos from YouTube. It provides a simple and intuitive interface for accessing YouTube content, extracting metadata from videos, and downloading video or audio streams in various formats. Pytube makes it easy to handle YouTube videos programmatically, offering functionalities like video search, resolution selection, and format conversion, which are essential for managing and utilizing online video content efficiently.</p><p>‍</p><div class=""w-embed""><pre>
<code class=""language-py"">
!pip install bertopic
!pip install pytube
!pip install --upgrade git+https://github.com/openai/whisper.git
</code>
</pre></div><p>‍</p><h4>Data Ingestion</h4><p>‍</p><p>We are performing topic modeling on YouTube videos from the 'Take U Forward' channel, specifically focusing on the Binary Search Playlist. To proceed with this task, we need to gather the video URLs from this playlist. This can be achieved using the YouTube Data API with an API key, which allows programmatic access to YouTube content, or by manually inputting some video URLs.</p><p>‍</p><div class=""w-embed""><pre>
<code class=""language-py"">
video_urls = [ 
  'https://www.youtube.com/watch?v=MHf6awe89xw', 
  'https://www.youtube.com/watch?v=6zhGS79oQ4k', 
  'https://www.youtube.com/watch?v=hjR1IYVx9lY', 
  'https://www.youtube.com/watch?v=5qGrJbHhqFs', 
  'https://www.youtube.com/watch?v=w2G2W8l__pc'
  ]
</code>
</pre></div><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/664e354369f990fbb68a0bf4_TOwX35c4fSppG2KH1U5vSAMP3B9MNrTyopP75PHcc96c_5XMYNKf6o_LW1lPJIzKw8NWrmDnU1L7pg_rpKauiSMC3cfYXJoUhd7NX0J7OfhRZyy0xPOwgbVt3jUtVuXxM3ZHxVebyeIgKm-LXPtUIeA.png""/></div></figure><p>‍</p><p>When we have our URLs, we can start downloading the videos and extracting the transcripts. To create those transcripts, we make use of the recently released Whisper.</p><p>‍</p><p>Below, we will import our Whisper model</p><p>‍</p><div class=""w-embed""><pre>
<code class=""language-py"">
import whisper
whisper_model = whisper.load_model(""tiny"")‍
</code>
</pre></div><p>‍</p><p>Then, we iterate over our YouTube URLs, download the audio, and finally pass them through our Whisper model in order to generate the transcriptions.</p><p>‍</p><p>The YouTube(url) creates an instance of the YouTube class from the pytube library with the specified URL, granting access to the video's streams and metadata. The streams.filter(only_audio=True) method filters to only audio streams, and [0] selects the first audio stream from the filtered list. The download(filename=""audio.mp4"") method downloads this selected audio stream as ""audio.mp4"", with the path to the downloaded file stored in the variable path.</p><p>‍</p><div class=""w-embed""><pre>
<code class=""language-py"">
from pytube import YouTube
docs = []

# Loop through the video URLs, transcribe them, and store in docs list
for url in video_urls:
	path = YouTube(url).streams.filter(only_audio=True)[0].download(filename=""audio.mp4"")  
	transcription = whisper_model.transcribe(path)  
  docs.append(transcription[""text""])

</code>
</pre></div><p>‍</p><h4>Text Preprocessing</h4><p>‍</p><p>We have split our text based on full stops and question marks to increase our dataset, thereby improving the clustering for topic modeling</p><p>‍</p><div class=""w-embed""><pre>
<code class=""language-py"">
def split_text_into_sentences(text):
    # Split text into sentences based on full stop and question mark
    import re
    chunks = re.split(r'(?&lt;=[.?\n])\s+', text.strip())
    return chunks


texts = []


for doc in docs:
  chunks = split_text_into_sentences(doc)
  texts += chunks


print(len(texts))
# Output - 2162


</code>
</pre></div><p>‍</p><h4>BERTopic Pipeline</h4><p>These components form a comprehensive topic modeling pipeline. First, text documents are converted into dense embeddings and then reduced to a lower-dimensional space for easier clustering. Next, the reduced embeddings are clustered into topics, and the text is tokenized to create a matrix of token counts. Finally, the token counts are transformed into a class-based TF-IDF matrix to clearly represent topics.</p><p>‍</p><div class=""w-embed""><pre>
<code class=""language-py"">

from umap import UMAP
from hdbscan import HDBSCAN
from sentence_transformers import SentenceTransformer
from sklearn.feature_extraction.text import CountVectorizer

from bertopic import BERTopic
from bertopic.representation import KeyBERTInspired
from bertopic.vectorizers import ClassTfidfTransformer

</code>
</pre></div><p>‍</p><p>First, we initialize a sentence embedding model using the <a href=""https://huggingface.co/sentence-transformers"">SentenceTransformer</a> library with the all-MiniLM-L6-v2 model. This model is used to convert sentences into numerical vectors. These embeddings capture the semantic meanings of the sentences, which can be used for comparison and clustering in later steps.</p><p>‍</p><p>Here, a UMAP (Uniform Manifold Approximation and Projection) model is set up to reduce the dimensionality of the high-dimensional embeddings from the previous step. This reduction makes the data easier to handle and visualize. The parameters define how the UMAP model behaves:</p><p>‍</p><ul role=""list""><li>n_neighbors=5: The number of neighboring points used in manifold approximation.</li><li>n_components=3: The number of dimensions to reduce the data to.</li><li>min_dist=0.0: The minimum distance between points in low-dimensional space.</li><li>metric='cosine': The metric used to measure distance in high-dimensional space, focusing on angles instead of Euclidean distance.</li></ul><p>‍</p><p>The clustering step involves clustering the dimensionally reduced embeddings using HDBSCAN, a density-based clustering algorithm. It helps to group the data into clusters based on density, with the following parameters:</p><p>‍</p><ul role=""list""><li>min_cluster_size=5: The smallest size a cluster can be.</li><li>metric='euclidean': The distance metric for clustering (in this case, standard Euclidean distance).</li><li>cluster_selection_method='eom': The method for selecting clusters from the cluster hierarchy.</li><li>prediction_data=True: Allows the model to predict which cluster new data points belong to.</li></ul><p>‍</p><p>A CountVectorizer is initialized to convert text data into a token count matrix, effectively tokenizing the text. It removes common English stop words to focus on more meaningful words. ClassTfidfTransformer is a modification of the TF-IDF approach to be more suited for topic modeling, emphasizing words that are more unique to each topic.</p><p>‍</p><div class=""w-embed""><pre>
<code class=""language-py"">

# Step 1 - Extract embeddings
embedding_model = SentenceTransformer(""all-MiniLM-L6-v2"")

# Step 2 - Reduce dimensionality
umap_model = UMAP(n_neighbors=5, n_components=3, min_dist=0.0, metric='cosine')

# Step 3 - Cluster reduced embeddings
hdbscan_model = HDBSCAN(min_cluster_size=5, metric='euclidean', cluster_selection_method='eom', prediction_data=True)

# Step 4 - Tokenize topics
vectorizer_model = CountVectorizer(stop_words=""english"")

# Step 5 - Create topic representation
ctfidf_model = ClassTfidfTransformer()

# Step 6 - (Optional) Fine-tune topic representations with
# a `bertopic.representation` model
representation_model = KeyBERTInspired()

</code>
</pre></div><p>‍</p><p>Initializes a BERTopic model with specified embedding, dimensionality reduction, clustering, tokenization, and representation models. It then fits this model to a set of texts, producing topics and their associated probabilities.</p><p>‍</p><div class=""w-embed""><pre>
<code class=""language-py"">

topic_model = BERTopic(
  embedding_model=embedding_model,          
  umap_model=umap_model,                    
  hdbscan_model=hdbscan_model,              
  vectorizer_model=vectorizer_model,        
  ctfidf_model=ctfidf_model,                
  representation_model=representation_model
)


topics, probs = topic_model.fit_transform(texts)

topic_model.get_topic_info()

</code>
</pre></div><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1448pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/664e3543ad1aa42b0d5725dd_2bnTKe14Co1zZaNjxhLI58IEi7j8SQaf9hwA6qE4clwdLZ8tZc3KBH39306L-Y9dv1PPeHfjpCTN87Tx1VTkdWfyRJ52wKMMuvyCtylmGcvy4Qc7VpYgpdOGewAkX1XWmySP63yuxMfek8e6jdI7Ntk.png""/></div></figure><p>‍</p><p>topic_model.get_topic(0)</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:432pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/664e354227fb34f1f9ba641d_fe6ON2s0bB52ckmGqdh4eHFuUGiXQZTHmwa6F6dfyzPWzP2y3yavilcuZRNPLz5c1sIjynfJoZu-5t0VxSGT-evdow22m2gsY-CdbQozszmwRytiNq1t3XhslrmE_-cKwne3U5R7gpZ651kNuzbwREo.png""/></div></figure><p>‍</p><div class=""w-embed""><pre>
<code class=""language-py"">

import pandas as pd
df = pd.DataFrame({""topic"": topics, ""document"": texts})
df


</code>
</pre></div><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/664e35437ed8244753e96de0_9VeJ6uqJN2Egl5yIVj7KGpuSQqlzHfLVtWTjJ3K6Kb0TUI6BnRKsH5-7HwaX6cGrcPaUt42UOAeF0fMrem3mSgTkzls3JCWe3INd3emMLeBdVIHIdsKvRJQahsXssF0Z1jMVGv_1zEXtEl6pXQneSQ0.png""/></div></figure><p>‍</p><h4>Visualization</h4><p>‍</p><p>There are many visualization techniques, but some of the important are:</p><p>‍</p><p>Intertopic Distance Map:</p><p>‍</p><p>An Intertopic Distance Map is a visualization tool used in topic modeling to represent the relationships and distances between different topics in a two-dimensional space. This map helps to understand how similar or different the topics are to each other, providing valuable insights into the structure of the data.</p><p>‍</p><div class=""w-embed""><pre>
<code class=""language-py"">

topic_model.visualize_topics()

</code>
</pre></div><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:650pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/664e3543e10ac48445216476_iDx9NHE5-KdvQSNUbNYbJAGRfxm_SJqJfGBuH7Qiy-06M84ujdNHypMnI4qGQEXh4LqawXVwUidFATN2qFrnKDx8BlxoUP6gil2Bx5AMF7yt9JNPX8r_wtwvcX6_iBs-O7os_q7t9TeftF53WYAbkYw.png""/></div></figure><p>‍</p><p>Bar Chart:</p><p>‍</p><p>The bar chart tool in BERTopic is used to visualize the most frequent words within each topic, providing a clear representation of the key terms that define each topic. This visualization helps in understanding the essence of each topic by highlighting the top words associated with it.</p><div class=""w-embed""><pre>
<code class=""language-py"">

topic_model.visualize_barchart()

</code>
</pre></div><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1528pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/664e3542771831388faf597c_9fPopZntpHf6FFwxQ_Q3TmpjEhNSXaYp3vQ1UYNUKlDcqUJz1UzoE2rxa_1-6h21gwv7yKOjJfcSd97K-mIY6rD_5RCbz_tTmmv7zhLp_AekkO0p-g9dshdsiPXmYnS8taYu-rhoV27WqQxkGwSUfzQ.png""/></div></figure><p>By analyzing the topic modeling results and identifying the most frequent words for Topic 0, we can confidently predict that the topic is centered around binary search algorithms in coding.</p><p>‍</p><h2>Do you want to add Topic Modeling to your Application?</h2><p>‍</p><p>If you are looking to integrate Topic Modeling to your application, Mercity.ai can help. We specialize in developing highly tailored NLP solutions for various industries and business domains. <a href=""https://www.mercity.ai/contacts"">Contact us</a> today and let us create a Topic Modeling solution for your application.</p><p>‍</p></div>"
Understanding and Training IP Adapters for Diffusion Models,understanding-and-training-ip-adapters-for-diffusion-models,640f56f76d313b2faa631c11,68c5a129c9308bc876315a4f,False,False,Sat Sep 13 2025 16:51:53 GMT+0000 (Coordinated Universal Time),Sat Sep 13 2025 16:51:53 GMT+0000 (Coordinated Universal Time),Sat Sep 13 2025 16:51:53 GMT+0000 (Coordinated Universal Time),"<p id="""">Text prompts alone are not very good at explaining visual concepts and styles. You can imagine trying to explain Michelangelo's art through words; it's simply not possible. We humans don’t think in terms of words when we imagine images. The same goes for the diffusion models.</p><p id="""">Diffusion models can generate amazing images from text prompts alone, but sometimes that's not enough. Image inputs are needed to properly follow a very specific style, which cannot be properly conveyed in text. IP-Adapter allows diffusion models to accept images as a part of their prompts, which is not directly possible, as diffusion models are trained as text-to-image only.&nbsp;&nbsp;</p><h2 id="""">What is IP-Adapter&nbsp;</h2><p id="""">IP-Adapter is a small, trainable add-on that lets a text-to-image diffusion model accept pictures as well as words. Normally, you type something like “a fluffy cat on a chair” and hope the model generates the exact stripes, fur texture, and velvet glow you had in mind. But the model will not do that because text alone often isn’t precise enough. The model might give you a different fur pattern, the wrong chair, or completely miss the mood you imagined. You can try improving the prompt, but it’ll be very difficult to get the output to match your imagination, if not impossible.</p><p id="""">That's where the IP adapter helps. The adapter takes your reference image, extracts the key details using an image encoder, and feeds them directly into the diffusion process. It translates the visual language of images into a form the model already understands, so the generated result matches both your text and your picture.</p><p id="""">So, unlike heavy fine-tuning or ControlNets, IP-Adapter is lightweight. You can plug it into existing diffusion models without breaking their text-to-image ability.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c5a129c9308bc876315a3d_d234e195.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><h3 id="""">Other Ways to Use Images in Diffusion Models</h3><p id="""">There are obviously other ways to sneak image information into the pipeline. Others specialize in style transfer or identity preservation, which lets the model mimic a painting style or reproduce a specific person across scenes.</p><p id="""">‍</p><p id="""">Various methods specialize in style transfer, structural control, or identity preservation, which allows the models to mimic painting styles, maintain specific compositions, or reproduce particular subjects across scenes.</p><ul id=""""><li id=""""><strong id="""">ControlNet</strong>: It focuses on structural control (poses, edges, and depth) rather than style and content. Even though it's great for maintaining specific compositions, it doesn't capture artistic styles or textures as effectively.</li></ul><ul id=""""><li id=""""><strong id="""">Full Fine-tuning</strong>: It requires retraining the entire model on specific styles or concepts, which is computationally expensive and can overwrite existing knowledge.</li></ul><ul id=""""><li id=""""><strong id="""">LoRA: </strong>LoRA<strong id=""""> </strong>adapts models to learn specific subjects or styles, but you still need to craft detailed text descriptions, and it often stumbles when handling complex artistic styles or generalizing to new scenarios.</li></ul><h2 id="""">​​Primer on diffusion</h2><p id="""">To understand how the IP Adapter works, you first need to revisit how diffusion models generate images and where text and image information get plugged in. You can think of a diffusion model as having several key components where information flows through different processing stages to create the final image.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c5a129c9308bc876315a3a_4562473d.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">Diffusion models create images out of random chaos; they begin with freshly sampled Gaussian noise, then iteratively remove it under the guidance of a text prompt. With each step the network predicts a slightly cleaner image, nudging pixels toward the described scene. Here's how the core components interact:</p><ul id=""""><li id=""""><strong id="""">CLIP Text Encoder</strong>: This converts your text prompt into mathematical representations called embeddings. Since CLIP was trained on millions of image-text pairs, these embeddings don’t just capture word meanings; they also carry an understanding of what those words should look like visually.</li><li id=""""><strong id="""">U-Net: </strong>It is the main component that generates the image. It starts with random noise in latent space (a compressed version of the image, for example, 64×64×4 instead of 512×512×3) and, at each step, predicts the noise in this latent while using the text embeddings as guidance.</li><li id=""""><strong id="""">Sampler: </strong>Takes the U-Net’s noise prediction and, using a numerical rule (for example, DDIM), subtracts part of the noise and updates the latent from 𝑧 𝑡 → z t−1 to gradually remove noise, step by step.</li><li id=""""><strong id="""">VAE Decoder: </strong>Once denoising is complete, you have a clean latent 𝑧₀. The VAE decoder converts this latent back into a full-resolution pixel image. It basically acts as an image translator.</li></ul><h2 id="""">How IP adapter works&nbsp;</h2><p id="""">The key to IP-Adapter's functionality lies in understanding how cross-attention layers operate within the U-Net architecture.&nbsp;</p><h3 id="""">Cross-Attention in IP adapter</h3><p id="""">Cross attention allows one sequence or modality to focus on and extract information from another. It connects your text embeddings and the developing image. Unlike self-attention, where the queries, keys, and values all come from the same source, in cross-attention the queries (Q) are taken from one sequence (for example, a developing image’s latent features), while the keys (K) and values (V) come from another sequence, such as text embedding.</p><p id="""">At each denoising step, this mechanism determines which words from your prompt should influence which regions of the image. However, this text-to-image attention works perfectly for text prompts but is designed only for text embeddings.</p><p id="""">IP-Adapter extends this by introducing dual cross-attention branches: the original text cross-attention, where K and V come from text embeddings, and a new image cross-attention, where K and V come from image encoder features instead. This allows the model to simultaneously pay attention to both your text prompt and visual references, creating a more comprehensive understanding of what you want to generate.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1586px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1586px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c5a129c9308bc876315a40_154371dd.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><h3 id="""">IP-Adapter's Decoupled Attention</h3><p id="""">‍</p><p id="""">This decoupled approach creates two distinct but complementary pathways that work together to produce more precise and controllable image generation. The unreasonable effectiveness of decoupling comes from the fact that when you pass image features into text attention, you force the model to find a common representation space (solving the alignment fallacy) since text and image features carry very different types of information. Keeping these separate, the IP adapter lets each modality contribute its strengths without interference.</p><p id="""">‍</p><p id=""""><strong id="""">Pathway 1: Semantic Control (Original Text Cross-Attention)</strong></p><p id="""">‍</p><p id="""">The first cross-attention mechanism maintains Stable Diffusion's original strength in semantic control. This pathway tells the U-Net what to generate by handling object identity, recognition, scene composition, layout, and a kind of high-level conceptual understanding.</p><p id="""">‍</p><p id=""""><strong id="""">Pathway 2: Visual Style Control (Image Cross-Attention)</strong></p><p id="""">‍</p><p id="""">The second cross-attention mechanism introduces an entirely new visual control path that tells the U-Net how the output should look. This pathway focuses on visual style and aesthetic qualities, texture details and surface properties, and color schemes and artistic techniques. It also captures compositional elements like visual hierarchy and more complex artistic concepts such as the metamorphosis and transformation of familiar objects.</p><h3 id="""">Training Process</h3><p id="""">The training process of IP adapters is straightforward. Rather than updating the entire model architecture, the training process employs a selective parameter strategy that maximizes efficiency while preserving the foundational capabilities of the pre-trained diffusion model.</p><p id="""">‍</p><h4 id="""">Selective Parameter Training Architecture</h4><p id="""">The training process strategically freezes the majority of model parameters, specifically the pre-trained U-Net diffusion, CLIP text encoder, and CLIP image encoder, while exclusively training the newly introduced components. This includes approximately 22 million parameters,&nbsp; which includes the image cross-attention layers, linear projection modules that map image embeddings to the appropriate dimensional space, and associated LayerNorm layers for training stability. This approach makes sure only 3-5% of the total model parameters undergo training, which dramatically reduces computational requirements.</p><p id="""">‍</p><h4 id="""">Training Pipeline</h4><p id="""">The training pipeline begins with data preparation, where reference images are processed through the frozen CLIP image encoder to generate image embeddings, while corresponding captions are encoded via the CLIP text encoder. During each training iteration, Gaussian noise is added to target images at randomly sampled timesteps, creating the noisy latents that serve as input to the U-Net.&nbsp;</p><p id="""">‍</p><p id="""">The forward pass incorporates both conditioning modalities through parallel cross-attention mechanisms: text cross-attention operates as Attention(Q=latent_features, K=text_embeddings, V=text_embeddings), while the new image cross-attention functions as Attention(Q=latent_features, K=image_embeddings, V=image_embeddings).&nbsp;</p><p id="""">‍</p><p id="""">The optimization process minimizes L2 loss between the model's noise predictions and ground truth noise, with gradients updating only the trainable adapter parameters while leaving the frozen base model untouched.</p><h2 id="""">Training your own IP adapter</h2><p id="""">We prepare a dataset of images (and captions) that represent the kinds of visual content and styles you want the adapter to handle. Then we fine-tune using the standard diffusion noise-prediction objective (L2 on predicted noise).</p><p id="""">‍</p><p id="""">This lightweight approach lets you specialize in domains like architectural photography or specific artistic styles without computational overhead or risk of degrading the base model's capabilities.</p><p id="""">‍</p><p id="""">You can get started by trying out these code snippets below for training an ip adapter</p><pre id=""""></pre><p id="""">‍</p><p id="""">Then download accelerate using</p><pre id=""""></pre><p id="""">‍</p><p id="""">You also need to make your own dataset into a json file.</p><p id="""">Here’s a helper script to produce a data.json with default captions as filenames.</p><pre id=""""></pre><p id="""">Then set image_encoder_path path to a pretrained <strong id="""">image encoder</strong> (e.g., CLIP-ViT) and&nbsp;</p><p id="""">And image_path folder to where the images in your dataset are stored.</p><p id="""">‍</p><p id="""">Then run this script below, it trains an IP adapter that connects an image encoder (like CLIP) with stable diffusion v1.5 so the diffusion model can be conditioned on both text prompts and reference images.</p><pre id=""""></pre><p id="""">‍</p><p id="""">You can get started by trying out these code snippets below for testing out these pretrained IP adapters models in Google Colab for inference.</p><p id="""">‍</p><p id="""">Here are the codes below<br><br></p><pre id=""""></pre><p id="""">‍</p><pre id=""""></pre><p id="""">‍</p><p id=""""><strong id="""">Inference code block:</strong></p><pre id=""""></pre><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1162px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1162px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c5a129c9308bc876315a43_ee813462.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><h2 id="""">IP-Adapter vs. Other Methods</h2><p id="""">IP-Adapter is usually orders of magnitude cheaper/faster to get working than full fine-tuning and somewhat cheaper than training a ControlNet while giving better style transfer than naive adapters and far better composability than full fine-tuning.</p><p id="""">‍</p><p id=""""><strong id="""">Full Fine-Tuning: </strong>This means retraining the entire diffusion model on a specific dataset. It needs cloud GPU and tons of data, and the model loses its text-prompt ability because of facing issues like<strong id=""""> </strong>catastrophic forgetting, where generalization erodes as the model forgets prior knowledge, and after fine-tuning we can’t combine it with other tricks (like ControlNet) without retraining. Some research also showed that these models often “forget” general, broad features across the reverse process when fine-tuned on a specific domain, especially in earlier (closer to raw noise) steps. It’s also called the chain of forgetting, which harms the generalization.</p><p id="""">‍</p><p id=""""><strong id="""">ControlNet:</strong> It was originally designed for low-level structural control (edges, depth, pose, etc.). Issues with ControlNet come because it doesn’t inherently convey style or content from a reference image. Strong ControlNet weights can also completely dominate the model, producing stiff or similar outputs that lack natural variation. There has been some research in combining IP-adapter with Depth ControlNet to preserve structure while restyling an image.</p><h2 id="""">Challenges of using IP Adapter</h2><p id=""""><strong id="""">Feature Mixing</strong>: Sometimes IP-Adapter picks up unintended elements from your reference image. If you want just the lighting style from a portrait, you might accidentally get the person's clothing or background elements too.</p><p id="""">‍</p><p id=""""><strong id="""">Complex Compositions</strong>: When your reference image has multiple distinct elements, IP-Adapter might struggle to understand which aspects you actually want to transfer to the new generation.</p><p id="""">‍</p><p id=""""><strong id="""">Style-Content Entanglement</strong>: IP-Adapter sometimes struggles to separate artistic style from subject matter, making it challenging to extract pure aesthetic qualities without copying content elements.</p><p id="""">‍</p><p id=""""><strong id="""">Limited Semantic Control</strong>: Unlike text prompts, you can't precisely specify which aspects of an image to emphasize or ignore.</p><h3 id="""">Alternative and Specialized Methods</h3><p id="""">‍</p><p id=""""><strong id="""">DreamBooth</strong>: When you need perfect identity preservation across multiple images, DreamBooth fine-tunes the entire model to learn specific subjects through custom tokens. More computationally expensive but offers precise control over character consistency.</p><p id="""">‍</p><p id=""""><strong id="""">Textual Inversion</strong>: Creates custom embedding tokens for specific concepts or styles, allowing precise text-based control over learned visual elements. Lightweight but limited to concepts that can be tokenized.</p><p id=""""><strong id="""">ControlNet + IP-Adapter Combination</strong>: Many practitioners combine both—ControlNet handles structural elements (pose, composition), while IP-Adapter manages aesthetic qualities (style, lighting, mood).</p><p id="""">We talk about these methods in our <a href=""https://www.mercity.ai/blog-post/use-stable-diffusion-to-generate-product-images"" id="""">Product Image Generation</a> Blog.</p><p id="""">‍</p><h2 id="""">Real-World Applications</h2><p id="""">It's used a lot in real-world use cases where preserving fine visual detail while staying compatible with large pre-trained text-to-image models matters.</p><p id="""">‍</p><p id="""">1. Identity preservation: IP adapter variants like IP-Adapter-FaceID focus on creating highly realistic and consistent depictions of individuals across a wide array of contexts and styles.</p><p id="""">2. Multi-Subject Style Transfer: The ICAS (IP-Adapter and ControlNet-based Attention Structure) framework is recent research towards creating more complex and coherent stylized scenes.</p><p id="""">3. Video / identity-preserving video:- Researchers have adopted Image-conditioning ideas (IP-Adapter style) to video diffusion so a reference image is preserved across frames while motion is generated</p><p id="""">4. Spatial/3D cues via depth &amp; maps: Researchers also combine IP-Adapter conditioning with depth maps/multi-view cues so the adapter encodes explicit 3D layout, which improves 3D placement, occlusion handling, and realism.</p><p id="""">‍</p><h2 id="""">CTA</h2><p id="""">Looking to train stable diffusion models on your images? Please <a href=""https://www.mercity.ai/contacts"" id="""">reach out to Mercity</a> to build stable diffusion based products. We have done a lot of research in diffusion based image generation and also around optimizing diffusion models.</p><p id="""">‍</p>",https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c59d0d8051364d2bc42326_Screenshot%202025-09-13%20at%2019.57.31.png,Mayank,Stable Diffusion,"Learn how IP-Adapter enhances diffusion models by enabling image prompts. Explore its architecture, training process, and comparison to ControlNet and LoRA.",False,"<div class=""rich-text w-richtext""><p>Text prompts alone are not very good at explaining visual concepts and styles. You can imagine trying to explain Michelangelo's art through words; it's simply not possible. We humans don’t think in terms of words when we imagine images. The same goes for the diffusion models.</p><p>Diffusion models can generate amazing images from text prompts alone, but sometimes that's not enough. Image inputs are needed to properly follow a very specific style, which cannot be properly conveyed in text. IP-Adapter allows diffusion models to accept images as a part of their prompts, which is not directly possible, as diffusion models are trained as text-to-image only.  </p><h2>What is IP-Adapter </h2><p>IP-Adapter is a small, trainable add-on that lets a text-to-image diffusion model accept pictures as well as words. Normally, you type something like “a fluffy cat on a chair” and hope the model generates the exact stripes, fur texture, and velvet glow you had in mind. But the model will not do that because text alone often isn’t precise enough. The model might give you a different fur pattern, the wrong chair, or completely miss the mood you imagined. You can try improving the prompt, but it’ll be very difficult to get the output to match your imagination, if not impossible.</p><p>That's where the IP adapter helps. The adapter takes your reference image, extracts the key details using an image encoder, and feeds them directly into the diffusion process. It translates the visual language of images into a form the model already understands, so the generated result matches both your text and your picture.</p><p>So, unlike heavy fine-tuning or ControlNets, IP-Adapter is lightweight. You can plug it into existing diffusion models without breaking their text-to-image ability.</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c5a129c9308bc876315a3d_d234e195.png""/></div></figure><p>‍</p><h3>Other Ways to Use Images in Diffusion Models</h3><p>There are obviously other ways to sneak image information into the pipeline. Others specialize in style transfer or identity preservation, which lets the model mimic a painting style or reproduce a specific person across scenes.</p><p>‍</p><p>Various methods specialize in style transfer, structural control, or identity preservation, which allows the models to mimic painting styles, maintain specific compositions, or reproduce particular subjects across scenes.</p><ul role=""list""><li><strong>ControlNet</strong>: It focuses on structural control (poses, edges, and depth) rather than style and content. Even though it's great for maintaining specific compositions, it doesn't capture artistic styles or textures as effectively.</li></ul><ul role=""list""><li><strong>Full Fine-tuning</strong>: It requires retraining the entire model on specific styles or concepts, which is computationally expensive and can overwrite existing knowledge.</li></ul><ul role=""list""><li><strong>LoRA: </strong>LoRA<strong> </strong>adapts models to learn specific subjects or styles, but you still need to craft detailed text descriptions, and it often stumbles when handling complex artistic styles or generalizing to new scenarios.</li></ul><h2>​​Primer on diffusion</h2><p>To understand how the IP Adapter works, you first need to revisit how diffusion models generate images and where text and image information get plugged in. You can think of a diffusion model as having several key components where information flows through different processing stages to create the final image.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c5a129c9308bc876315a3a_4562473d.png""/></div></figure><p>Diffusion models create images out of random chaos; they begin with freshly sampled Gaussian noise, then iteratively remove it under the guidance of a text prompt. With each step the network predicts a slightly cleaner image, nudging pixels toward the described scene. Here's how the core components interact:</p><ul role=""list""><li><strong>CLIP Text Encoder</strong>: This converts your text prompt into mathematical representations called embeddings. Since CLIP was trained on millions of image-text pairs, these embeddings don’t just capture word meanings; they also carry an understanding of what those words should look like visually.</li><li><strong>U-Net: </strong>It is the main component that generates the image. It starts with random noise in latent space (a compressed version of the image, for example, 64×64×4 instead of 512×512×3) and, at each step, predicts the noise in this latent while using the text embeddings as guidance.</li><li><strong>Sampler: </strong>Takes the U-Net’s noise prediction and, using a numerical rule (for example, DDIM), subtracts part of the noise and updates the latent from 𝑧 𝑡 → z t−1 to gradually remove noise, step by step.</li><li><strong>VAE Decoder: </strong>Once denoising is complete, you have a clean latent 𝑧₀. The VAE decoder converts this latent back into a full-resolution pixel image. It basically acts as an image translator.</li></ul><h2>How IP adapter works </h2><p>The key to IP-Adapter's functionality lies in understanding how cross-attention layers operate within the U-Net architecture. </p><h3>Cross-Attention in IP adapter</h3><p>Cross attention allows one sequence or modality to focus on and extract information from another. It connects your text embeddings and the developing image. Unlike self-attention, where the queries, keys, and values all come from the same source, in cross-attention the queries (Q) are taken from one sequence (for example, a developing image’s latent features), while the keys (K) and values (V) come from another sequence, such as text embedding.</p><p>At each denoising step, this mechanism determines which words from your prompt should influence which regions of the image. However, this text-to-image attention works perfectly for text prompts but is designed only for text embeddings.</p><p>IP-Adapter extends this by introducing dual cross-attention branches: the original text cross-attention, where K and V come from text embeddings, and a new image cross-attention, where K and V come from image encoder features instead. This allows the model to simultaneously pay attention to both your text prompt and visual references, creating a more comprehensive understanding of what you want to generate.</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1586pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c5a129c9308bc876315a40_154371dd.png""/></div></figure><h3>IP-Adapter's Decoupled Attention</h3><p>‍</p><p>This decoupled approach creates two distinct but complementary pathways that work together to produce more precise and controllable image generation. The unreasonable effectiveness of decoupling comes from the fact that when you pass image features into text attention, you force the model to find a common representation space (solving the alignment fallacy) since text and image features carry very different types of information. Keeping these separate, the IP adapter lets each modality contribute its strengths without interference.</p><p>‍</p><p><strong>Pathway 1: Semantic Control (Original Text Cross-Attention)</strong></p><p>‍</p><p>The first cross-attention mechanism maintains Stable Diffusion's original strength in semantic control. This pathway tells the U-Net what to generate by handling object identity, recognition, scene composition, layout, and a kind of high-level conceptual understanding.</p><p>‍</p><p><strong>Pathway 2: Visual Style Control (Image Cross-Attention)</strong></p><p>‍</p><p>The second cross-attention mechanism introduces an entirely new visual control path that tells the U-Net how the output should look. This pathway focuses on visual style and aesthetic qualities, texture details and surface properties, and color schemes and artistic techniques. It also captures compositional elements like visual hierarchy and more complex artistic concepts such as the metamorphosis and transformation of familiar objects.</p><h3>Training Process</h3><p>The training process of IP adapters is straightforward. Rather than updating the entire model architecture, the training process employs a selective parameter strategy that maximizes efficiency while preserving the foundational capabilities of the pre-trained diffusion model.</p><p>‍</p><h4>Selective Parameter Training Architecture</h4><p>The training process strategically freezes the majority of model parameters, specifically the pre-trained U-Net diffusion, CLIP text encoder, and CLIP image encoder, while exclusively training the newly introduced components. This includes approximately 22 million parameters,  which includes the image cross-attention layers, linear projection modules that map image embeddings to the appropriate dimensional space, and associated LayerNorm layers for training stability. This approach makes sure only 3-5% of the total model parameters undergo training, which dramatically reduces computational requirements.</p><p>‍</p><h4>Training Pipeline</h4><p>The training pipeline begins with data preparation, where reference images are processed through the frozen CLIP image encoder to generate image embeddings, while corresponding captions are encoded via the CLIP text encoder. During each training iteration, Gaussian noise is added to target images at randomly sampled timesteps, creating the noisy latents that serve as input to the U-Net. </p><p>‍</p><p>The forward pass incorporates both conditioning modalities through parallel cross-attention mechanisms: text cross-attention operates as Attention(Q=latent_features, K=text_embeddings, V=text_embeddings), while the new image cross-attention functions as Attention(Q=latent_features, K=image_embeddings, V=image_embeddings). </p><p>‍</p><p>The optimization process minimizes L2 loss between the model's noise predictions and ground truth noise, with gradients updating only the trainable adapter parameters while leaving the frozen base model untouched.</p><h2>Training your own IP adapter</h2><p>We prepare a dataset of images (and captions) that represent the kinds of visual content and styles you want the adapter to handle. Then we fine-tune using the standard diffusion noise-prediction objective (L2 on predicted noise).</p><p>‍</p><p>This lightweight approach lets you specialize in domains like architectural photography or specific artistic styles without computational overhead or risk of degrading the base model's capabilities.</p><p>‍</p><p>You can get started by trying out these code snippets below for training an ip adapter</p><pre class=""w-code-block"" contenteditable=""false"" style=""display:block;overflow-x:auto;background:#2b2b2b;color:#f8f8f2;padding:0.5em""><code class=""language-javascript"" style=""white-space:pre""><span>git clone https:</span><span style=""color:#d4d0ab"">//github.com/tencent-ailab/IP-Adapter</span></code></pre><p>‍</p><p>Then download accelerate using</p><pre class=""w-code-block"" contenteditable=""false"" style=""display:block;overflow-x:auto;background:#2b2b2b;color:#f8f8f2;padding:0.5em""><code class=""language-javascript"" style=""white-space:pre""><span>pip install accelerate</span></code></pre><p>‍</p><p>You also need to make your own dataset into a json file.</p><p>Here’s a helper script to produce a data.json with default captions as filenames.</p><pre class=""w-code-block"" contenteditable=""false"" style=""display:block;overflow-x:auto;background:#2b2b2b;color:#f8f8f2;padding:0.5em""><code class=""language-javascript"" style=""white-space:pre""><span style=""color:#dcc6e0"">import</span><span> os
</span><span></span><span style=""color:#dcc6e0"">import</span><span> json
</span>
<!-- --># paths
<span>image_folder = </span><span style=""color:#abe338"">""dataset/images""</span><span>
</span><span>output_json = </span><span style=""color:#abe338"">""dataset/data.json""</span><span>
</span>
<span># create a list </span><span style=""color:#dcc6e0"">for</span><span> entries
</span>data = []
<!-- -->
<!-- --># loop through images
<span></span><span style=""color:#dcc6e0"">for</span><span> img_name </span><span style=""color:#dcc6e0"">in</span><span> os.listdir(image_folder):
</span><span>    </span><span style=""color:#dcc6e0"">if</span><span> img_name.lower().endswith((</span><span style=""color:#abe338"">"".jpg""</span><span>, </span><span style=""color:#abe338"">"".png""</span><span>, </span><span style=""color:#abe338"">"".jpeg""</span><span>)):
</span><span>        # </span><span style=""color:#dcc6e0"">default</span><span> caption = filename (you can edit later)
</span><span>        caption = os.path.splitext(img_name)[</span><span style=""color:#f5ab35"">0</span><span>]
</span>        
<!-- -->        data.append({
<span>            </span><span style=""color:#abe338"">""file_name""</span><span>: img_name,
</span><span>            </span><span style=""color:#abe338"">""text""</span><span>: caption
</span>        })
<!-- -->
<span># save </span><span style=""color:#dcc6e0"">as</span><span> json
</span><span></span><span style=""color:#dcc6e0"">with</span><span> open(output_json, </span><span style=""color:#abe338"">""w""</span><span>) </span><span style=""color:#dcc6e0"">as</span><span> f:
</span><span>    json.dump(data, f, indent=</span><span style=""color:#f5ab35"">2</span><span>)
</span>
<span>print(f</span><span style=""color:#abe338"">""Saved {len(data)} entries to {output_json}""</span><span>)</span></code></pre><p>Then set image_encoder_path path to a pretrained <strong>image encoder</strong> (e.g., CLIP-ViT) and </p><p>And image_path folder to where the images in your dataset are stored.</p><p>‍</p><p>Then run this script below, it trains an IP adapter that connects an image encoder (like CLIP) with stable diffusion v1.5 so the diffusion model can be conditioned on both text prompts and reference images.</p><pre class=""w-code-block"" contenteditable=""false"" style=""display:block;overflow-x:auto;background:#2b2b2b;color:#f8f8f2;padding:0.5em""><code class=""language-javascript"" style=""white-space:pre""><span>accelerate launch --num_processes </span><span style=""color:#f5ab35"">8</span><span> --multi_gpu --mixed_precision </span><span style=""color:#abe338"">""fp16""</span><span> \
</span>  tutorial_train.py \
<span>  --pretrained_model_name_or_path=</span><span style=""color:#abe338"">""runwayml/stable-diffusion-v1-5/""</span><span> \
</span><span>  --image_encoder_path=</span><span style=""color:#abe338"">""{image_encoder_path}""</span><span> \
</span><span>  --data_json_file=</span><span style=""color:#abe338"">""{data.json}""</span><span> \
</span><span>  --data_root_path=</span><span style=""color:#abe338"">""{image_path}""</span><span> \
</span><span>  --mixed_precision=</span><span style=""color:#abe338"">""fp16""</span><span> \
</span><span>  --resolution=</span><span style=""color:#f5ab35"">512</span><span> \
</span><span>  --train_batch_size=</span><span style=""color:#f5ab35"">8</span><span> \
</span><span>  --dataloader_num_workers=</span><span style=""color:#f5ab35"">4</span><span> \
</span><span>  --learning_rate=</span><span style=""color:#f5ab35"">1e-04</span><span> \
</span><span>  --weight_decay=</span><span style=""color:#f5ab35"">0.01</span><span> \
</span><span>  --output_dir=</span><span style=""color:#abe338"">""{output_dir}""</span><span> \
</span><span>  --save_steps=</span><span style=""color:#f5ab35"">10000</span></code></pre><p>‍</p><p>You can get started by trying out these code snippets below for testing out these pretrained IP adapters models in Google Colab for inference.</p><p>‍</p><p>Here are the codes below<br/><br/></p><pre class=""w-code-block"" contenteditable=""false"" style=""display:block;overflow-x:auto;background:#2b2b2b;color:#f8f8f2;padding:0.5em""><code class=""language-javascript"" style=""white-space:pre""><span style=""color:#dcc6e0"">import</span><span> torch
</span><span></span><span style=""color:#dcc6e0"">from</span><span> diffusers </span><span style=""color:#dcc6e0"">import</span><span> AutoPipelineForImage2Image
</span><span></span><span style=""color:#dcc6e0"">from</span><span> PIL </span><span style=""color:#dcc6e0"">import</span><span> Image
</span>
<!-- -->
<!-- -->pipeline = AutoPipelineForImage2Image.from_pretrained(
<span> </span><span style=""color:#abe338"">""stabilityai/stable-diffusion-xl-base-1.0""</span><span>,
</span> torch_dtype=torch.float16
<span>).to(</span><span style=""color:#abe338"">""cuda""</span><span>)
</span>
<!-- -->
<span>pipeline.load_ip_adapter(</span><span style=""color:#abe338"">""h94/IP-Adapter""</span><span>,
</span><span>                        subfolder=</span><span style=""color:#abe338"">""sdxl_models""</span><span>,
</span><span>                        weight_name=</span><span style=""color:#abe338"">""ip-adapter_sdxl.bin""</span><span>)
</span></code></pre><p>‍</p><pre class=""w-code-block"" contenteditable=""false"" style=""display:block;overflow-x:auto;background:#2b2b2b;color:#f8f8f2;padding:0.5em""><code class=""language-javascript"" style=""white-space:pre""><span>image = Image.open(</span><span style=""color:#abe338"">""image.png""</span><span>).convert(</span><span style=""color:#abe338"">""RGB""</span><span>)
</span>
<!-- -->
<!-- -->image_embeds = pipeline.prepare_ip_adapter_image_embeds(
<!-- -->   ip_adapter_image=image,
<!-- -->   ip_adapter_image_embeds=None,
<span>   device=</span><span style=""color:#abe338"">""cuda""</span><span>,
</span><span>   num_images_per_prompt=</span><span style=""color:#f5ab35"">1</span><span>,
</span>   do_classifier_free_guidance=True,
<!-- -->)
<!-- -->
<!-- -->
<span>torch.save(image_embeds, </span><span style=""color:#abe338"">""image_embeds.ipadpt""</span><span>)</span></code></pre><p>‍</p><p><strong>Inference code block:</strong></p><pre class=""w-code-block"" contenteditable=""false"" style=""display:block;overflow-x:auto;background:#2b2b2b;color:#f8f8f2;padding:0.5em""><code class=""language-javascript"" style=""white-space:pre""><span>pipeline.set_ip_adapter_scale(</span><span style=""color:#f5ab35"">0.8</span><span>)
</span><span>image_embeds = torch.load(</span><span style=""color:#abe338"">""image_embeds.ipadpt""</span><span>)
</span>
<!-- -->
<!-- -->pipeline(
<span>   prompt=</span><span style=""color:#abe338"">""a girl drinking milkshake near volcano attempting to fly""</span><span>,
</span>   image=image,
<!-- -->   ip_adapter_image_embeds=image_embeds,
<span>   negative_prompt=</span><span style=""color:#abe338"">""deformed, ugly, low res, bad anatomy, worst quality""</span><span>,
</span><span>   num_inference_steps=</span><span style=""color:#f5ab35"">100</span><span>,
</span><span>   generator=torch.Generator(device=</span><span style=""color:#abe338"">""cuda""</span><span>).manual_seed(</span><span style=""color:#f5ab35"">42</span><span>),
</span><span>).images[</span><span style=""color:#f5ab35"">0</span><span>]
</span></code></pre><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1162pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/68c5a129c9308bc876315a43_ee813462.png""/></div></figure><h2>IP-Adapter vs. Other Methods</h2><p>IP-Adapter is usually orders of magnitude cheaper/faster to get working than full fine-tuning and somewhat cheaper than training a ControlNet while giving better style transfer than naive adapters and far better composability than full fine-tuning.</p><p>‍</p><p><strong>Full Fine-Tuning: </strong>This means retraining the entire diffusion model on a specific dataset. It needs cloud GPU and tons of data, and the model loses its text-prompt ability because of facing issues like<strong> </strong>catastrophic forgetting, where generalization erodes as the model forgets prior knowledge, and after fine-tuning we can’t combine it with other tricks (like ControlNet) without retraining. Some research also showed that these models often “forget” general, broad features across the reverse process when fine-tuned on a specific domain, especially in earlier (closer to raw noise) steps. It’s also called the chain of forgetting, which harms the generalization.</p><p>‍</p><p><strong>ControlNet:</strong> It was originally designed for low-level structural control (edges, depth, pose, etc.). Issues with ControlNet come because it doesn’t inherently convey style or content from a reference image. Strong ControlNet weights can also completely dominate the model, producing stiff or similar outputs that lack natural variation. There has been some research in combining IP-adapter with Depth ControlNet to preserve structure while restyling an image.</p><h2>Challenges of using IP Adapter</h2><p><strong>Feature Mixing</strong>: Sometimes IP-Adapter picks up unintended elements from your reference image. If you want just the lighting style from a portrait, you might accidentally get the person's clothing or background elements too.</p><p>‍</p><p><strong>Complex Compositions</strong>: When your reference image has multiple distinct elements, IP-Adapter might struggle to understand which aspects you actually want to transfer to the new generation.</p><p>‍</p><p><strong>Style-Content Entanglement</strong>: IP-Adapter sometimes struggles to separate artistic style from subject matter, making it challenging to extract pure aesthetic qualities without copying content elements.</p><p>‍</p><p><strong>Limited Semantic Control</strong>: Unlike text prompts, you can't precisely specify which aspects of an image to emphasize or ignore.</p><h3>Alternative and Specialized Methods</h3><p>‍</p><p><strong>DreamBooth</strong>: When you need perfect identity preservation across multiple images, DreamBooth fine-tunes the entire model to learn specific subjects through custom tokens. More computationally expensive but offers precise control over character consistency.</p><p>‍</p><p><strong>Textual Inversion</strong>: Creates custom embedding tokens for specific concepts or styles, allowing precise text-based control over learned visual elements. Lightweight but limited to concepts that can be tokenized.</p><p><strong>ControlNet + IP-Adapter Combination</strong>: Many practitioners combine both—ControlNet handles structural elements (pose, composition), while IP-Adapter manages aesthetic qualities (style, lighting, mood).</p><p>We talk about these methods in our <a href=""https://www.mercity.ai/blog-post/use-stable-diffusion-to-generate-product-images"">Product Image Generation</a> Blog.</p><p>‍</p><h2>Real-World Applications</h2><p>It's used a lot in real-world use cases where preserving fine visual detail while staying compatible with large pre-trained text-to-image models matters.</p><p>‍</p><p>1. Identity preservation: IP adapter variants like IP-Adapter-FaceID focus on creating highly realistic and consistent depictions of individuals across a wide array of contexts and styles.</p><p>2. Multi-Subject Style Transfer: The ICAS (IP-Adapter and ControlNet-based Attention Structure) framework is recent research towards creating more complex and coherent stylized scenes.</p><p>3. Video / identity-preserving video:- Researchers have adopted Image-conditioning ideas (IP-Adapter style) to video diffusion so a reference image is preserved across frames while motion is generated</p><p>4. Spatial/3D cues via depth &amp; maps: Researchers also combine IP-Adapter conditioning with depth maps/multi-view cues so the adapter encodes explicit 3D layout, which improves 3D placement, occlusion handling, and realism.</p><p>‍</p><h2>CTA</h2><p>Looking to train stable diffusion models on your images? Please <a href=""https://www.mercity.ai/contacts"">reach out to Mercity</a> to build stable diffusion based products. We have done a lot of research in diffusion based image generation and also around optimizing diffusion models.</p><p>‍</p></div>"
How to use  AI for Predictive Maintenance,use-ai-for-predictive-maintenance,640f56f76d313b2faa631c11,67955dace8aee4ed4466ee8c,False,False,Sat Jan 25 2025 21:54:52 GMT+0000 (Coordinated Universal Time),Sat Jan 25 2025 21:57:31 GMT+0000 (Coordinated Universal Time),Sat Jan 25 2025 21:57:37 GMT+0000 (Coordinated Universal Time),"<h1 id="""">AI Powered Predictive Maintenance</h1><p id="""">‍</p><p id="""">AI-powered predictive maintenance helps prevent equipment failures by analyzing real-time machine data. This technology is crucial for modern industries, as it can detect problems before they cause expensive breakdowns. Recent company failures highlight this importance, where <a href=""https://gfmag.com/data/worlds-biggest-bankruptcies/"" id="""">Altera Infrastructure</a> filed for bankruptcy in 2022 due to heavy debt and operational inefficiencies from poor equipment maintenance, while Cineworld accumulated nearly $5 billion in debt partly due to inefficient theater maintenance and equipment breakdowns. Similarly, <a href=""https://www.businesstoday.in/industry/aviation/story/gofirst-latest-airlines-to-go-bankrupt-heres-a-list-of-air-carriers-that-went-bust-in-the-past-379829-2023-05-03"" id="""">Kingfisher Airlines</a> collapsed under high maintenance costs and frequent technical issues that led to flight cancellations. By implementing AI maintenance systems, companies can better protect their operations and avoid similar financial disasters and public institutions like <a href=""https://timesofindia.indiatimes.com/india/poor-maintenance-of-nh-stretches-will-invite-blacklisting-strict-action-gadkari/articleshow/113434588.cms"" id="""">Indian Railways</a>, which has encountered accidents due to poor track and rolling stock maintenance which shows the critical need for AI solutions to predict and address maintenance needs proactively.</p><h2 id="""">What is Predictive Maintenance (PdM)?</h2><p id="""">AI-powered predictive maintenance (PdM) leverages real-time data and analytics to predict potential equipment failures, reducing unplanned downtime and extending machinery lifespan by addressing issues before they escalate. This approach marks a shift from traditional reactive and preventive maintenance, which either waits for breakdowns or follows routine schedules without regard to actual equipment condition.&nbsp;</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/679556003a18788010b007fc_AD_4nXey3G5HnBNO-z3iBVWZrS3K1J2pevvt8F6CS_oj66LQH14kyL3H8IMjtvlE3I5c7FkRXLQPQ6ojJHzsQTOGJBthTzj6ZiRaBgPEMqwTK6uXIMDotsPYOSlBCUMW74f9yntt5bJa4A.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><p id="""">PdM’s importance is highlighted by success stories from companies like <a href=""https://www.wsj.com/articles/predictive-maintenance-tech-is-taking-off-as-manufacturers-seek-more-efficiency-11662543000#"" id="""">PepsiCo's Frito-Lay</a>, Noranda Alumina, and San Diego Gas &amp; Electric (SDG&amp;E), each of which harnessed PdM to avoid costly interruptions and achieve impressive efficiency gains. For example, Frito-Lay's Tennessee plant prevented equipment failures through vibration and ultrasound analyses, while SDG&amp;E's AI-driven PdM program accurately predicted failures in underground assets, showcasing the utility of condition-based monitoring over outdated methods as covered in the <a href=""https://www.plantservices.com/predictive-maintenance/predictive-maintenance/article/11288555/push-the-needle-how-6-companies-are-achieving-predictive-maintenance-success"" id="""">source</a>. Predictive maintenance can significantly improve asset management by foreseeing issues and optimizing maintenance schedules, saving companies millions in potential losses and ensuring higher operational continuity and safety across sectors like manufacturing, energy, and transportation.</p><p id="""">‍</p><h2 id="""">&nbsp;Why Predictive Maintenance?</h2><p id="""">‍</p><p id="""">Predictive Maintenance (PdM) is revolutionizing asset management by delivering substantial cost savings, maximizing equipment reliability, reducing unplanned downtime, and extending the lifespan of critical assets. Unlike traditional maintenance methods, PdM uses advanced analytics and real-time monitoring to perform maintenance only when truly necessary, optimizing both efficiency and operational costs.<br><br></p><p id="""">By leveraging technologies such as IoT sensors, machine learning algorithms, and big data, PdM provides actionable insights into asset health. This proactive approach not only minimizes the risk of sudden failures but also enhances safety, improves resource allocation, and ensures better compliance with industry regulations. Organizations adopting PdM are witnessing transformative results, driving competitiveness in an increasingly dynamic market landscape.</p><p id="""">‍</p><h3 id="""">Cost Savings and Equipment Reliability</h3><p id=""""><a href=""https://www.wsj.com/articles/predictive-maintenance-tech-is-taking-off-as-manufacturers-seek-more-efficiency-11662543000"" id="""">PepsiCo’s Frito-Lay</a> facilities leveraged PdM to gain over 4,000 additional hours of annual production, effectively translating into millions of pounds of extra product without the cost burden of unexpected part replacements. By minimizing unexpected breakdowns and proactively addressing minor issues, Frito-Lay maximized asset reliability and optimized production capacity across its operations.<br><br></p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:829px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""829px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/679555ffcaa1631e8c68ae29_AD_4nXeLQRwqylDgS7gAV6IGRMwjRVWxJLj1f1q81ISN_c7ENOaTrK3U_8R_HOY3m3XZnprKRtl-uiyo_qH38y00tdGmVT4dH3i8QiLvShuLmK6-D5cKbOfy_KIJeSsvBwEY_QlrQW5lTg.jpeg"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><h3 id="""">Reduction in Unplanned Downtime</h3><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1280px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1280px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/679555ff650ea7503dec26ec_AD_4nXcLM1NMACRwLVa1LWQlc02NkhxkbAxKxEyIDjgq1sWzFe_sZeLRkyHWURVDilPixY29hL2waY76dJeLXneliiNkIHmrs8fbteJ5rvHY34BKBuG7-ZohsXO9i7w85Gbsn__I4iFuaA.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><p id="""">According to a <a href=""https://www2.deloitte.com/us/en/pages/operations/articles/predictive-maintenance-and-the-smart-factory.html"" id="""">study by Deloitte</a>, predictive maintenance can increase equipment uptime by as much as 20% and reduce overall maintenance costs by 10%. Companies that adopt PdM often experience significant reductions in unplanned downtime, which can lead to substantial cost savings. For instance, a leading offshore oil and gas operator introduced a sophisticated PdM system across nine platforms, achieving an average reduction in downtime of 20% and increasing production by over 500,000 barrels of oil annually reported in <a href=""https://www.mckinsey.com/capabilities/operations/our-insights/a-smarter-way-to-digitize-maintenance-and-reliability"" id="""">mckinsey</a>.</p><p id="""">‍</p><h3 id="""">Enhanced Operational Efficiency</h3><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/679556003d6035bcb0720184_AD_4nXe88XyW9JjcXlpVZWqoQrBZAhz3c6v7QomHCJfXQioePnzGiCc8BmLaJdtz_1u5Fll4gbYk9VwSZ7tISzhBmDg1o9_4SYbJGsUH_CQWTgN2d1fZa61H36eEDpod_ei4q3krjrfE3Q.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">Predictive maintenance optimizes maintenance schedules based on real-time data analysis. This proactive approach allows companies to schedule maintenance during planned downtimes or low-demand periods, maximizing asset availability. For example, a global chemicals company saw a 30% productivity boost in maintenance planning and scheduling after implementing a digital work management system, underscoring PdM’s role in enhancing operational efficiency reported in <a href=""https://www.mckinsey.com/capabilities/operations/our-insights/a-smarter-way-to-digitize-maintenance-and-reliability"" id="""">mckinsey</a>. Companies across industries are increasingly recognizing PdM’s value, driving the market’s growth at a projected CAGR of 23.77%, with an estimated value of $10.47 billion in 2023. This surge in adoption is powered by advancements in IoT, AI, and machine learning, making predictive maintenance indispensable for ensuring uptime, reliability, and cost-effectiveness in critical sectors like manufacturing, energy, and transportation reported in <a href=""https://www.einnews.com/pr_news/755482787/future-proofing-industries-the-predictive-maintenance-market-driving-operational-excellence-says-evolvebi"" id="""">source</a>.</p><p id="""">‍</p><h2 id="""">Techniques for Predictive Maintenance</h2><p id="""">‍</p><h3 id="""">Reactive Maintenance</h3><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/679556003d3f3dc62ddb8a69_AD_4nXcvCWCWCYjocIvSYir_yI6vKWXOCEI9iYIozdM9eayvo8gXckDmTXa9wfNb4dwDC9EkDbQCbJm54ZehqGi75H0kiDVA54RtjyxGM0haH7ZmeDGpwNnN1TjwXqmk3fE28Jvz5OJWDw.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><p id="""">Reactive maintenance is the earliest form of maintenance strategy, addressing equipment issues only after they occur. This ""run-to-failure"" approach often leads to unexpected downtime and costly repairs. Historically, reactive maintenance was the standard practice before the development of more structured strategies like preventive maintenance. Without anticipating or preventing issues, equipment was simply repaired or replaced after failure, which could cause significant disruptions, particularly in critical operations.</p><p id="""">As industries grew and equipment complexity increased, the limitations of reactive maintenance became apparent, paving the way for preventive maintenance. This strategy introduced scheduled upkeep to mitigate unexpected failures, thus improving reliability and extending asset life.</p><p id="""">‍</p><h3 id="""">Anomaly Detection</h3><p id="""">‍</p><p id="""">Anomaly detection in predictive maintenance identifies irregularities in real-time equipment data, enabling early intervention before issues escalate. Techniques for anomaly detection include statistical analysis, time series analysis, and pattern recognition, utilizing both supervised and unsupervised machine learning methods. For instance, statistical approaches such as Z-score analysis flag unusual deviations in sensor readings, while time series models like ARIMA and deep learning architectures like LSTM networks are valuable for detecting complex temporal patterns in machine data. Pattern analysis further examines operational sequences, identifying deviations that may signal mechanical wear or impending faults.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/67955600e282f9bf5eb6bbf3_AD_4nXcukiZJe3x3Rp0MCzsx9F7nEmcCqt2JSzTE1J3J-9c4c2ed_sTvpro6YYBMdsaModyTl9ugyJWoRaqjuOdQpEsnFyzJByBClbitny98lAO3v5mgV7nuekhGy2uR0ltq1d6-vbGb9A.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><p id="""">Here’s an example graph showing vibration intensity over time, with red markers indicating anomaly points. This illustrates how certain data points deviate significantly from the regular pattern, serving as potential indicators of abnormal equipment behavior in predictive maintenance. ​</p><p id="""">‍</p><h3 id="""">Condition-based maintenance (CBM)</h3><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/679559a4650ea7503deeda23_AD_4nXfmCA5v-KU3g77KAPAHDfBoyJRjzd-J5cRYMj_kbTfCoOpIoi14DEgjYGoKCtMr4WTp5N01iNLNnEPGLLE2nSTKVUjRcErP97CWBzlb8gnfVA5Xi1T7B-GMgbudQwIwr2ZcZPGY.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><p id="""">Condition-based maintenance (CBM) is a proactive strategy that schedules maintenance actions based on real-time equipment monitoring. Sensors capture key parameters like vibration, temperature, pressure, and acoustic signals, which are continuously analyzed to detect wear or deterioration. When these readings indicate a decline in performance, CBM triggers maintenance, minimizing downtime and extending equipment lifespan. Unlike preventive maintenance, which operates on set schedules, CBM responds to the actual health of equipment, optimizing resource use and reducing unnecessary interventions.</p><p id="""">CBM employs several survival analysis algorithms to predict equipment failure and better understand maintenance timing:</p><h4 id="""">Kaplan-Meier Survival Analysis</h4><p id="""">‍</p><p id="""">The Kaplan-Meier survival analysis is a non-parametric method used to estimate the probability of equipment survival over time. This model helps maintenance teams assess the likelihood of equipment operating without failure over specific periods, making it particularly useful for planning maintenance activities based on potential failure points in the equipment's life cycle.</p><p id="""">‍</p><p id="""">Consider an example scenario, a company monitors the operational lifespan of its instruments, tracking the number of days each instrument remains operational before either failing or continuing beyond the observation period. The sample data includes operational days [120, 200, 95, 150, 240, 180, 300, 210, 130, 275] and corresponding failure events [1, 0, 1, 1, 0, 1, 1, 0, 1, 1], where 1 indicates a failure and 0 represents censoring (the instrument continued functioning).</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:846px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""846px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/679555fff45812ba0d4eaa17_AD_4nXd5gt2poEFaZTP7LGia8MfmK_4Vx4DUnxpeDgF-CdYN3X_HJQviab4qIRrkdyb-BW5hm-fh-M88zApnQb49Z26dnw_Y0kq_9_ZOEfYFykqrrOhRrvtLNGzfJXwfjFvzm2r_McS8Nw.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><p id="""">The Kaplan-Meier survival curve generated from this data illustrates the probability of instrument survival over time. The curve shows a steep decline in survival probability after around 100 operational days. At approximately 120 days, about 90% of the instruments are still operational, but this probability sharply declines between 100 and 200 days, reaching around 60% by 200 days. Beyond 200 days, the survival probability drops further, approaching zero by around 275 days, suggesting that most instruments are expected to fail within this timeframe.</p><h5 id="""">Strategic Insights</h5><p id="""">Based on this analysis, the company can take several proactive steps:</p><ol id=""""><li id="""">Schedule preventive maintenance activities around the 100-day mark to address potential failures before they become critical.</li><li id="""">Allocate budget for replacement or repair costs as instruments are likely to require attention beyond 200 operational days.</li><li id="""">Inform end-users and stakeholders about expected equipment performance over time to set realistic expectations for the operational lifespan of the instruments.</li></ol><p id="""">‍</p><h4 id="""">Cox Proportional Hazards Model</h4><p id="""">‍</p><p id="""">The Cox Proportional Hazards Model is a statistical technique widely used in predictive maintenance to evaluate how different conditions impact equipment failure over time. Unlike the Kaplan-Meier method, which focuses on estimating survival probability for a single variable, the Cox model accommodates multiple factors, making it particularly suitable for condition-based maintenance (CBM). This model can incorporate variables such as temperature, vibration level, and operation frequency, providing a nuanced understanding of how each factor affects the likelihood of equipment failure.</p><p id="""">‍</p><p id="""">Let’s assume a company has a dataset with specific values collected from machinery in an industrial setting. Below is a sample dataset with 10 entries representing machine conditions and their relationship to time-to-failure events.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1410px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1410px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/679555ff9cc3c0bf31c173c7_AD_4nXf-wWmmGm1ZEQ1glCoHrzftrGhdXc2iJXNwZ4rdGwAKJIgIILUxhhlUBVLKljAXLRoNArKZYHn0vjAKEHMTcb3kAR_Mjg9iZ0d5qd7erBNFfKuEW6ZORLaiF16dtD4bp6Ja1CcBbw.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><p id="""">Using this dataset, the Cox Proportional Hazards Model was applied to examine the impact of temperature, vibration level, and operation frequency on equipment failure risk.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/679556003d6035bcb072016b_AD_4nXd1pMA9BHOHEtY0tLtmmWD-MqBfKZhX_bUa_tVzhidlCCkfoaHqxA3niyemX94Ew2GkdXqUBJzU6qbmKZAz8fjjavZjVB6X1VWB4UUuIm437CfML-DG7jiEA7zcMuaCVbvhFbFpWg.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><ol id=""""><li id=""""><strong id="""">Coef (Coefficient):<br></strong>Represents the change in the log hazard ratio for each unit increase in a covariate. Positive coefficients indicate an increased hazard with an increase in the covariate, while negative coefficients indicate a reduced hazard.<br><br></li><li id=""""><strong id="""">Exp(Coef):<br></strong>The exponentiated coefficient translates the log hazard ratio into a multiplicative effect on the hazard. Values greater than 1 imply an increased hazard, while values less than 1 suggest a decreased hazard.<br><br></li><li id=""""><strong id="""">Confidence Intervals (Lower 95% and Upper 95%):<br></strong>Provide a range within which the true effect size is likely to fall with 95% confidence. Confidence intervals that span 1 imply the effect may not be statistically significant.<br><br></li><li id=""""><strong id="""">Z-Score:<br></strong>Indicates how many standard deviations the coefficient is from zero. Larger absolute values suggest stronger evidence against the null hypothesis.<br><br></li><li id=""""><strong id="""">P-Value:<br></strong>Measures the statistical significance of the coefficient. A p-value below 0.05 typically indicates statistical significance.<br><br></li><li id=""""><strong id="""">-log2(P):<br></strong>Represents the significance of the covariate on a logarithmic scale. Higher values indicate greater significance.</li></ol><p id="""">‍</p><h2 id="""">How does AI Powered Maintenance work?</h2><p id="""">‍</p><p id="""">AI-powered maintenance uses advanced technologies like machine learning, IoT sensors, and data analytics to predict and prevent equipment failures. Sensors collect real-time data on parameters such as <em id="""">temperature, vibration, and operational frequency</em>, which are then analyzed by AI algorithms to identify patterns and anomalies. These insights help forecast potential issues and schedule maintenance only when needed, reducing unplanned downtime and optimizing costs. Over time, AI models become more accurate by learning from historical data, enhancing reliability and efficiency across operations.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/679555ff9cf5e7cf020c5508_AD_4nXc-UfHZOD4dVqkgAyv6VYJjfFL56NGKr0EEahyg2ICjuEMSJILd4CzEpVjykvg8q9E4sc0JhkGBdOxUjPcb6onoylwpTjHSfZjDq-YaC506hx4GJT7M19ljeoYUCXBnle4X-SPS.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><h3 id="""">Data Collection and Storage</h3><p id="""">The process begins with collecting data from various sources, including IoT sensors, operational logs, and historical maintenance records from the equipment manufacturer. This data encompasses crucial parameters like temperature, vibration, rotational speed, and more, which are pivotal for predictive maintenance. For this example, we'll utilize data from a <a href=""https://www.kaggle.com/datasets/stephanmatzka/predictive-maintenance-dataset-ai4i-2020/code?datasetId=2609801&sortBy=voteCount"" id="""">Kaggle dataset</a> to illustrate the process of gathering and storing data effectively.</p><div data-rt-embed-type='true'><pre>
<code class=""language-py"">
import pandas as pd

# Load the dataset
url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00601/ai4i2020.csv'
data = pd.read_csv(url)

print(data.head(5))
</code>
</pre></div><p id="""">‍</p><p id="""">‍</p><p id="""">For this example, we'll utilize a synthetic dataset modeled after an existing milling machine, consisting of 10,000 data points with 14 features. Here are the key attributes of the dataset:</p><ul id=""""><li id=""""><strong id="""">UID:</strong> Unique identifier for each data point (1 to 10,000).<br><br></li><li id=""""><strong id="""">Product ID:</strong> Combines product quality (L, M, H for Low, Medium, High) with a serial number. L represents 50% of products, M 30%, and H 20%.<br><br></li><li id=""""><strong id="""">Type:</strong> Product quality type (L, M, or H).<br><br></li><li id=""""><strong id="""">Air Temperature [K]:</strong> Randomly generated around a mean of 300 K with minor fluctuations.<br><br></li><li id=""""><strong id="""">Process Temperature [K]:</strong> Air temperature plus 10 K, with small variations.<br><br></li><li id=""""><strong id="""">Rotational Speed [rpm]:</strong> Derived from power (2860 W) with added noise.<br><br></li><li id=""""><strong id="""">Torque [Nm]:</strong> Normally distributed around 40 Nm, ensuring no negative values.<br><br></li><li id=""""><strong id="""">Tool Wear [min]:</strong> Increased wear time based on product quality (H/M/L).<br><br></li><li id=""""><strong id="""">Machine Failure Label:</strong> Indicates failure due to one of five modes:<br><br> <br><ul id=""""><li id=""""><strong id="""">Tool Wear Failure (TWF):</strong> Tool wear exceeds limits.</li><li id=""""><strong id="""">Heat Dissipation Failure (HDF):</strong> Insufficient temperature difference and low speed.</li><li id=""""><strong id="""">Power Failure (PWF):</strong> Torque and speed outside acceptable power range.</li><li id=""""><strong id="""">Overstrain Failure (OSF):</strong> Tool wear and torque exceed thresholds for product quality.</li><li id=""""><strong id="""">Random Failures (RNF):</strong> Small random chance of failure (0.1%).</li></ul></li></ul><p id="""">If the above failure modes are true, the 'machine failure' label is set to 1. Importantly, it is not transparent to the machine learning method in which specific failure mode caused the process to fail, adding complexity to the predictive task.</p><p id="""">‍</p><h3 id="""">Data Preprocessing and Exploratory Data Analysis (EDA)</h3><p id="""">The data extracted from IoT sensors, operational logs, or manufacturer history data is often prone to outliers, missing values, or data misinterpretations. To ensure the data is accurate, consistent, and suitable for model training, we need to perform data preprocessing and exploratory data analysis (EDA). This is a key step to make the data standardized, remove inconsistencies, and gain insights into the relationships between features.</p><p id="""">Handling missing values involves replacing missing data with appropriate fill values such as the mean, median, or forward fill, or removing rows and columns altogether if necessary. Outlier detection and removal can be achieved using statistical methods like the interquartile range (IQR) or by visualizing outliers through boxplots. Data encoding transforms categorical variables, such as 'Type', into binary or numerical formats to make them suitable for machine learning models. Normalization or standardization of numerical features ensures consistency across the dataset, making it easier for algorithms to process and interpret the data effectively.</p><p id="""">‍</p><div data-rt-embed-type='true'><pre>
<code class=""language-py"">
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt


# 1. Data Overview
# Display basic information and summary statistics
print(data.info())  # Shows data types, non-null counts
print(data.describe(include='all').T)  # Full statistical summary for numeric and categorical columns

# 2. Replace '?' with NaN (if applicable)
# Replace any placeholders for missing values (like '?') with NaN to handle them correctly
data.replace('?', np.nan, inplace=True)

# 3. Convert columns to float where applicable
# Attempt to convert columns to float for uniform processing
for col in data.columns:
    try:
        data[col] = data[col].astype(float)
    except:
        pass

# 4. Identify Numeric Features and Display Descriptions
# Select numeric columns for further analysis and display their statistics
numeric_features = data.select_dtypes(include=[np.number])
print(numeric_features.describe().T)

# 5. Check for Missing Values
# Visualize missing values with a heatmap to understand data completeness
plt.figure(figsize=(10, 8))
sns.heatmap(data.isnull(), cbar=False, cmap='viridis')
plt.title('Missing Value Heatmap')
plt.show()


# 6. Data Cleaning and Feature Selection
# Drop indices as they have no predictive power
data.drop(['UDI', 'Product ID'], axis=1, inplace=True)

# Drop individual failure modes, focusing solely on whether there is a machine failure
data.drop(['TWF', 'HDF', 'PWF', 'OSF', 'RNF'], axis=1, inplace=True)

# Drop the 'Type' column to avoid strong dominance from the 'Type = L' category
data.drop(['Type'], axis=1, inplace=True)

# List remaining features to verify the changes
remaining_features = list(data.columns)
print(""Remaining features:"", remaining_features)

""""""
Remaining Features:

'Air temperature [K]'
'Process temperature [K]'
'Rotational speed [rpm]'
'Torque [Nm]'
'Tool wear [min]'
'Machine failure'
""""""
</code>
</pre></div><h3 id="""">Feature Engineering&nbsp;</h3><p id="""">‍</p><p id="""">Feature Engineering is a crucial step in the data preparation process for building predictive models. It involves creating new features or modifying existing ones to enhance the predictive power of a dataset. Well-engineered features can make a significant difference in model accuracy and robustness.</p><h4 id="""">Common Feature Engineering Techniques</h4><p id="""">‍</p><p id="""">Feature Creation involves generating new features based on existing ones, often by combining or transforming data to highlight underlying relationships. For example, multiplying torque and rotational speed in a machine dataset can yield a new feature indicative of power output. This technique can capture interactions between variables, enhancing predictive performance.&nbsp;</p><p id="""">Encoding Categorical Features transforms non-numeric data (like categories or labels) into a numeric format that machine learning models can understand. Common approaches include one-hot encoding, which creates binary columns for each category, and label encoding, which assigns integer values to categories. Proper encoding ensures categorical variables are effectively utilized during modeling.</p><p id="""">Binning converts continuous numerical data into discrete categories or intervals, often making patterns or trends more apparent. For instance, dividing 'tool wear' into 'low', 'medium', and 'high' categories can simplify the modeling process by segmenting data into more interpretable ranges.&nbsp;</p><p id="""">‍</p><p id="""">Feature Aggregation combines multiple data points or features into a single representative value, such as calculating the mean, sum, or maximum of related variables. This approach is useful when summarizing data over a time period or when consolidating redundant features, ultimately reducing dimensionality and complexity.</p><p id="""">‍</p><div data-rt-embed-type='true'><pre>
<code class=""language-py"">
import pandas as pd
import numpy as np

# Load the dataset (assume 'data' is already loaded as a DataFrame)

# 1. Creating Interaction Terms
data['Torque_Speed'] = data['Torque [Nm]'] * data['Rotational speed [rpm]']

# 2. Generating Polynomial Features
data['Torque_squared'] = data['Torque [Nm]'] ** 2

# 3. Binning a Continuous Variable
data['Tool_wear_category'] = pd.cut(data['Tool wear [min]'], bins=[0, 50, 150, 250], labels=['Low', 'Medium', 'High'])

# 4. Encoding Categorical Variables
data = pd.get_dummies(data, columns=['Tool_wear_category'], drop_first=True)

# 5. Normalization
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
data[['Air temperature [K]', 'Process temperature [K]', 'Rotational speed [rpm]', 'Torque [Nm]']] = scaler.fit_transform(
    data[['Air temperature [K]', 'Process temperature [K]', 'Rotational speed [rpm]', 'Torque [Nm]']]
)

# Display the engineered features
print(data.head())
</code>
</pre></div><h3 id="""">Model Training and Evaluation</h3><p id="""">‍</p><p id="""">In predictive maintenance, our goal is to build models that accurately predict machine failures based on various sensor readings and operational parameters. To achieve this, we will use two approaches: a Survival Analysis model (Cox Proportional Hazards Model) and a Machine Learning model (Random Forest Classifier). Survival analysis helps estimate the time until an event occurs, while traditional ML algorithms focus on binary classification for failure prediction.</p><p id="""">The Cox Proportional Hazards Model is a popular survival analysis technique that estimates the hazard (or risk) of an event, such as machine failure, occurring at a specific time point, given various covariates. This model is useful for understanding how different factors influence the time to an event.</p><p id="""">‍</p><div data-rt-embed-type='true'><pre>
<code class=""language-py"">
from lifelines import CoxPHFitter

# Load and preprocess data
# Assuming 'data' DataFrame has already been prepared
# Duration column represents the time to failure (e.g., tool wear [min])
data['Duration'] = data['Tool wear [min]']
data['Event'] = data['Machine failure']  # 1 for failure, 0 for no failure

# Select relevant features
features = ['Air temperature [K]', 'Process temperature [K]', 'Rotational speed [rpm]', 
            'Torque [Nm]', 'Duration', 'Event']

# Fit Cox Proportional Hazards Model
cph = CoxPHFitter()
cph.fit(data[features], duration_col='Duration', event_col='Event')
cph.print_summary()  # Display summary of model results
</code>
</pre></div><p id="""">‍</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1092px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1092px""><div id=""""><img src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/67955cde8e65f007f7ed2167_AD_4nXdjajERGvx1508WAqdLY7OpIolkNtY-ZPjTi61uOodDGCnS0w6kOsBdh6wPZNcVyDBKGJ6NJD3WbzFV08eUKv5D-HW8FHgdUjWAOqWtO5Uc7iQ4ea-OLXLgwtGJ8UBtXbVbjrMC7g.png"" width=""auto"" height=""auto"" alt="""" loading=""auto"" id=""""></div></figure><p id="""">‍</p><p id="""">The Cox Proportional Hazards Model helps in understanding the factors that affect the timing of failures, making it particularly suitable for survival analysis tasks where estimating the time until an event occurs is critical. On the other hand, the Random Forest Classifier provides robust classification predictions, making it well-suited for binary outcomes such as predicting whether a machine will fail or not. By combining insights from both approaches, predictive maintenance models can gain a deeper understanding of machine failure risks and optimize maintenance schedules, ultimately improving operational efficiency and reducing downtime.</p><h2 id="""">Optimize Your Equipment with AI-Powered Predictive Maintenance!</h2><p id="""">Ready to reduce downtime, cut maintenance costs, and boost operational efficiency? Our AI-driven solutions predict equipment failures before they happen, saving you time and money. Reach out <a href=""http://mercity.ai"" id="""">Mercity</a> today, and let's transform your maintenance strategy with cutting-edge predictive technology!</p><p id="""">‍</p>",https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/67955e35406d3f1f2eb2a783_predictive%20maintenance.png,Mathavan,AI in Maintenance,In this study we show how AI and Machine Learning can be used to preserve industrial machinery and do maintenance before the breakdown happens,False,"<div class=""rich-text w-richtext""><h1>AI Powered Predictive Maintenance</h1><p>‍</p><p>AI-powered predictive maintenance helps prevent equipment failures by analyzing real-time machine data. This technology is crucial for modern industries, as it can detect problems before they cause expensive breakdowns. Recent company failures highlight this importance, where <a href=""https://gfmag.com/data/worlds-biggest-bankruptcies/"">Altera Infrastructure</a> filed for bankruptcy in 2022 due to heavy debt and operational inefficiencies from poor equipment maintenance, while Cineworld accumulated nearly $5 billion in debt partly due to inefficient theater maintenance and equipment breakdowns. Similarly, <a href=""https://www.businesstoday.in/industry/aviation/story/gofirst-latest-airlines-to-go-bankrupt-heres-a-list-of-air-carriers-that-went-bust-in-the-past-379829-2023-05-03"">Kingfisher Airlines</a> collapsed under high maintenance costs and frequent technical issues that led to flight cancellations. By implementing AI maintenance systems, companies can better protect their operations and avoid similar financial disasters and public institutions like <a href=""https://timesofindia.indiatimes.com/india/poor-maintenance-of-nh-stretches-will-invite-blacklisting-strict-action-gadkari/articleshow/113434588.cms"">Indian Railways</a>, which has encountered accidents due to poor track and rolling stock maintenance which shows the critical need for AI solutions to predict and address maintenance needs proactively.</p><h2>What is Predictive Maintenance (PdM)?</h2><p>AI-powered predictive maintenance (PdM) leverages real-time data and analytics to predict potential equipment failures, reducing unplanned downtime and extending machinery lifespan by addressing issues before they escalate. This approach marks a shift from traditional reactive and preventive maintenance, which either waits for breakdowns or follows routine schedules without regard to actual equipment condition. </p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/679556003a18788010b007fc_AD_4nXey3G5HnBNO-z3iBVWZrS3K1J2pevvt8F6CS_oj66LQH14kyL3H8IMjtvlE3I5c7FkRXLQPQ6ojJHzsQTOGJBthTzj6ZiRaBgPEMqwTK6uXIMDotsPYOSlBCUMW74f9yntt5bJa4A.png""/></div></figure><p>‍</p><p>PdM’s importance is highlighted by success stories from companies like <a href=""https://www.wsj.com/articles/predictive-maintenance-tech-is-taking-off-as-manufacturers-seek-more-efficiency-11662543000#"">PepsiCo's Frito-Lay</a>, Noranda Alumina, and San Diego Gas &amp; Electric (SDG&amp;E), each of which harnessed PdM to avoid costly interruptions and achieve impressive efficiency gains. For example, Frito-Lay's Tennessee plant prevented equipment failures through vibration and ultrasound analyses, while SDG&amp;E's AI-driven PdM program accurately predicted failures in underground assets, showcasing the utility of condition-based monitoring over outdated methods as covered in the <a href=""https://www.plantservices.com/predictive-maintenance/predictive-maintenance/article/11288555/push-the-needle-how-6-companies-are-achieving-predictive-maintenance-success"">source</a>. Predictive maintenance can significantly improve asset management by foreseeing issues and optimizing maintenance schedules, saving companies millions in potential losses and ensuring higher operational continuity and safety across sectors like manufacturing, energy, and transportation.</p><p>‍</p><h2> Why Predictive Maintenance?</h2><p>‍</p><p>Predictive Maintenance (PdM) is revolutionizing asset management by delivering substantial cost savings, maximizing equipment reliability, reducing unplanned downtime, and extending the lifespan of critical assets. Unlike traditional maintenance methods, PdM uses advanced analytics and real-time monitoring to perform maintenance only when truly necessary, optimizing both efficiency and operational costs.<br/><br/></p><p>By leveraging technologies such as IoT sensors, machine learning algorithms, and big data, PdM provides actionable insights into asset health. This proactive approach not only minimizes the risk of sudden failures but also enhances safety, improves resource allocation, and ensures better compliance with industry regulations. Organizations adopting PdM are witnessing transformative results, driving competitiveness in an increasingly dynamic market landscape.</p><p>‍</p><h3>Cost Savings and Equipment Reliability</h3><p><a href=""https://www.wsj.com/articles/predictive-maintenance-tech-is-taking-off-as-manufacturers-seek-more-efficiency-11662543000"">PepsiCo’s Frito-Lay</a> facilities leveraged PdM to gain over 4,000 additional hours of annual production, effectively translating into millions of pounds of extra product without the cost burden of unexpected part replacements. By minimizing unexpected breakdowns and proactively addressing minor issues, Frito-Lay maximized asset reliability and optimized production capacity across its operations.<br/><br/></p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:829pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/679555ffcaa1631e8c68ae29_AD_4nXeLQRwqylDgS7gAV6IGRMwjRVWxJLj1f1q81ISN_c7ENOaTrK3U_8R_HOY3m3XZnprKRtl-uiyo_qH38y00tdGmVT4dH3i8QiLvShuLmK6-D5cKbOfy_KIJeSsvBwEY_QlrQW5lTg.jpeg""/></div></figure><p>‍</p><h3>Reduction in Unplanned Downtime</h3><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1280pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/679555ff650ea7503dec26ec_AD_4nXcLM1NMACRwLVa1LWQlc02NkhxkbAxKxEyIDjgq1sWzFe_sZeLRkyHWURVDilPixY29hL2waY76dJeLXneliiNkIHmrs8fbteJ5rvHY34BKBuG7-ZohsXO9i7w85Gbsn__I4iFuaA.png""/></div></figure><p>‍</p><p>According to a <a href=""https://www2.deloitte.com/us/en/pages/operations/articles/predictive-maintenance-and-the-smart-factory.html"">study by Deloitte</a>, predictive maintenance can increase equipment uptime by as much as 20% and reduce overall maintenance costs by 10%. Companies that adopt PdM often experience significant reductions in unplanned downtime, which can lead to substantial cost savings. For instance, a leading offshore oil and gas operator introduced a sophisticated PdM system across nine platforms, achieving an average reduction in downtime of 20% and increasing production by over 500,000 barrels of oil annually reported in <a href=""https://www.mckinsey.com/capabilities/operations/our-insights/a-smarter-way-to-digitize-maintenance-and-reliability"">mckinsey</a>.</p><p>‍</p><h3>Enhanced Operational Efficiency</h3><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/679556003d6035bcb0720184_AD_4nXe88XyW9JjcXlpVZWqoQrBZAhz3c6v7QomHCJfXQioePnzGiCc8BmLaJdtz_1u5Fll4gbYk9VwSZ7tISzhBmDg1o9_4SYbJGsUH_CQWTgN2d1fZa61H36eEDpod_ei4q3krjrfE3Q.png""/></div></figure><p>Predictive maintenance optimizes maintenance schedules based on real-time data analysis. This proactive approach allows companies to schedule maintenance during planned downtimes or low-demand periods, maximizing asset availability. For example, a global chemicals company saw a 30% productivity boost in maintenance planning and scheduling after implementing a digital work management system, underscoring PdM’s role in enhancing operational efficiency reported in <a href=""https://www.mckinsey.com/capabilities/operations/our-insights/a-smarter-way-to-digitize-maintenance-and-reliability"">mckinsey</a>. Companies across industries are increasingly recognizing PdM’s value, driving the market’s growth at a projected CAGR of 23.77%, with an estimated value of $10.47 billion in 2023. This surge in adoption is powered by advancements in IoT, AI, and machine learning, making predictive maintenance indispensable for ensuring uptime, reliability, and cost-effectiveness in critical sectors like manufacturing, energy, and transportation reported in <a href=""https://www.einnews.com/pr_news/755482787/future-proofing-industries-the-predictive-maintenance-market-driving-operational-excellence-says-evolvebi"">source</a>.</p><p>‍</p><h2>Techniques for Predictive Maintenance</h2><p>‍</p><h3>Reactive Maintenance</h3><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/679556003d3f3dc62ddb8a69_AD_4nXcvCWCWCYjocIvSYir_yI6vKWXOCEI9iYIozdM9eayvo8gXckDmTXa9wfNb4dwDC9EkDbQCbJm54ZehqGi75H0kiDVA54RtjyxGM0haH7ZmeDGpwNnN1TjwXqmk3fE28Jvz5OJWDw.png""/></div></figure><p>‍</p><p>Reactive maintenance is the earliest form of maintenance strategy, addressing equipment issues only after they occur. This ""run-to-failure"" approach often leads to unexpected downtime and costly repairs. Historically, reactive maintenance was the standard practice before the development of more structured strategies like preventive maintenance. Without anticipating or preventing issues, equipment was simply repaired or replaced after failure, which could cause significant disruptions, particularly in critical operations.</p><p>As industries grew and equipment complexity increased, the limitations of reactive maintenance became apparent, paving the way for preventive maintenance. This strategy introduced scheduled upkeep to mitigate unexpected failures, thus improving reliability and extending asset life.</p><p>‍</p><h3>Anomaly Detection</h3><p>‍</p><p>Anomaly detection in predictive maintenance identifies irregularities in real-time equipment data, enabling early intervention before issues escalate. Techniques for anomaly detection include statistical analysis, time series analysis, and pattern recognition, utilizing both supervised and unsupervised machine learning methods. For instance, statistical approaches such as Z-score analysis flag unusual deviations in sensor readings, while time series models like ARIMA and deep learning architectures like LSTM networks are valuable for detecting complex temporal patterns in machine data. Pattern analysis further examines operational sequences, identifying deviations that may signal mechanical wear or impending faults.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/67955600e282f9bf5eb6bbf3_AD_4nXcukiZJe3x3Rp0MCzsx9F7nEmcCqt2JSzTE1J3J-9c4c2ed_sTvpro6YYBMdsaModyTl9ugyJWoRaqjuOdQpEsnFyzJByBClbitny98lAO3v5mgV7nuekhGy2uR0ltq1d6-vbGb9A.png""/></div></figure><p>‍</p><p>Here’s an example graph showing vibration intensity over time, with red markers indicating anomaly points. This illustrates how certain data points deviate significantly from the regular pattern, serving as potential indicators of abnormal equipment behavior in predictive maintenance. ​</p><p>‍</p><h3>Condition-based maintenance (CBM)</h3><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/679559a4650ea7503deeda23_AD_4nXfmCA5v-KU3g77KAPAHDfBoyJRjzd-J5cRYMj_kbTfCoOpIoi14DEgjYGoKCtMr4WTp5N01iNLNnEPGLLE2nSTKVUjRcErP97CWBzlb8gnfVA5Xi1T7B-GMgbudQwIwr2ZcZPGY.png""/></div></figure><p>‍</p><p>Condition-based maintenance (CBM) is a proactive strategy that schedules maintenance actions based on real-time equipment monitoring. Sensors capture key parameters like vibration, temperature, pressure, and acoustic signals, which are continuously analyzed to detect wear or deterioration. When these readings indicate a decline in performance, CBM triggers maintenance, minimizing downtime and extending equipment lifespan. Unlike preventive maintenance, which operates on set schedules, CBM responds to the actual health of equipment, optimizing resource use and reducing unnecessary interventions.</p><p>CBM employs several survival analysis algorithms to predict equipment failure and better understand maintenance timing:</p><h4>Kaplan-Meier Survival Analysis</h4><p>‍</p><p>The Kaplan-Meier survival analysis is a non-parametric method used to estimate the probability of equipment survival over time. This model helps maintenance teams assess the likelihood of equipment operating without failure over specific periods, making it particularly useful for planning maintenance activities based on potential failure points in the equipment's life cycle.</p><p>‍</p><p>Consider an example scenario, a company monitors the operational lifespan of its instruments, tracking the number of days each instrument remains operational before either failing or continuing beyond the observation period. The sample data includes operational days [120, 200, 95, 150, 240, 180, 300, 210, 130, 275] and corresponding failure events [1, 0, 1, 1, 0, 1, 1, 0, 1, 1], where 1 indicates a failure and 0 represents censoring (the instrument continued functioning).</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:846pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/679555fff45812ba0d4eaa17_AD_4nXd5gt2poEFaZTP7LGia8MfmK_4Vx4DUnxpeDgF-CdYN3X_HJQviab4qIRrkdyb-BW5hm-fh-M88zApnQb49Z26dnw_Y0kq_9_ZOEfYFykqrrOhRrvtLNGzfJXwfjFvzm2r_McS8Nw.png""/></div></figure><p>‍</p><p>The Kaplan-Meier survival curve generated from this data illustrates the probability of instrument survival over time. The curve shows a steep decline in survival probability after around 100 operational days. At approximately 120 days, about 90% of the instruments are still operational, but this probability sharply declines between 100 and 200 days, reaching around 60% by 200 days. Beyond 200 days, the survival probability drops further, approaching zero by around 275 days, suggesting that most instruments are expected to fail within this timeframe.</p><h5>Strategic Insights</h5><p>Based on this analysis, the company can take several proactive steps:</p><ol role=""list""><li>Schedule preventive maintenance activities around the 100-day mark to address potential failures before they become critical.</li><li>Allocate budget for replacement or repair costs as instruments are likely to require attention beyond 200 operational days.</li><li>Inform end-users and stakeholders about expected equipment performance over time to set realistic expectations for the operational lifespan of the instruments.</li></ol><p>‍</p><h4>Cox Proportional Hazards Model</h4><p>‍</p><p>The Cox Proportional Hazards Model is a statistical technique widely used in predictive maintenance to evaluate how different conditions impact equipment failure over time. Unlike the Kaplan-Meier method, which focuses on estimating survival probability for a single variable, the Cox model accommodates multiple factors, making it particularly suitable for condition-based maintenance (CBM). This model can incorporate variables such as temperature, vibration level, and operation frequency, providing a nuanced understanding of how each factor affects the likelihood of equipment failure.</p><p>‍</p><p>Let’s assume a company has a dataset with specific values collected from machinery in an industrial setting. Below is a sample dataset with 10 entries representing machine conditions and their relationship to time-to-failure events.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1410pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/679555ff9cc3c0bf31c173c7_AD_4nXf-wWmmGm1ZEQ1glCoHrzftrGhdXc2iJXNwZ4rdGwAKJIgIILUxhhlUBVLKljAXLRoNArKZYHn0vjAKEHMTcb3kAR_Mjg9iZ0d5qd7erBNFfKuEW6ZORLaiF16dtD4bp6Ja1CcBbw.png""/></div></figure><p>‍</p><p>Using this dataset, the Cox Proportional Hazards Model was applied to examine the impact of temperature, vibration level, and operation frequency on equipment failure risk.</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/679556003d6035bcb072016b_AD_4nXd1pMA9BHOHEtY0tLtmmWD-MqBfKZhX_bUa_tVzhidlCCkfoaHqxA3niyemX94Ew2GkdXqUBJzU6qbmKZAz8fjjavZjVB6X1VWB4UUuIm437CfML-DG7jiEA7zcMuaCVbvhFbFpWg.png""/></div></figure><p>‍</p><ol role=""list""><li><strong>Coef (Coefficient):<br/></strong>Represents the change in the log hazard ratio for each unit increase in a covariate. Positive coefficients indicate an increased hazard with an increase in the covariate, while negative coefficients indicate a reduced hazard.<br/><br/></li><li><strong>Exp(Coef):<br/></strong>The exponentiated coefficient translates the log hazard ratio into a multiplicative effect on the hazard. Values greater than 1 imply an increased hazard, while values less than 1 suggest a decreased hazard.<br/><br/></li><li><strong>Confidence Intervals (Lower 95% and Upper 95%):<br/></strong>Provide a range within which the true effect size is likely to fall with 95% confidence. Confidence intervals that span 1 imply the effect may not be statistically significant.<br/><br/></li><li><strong>Z-Score:<br/></strong>Indicates how many standard deviations the coefficient is from zero. Larger absolute values suggest stronger evidence against the null hypothesis.<br/><br/></li><li><strong>P-Value:<br/></strong>Measures the statistical significance of the coefficient. A p-value below 0.05 typically indicates statistical significance.<br/><br/></li><li><strong>-log2(P):<br/></strong>Represents the significance of the covariate on a logarithmic scale. Higher values indicate greater significance.</li></ol><p>‍</p><h2>How does AI Powered Maintenance work?</h2><p>‍</p><p>AI-powered maintenance uses advanced technologies like machine learning, IoT sensors, and data analytics to predict and prevent equipment failures. Sensors collect real-time data on parameters such as <em>temperature, vibration, and operational frequency</em>, which are then analyzed by AI algorithms to identify patterns and anomalies. These insights help forecast potential issues and schedule maintenance only when needed, reducing unplanned downtime and optimizing costs. Over time, AI models become more accurate by learning from historical data, enhancing reliability and efficiency across operations.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/679555ff9cf5e7cf020c5508_AD_4nXc-UfHZOD4dVqkgAyv6VYJjfFL56NGKr0EEahyg2ICjuEMSJILd4CzEpVjykvg8q9E4sc0JhkGBdOxUjPcb6onoylwpTjHSfZjDq-YaC506hx4GJT7M19ljeoYUCXBnle4X-SPS.png""/></div></figure><h3>Data Collection and Storage</h3><p>The process begins with collecting data from various sources, including IoT sensors, operational logs, and historical maintenance records from the equipment manufacturer. This data encompasses crucial parameters like temperature, vibration, rotational speed, and more, which are pivotal for predictive maintenance. For this example, we'll utilize data from a <a href=""https://www.kaggle.com/datasets/stephanmatzka/predictive-maintenance-dataset-ai4i-2020/code?datasetId=2609801&amp;sortBy=voteCount"">Kaggle dataset</a> to illustrate the process of gathering and storing data effectively.</p><div class=""w-embed""><pre>
<code class=""language-py"">
import pandas as pd

# Load the dataset
url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00601/ai4i2020.csv'
data = pd.read_csv(url)

print(data.head(5))
</code>
</pre></div><p>‍</p><p>‍</p><p>For this example, we'll utilize a synthetic dataset modeled after an existing milling machine, consisting of 10,000 data points with 14 features. Here are the key attributes of the dataset:</p><ul role=""list""><li><strong>UID:</strong> Unique identifier for each data point (1 to 10,000).<br/><br/></li><li><strong>Product ID:</strong> Combines product quality (L, M, H for Low, Medium, High) with a serial number. L represents 50% of products, M 30%, and H 20%.<br/><br/></li><li><strong>Type:</strong> Product quality type (L, M, or H).<br/><br/></li><li><strong>Air Temperature [K]:</strong> Randomly generated around a mean of 300 K with minor fluctuations.<br/><br/></li><li><strong>Process Temperature [K]:</strong> Air temperature plus 10 K, with small variations.<br/><br/></li><li><strong>Rotational Speed [rpm]:</strong> Derived from power (2860 W) with added noise.<br/><br/></li><li><strong>Torque [Nm]:</strong> Normally distributed around 40 Nm, ensuring no negative values.<br/><br/></li><li><strong>Tool Wear [min]:</strong> Increased wear time based on product quality (H/M/L).<br/><br/></li><li><strong>Machine Failure Label:</strong> Indicates failure due to one of five modes:<br/><br/> <br/><ul role=""list""><li><strong>Tool Wear Failure (TWF):</strong> Tool wear exceeds limits.</li><li><strong>Heat Dissipation Failure (HDF):</strong> Insufficient temperature difference and low speed.</li><li><strong>Power Failure (PWF):</strong> Torque and speed outside acceptable power range.</li><li><strong>Overstrain Failure (OSF):</strong> Tool wear and torque exceed thresholds for product quality.</li><li><strong>Random Failures (RNF):</strong> Small random chance of failure (0.1%).</li></ul></li></ul><p>If the above failure modes are true, the 'machine failure' label is set to 1. Importantly, it is not transparent to the machine learning method in which specific failure mode caused the process to fail, adding complexity to the predictive task.</p><p>‍</p><h3>Data Preprocessing and Exploratory Data Analysis (EDA)</h3><p>The data extracted from IoT sensors, operational logs, or manufacturer history data is often prone to outliers, missing values, or data misinterpretations. To ensure the data is accurate, consistent, and suitable for model training, we need to perform data preprocessing and exploratory data analysis (EDA). This is a key step to make the data standardized, remove inconsistencies, and gain insights into the relationships between features.</p><p>Handling missing values involves replacing missing data with appropriate fill values such as the mean, median, or forward fill, or removing rows and columns altogether if necessary. Outlier detection and removal can be achieved using statistical methods like the interquartile range (IQR) or by visualizing outliers through boxplots. Data encoding transforms categorical variables, such as 'Type', into binary or numerical formats to make them suitable for machine learning models. Normalization or standardization of numerical features ensures consistency across the dataset, making it easier for algorithms to process and interpret the data effectively.</p><p>‍</p><div class=""w-embed""><pre>
<code class=""language-py"">
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt


# 1. Data Overview
# Display basic information and summary statistics
print(data.info())  # Shows data types, non-null counts
print(data.describe(include='all').T)  # Full statistical summary for numeric and categorical columns

# 2. Replace '?' with NaN (if applicable)
# Replace any placeholders for missing values (like '?') with NaN to handle them correctly
data.replace('?', np.nan, inplace=True)

# 3. Convert columns to float where applicable
# Attempt to convert columns to float for uniform processing
for col in data.columns:
    try:
        data[col] = data[col].astype(float)
    except:
        pass

# 4. Identify Numeric Features and Display Descriptions
# Select numeric columns for further analysis and display their statistics
numeric_features = data.select_dtypes(include=[np.number])
print(numeric_features.describe().T)

# 5. Check for Missing Values
# Visualize missing values with a heatmap to understand data completeness
plt.figure(figsize=(10, 8))
sns.heatmap(data.isnull(), cbar=False, cmap='viridis')
plt.title('Missing Value Heatmap')
plt.show()


# 6. Data Cleaning and Feature Selection
# Drop indices as they have no predictive power
data.drop(['UDI', 'Product ID'], axis=1, inplace=True)

# Drop individual failure modes, focusing solely on whether there is a machine failure
data.drop(['TWF', 'HDF', 'PWF', 'OSF', 'RNF'], axis=1, inplace=True)

# Drop the 'Type' column to avoid strong dominance from the 'Type = L' category
data.drop(['Type'], axis=1, inplace=True)

# List remaining features to verify the changes
remaining_features = list(data.columns)
print(""Remaining features:"", remaining_features)

""""""
Remaining Features:

'Air temperature [K]'
'Process temperature [K]'
'Rotational speed [rpm]'
'Torque [Nm]'
'Tool wear [min]'
'Machine failure'
""""""
</code>
</pre></div><h3>Feature Engineering </h3><p>‍</p><p>Feature Engineering is a crucial step in the data preparation process for building predictive models. It involves creating new features or modifying existing ones to enhance the predictive power of a dataset. Well-engineered features can make a significant difference in model accuracy and robustness.</p><h4>Common Feature Engineering Techniques</h4><p>‍</p><p>Feature Creation involves generating new features based on existing ones, often by combining or transforming data to highlight underlying relationships. For example, multiplying torque and rotational speed in a machine dataset can yield a new feature indicative of power output. This technique can capture interactions between variables, enhancing predictive performance. </p><p>Encoding Categorical Features transforms non-numeric data (like categories or labels) into a numeric format that machine learning models can understand. Common approaches include one-hot encoding, which creates binary columns for each category, and label encoding, which assigns integer values to categories. Proper encoding ensures categorical variables are effectively utilized during modeling.</p><p>Binning converts continuous numerical data into discrete categories or intervals, often making patterns or trends more apparent. For instance, dividing 'tool wear' into 'low', 'medium', and 'high' categories can simplify the modeling process by segmenting data into more interpretable ranges. </p><p>‍</p><p>Feature Aggregation combines multiple data points or features into a single representative value, such as calculating the mean, sum, or maximum of related variables. This approach is useful when summarizing data over a time period or when consolidating redundant features, ultimately reducing dimensionality and complexity.</p><p>‍</p><div class=""w-embed""><pre>
<code class=""language-py"">
import pandas as pd
import numpy as np

# Load the dataset (assume 'data' is already loaded as a DataFrame)

# 1. Creating Interaction Terms
data['Torque_Speed'] = data['Torque [Nm]'] * data['Rotational speed [rpm]']

# 2. Generating Polynomial Features
data['Torque_squared'] = data['Torque [Nm]'] ** 2

# 3. Binning a Continuous Variable
data['Tool_wear_category'] = pd.cut(data['Tool wear [min]'], bins=[0, 50, 150, 250], labels=['Low', 'Medium', 'High'])

# 4. Encoding Categorical Variables
data = pd.get_dummies(data, columns=['Tool_wear_category'], drop_first=True)

# 5. Normalization
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
data[['Air temperature [K]', 'Process temperature [K]', 'Rotational speed [rpm]', 'Torque [Nm]']] = scaler.fit_transform(
    data[['Air temperature [K]', 'Process temperature [K]', 'Rotational speed [rpm]', 'Torque [Nm]']]
)

# Display the engineered features
print(data.head())
</code>
</pre></div><h3>Model Training and Evaluation</h3><p>‍</p><p>In predictive maintenance, our goal is to build models that accurately predict machine failures based on various sensor readings and operational parameters. To achieve this, we will use two approaches: a Survival Analysis model (Cox Proportional Hazards Model) and a Machine Learning model (Random Forest Classifier). Survival analysis helps estimate the time until an event occurs, while traditional ML algorithms focus on binary classification for failure prediction.</p><p>The Cox Proportional Hazards Model is a popular survival analysis technique that estimates the hazard (or risk) of an event, such as machine failure, occurring at a specific time point, given various covariates. This model is useful for understanding how different factors influence the time to an event.</p><p>‍</p><div class=""w-embed""><pre>
<code class=""language-py"">
from lifelines import CoxPHFitter

# Load and preprocess data
# Assuming 'data' DataFrame has already been prepared
# Duration column represents the time to failure (e.g., tool wear [min])
data['Duration'] = data['Tool wear [min]']
data['Event'] = data['Machine failure']  # 1 for failure, 0 for no failure

# Select relevant features
features = ['Air temperature [K]', 'Process temperature [K]', 'Rotational speed [rpm]', 
            'Torque [Nm]', 'Duration', 'Event']

# Fit Cox Proportional Hazards Model
cph = CoxPHFitter()
cph.fit(data[features], duration_col='Duration', event_col='Event')
cph.print_summary()  # Display summary of model results
</code>
</pre></div><p>‍</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1092pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/67955cde8e65f007f7ed2167_AD_4nXdjajERGvx1508WAqdLY7OpIolkNtY-ZPjTi61uOodDGCnS0w6kOsBdh6wPZNcVyDBKGJ6NJD3WbzFV08eUKv5D-HW8FHgdUjWAOqWtO5Uc7iQ4ea-OLXLgwtGJ8UBtXbVbjrMC7g.png""/></div></figure><p>‍</p><p>The Cox Proportional Hazards Model helps in understanding the factors that affect the timing of failures, making it particularly suitable for survival analysis tasks where estimating the time until an event occurs is critical. On the other hand, the Random Forest Classifier provides robust classification predictions, making it well-suited for binary outcomes such as predicting whether a machine will fail or not. By combining insights from both approaches, predictive maintenance models can gain a deeper understanding of machine failure risks and optimize maintenance schedules, ultimately improving operational efficiency and reducing downtime.</p><h2>Optimize Your Equipment with AI-Powered Predictive Maintenance!</h2><p>Ready to reduce downtime, cut maintenance costs, and boost operational efficiency? Our AI-driven solutions predict equipment failures before they happen, saving you time and money. Reach out <a href=""http://mercity.ai"">Mercity</a> today, and let's transform your maintenance strategy with cutting-edge predictive technology!</p><p>‍</p></div>"
How to use Stable Diffusion to generate product images,use-stable-diffusion-to-generate-product-images,640f56f76d313b2faa631c11,64fdede8bd89df5714d9aa79,False,False,Sun Sep 10 2023 16:25:12 GMT+0000 (Coordinated Universal Time),Sun Sep 10 2023 16:27:11 GMT+0000 (Coordinated Universal Time),Sun Sep 10 2023 16:27:11 GMT+0000 (Coordinated Universal Time),"<p id="""">Diffusion models have revolutionized generative AI, enabling diverse content creation from textual inputs. Among them, Stable Diffusion emerged as a breakthrough. It enhances the quality and efficiency of content generation. It finds use in various domains, from art and design to marketing. In this article, we will explore stable diffusion and how you can use it to create product images to enhance your business.</p><h2 id="""">What is Stable diffusion?</h2><p id=""""><a href=""https://stability.ai/stablediffusion"" id="""">Stable Diffusion by StabilityAI</a> is a text-to-image deep-learning model. It is open source under the Apache License 2.0. Its 3.0 models are still under development. Stable Diffusion was trained on the LAION-5B dataset derived from CommonCrawl. It is based on a latent diffusion process. It starts with an image with noise and then gradually adds detail to the image until it becomes a realistic image. It runs on any hardware with a GPU having at least 8 GB VRAM compared to only cloud-accessible models such as DALL-E.&nbsp;</p><p id="""">‍</p><p id="""">Recently Stability AI released <a href=""https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0"" id="""">Stable Diffusion XL</a>, the best open-source diffusion model so far. SDXL generates stunning images at a resolution of 1024 x 1024 and also uses a refiner model to increase quality and control over generated images.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64fdeda52900d8b57c609826_GFalimg_Sdfs7P96zgN3URK0zPxUpSvUxZKnF_MTaQaBeBffnjlaRncneCb7clfE_f2KqeYtSKuxQbDJI751OXcXVhp2iT-5TE13MzPU8IjliUfDZBUdoD_dP5sY7QojhMY9M7XI4BvN6shr9EAPaW8.png"" id="""" width=""auto"" height=""auto"" alt="""" loading=""auto""></div></figure><h3 id="""">How does Stable Diffusion Work?</h3><p id="""">Stable diffusion has two core parts, the <strong id="""">Diffusion Process</strong> and the <strong id="""">Reverse Diffusion Process</strong>. The diffusion process adds noise to images and generates slightly noisy images with every step. The reverse diffusion process then reverses this process by predicting the amount of noise required to subtract from the noisy image to get the original image. This process is then repeated several times until a coherent image is obtained.</p><p id="""">‍</p><p id="""">Let’s dive deeper into both of the concepts:</p><h4 id="""">Forward Diffusion</h4><p id="""">As mentioned above, forward diffusion is the process of adding noise to an image. This happens in steps, so a number of steps are predetermined, and then a distribution of strength of noise is calculated for those steps. The higher the step, the more noise is added. This is an iterative process, but the authors of the <a href=""https://arxiv.org/abs/2112.10752"" id="""">LDM paper</a> show this can be done in one go. Given a step and the image, you can skip the previous iterations of adding noise.</p><p id="""">‍</p><p id="""">The authors also note that instead of adding noise directly to the image, we can encode the image and do the same operation in a latent space instead and then decode the image later on. This HUGELY reduces the computational power required to train and use diffusion models.</p><p id="""">‍</p><p id="""">This process is used to generate training samples for the reverse diffusion process.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1323px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1323px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64fdeda5bd89df5714d96986_Zj568y3X3efFdGhQpDmuBUPkg6Yhn5hRYct7huMMFnmtmgKsc2NTqLSjUADntyMarSG8SQ_vsYS-jlYDpqk6KXo6meFx3-W_Uj67gf4M_XXfpa_CGXgGBpvfmxIuI1DTwWjjirKY6lzs_4xplfFwuQo.png"" id="""" width=""auto"" height=""auto"" alt="""" loading=""auto""></div></figure><p id="""">‍</p><h4 id="""">Reverse Diffusion</h4><p id="""">Once we have the training samples from the forward diffusion process. We can start training the generative model to generate images. The generative model generates images by removing noise from a completely noisy image. This happens in an iterative fashion. The generative model predicts the noise that has to be subtracted from the noisy image to get an actual image. Then a fraction of this noise is subtracted from the noisy image. The resultant image is then again fed to the network. This process is repeated for a set amount of steps.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1265px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1265px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64fdeda5b2b752f61fc308cb_ikW4nmBsI68Iv_7hO_89po3k35L2jN92nlPFyPGugNlqdJdu6NDeTwpDPDQbh331iwHUnJF9tr5xQgqXRzIPDbuJLjkPkwWri-2o46ICNWuMHAJknKzbD2s2RjH1vj0Te7vY0Exdq-0UeJOkKbQd31w.png"" id="""" width=""auto"" height=""auto"" alt="""" loading=""auto""></div></figure><p id="""">‍</p><h3 id="""">Alternatives to Stable Diffusion</h3><h4 id="""">DALL-E</h4><p id=""""><a href=""https://arxiv.org/pdf/2102.12092.pdf"" id="""">DALL-E</a> is a text-to-image model by OpenAI. It has two model versions, DALL-E and <a href=""https://openai.com/dall-e-2"" id="""">DALL-E 2</a>. DALL-E 2 is an encoder-decoder architecture. The text encoder takes text as input and generates text embeddings. They are passed to a prior model which is a diffusion model. It generates the corresponding CLIP image embeddings. The embeddings are passed to an image decoder which then generates actual images from the embeddings.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64fdeda55e79a21ef5d6574e_89XLUW1IJqdl6iKrYovHq6qrULJHEM31x6wP1lAJeub_lj4I-3EkjqdIU9lxypCd_wkvPTchPDj3duZ3pzEhunm5IeFmT--mxeEgdVj04UsQkIBiakld5RgRHPMIHxEBHaKtYWg3n1cX7AoIcJJec0E.jpeg"" id="""" width=""auto"" height=""auto"" alt="""" loading=""auto""></div></figure><p id="""">Figure From <a href=""https://medium.com/augmented-startups/how-does-dall-e-2-work-e6d492a2667f"" id=""""><em id="""">Medium</em></a></p><p id="""">‍</p><p id="""">The embeddings come from CLIP (Contrastive Language-Image Pretraining). CLIP is trained on millions of images and their captions, to understand the relation between text and images. The model is designed to test how well a given caption matches an image, rather than predicting captions based on images. CLIP generates text and image encodings of each image-caption pair. It then calculates the cosine similarity of each of these embeddings. It minimizes the similarity between incorrect pairs and maximizes that of correct pairs. It then freezes and DALL-E 2 moves to the next task. CLIP guides the prior that takes text embeddings and turns them into image embeddings.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1297px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1297px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64fdeda61e51df93e3a5d9e1_IdulUm6cSvUGY4X-y3z_V-YQA9gpMwJAMVMZZP8MCRcizndJN48tBgxxr5MGSqPGNwYlQ8JKwXrgXo6fdATo6WHO9iqeTDMYn3AmoVlar8V_SZU9VZpfRBVU9dcGxI2r4gtDhaO2PHUkiZBSXX-e0dM.jpeg"" id="""" width=""auto"" height=""auto"" alt="""" loading=""auto""></div></figure><p id="""">‍</p><p id="""">Lastly, a decoder generates an image. It uses a modified diffusion model called GLIDE (Guided Language to Image Diffusion for Generate and Editing). Glide improves diffusion models by adding text inputs. It creates images based on both text and diffusion methods. This model is adapted to decode the CLIP image embeddings into coherent images, maintaining the essence of the original prompts. A diffusion model, employed by GLIDE, ensures the creation of photorealistic images.</p><p id="""">‍</p><p id="""">DALL-E still faces some limitations. For example, it might struggle to create images where text and visuals align coherently. It also faces challenges in linking attributes to objects or generating complex scenes. Additionally, it may inherit biases from the data it is trained on. However, it is still a powerful generative AI model that can create realistic and creative images from text descriptions.&nbsp;</p><h4 id="""">Midjourney</h4><p id=""""><a href=""https://arxiv.org/pdf/2206.02904.pdf"" id="""">Midjourney</a> is a self-funded and independent generative AI program. It can generate high-quality images from text descriptions. It is hosted by an independent research lab,<a href=""https://www.midjourney.com/home/?callbackUrl=%2Fapp%2F"" id=""""> Midjourney</a>, Inc., and operates entirely on Discord or third-party servers. To use Midjourney, you need to have a Discord account. You do not need any specialized hardware or software, and you do not need to download any files. Currently, the lab is working to make it accessible through a web interface.</p><p id="""">‍</p><p id="""">Midjourney is a closed-source program. No proper knowledge of its underlying working is available. However, it can be said that it uses large language and diffusion models. The former is used by Midjourney to understand the meaning of a text prompt. The language model then converts the prompt into a vector, which is used to guide a diffusion process. The diffusion process gradually generates an image that is consistent with the meaning of the prompt. To generate images, users input prompts using the<strong id=""""> /imagine</strong> command. Advanced techniques, such as using Upscale, Vary, and Redo buttons, empower users to enhance results. Midjourney's unique artistic style is evident in its outcomes, similar to paintings rather than photographs.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1440px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1440px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64fdeda64c01d1a2dc7233f4_C1UflCpRMY-R2t-0du1jDJUrsupN1WkPJXpA8eP_I0LgULzApawMEbGExAQt-Tj2oIz_f5yxSB9JH0saebiA2WotV40V5-dH0o29YMMACQb7Q_UE4vmtSDfbuMNAVRGFSE0QVO6OJ0VEyOq2mB49zU4.png"" id="""" width=""auto"" height=""auto"" alt="""" loading=""auto""></div></figure><p id="""">‍</p><p id=""""><a href=""https://github.com/midjourney"" id="""">Midjourney</a> is still in the beta stage. It continually refines its algorithms, introducing new model versions every few months. Midjourney requires payment upfront, unlike many other image generators. This is because its image generation process is resource-intensive. It requires the use of GPUs and large memory for denoising. Midjourney is a promising AI art generation program that is still under development.</p><h2 id="""">Techniques to generate product images using Stable Diffusion</h2><h3 id="""">Textual Inversion</h3><p id="""">Textual Inversion is a method used in machine learning, specifically in text-to-image generation. It's a way to introduce new concepts to a model using a few example images. The model learns these new concepts by creating new 'words' in the embedding space of the text encoder. These words can then be used in text prompts to generate images with a high degree of control. The technique was first introduced in this research <a href=""https://arxiv.org/abs/2208.01618"" id="""">paper</a> and has since been applied to various models, including the Stable Diffusion models.</p><p id="""">‍</p><p id="""">The assumption here is that the embedding space of the text encoder is vast enough to encode the aspects of the new images that one would want to introduce and generate using the image decoder.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1420px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1420px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64fdeda63c8f65ea1cd07c01_8Nh31ivXJwlnwuFbWsu-t7SC_owokiN-XNEJsqfCfQ80khnsmSw5yeMkyx-c0TkSIwrk29Hg3ThyUU2YNelwaebgAYXp1KwczmtVUkR3vg2V09X_4slTYgLJBOJiL_di4YUZQJ8Ojra5GpwAmjKDVUM.png"" id="""" width=""auto"" height=""auto"" alt="""" loading=""auto""></div></figure><p id="""">‍</p><p id="""">To generate product images using this process, one can upload a few images of the product, preferably in different positions and lighting. Once we have that we can start training the model on our new images, but the embeddings for the new token will be the only trainable params. This means all the loss of the model will be focused on learning the embeddings that are required to generate the given images. This will find the best vectors responsible in the text encoder latent space which can best represent the given images or concepts in the image.</p><p id="""">‍</p><p id="""">The advantage of this process is that we are not expanding the space of the model. We are learning the new embedding required specifically for our product. This means we can use the same model for multiple products, just assigning a new word to every specific product.</p><p id="""">‍</p><h3 id="""">DreamBooth</h3><p id="""">Dreambooth is another method to add new concepts to image generative models. This one came out of Google in 2022. Similar to Textual Inversion, DreamBooth also lets you introduce new objects/concepts in the model and assign unique identifiers to them. Once the model has learned the new concept, you can use the associated unique identifier to generate images of the object in different settings and scenarios.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64fdeda75a21476b34ddec16_izeMmgYJk8Gk8ta7NyWxn1uoLf9TBH8SNVKR46j3WPD4Hz2ygf97_pHSEWdRDJnu5PEWuDFDtoEhBGFHVP6gIM7ufpkXxdqw5l6Afsd2niGzIX5Yw-QDf5kKvtoLyk4sx-7MTebMhnSBS7p1aAk6SfU.png"" id="""" width=""auto"" height=""auto"" alt="""" loading=""auto""></div></figure><p id="""">‍</p><p id="""">Although similar in concept and goal, DreamBooth differs completely in approach from Textual Inversion. Instead of just finetuning the text embedding space and learning the embedding for the new object. We finetune the whole network, in a few-shot fashion. And that’s The core problem here is to finetune the network in a few-shot fashion. Authors show that you can use this method with as little as 3 images, although the more the better. But finetuning with such a small amount of data can lead to major issues in the network. The authors introduce a few ways to make this possible.</p><p>‍</p><figure class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1837px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1837px""><div><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64fdee579732cbbaa52dd5d2_dreambooth.png"" loading=""lazy""></div></figure><p>‍</p><p id="""">To generate custom product images using this process, you would first need a few images of the product or person that you want to target. Then, along with that, you would need some images of the same <strong id="""">class</strong> as the product. The class of the product is basically what the product is, bag, shoes, water bottle, etc. As shown in the diagram above, the training is comprised of the new images AND the images from the same class. This is necessary to make sure that the model retains the original information and doesn’t overfit. To ensure the balance between preserving the old concepts and learning the new concepts, the authors introduce a <strong id="""">class-preservation loss</strong>, which is a loss term with an additional parameter to control the weight of the loss from the old images. One can reduce or increase this parameter according to the needs. This also helps in preventing <em id="""">language drift.</em> This is the phenomenon when a language model trained on a large corpus is finetuned on a smaller more specific corpus, it starts to lose the syntactical and semantic knowledge of the language.</p><p id="""">‍</p><p id="""">Along with the class preservation loss, the authors also put emphasis on the specific technique to use when building prompts. They suggest using prompts in the specific format of: <strong id="""">“A [V] [class noun]”</strong> where [V] is the unique identifier, and the [class noun] is the class the object belongs to. Using class nouns helps greatly with learning as it helps the model tie the properties of the new images to something that it has already learned. This is because this way the image and the embeddings are more closely related right off the bat instead of being learned slowly.</p><p id="""">‍</p><p id="""">These two are the core of the finetuning with dreambooth. Once these are in place, one can finetune the smaller resolution model with class preservation loss and class noun prompting technique. And then finetune the larger model with the new images to ensure the fidelity of the generated images.</p><h4 id="""">Dreambooth vs Textual Inversion</h4><p id="""">Dreambooth when compared to Textual Inversion shows much better results. This is primarily because Dreambooth finetunes the whole network, instead of just the text encoder space. But because of this, finetuning with dreambooth can be notoriously difficult. It is very easy to overfit and can lead to language drift. There are many hyperparameters to control. Many people have been running experiments, you can read this amazing article from hugging face <a href=""https://huggingface.co/blog/dreambooth"" id="""">here</a>.&nbsp; Another big issue with Dreambooth is the high number of trainable parameters. This issue can be solved by finetuning with <a href=""https://www.mercity.ai/blog-post/fine-tuning-llms-using-peft-and-lora"" id="""">Peft Techniques</a> like LoRA.</p><p id="""">‍</p><p id="""">This being said, DreamBooth is hugely superior to Textual Inversion. If you want to generate product images using Stable Diffusion, definitely use DreamBooth finetuning with LoRA, but if you only need the model to learn the basic concept, without very high accuracy, Textual Inversion would be better.</p><p id="""">‍</p><h3 id="""">Outpainting</h3><p id="""">Outpainting is a very basic method of extending a passed image. Stable Diffusion is able to perform this operation using the techniques described in another paper, <a href=""https://arxiv.org/abs/2109.07161"" id="""">LaMa - Large Mask Inpainting</a>. The authors generate data and evaluate the performance of the stable diffusion model based on the LaMa paper.&nbsp;</p><p id="""">‍</p><p id="""">This is very important to note that Outpainting is an extension of <strong id="""">Inpainting</strong>, which is a technique to <strong id="""">remove</strong> parts from images, not add to them. This works by passing the original image and a mask image to the model, the model will then erase the parts from the original image which are highlighted in the mask. LaMa was a SOTA method at the time, new papers like <a href=""https://arxiv.org/abs/2206.13644"" id="""">Feature Refinement</a> have come out since.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1000px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1000px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64fdeda6b2b752f61fc3094f_KQDAT283BurhvQPCpO9sJ170OQIZUKXXP_coMAXw4UGGWBdzLk8x0xdF8pelUcjvAoDxbRpU0cZXi_a6Go77yduzc9KsptjGhRa0L2JTfAk6kYIrTrFxzDGoxK8LPiR5zNJ0Rh87Hq0HIJsKwN9mfzI.png"" id="""" width=""auto"" height=""auto"" alt="""" loading=""auto""></div></figure><p id="""">‍</p><p id="""">To perform outpainting using this, the mask is made bigger than than original image and is added around the image, not over the original image. This forces the model to add to the image and hence extend the original image.&nbsp;</p><h3 id="""">ControlNet</h3><p id="""">Control net is not a technique to extend images, but rather a technique to control the output of a generative model. It is a type of generative model that uses a control vector to control the output of the model. This control vector is a set of parameters that are used to control the output of the model. The control vector is used to control the output of the model in terms of the desired features, such as color, texture, shape, etc. This allows for more control over the output of the model and can be used to generate more realistic images.&nbsp;</p><p id="""">‍</p><p id="""">Control net can be combined with dream booth, textual inversion, and other stable diffusion models to generate finely controlled images of desired products. Here is an example of generating shoe images using nothing but scribble:</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:500px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""500px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64fdeda62900d8b57c6098ae_6PGpErIA2MveL68PTv7NgI_MOwsOccPnRAikVusF_f1xoYWd5iAentBtay8n99gmgmgqg2Wnv6USoItp-9sL-_pJMMfqqcKtodH7lQZZA2NlGVJ43X1X3sV7TsScl-OmlyroS_8M-2qDoaSC6Ge1Jxc.png"" id="""" width=""auto"" height=""auto"" alt="""" loading=""auto""></div></figure><h2 id="""">Why Use Stable Diffusion To Generate Product Images?</h2><p id="""">In rapidly evolving product marketing, visual content is essential for businesses to stand out from the competition. Stable Diffusion is a cutting-edge image generation technique that can help businesses create high-quality product images quickly and easily. It offers several advantages over traditional image creation methods.</p><h3 id="""">Costly-Effective Than Product Photography</h3><p id="""">Traditional product photography can be expensive, especially when you factor in the cost of hiring a photographer, renting equipment, and paying for post-processing. Stable Diffusion can help you eliminate these costs by generating high-quality product images from text prompts. Also with traditional product photography, there can be a long wait between the time you take the photos and the time you receive them. Stable Diffusion can generate images much more quickly, so you can get your products listed online faster. You are not limited by the time and location of the photoshoot. Stable Diffusion allows you to generate images of any product, in any setting, at any time. This gives you more flexibility to create the perfect images for your marketing campaigns. Traditional product photography can be inconsistent, depending on the photographer's skills and the lighting conditions. Stable Diffusion can help you create consistent, high-quality images every time.</p><div data-rt-embed-type='true'><blockquote class=""twitter-twee tw-align-centert""><p lang=""en"" dir=""ltr"">Gonna ship a Figma plugin to go from prompts + simple shapes to design ideas using <a href=""https://twitter.com/hashtag/stablediffusion?src=hash&amp;ref_src=twsrc%5Etfw"">#stablediffusion</a> <a href=""https://twitter.com/hashtag/aiart?src=hash&amp;ref_src=twsrc%5Etfw"">#aiart</a> <a href=""https://t.co/0VYais9C6X"">pic.twitter.com/0VYais9C6X</a></p>&mdash; Antonio Cao (@RemitNotPaucity) <a href=""https://twitter.com/RemitNotPaucity/status/1562319004563173376?ref_src=twsrc%5Etfw"">August 24, 2022</a></blockquote> <script async src=""https://platform.twitter.com/widgets.js"" charset=""utf-8""></script></div><h3 id="""">Adaptable To Trends</h3><p id="""">In dynamic markets, where trends are constantly changing, businesses need to be adaptable. Stable Diffusion can help businesses quickly align with the latest trends and product variations. It does so by generating realistic images from simple sketches, textual descriptions, or input images. It makes it ideal for tasks such as image inpainting, style transfer, and upscaling. It can also be used for complex image segmentation tasks. This involves dividing an image into distinct regions based on contrasts, colors, or features. The iterative nature makes it particularly effective for this task. This is because it can gradually refine the segmentation results until they are precise and intricate. Its inherent adaptability to evolving trends makes it a valuable asset for businesses that want to stay ahead of the competition.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:964px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""964px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64fdeda7277733d3ac07e9b7_i5L6eGJIWwQCB4LjhiGzOZ0Bdyx8nX1v2hmTx2_6sq9fxCwnFl-64GBgupdWSEz3ST9NMS9O5a0b-TeYQf_1Tti2toKnsv0bSE6SAZXBnE9QrkKGdm37EEpO2EC4BuDkU63MUzchrJHgoXU-eyavZxg.png"" id="""" width=""auto"" height=""auto"" alt="""" loading=""auto""></div></figure><p id="""">Figure From <a href=""https://www.reddit.com/media?url=https%3A%2F%2Fpreview.redd.it%2Fusing-sd-to-create-ecommerce-product-images-v0-br1s5vxlhd4a1.png%3Fwidth%3D1080%26crop%3Dsmart%26auto%3Dwebp%26s%3Da4cea26c02345c5241f85a4a65c2b425e0914e43"" id=""""><em id="""">Reddit</em></a></p><h3 id="""">Efficient And Speed</h3><p id="""">Efficiency and speed are essential qualities for businesses in today's competitive landscape. Stable Diffusion can help businesses achieve both of these goals by accelerating the image creation process. Traditional methods of image creation can be time-consuming and expensive. Businesses may need to hire photographers or graphic designers to create high-quality images. However, Stable Diffusion can generate realistic images from simple text descriptions or input images. This can save businesses time and money, while also giving them more control over the process. It can generate multiple images in a short amount of time. This allows businesses to quickly create various images for different marketing campaigns or e-commerce platforms.</p><h3 id="""">Customizable And Diverse</h3><p id="""">Stable Diffusion offers a wide range of customization and diversity, making it easy to create images that meet specific requirements. For businesses, it can generate high-quality visuals for advertising, creative projects, and product design, and streamline image and video editing, increasing efficiency. It gives users and product designers more control over design choices, quickly generating new and engaging designs. Stable Diffusion can also be used to extract appealing designs and color palettes for web pages, apps, and themes.</p><p id="""">‍</p><p id="""">In marketing, Stable Diffusion networks can develop new designs for logos, promotional materials, and content illustrations. For example, a furniture store could use Stable Diffusion to create images of its products in different room settings, helping customers visualize how the furniture would look in their homes. A clothing brand could use Stable Diffusion to create images of its clothes on different models, helping customers see how the clothes would look on them.</p><div data-rt-embed-type='true'><blockquote class=""twitter-tweet tw-align-center""><p lang=""en"" dir=""ltr"">1/ As soon as StableDiffusion landed we dropped everything to build a GENIE! 🧞‍♂️<br><br>🎙 Voice UI<br>🖼 AI Art<br>😎 AR previews<br>🤑 Instant purchasing<br>📦 On-demand production<br><br>Could AI make every shopper’s wish come true? 🤔👇 <a href=""https://twitter.com/hashtag/ai?src=hash&amp;ref_src=twsrc%5Etfw"">#ai</a> <a href=""https://twitter.com/hashtag/aiart?src=hash&amp;ref_src=twsrc%5Etfw"">#aiart</a> <a href=""https://twitter.com/hashtag/stablediffusion?src=hash&amp;ref_src=twsrc%5Etfw"">#stablediffusion</a> <a href=""https://twitter.com/hashtag/dalle?src=hash&amp;ref_src=twsrc%5Etfw"">#dalle</a> <a href=""https://twitter.com/hashtag/dalle2?src=hash&amp;ref_src=twsrc%5Etfw"">#dalle2</a> <a href=""https://twitter.com/hashtag/vui?src=hash&amp;ref_src=twsrc%5Etfw"">#vui</a> <a href=""https://t.co/0rHLH9Y5aI"">pic.twitter.com/0rHLH9Y5aI</a></p>&mdash; Russ Maschmeyer (@StrangeNative) <a href=""https://twitter.com/StrangeNative/status/1569700294673702912?ref_src=twsrc%5Etfw"">September 13, 2022</a></blockquote> <script async src=""https://platform.twitter.com/widgets.js"" charset=""utf-8""></script></div><h3 id="""">Efficiency In Iteration&nbsp;</h3><p id="""">Stable Diffusion streamlines the iterative design process by swiftly generating multiple product images with slight variations, such as different colors, poses, or backgrounds. It allows designers to quickly compare different options and make informed decisions about the final design. It is easier to refine the design. For example, if a designer is not happy with the color of a product, they can quickly generate a new image with a different color. It can save a lot of time and effort compared to traditional methods of design, which often require manual editing of images. Stable Diffusion can also be used to optimize visual assets. For example, it can be used to resize images for different platforms or to add text or graphics to images. It can help to ensure that visual assets are consistent and effective across all channels.</p><p id="""">‍</p><h2 id="""">Want to build Stable Diffusion Products?</h2><p id="""">If you want to build products with Stable diffusion or other with image generation algorithms, <a href=""https://www.mercity.ai/contacts"" id="""">reach out</a> to us.&nbsp; We have a ton of experience in working with Stable Diffusion, VQ-GANs, VAEs, and other generative AI technologies. We would love to see how we can help you.</p><p id="""">‍</p>",https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/64fded91f4607f9a8fc32c09_stable%20diffusion.png,Pranav,Stable Diffusion,In this blog post we will show you how you to generate product images using stable diffusion,False,"<div class=""rich-text w-richtext""><p>Diffusion models have revolutionized generative AI, enabling diverse content creation from textual inputs. Among them, Stable Diffusion emerged as a breakthrough. It enhances the quality and efficiency of content generation. It finds use in various domains, from art and design to marketing. In this article, we will explore stable diffusion and how you can use it to create product images to enhance your business.</p><h2>What is Stable diffusion?</h2><p><a href=""https://stability.ai/stablediffusion"">Stable Diffusion by StabilityAI</a> is a text-to-image deep-learning model. It is open source under the Apache License 2.0. Its 3.0 models are still under development. Stable Diffusion was trained on the LAION-5B dataset derived from CommonCrawl. It is based on a latent diffusion process. It starts with an image with noise and then gradually adds detail to the image until it becomes a realistic image. It runs on any hardware with a GPU having at least 8 GB VRAM compared to only cloud-accessible models such as DALL-E. </p><p>‍</p><p>Recently Stability AI released <a href=""https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0"">Stable Diffusion XL</a>, the best open-source diffusion model so far. SDXL generates stunning images at a resolution of 1024 x 1024 and also uses a refiner model to increase quality and control over generated images.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64fdeda52900d8b57c609826_GFalimg_Sdfs7P96zgN3URK0zPxUpSvUxZKnF_MTaQaBeBffnjlaRncneCb7clfE_f2KqeYtSKuxQbDJI751OXcXVhp2iT-5TE13MzPU8IjliUfDZBUdoD_dP5sY7QojhMY9M7XI4BvN6shr9EAPaW8.png""/></div></figure><h3>How does Stable Diffusion Work?</h3><p>Stable diffusion has two core parts, the <strong>Diffusion Process</strong> and the <strong>Reverse Diffusion Process</strong>. The diffusion process adds noise to images and generates slightly noisy images with every step. The reverse diffusion process then reverses this process by predicting the amount of noise required to subtract from the noisy image to get the original image. This process is then repeated several times until a coherent image is obtained.</p><p>‍</p><p>Let’s dive deeper into both of the concepts:</p><h4>Forward Diffusion</h4><p>As mentioned above, forward diffusion is the process of adding noise to an image. This happens in steps, so a number of steps are predetermined, and then a distribution of strength of noise is calculated for those steps. The higher the step, the more noise is added. This is an iterative process, but the authors of the <a href=""https://arxiv.org/abs/2112.10752"">LDM paper</a> show this can be done in one go. Given a step and the image, you can skip the previous iterations of adding noise.</p><p>‍</p><p>The authors also note that instead of adding noise directly to the image, we can encode the image and do the same operation in a latent space instead and then decode the image later on. This HUGELY reduces the computational power required to train and use diffusion models.</p><p>‍</p><p>This process is used to generate training samples for the reverse diffusion process.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1323pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64fdeda5bd89df5714d96986_Zj568y3X3efFdGhQpDmuBUPkg6Yhn5hRYct7huMMFnmtmgKsc2NTqLSjUADntyMarSG8SQ_vsYS-jlYDpqk6KXo6meFx3-W_Uj67gf4M_XXfpa_CGXgGBpvfmxIuI1DTwWjjirKY6lzs_4xplfFwuQo.png""/></div></figure><p>‍</p><h4>Reverse Diffusion</h4><p>Once we have the training samples from the forward diffusion process. We can start training the generative model to generate images. The generative model generates images by removing noise from a completely noisy image. This happens in an iterative fashion. The generative model predicts the noise that has to be subtracted from the noisy image to get an actual image. Then a fraction of this noise is subtracted from the noisy image. The resultant image is then again fed to the network. This process is repeated for a set amount of steps.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1265pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64fdeda5b2b752f61fc308cb_ikW4nmBsI68Iv_7hO_89po3k35L2jN92nlPFyPGugNlqdJdu6NDeTwpDPDQbh331iwHUnJF9tr5xQgqXRzIPDbuJLjkPkwWri-2o46ICNWuMHAJknKzbD2s2RjH1vj0Te7vY0Exdq-0UeJOkKbQd31w.png""/></div></figure><p>‍</p><h3>Alternatives to Stable Diffusion</h3><h4>DALL-E</h4><p><a href=""https://arxiv.org/pdf/2102.12092.pdf"">DALL-E</a> is a text-to-image model by OpenAI. It has two model versions, DALL-E and <a href=""https://openai.com/dall-e-2"">DALL-E 2</a>. DALL-E 2 is an encoder-decoder architecture. The text encoder takes text as input and generates text embeddings. They are passed to a prior model which is a diffusion model. It generates the corresponding CLIP image embeddings. The embeddings are passed to an image decoder which then generates actual images from the embeddings.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64fdeda55e79a21ef5d6574e_89XLUW1IJqdl6iKrYovHq6qrULJHEM31x6wP1lAJeub_lj4I-3EkjqdIU9lxypCd_wkvPTchPDj3duZ3pzEhunm5IeFmT--mxeEgdVj04UsQkIBiakld5RgRHPMIHxEBHaKtYWg3n1cX7AoIcJJec0E.jpeg""/></div></figure><p>Figure From <a href=""https://medium.com/augmented-startups/how-does-dall-e-2-work-e6d492a2667f""><em>Medium</em></a></p><p>‍</p><p>The embeddings come from CLIP (Contrastive Language-Image Pretraining). CLIP is trained on millions of images and their captions, to understand the relation between text and images. The model is designed to test how well a given caption matches an image, rather than predicting captions based on images. CLIP generates text and image encodings of each image-caption pair. It then calculates the cosine similarity of each of these embeddings. It minimizes the similarity between incorrect pairs and maximizes that of correct pairs. It then freezes and DALL-E 2 moves to the next task. CLIP guides the prior that takes text embeddings and turns them into image embeddings.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1297pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64fdeda61e51df93e3a5d9e1_IdulUm6cSvUGY4X-y3z_V-YQA9gpMwJAMVMZZP8MCRcizndJN48tBgxxr5MGSqPGNwYlQ8JKwXrgXo6fdATo6WHO9iqeTDMYn3AmoVlar8V_SZU9VZpfRBVU9dcGxI2r4gtDhaO2PHUkiZBSXX-e0dM.jpeg""/></div></figure><p>‍</p><p>Lastly, a decoder generates an image. It uses a modified diffusion model called GLIDE (Guided Language to Image Diffusion for Generate and Editing). Glide improves diffusion models by adding text inputs. It creates images based on both text and diffusion methods. This model is adapted to decode the CLIP image embeddings into coherent images, maintaining the essence of the original prompts. A diffusion model, employed by GLIDE, ensures the creation of photorealistic images.</p><p>‍</p><p>DALL-E still faces some limitations. For example, it might struggle to create images where text and visuals align coherently. It also faces challenges in linking attributes to objects or generating complex scenes. Additionally, it may inherit biases from the data it is trained on. However, it is still a powerful generative AI model that can create realistic and creative images from text descriptions. </p><h4>Midjourney</h4><p><a href=""https://arxiv.org/pdf/2206.02904.pdf"">Midjourney</a> is a self-funded and independent generative AI program. It can generate high-quality images from text descriptions. It is hosted by an independent research lab,<a href=""https://www.midjourney.com/home/?callbackUrl=%2Fapp%2F""> Midjourney</a>, Inc., and operates entirely on Discord or third-party servers. To use Midjourney, you need to have a Discord account. You do not need any specialized hardware or software, and you do not need to download any files. Currently, the lab is working to make it accessible through a web interface.</p><p>‍</p><p>Midjourney is a closed-source program. No proper knowledge of its underlying working is available. However, it can be said that it uses large language and diffusion models. The former is used by Midjourney to understand the meaning of a text prompt. The language model then converts the prompt into a vector, which is used to guide a diffusion process. The diffusion process gradually generates an image that is consistent with the meaning of the prompt. To generate images, users input prompts using the<strong> /imagine</strong> command. Advanced techniques, such as using Upscale, Vary, and Redo buttons, empower users to enhance results. Midjourney's unique artistic style is evident in its outcomes, similar to paintings rather than photographs.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1440pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64fdeda64c01d1a2dc7233f4_C1UflCpRMY-R2t-0du1jDJUrsupN1WkPJXpA8eP_I0LgULzApawMEbGExAQt-Tj2oIz_f5yxSB9JH0saebiA2WotV40V5-dH0o29YMMACQb7Q_UE4vmtSDfbuMNAVRGFSE0QVO6OJ0VEyOq2mB49zU4.png""/></div></figure><p>‍</p><p><a href=""https://github.com/midjourney"">Midjourney</a> is still in the beta stage. It continually refines its algorithms, introducing new model versions every few months. Midjourney requires payment upfront, unlike many other image generators. This is because its image generation process is resource-intensive. It requires the use of GPUs and large memory for denoising. Midjourney is a promising AI art generation program that is still under development.</p><h2>Techniques to generate product images using Stable Diffusion</h2><h3>Textual Inversion</h3><p>Textual Inversion is a method used in machine learning, specifically in text-to-image generation. It's a way to introduce new concepts to a model using a few example images. The model learns these new concepts by creating new 'words' in the embedding space of the text encoder. These words can then be used in text prompts to generate images with a high degree of control. The technique was first introduced in this research <a href=""https://arxiv.org/abs/2208.01618"">paper</a> and has since been applied to various models, including the Stable Diffusion models.</p><p>‍</p><p>The assumption here is that the embedding space of the text encoder is vast enough to encode the aspects of the new images that one would want to introduce and generate using the image decoder.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1420pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64fdeda63c8f65ea1cd07c01_8Nh31ivXJwlnwuFbWsu-t7SC_owokiN-XNEJsqfCfQ80khnsmSw5yeMkyx-c0TkSIwrk29Hg3ThyUU2YNelwaebgAYXp1KwczmtVUkR3vg2V09X_4slTYgLJBOJiL_di4YUZQJ8Ojra5GpwAmjKDVUM.png""/></div></figure><p>‍</p><p>To generate product images using this process, one can upload a few images of the product, preferably in different positions and lighting. Once we have that we can start training the model on our new images, but the embeddings for the new token will be the only trainable params. This means all the loss of the model will be focused on learning the embeddings that are required to generate the given images. This will find the best vectors responsible in the text encoder latent space which can best represent the given images or concepts in the image.</p><p>‍</p><p>The advantage of this process is that we are not expanding the space of the model. We are learning the new embedding required specifically for our product. This means we can use the same model for multiple products, just assigning a new word to every specific product.</p><p>‍</p><h3>DreamBooth</h3><p>Dreambooth is another method to add new concepts to image generative models. This one came out of Google in 2022. Similar to Textual Inversion, DreamBooth also lets you introduce new objects/concepts in the model and assign unique identifiers to them. Once the model has learned the new concept, you can use the associated unique identifier to generate images of the object in different settings and scenarios.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64fdeda75a21476b34ddec16_izeMmgYJk8Gk8ta7NyWxn1uoLf9TBH8SNVKR46j3WPD4Hz2ygf97_pHSEWdRDJnu5PEWuDFDtoEhBGFHVP6gIM7ufpkXxdqw5l6Afsd2niGzIX5Yw-QDf5kKvtoLyk4sx-7MTebMhnSBS7p1aAk6SfU.png""/></div></figure><p>‍</p><p>Although similar in concept and goal, DreamBooth differs completely in approach from Textual Inversion. Instead of just finetuning the text embedding space and learning the embedding for the new object. We finetune the whole network, in a few-shot fashion. And that’s The core problem here is to finetune the network in a few-shot fashion. Authors show that you can use this method with as little as 3 images, although the more the better. But finetuning with such a small amount of data can lead to major issues in the network. The authors introduce a few ways to make this possible.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1837px""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64fdee579732cbbaa52dd5d2_dreambooth.png""/></div></figure><p>‍</p><p>To generate custom product images using this process, you would first need a few images of the product or person that you want to target. Then, along with that, you would need some images of the same <strong>class</strong> as the product. The class of the product is basically what the product is, bag, shoes, water bottle, etc. As shown in the diagram above, the training is comprised of the new images AND the images from the same class. This is necessary to make sure that the model retains the original information and doesn’t overfit. To ensure the balance between preserving the old concepts and learning the new concepts, the authors introduce a <strong>class-preservation loss</strong>, which is a loss term with an additional parameter to control the weight of the loss from the old images. One can reduce or increase this parameter according to the needs. This also helps in preventing <em>language drift.</em> This is the phenomenon when a language model trained on a large corpus is finetuned on a smaller more specific corpus, it starts to lose the syntactical and semantic knowledge of the language.</p><p>‍</p><p>Along with the class preservation loss, the authors also put emphasis on the specific technique to use when building prompts. They suggest using prompts in the specific format of: <strong>“A [V] [class noun]”</strong> where [V] is the unique identifier, and the [class noun] is the class the object belongs to. Using class nouns helps greatly with learning as it helps the model tie the properties of the new images to something that it has already learned. This is because this way the image and the embeddings are more closely related right off the bat instead of being learned slowly.</p><p>‍</p><p>These two are the core of the finetuning with dreambooth. Once these are in place, one can finetune the smaller resolution model with class preservation loss and class noun prompting technique. And then finetune the larger model with the new images to ensure the fidelity of the generated images.</p><h4>Dreambooth vs Textual Inversion</h4><p>Dreambooth when compared to Textual Inversion shows much better results. This is primarily because Dreambooth finetunes the whole network, instead of just the text encoder space. But because of this, finetuning with dreambooth can be notoriously difficult. It is very easy to overfit and can lead to language drift. There are many hyperparameters to control. Many people have been running experiments, you can read this amazing article from hugging face <a href=""https://huggingface.co/blog/dreambooth"">here</a>.  Another big issue with Dreambooth is the high number of trainable parameters. This issue can be solved by finetuning with <a href=""https://www.mercity.ai/blog-post/fine-tuning-llms-using-peft-and-lora"">Peft Techniques</a> like LoRA.</p><p>‍</p><p>This being said, DreamBooth is hugely superior to Textual Inversion. If you want to generate product images using Stable Diffusion, definitely use DreamBooth finetuning with LoRA, but if you only need the model to learn the basic concept, without very high accuracy, Textual Inversion would be better.</p><p>‍</p><h3>Outpainting</h3><p>Outpainting is a very basic method of extending a passed image. Stable Diffusion is able to perform this operation using the techniques described in another paper, <a href=""https://arxiv.org/abs/2109.07161"">LaMa - Large Mask Inpainting</a>. The authors generate data and evaluate the performance of the stable diffusion model based on the LaMa paper. </p><p>‍</p><p>This is very important to note that Outpainting is an extension of <strong>Inpainting</strong>, which is a technique to <strong>remove</strong> parts from images, not add to them. This works by passing the original image and a mask image to the model, the model will then erase the parts from the original image which are highlighted in the mask. LaMa was a SOTA method at the time, new papers like <a href=""https://arxiv.org/abs/2206.13644"">Feature Refinement</a> have come out since.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1000pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64fdeda6b2b752f61fc3094f_KQDAT283BurhvQPCpO9sJ170OQIZUKXXP_coMAXw4UGGWBdzLk8x0xdF8pelUcjvAoDxbRpU0cZXi_a6Go77yduzc9KsptjGhRa0L2JTfAk6kYIrTrFxzDGoxK8LPiR5zNJ0Rh87Hq0HIJsKwN9mfzI.png""/></div></figure><p>‍</p><p>To perform outpainting using this, the mask is made bigger than than original image and is added around the image, not over the original image. This forces the model to add to the image and hence extend the original image. </p><h3>ControlNet</h3><p>Control net is not a technique to extend images, but rather a technique to control the output of a generative model. It is a type of generative model that uses a control vector to control the output of the model. This control vector is a set of parameters that are used to control the output of the model. The control vector is used to control the output of the model in terms of the desired features, such as color, texture, shape, etc. This allows for more control over the output of the model and can be used to generate more realistic images. </p><p>‍</p><p>Control net can be combined with dream booth, textual inversion, and other stable diffusion models to generate finely controlled images of desired products. Here is an example of generating shoe images using nothing but scribble:</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:500pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64fdeda62900d8b57c6098ae_6PGpErIA2MveL68PTv7NgI_MOwsOccPnRAikVusF_f1xoYWd5iAentBtay8n99gmgmgqg2Wnv6USoItp-9sL-_pJMMfqqcKtodH7lQZZA2NlGVJ43X1X3sV7TsScl-OmlyroS_8M-2qDoaSC6Ge1Jxc.png""/></div></figure><h2>Why Use Stable Diffusion To Generate Product Images?</h2><p>In rapidly evolving product marketing, visual content is essential for businesses to stand out from the competition. Stable Diffusion is a cutting-edge image generation technique that can help businesses create high-quality product images quickly and easily. It offers several advantages over traditional image creation methods.</p><h3>Costly-Effective Than Product Photography</h3><p>Traditional product photography can be expensive, especially when you factor in the cost of hiring a photographer, renting equipment, and paying for post-processing. Stable Diffusion can help you eliminate these costs by generating high-quality product images from text prompts. Also with traditional product photography, there can be a long wait between the time you take the photos and the time you receive them. Stable Diffusion can generate images much more quickly, so you can get your products listed online faster. You are not limited by the time and location of the photoshoot. Stable Diffusion allows you to generate images of any product, in any setting, at any time. This gives you more flexibility to create the perfect images for your marketing campaigns. Traditional product photography can be inconsistent, depending on the photographer's skills and the lighting conditions. Stable Diffusion can help you create consistent, high-quality images every time.</p><div class=""w-embed w-script""><blockquote class=""twitter-twee tw-align-centert""><p dir=""ltr"" lang=""en"">Gonna ship a Figma plugin to go from prompts + simple shapes to design ideas using <a href=""https://twitter.com/hashtag/stablediffusion?src=hash&amp;ref_src=twsrc%5Etfw"">#stablediffusion</a> <a href=""https://twitter.com/hashtag/aiart?src=hash&amp;ref_src=twsrc%5Etfw"">#aiart</a> <a href=""https://t.co/0VYais9C6X"">pic.twitter.com/0VYais9C6X</a></p>— Antonio Cao (@RemitNotPaucity) <a href=""https://twitter.com/RemitNotPaucity/status/1562319004563173376?ref_src=twsrc%5Etfw"">August 24, 2022</a></blockquote> <script async="""" charset=""utf-8"" src=""https://platform.twitter.com/widgets.js""></script></div><h3>Adaptable To Trends</h3><p>In dynamic markets, where trends are constantly changing, businesses need to be adaptable. Stable Diffusion can help businesses quickly align with the latest trends and product variations. It does so by generating realistic images from simple sketches, textual descriptions, or input images. It makes it ideal for tasks such as image inpainting, style transfer, and upscaling. It can also be used for complex image segmentation tasks. This involves dividing an image into distinct regions based on contrasts, colors, or features. The iterative nature makes it particularly effective for this task. This is because it can gradually refine the segmentation results until they are precise and intricate. Its inherent adaptability to evolving trends makes it a valuable asset for businesses that want to stay ahead of the competition.</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:964pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/64fdeda7277733d3ac07e9b7_i5L6eGJIWwQCB4LjhiGzOZ0Bdyx8nX1v2hmTx2_6sq9fxCwnFl-64GBgupdWSEz3ST9NMS9O5a0b-TeYQf_1Tti2toKnsv0bSE6SAZXBnE9QrkKGdm37EEpO2EC4BuDkU63MUzchrJHgoXU-eyavZxg.png""/></div></figure><p>Figure From <a href=""https://www.reddit.com/media?url=https%3A%2F%2Fpreview.redd.it%2Fusing-sd-to-create-ecommerce-product-images-v0-br1s5vxlhd4a1.png%3Fwidth%3D1080%26crop%3Dsmart%26auto%3Dwebp%26s%3Da4cea26c02345c5241f85a4a65c2b425e0914e43""><em>Reddit</em></a></p><h3>Efficient And Speed</h3><p>Efficiency and speed are essential qualities for businesses in today's competitive landscape. Stable Diffusion can help businesses achieve both of these goals by accelerating the image creation process. Traditional methods of image creation can be time-consuming and expensive. Businesses may need to hire photographers or graphic designers to create high-quality images. However, Stable Diffusion can generate realistic images from simple text descriptions or input images. This can save businesses time and money, while also giving them more control over the process. It can generate multiple images in a short amount of time. This allows businesses to quickly create various images for different marketing campaigns or e-commerce platforms.</p><h3>Customizable And Diverse</h3><p>Stable Diffusion offers a wide range of customization and diversity, making it easy to create images that meet specific requirements. For businesses, it can generate high-quality visuals for advertising, creative projects, and product design, and streamline image and video editing, increasing efficiency. It gives users and product designers more control over design choices, quickly generating new and engaging designs. Stable Diffusion can also be used to extract appealing designs and color palettes for web pages, apps, and themes.</p><p>‍</p><p>In marketing, Stable Diffusion networks can develop new designs for logos, promotional materials, and content illustrations. For example, a furniture store could use Stable Diffusion to create images of its products in different room settings, helping customers visualize how the furniture would look in their homes. A clothing brand could use Stable Diffusion to create images of its clothes on different models, helping customers see how the clothes would look on them.</p><div class=""w-embed w-script""><blockquote class=""twitter-tweet tw-align-center""><p dir=""ltr"" lang=""en"">1/ As soon as StableDiffusion landed we dropped everything to build a GENIE! 🧞‍♂️<br/><br/>🎙 Voice UI<br/>🖼 AI Art<br/>😎 AR previews<br/>🤑 Instant purchasing<br/>📦 On-demand production<br/><br/>Could AI make every shopper’s wish come true? 🤔👇 <a href=""https://twitter.com/hashtag/ai?src=hash&amp;ref_src=twsrc%5Etfw"">#ai</a> <a href=""https://twitter.com/hashtag/aiart?src=hash&amp;ref_src=twsrc%5Etfw"">#aiart</a> <a href=""https://twitter.com/hashtag/stablediffusion?src=hash&amp;ref_src=twsrc%5Etfw"">#stablediffusion</a> <a href=""https://twitter.com/hashtag/dalle?src=hash&amp;ref_src=twsrc%5Etfw"">#dalle</a> <a href=""https://twitter.com/hashtag/dalle2?src=hash&amp;ref_src=twsrc%5Etfw"">#dalle2</a> <a href=""https://twitter.com/hashtag/vui?src=hash&amp;ref_src=twsrc%5Etfw"">#vui</a> <a href=""https://t.co/0rHLH9Y5aI"">pic.twitter.com/0rHLH9Y5aI</a></p>— Russ Maschmeyer (@StrangeNative) <a href=""https://twitter.com/StrangeNative/status/1569700294673702912?ref_src=twsrc%5Etfw"">September 13, 2022</a></blockquote> <script async="""" charset=""utf-8"" src=""https://platform.twitter.com/widgets.js""></script></div><h3>Efficiency In Iteration </h3><p>Stable Diffusion streamlines the iterative design process by swiftly generating multiple product images with slight variations, such as different colors, poses, or backgrounds. It allows designers to quickly compare different options and make informed decisions about the final design. It is easier to refine the design. For example, if a designer is not happy with the color of a product, they can quickly generate a new image with a different color. It can save a lot of time and effort compared to traditional methods of design, which often require manual editing of images. Stable Diffusion can also be used to optimize visual assets. For example, it can be used to resize images for different platforms or to add text or graphics to images. It can help to ensure that visual assets are consistent and effective across all channels.</p><p>‍</p><h2>Want to build Stable Diffusion Products?</h2><p>If you want to build products with Stable diffusion or other with image generation algorithms, <a href=""https://www.mercity.ai/contacts"">reach out</a> to us.  We have a ton of experience in working with Stable Diffusion, VQ-GANs, VAEs, and other generative AI technologies. We would love to see how we can help you.</p><p>‍</p></div>"
Using ChatGPT to build Synthetic Datasets,using-chatgpt-to-build-synthetic-datasets,640f56f76d313b2faa631c11,645f776fd2bb513e256dab03,False,False,Sat May 13 2023 11:41:34 GMT+0000 (Coordinated Universal Time),Fri May 19 2023 09:28:54 GMT+0000 (Coordinated Universal Time),Fri May 19 2023 09:35:30 GMT+0000 (Coordinated Universal Time),"<p>Often when solving very specific business problems, it can be challenging to find a large and diverse dataset to train machine learning models.</p><p id="""">‍</p><p id="""">Real-world datasets can be costly and time-consuming to collect, and they may not always contain the necessary variety of data to create accurate models. In recent years, the use of synthetic datasets generated by large language models like GPT has become increasingly popular.</p><p id="""">‍</p><p id="""">In this article, we will generate a synthetic dataset of complicated sentences and their rewritten simpler versions using GPT models.</p><h2 id="""">Why use Synthetic Datasets?</h2><p id="""">Synthetic datasets are computer-generated datasets that can be tailored to specific needs and created quickly and inexpensively compared to collecting and labeling real-world data. This provides businesses with a time-saving and cost-effective solution for training their machine-learning models.</p><p id="""">‍</p><p id="""">Synthetic datasets are more diverse and representative than real-world datasets, offering businesses an advantage. Real-world datasets may be limited in their data variety, impacting the accuracy of machine-learning models. Synthetic datasets can be generated with specific properties, such as balanced class distributions, that are hard to achieve in real-world datasets, leading to more accurate models and informed decision-making.</p><p id="""">‍</p><p id="""">While synthetic datasets may not be able to fully capture the complexities of real-world data, they are still a powerful tool for businesses to overcome the challenges of collecting real-world data and developing machine-learning models that meet their specific needs.&nbsp;</p><h2 id="""">How to Generate Synthetic Data with ChatGPT</h2><p id="""">Models like GPT4 and ChatGPT are usually trained on huge datasets to learn multiple knowledge bases and can do various tasks. We can use this property of LLMs to generate synthetic datasets which are more specific to our needs.</p><p id="""">‍</p><p id="""">Now, we can generate synthetic datasets from scratch, it is always better to provide the model with some examples to begin with.</p><p id="""">‍</p><p id="""">Let’s discuss the EXACT pipeline we use to generate high-quality synthetic datasets.</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/645f77cd5e4b02b72cd112f9_33e6e0c1.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h3 id="""">Extracting properties from provided examples</h3><p id="""">The first step is to analyze the examples provided and understand what exactly we are seeking to achieve. This is not required, but we have found that this greatly increases the quality of generated outputs.</p><p id="""">‍</p><p id="""">The goal of this step is to generate and understand the key properties of the provided examples. In the synthetic dataset, pairs of inputs and outputs are generated, these properties are the guide for the model to generate them.</p><p id="""">‍</p><p id="""">Here’s an example of how to do this:</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/645f7640c1cf8ec3f58d1d0e_7f2769e3.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/645f763e885c34030d6a52a2_09a2a670.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h3 id="""">Generating the dataset</h3><p id="""">Once we have the properties we want our dataset to have, we can begin to generate the synthetic dataset we want. We can tweak the properties if we want, this gives us more control over the generated dataset.</p><p id="""">‍</p><p id="""">For this, we usually set the model parameters in a specific way:</p><p id="""">‍</p><ul id=""""><li id=""""><strong id="""">Temperature: </strong>The temperature is usually set to the max value, 1. This is because we want the model to be as “random” as possible, while still staying within the constraints provided, to generate the most diverse dataset possible.</li></ul><ul id=""""><li id=""""><strong id="""">Frequency Penalty: </strong>This value is also set to a higher number, 1.&nbsp; A higher frequency penalty value will encourage the model to produce more diverse and unique content by penalizing the repetition of the same words or phrases.<br>&nbsp;<strong id="""">‍</strong></li><li id=""""><strong id="""">Presence Penalty: </strong>Setting this parameter is a bit tricky. Higher presence penalty that model will be penalized if it generates the same word multiple times. This can be good if you want your dataset to be diverse and contain lots of different words. But if you are generating something very specific, you might want to have the same words present in your dataset multiple times in different contexts. The value of this parameter depends entirely on your requirement.</li></ul><p id="""">‍</p><p id="""">You can of course tweak these parameters according to your needs.</p><p id="""">‍</p><p id="""">Here’s the prompt to use for this step:</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1572px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1572px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/645f766e4f8e6541f3528f15_fa176408.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">‍</p><div data-rt-embed-type='true'><div style=""background-color: #2A2A2B;padding: 2.5%;"">

<code>
<i>
Goal is to generate difficult to read and complicated sentences and paraphrase them so they are easy to read and more clear. Use difficult and complicated words in the difficult sentences, and easier to understand words in the paraphrased sentences. The easy to read sentences must be easy to understand, short, should use easy words, and should be on a 5th-grade reading level. 
<br/>
<br/>

Here's an example of such a pair:[{""difficult"":""The new legislation, which was passed by a majority of the members of the legislature, will result in a drastic reduction of the number of individuals who are eligible for certain government benefits."",""easy"":""The lawmakers voted to pass a law that will make it so fewer people can get help from the government.""}]‍

<br/>
<br/>

Some VERY important things:Do not change the meaning of the sentences. Try to only change a few words, and use easier synonyms.Do not change the sentence a lot.Use heavy jargon and very complicated words in difficult sentences.Use easy words in the easy sentences.Difficult sentences should be difficult to parse and follow.Use a formal tone in both sentences. Formal words.All sentences should be very different from each other.‍

<br/>
<br/>


Important: Sentences must be long and complicatedTopic: legal jargon. Talk about different topics within the given topic.

<br/>
<br/>


JSON with 5 pairs, varying sentences:

</i>
</code>

</div></div><p id="""">‍</p><p id="""">You can change the <em id="""">topic</em> according to your needs.</p><p id="""">‍</p><p id="""">The prompts are changed to compress the information from the first output, initial testing shows that the results are good enough to work with. But specific usages will require specific changes and prompts will have to be changed accordingly.</p><p id="""">‍</p><p id="""">Here’s an example output of the above prompt with the topic set to <em id="""">business administration</em>:</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1570px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1570px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/645f764d159a448158e297d7_88411663.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><h3 id="""">Postprocessing and storing</h3><p id="""">Once we have the prompts ready, we can tie everything together and start generating the dataset. We usually ask GPT3 to generate data in a specific format so we can parse and store it. We are going to use the JSON structure in this article as it is very common and is easy to process.</p><p id="""">‍</p><p id="""">For this tutorial, we will take the output JSON from GPT3, parse it using python and store it inside a pandas dataframe. After that, we can store it as a CSV.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/645f78763a0d8e2c7a306133_bba7e9fe.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">‍</p><h2 id="""">Generating a Synthetic Dataset of Rephrased Sentences</h2><p id="""">Now that we have the pipeline and the prompts laid out, we can generate the dataset. Here we present an example of generating a dataset of rephrased sentences.</p><h3 id="""">Python code to call GPT APIs</h3><p id="""">Now we write a python function that calls the GPT APIs and generates the dataset. We will call this function repeatedly with different inputs for the <em id="""">topic</em> parameter to generate sentences with a diverse range of vocabulary.</p><p id="""">‍</p><p id="""">Here’s the code:</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/645f78888d05c01821cdca61_238d3efc.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">‍</p><p id="""">In this code, we first load the openai module which allows us to call the GPT APIs, then we load the <em id="""">OPENAI_KEY</em> and set it as the API key to authenticate our requests to OpenAI's GPT service.</p><p id="""">‍</p><p id="""">The infer_gpt() function takes two parameters: <em id="""">topic </em>and <em id="""">prompt_version</em>, with a default value of 0 for prompt_version.</p><p id="""">‍</p><p id="""">The prompts variable holds a list of different versions of the same prompt. In this case, the <em id="""">tkn_topic_tkn </em>placeholder within the prompt is replaced with the value of the topic parameter, which is the topic we want GPT to generate text about.</p><p id="""">‍</p><p id="""">Then, we call the <em id="""">openai.Completion.create()</em> method to generate the sentence pairs.</p><p id="""">‍</p><h3 id="""">Calling the API to generate the dataset</h3><p id="""">‍</p><p id="""">Once we have the code to generate the sentences, we can write a loop that will generate them in the context of a specific topic.</p><p id="""">‍</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-center"" data-rt-type=""image"" data-rt-align=""center""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/645f7648c147b29d6a0fb455_cee3d730.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">‍</p><p id="""">In this bit of code, we are looping through our list of <em id="""">topics</em> and calling the <em id="""">infer_gpt</em> for the pased amount of times every single topic in the list.</p><p id="""">‍</p><p id="""">Once done, we will have all the data in the <em id="""">df_gpt</em> variable. We can dump that data into a CSV:</p><figure id="""" class=""w-richtext-figure-type-image w-richtext-align-fullwidth"" style=""max-width:1600px"" data-rt-type=""image"" data-rt-align=""fullwidth"" data-rt-max-width=""1600px""><div id=""""><img src=""https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/645f78a3159a448158e45320_6088aca1.png"" id="""" width=""auto"" height=""auto"" loading=""auto""></div></figure><p id="""">‍</p><p id="""">You can access the data we generated <a href=""https://docs.google.com/spreadsheets/d/1TN0Cal-3QnblKIQwGi0hZdm88MW--DwkwkGV3OE5thI/edit?usp=sharing"" id="""">here</a>.</p><p id="""">‍</p><h3 id="""">Exploring the Dataset</h3><p id="""">For a dataset on rewritten sentences, this dataset works really well. For rewriting, we wanted a wide vocabulary and a lot of different scenarios where sentences could be rewritten.</p><p id="""">‍</p><p id="""">We achieved a wide range of words using the <em id="""">topic</em> parameter, it gave us some control over what kind of words we want to be generated.</p><p id="""">‍</p><p id="""">But when it comes to different types of scenarios, this is definitely not a good dataset. All the pairs are of one single sentence rewritten, in real life there might be multiple sentences or even paragraphs that need rewriting. The dataset also lacks variety in the topic of the sentences, this is most likely because of the passed example in the prompt. The generated sentences are all too similar to the given example.</p><p id="""">‍</p><h2 id="""">Synthetic Data vs Real Data</h2><p id="""">Synthetic data can indeed help solve a lot of problems in the industry, but it is important to note that synthetic data is always limited by how we are generating it and what underlying model is being used.</p><p id="""">‍</p><p id="""">In the above example, the synthetic data is pretty good but is not all that is required to train a model to rephrase sentences. There are many ways a sentence can be simplified, semantically, syntactically, and even by using simple conjunctions. The generated dataset does not contain all these variations. True, these can be generated too, but that requires more work and a new prompt.</p><p id="""">‍</p><p id="""">In real data, these variations will be present naturally. Synthetic data alone should not be used to train highly accurate systems, they should only be used to add to the data that is already present. It can also be used to introduce new variations and types to the existing data.</p><h2 id="""">Want to build synthetic datasets?</h2><p id="""">Looking to build synthetic datasets and train AI models on them? We have worked with synthetic datasets in different fields. We have internal pipelines to ensure the quality of the generated data and experience in using them properly to build and deploy AI systems in production.</p><p id="""">‍</p><p id="""">Book a call, let’s help you build your specific dataset!</p><p id="""">‍</p>",https://uploads-ssl.webflow.com/640f56f76d313bbe39631bfd/646741133d82451ca502c79b_synthetic-data-hero.png,Pranav,Datasets,In this practical guide you'll learn how to build high quality synthetic datasets using ChatGPT.,False,"<div class=""rich-text w-richtext""><p>Often when solving very specific business problems, it can be challenging to find a large and diverse dataset to train machine learning models.</p><p>‍</p><p>Real-world datasets can be costly and time-consuming to collect, and they may not always contain the necessary variety of data to create accurate models. In recent years, the use of synthetic datasets generated by large language models like GPT has become increasingly popular.</p><p>‍</p><p>In this article, we will generate a synthetic dataset of complicated sentences and their rewritten simpler versions using GPT models.</p><h2>Why use Synthetic Datasets?</h2><p>Synthetic datasets are computer-generated datasets that can be tailored to specific needs and created quickly and inexpensively compared to collecting and labeling real-world data. This provides businesses with a time-saving and cost-effective solution for training their machine-learning models.</p><p>‍</p><p>Synthetic datasets are more diverse and representative than real-world datasets, offering businesses an advantage. Real-world datasets may be limited in their data variety, impacting the accuracy of machine-learning models. Synthetic datasets can be generated with specific properties, such as balanced class distributions, that are hard to achieve in real-world datasets, leading to more accurate models and informed decision-making.</p><p>‍</p><p>While synthetic datasets may not be able to fully capture the complexities of real-world data, they are still a powerful tool for businesses to overcome the challenges of collecting real-world data and developing machine-learning models that meet their specific needs. </p><h2>How to Generate Synthetic Data with ChatGPT</h2><p>Models like GPT4 and ChatGPT are usually trained on huge datasets to learn multiple knowledge bases and can do various tasks. We can use this property of LLMs to generate synthetic datasets which are more specific to our needs.</p><p>‍</p><p>Now, we can generate synthetic datasets from scratch, it is always better to provide the model with some examples to begin with.</p><p>‍</p><p>Let’s discuss the EXACT pipeline we use to generate high-quality synthetic datasets.</p><figure class=""w-richtext-align-center w-richtext-figure-type-image""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/645f77cd5e4b02b72cd112f9_33e6e0c1.png""/></div></figure><h3>Extracting properties from provided examples</h3><p>The first step is to analyze the examples provided and understand what exactly we are seeking to achieve. This is not required, but we have found that this greatly increases the quality of generated outputs.</p><p>‍</p><p>The goal of this step is to generate and understand the key properties of the provided examples. In the synthetic dataset, pairs of inputs and outputs are generated, these properties are the guide for the model to generate them.</p><p>‍</p><p>Here’s an example of how to do this:</p><p>‍</p><figure class=""w-richtext-align-center w-richtext-figure-type-image""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/645f7640c1cf8ec3f58d1d0e_7f2769e3.png""/></div></figure><figure class=""w-richtext-align-center w-richtext-figure-type-image""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/645f763e885c34030d6a52a2_09a2a670.png""/></div></figure><h3>Generating the dataset</h3><p>Once we have the properties we want our dataset to have, we can begin to generate the synthetic dataset we want. We can tweak the properties if we want, this gives us more control over the generated dataset.</p><p>‍</p><p>For this, we usually set the model parameters in a specific way:</p><p>‍</p><ul role=""list""><li><strong>Temperature: </strong>The temperature is usually set to the max value, 1. This is because we want the model to be as “random” as possible, while still staying within the constraints provided, to generate the most diverse dataset possible.</li></ul><ul role=""list""><li><strong>Frequency Penalty: </strong>This value is also set to a higher number, 1.  A higher frequency penalty value will encourage the model to produce more diverse and unique content by penalizing the repetition of the same words or phrases.<br/> <strong>‍</strong></li><li><strong>Presence Penalty: </strong>Setting this parameter is a bit tricky. Higher presence penalty that model will be penalized if it generates the same word multiple times. This can be good if you want your dataset to be diverse and contain lots of different words. But if you are generating something very specific, you might want to have the same words present in your dataset multiple times in different contexts. The value of this parameter depends entirely on your requirement.</li></ul><p>‍</p><p>You can of course tweak these parameters according to your needs.</p><p>‍</p><p>Here’s the prompt to use for this step:</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1572pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/645f766e4f8e6541f3528f15_fa176408.png""/></div></figure><p>‍</p><div class=""w-embed""><div style=""background-color: #2A2A2B;padding: 2.5%;"">
<code>
<i>
Goal is to generate difficult to read and complicated sentences and paraphrase them so they are easy to read and more clear. Use difficult and complicated words in the difficult sentences, and easier to understand words in the paraphrased sentences. The easy to read sentences must be easy to understand, short, should use easy words, and should be on a 5th-grade reading level. 
<br/>
<br/>

Here's an example of such a pair:[{""difficult"":""The new legislation, which was passed by a majority of the members of the legislature, will result in a drastic reduction of the number of individuals who are eligible for certain government benefits."",""easy"":""The lawmakers voted to pass a law that will make it so fewer people can get help from the government.""}]‍

<br/>
<br/>

Some VERY important things:Do not change the meaning of the sentences. Try to only change a few words, and use easier synonyms.Do not change the sentence a lot.Use heavy jargon and very complicated words in difficult sentences.Use easy words in the easy sentences.Difficult sentences should be difficult to parse and follow.Use a formal tone in both sentences. Formal words.All sentences should be very different from each other.‍

<br/>
<br/>


Important: Sentences must be long and complicatedTopic: legal jargon. Talk about different topics within the given topic.

<br/>
<br/>


JSON with 5 pairs, varying sentences:

</i>
</code>
</div></div><p>‍</p><p>You can change the <em>topic</em> according to your needs.</p><p>‍</p><p>The prompts are changed to compress the information from the first output, initial testing shows that the results are good enough to work with. But specific usages will require specific changes and prompts will have to be changed accordingly.</p><p>‍</p><p>Here’s an example output of the above prompt with the topic set to <em>business administration</em>:</p><p>‍</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1570pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/645f764d159a448158e297d7_88411663.png""/></div></figure><h3>Postprocessing and storing</h3><p>Once we have the prompts ready, we can tie everything together and start generating the dataset. We usually ask GPT3 to generate data in a specific format so we can parse and store it. We are going to use the JSON structure in this article as it is very common and is easy to process.</p><p>‍</p><p>For this tutorial, we will take the output JSON from GPT3, parse it using python and store it inside a pandas dataframe. After that, we can store it as a CSV.</p><p>‍</p><figure class=""w-richtext-align-center w-richtext-figure-type-image""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/645f78763a0d8e2c7a306133_bba7e9fe.png""/></div></figure><p>‍</p><h2>Generating a Synthetic Dataset of Rephrased Sentences</h2><p>Now that we have the pipeline and the prompts laid out, we can generate the dataset. Here we present an example of generating a dataset of rephrased sentences.</p><h3>Python code to call GPT APIs</h3><p>Now we write a python function that calls the GPT APIs and generates the dataset. We will call this function repeatedly with different inputs for the <em>topic</em> parameter to generate sentences with a diverse range of vocabulary.</p><p>‍</p><p>Here’s the code:</p><figure class=""w-richtext-align-center w-richtext-figure-type-image""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/645f78888d05c01821cdca61_238d3efc.png""/></div></figure><p>‍</p><p>In this code, we first load the openai module which allows us to call the GPT APIs, then we load the <em>OPENAI_KEY</em> and set it as the API key to authenticate our requests to OpenAI's GPT service.</p><p>‍</p><p>The infer_gpt() function takes two parameters: <em>topic </em>and <em>prompt_version</em>, with a default value of 0 for prompt_version.</p><p>‍</p><p>The prompts variable holds a list of different versions of the same prompt. In this case, the <em>tkn_topic_tkn </em>placeholder within the prompt is replaced with the value of the topic parameter, which is the topic we want GPT to generate text about.</p><p>‍</p><p>Then, we call the <em>openai.Completion.create()</em> method to generate the sentence pairs.</p><p>‍</p><h3>Calling the API to generate the dataset</h3><p>‍</p><p>Once we have the code to generate the sentences, we can write a loop that will generate them in the context of a specific topic.</p><p>‍</p><figure class=""w-richtext-align-center w-richtext-figure-type-image""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/645f7648c147b29d6a0fb455_cee3d730.png""/></div></figure><p>‍</p><p>In this bit of code, we are looping through our list of <em>topics</em> and calling the <em>infer_gpt</em> for the pased amount of times every single topic in the list.</p><p>‍</p><p>Once done, we will have all the data in the <em>df_gpt</em> variable. We can dump that data into a CSV:</p><figure class=""w-richtext-align-fullwidth w-richtext-figure-type-image"" style=""max-width:1600pxpx""><div><img alt="""" loading=""lazy"" src=""https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/645f78a3159a448158e45320_6088aca1.png""/></div></figure><p>‍</p><p>You can access the data we generated <a href=""https://docs.google.com/spreadsheets/d/1TN0Cal-3QnblKIQwGi0hZdm88MW--DwkwkGV3OE5thI/edit?usp=sharing"">here</a>.</p><p>‍</p><h3>Exploring the Dataset</h3><p>For a dataset on rewritten sentences, this dataset works really well. For rewriting, we wanted a wide vocabulary and a lot of different scenarios where sentences could be rewritten.</p><p>‍</p><p>We achieved a wide range of words using the <em>topic</em> parameter, it gave us some control over what kind of words we want to be generated.</p><p>‍</p><p>But when it comes to different types of scenarios, this is definitely not a good dataset. All the pairs are of one single sentence rewritten, in real life there might be multiple sentences or even paragraphs that need rewriting. The dataset also lacks variety in the topic of the sentences, this is most likely because of the passed example in the prompt. The generated sentences are all too similar to the given example.</p><p>‍</p><h2>Synthetic Data vs Real Data</h2><p>Synthetic data can indeed help solve a lot of problems in the industry, but it is important to note that synthetic data is always limited by how we are generating it and what underlying model is being used.</p><p>‍</p><p>In the above example, the synthetic data is pretty good but is not all that is required to train a model to rephrase sentences. There are many ways a sentence can be simplified, semantically, syntactically, and even by using simple conjunctions. The generated dataset does not contain all these variations. True, these can be generated too, but that requires more work and a new prompt.</p><p>‍</p><p>In real data, these variations will be present naturally. Synthetic data alone should not be used to train highly accurate systems, they should only be used to add to the data that is already present. It can also be used to introduce new variations and types to the existing data.</p><h2>Want to build synthetic datasets?</h2><p>Looking to build synthetic datasets and train AI models on them? We have worked with synthetic datasets in different fields. We have internal pipelines to ensure the quality of the generated data and experience in using them properly to build and deploy AI systems in production.</p><p>‍</p><p>Book a call, let’s help you build your specific dataset!</p><p>‍</p></div>"
